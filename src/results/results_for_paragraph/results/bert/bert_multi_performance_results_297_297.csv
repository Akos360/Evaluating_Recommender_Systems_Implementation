book_index,paragraph_index,similarity_score,recommended_book,recommended_text
383,230,0.447,Elements of Risk Analysis with Applications in R,"R Output > str ( mydata ) 25 â data . frame â: 483 obs . of 4 variables : $ X : int 1 2 3 4 5 6 7 8 9 10 ... $ Month : chr "" JAN "" "" FEB "" "" MAR "" "" APR "" ... $ Year : int 1980 1980 1980 1980 1980 1980 1980 1980 1980 1980 ..."
356,375,0.421,Re-engineering the Uptake of ICT in Schools,DateTime This element is based on ISO 8601 and contains date and time information. The format follows Date and Time Formats as speciï¬ed by the W3 consortium. See http://www.w3.org/TR/NOTE-datetime or http://www.w3.org/TR/xmlschema-2/ #isoformats. YYYY[-MM[-DD[Thh[:mm[:ss[.s[TZD]]]]]]] where: YYYY = four-digit year MM = two-digit month DD = two-digit day of month hh = two digits of hour (00 through 23) mm = two digits of minute (00 through 59) ss = two digits of second (00 through 59) s = one or more digits representing a decimal fraction of a second TZD = time zone designator (âZâ for UTC or +hh:mm or âhh:mm)
383,243,0.415,Elements of Risk Analysis with Applications in R,"mydata . visits . ts <- ts ( mydata $ Visits , start = c (1980 ,1) , end = c (1981 ,12) , frequency =12) 11 # Create the Time for the first visit 12 dt1 . visits <- as . Date ( "" 1980 -01 -01 "" ) 13 dt1 . visits 14 # The length of our time series 15 T <- 24 # 24 months only 16 # Then we create a whole column of n days : 17 Time . visits <- seq ( dt1 . visits , length = 24 , by = "" months "" ) 18 str ( Time . visits ) # Checking that Time is indeed a date format . 19 # Fit the HW model with additive seasonality 20 hw . visits . additive <- hw ( mydata . visits . ts , seasonal = "" additive "" , h =24) 21 summary ( hw . visits . additive ) 22 # Fit the HW model with multi plicati ve seasonality 23 hw . visits . m ultipli cative <- hw ( mydata . visits . ts , seasonal = "" mult iplicati ve "" ,h =24) 24 summary ( hw . visits . mult iplicat ive ) 25 # Next , we plot the data and the fitted values from the two models . 26 plot ( Time . visits , mydata . visits . ts , ylab = "" Visits [ thousands ] "" , xlab = "" Year "" , lwd =2 , type = "" l "" ) 28 lines ( Time . visits , hw . visits . additive $ fitted , col = "" blue "" , lwd =2) 29 lines ( Time . visits , hw . visits . mul tiplicat ive $ fitted , col = "" pink "" , lwd =2) 30 # We redraw the original data , just to make the plot look nicer . 31 lines ( Time . visits , mydata . visits . ts ) 32 legend ( Time . visits [1] ,1700 , legend = c ( "" Time Series "" , "" HW additive "" , "" HW multipli cative "" ) , col = c ( "" black "" ,"" blue "" ,"" pink "" ) , lty = c (1 ,1 ,1) , lwd = c (2 ,2 ,2) , cex =0.8) 35 title ( "" Overseas data and fitted values "" )"
389,183,0.413,Impacts of the Fukushima Nuclear Accident on Fish and Fishing Grounds,Sampling Region Date Sardine 2011/3/28 2011/4/6 2011/4/11 2011/4/13 2011/4/25 2011/4/26 2011/5/5 2011/5/9 2011/5/16 2011/5/20 2011/5/25 2011/6/2 2011/6/4 2011/6/22 2011/6/29 2011/4/11 2011/6/6 Japanese anchovy 2011/3/24 2011/4/7 2011/4/14 2011/4/18
175,333,0.38,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","In the above equation, when t = 4, the last period of the year, the following period t + 1 = 1, the ï¬rst period in the following year. Each ESt is the storage volume in excess of the flood storage target volume, TF. Each DRt is the difference between the actual release, Rt, and the target release, TRt, when the release is less than the target. The excess storage, ESt, above the flood target storage TF at the beginning of each season t can be deï¬ned by the constraint: St  TF Ã¾ ESt"
294,276,0.37,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"In the code for updating the arrays S and V we get a term p(t[n])*S[n]. We can also let p.t/ be an array filled with correct values prior to the simulation. Then we need to allocate an array p of length N_t+1 and find the indices corresponding to the time period between 6 and 15 days. These indices are found from the time point divided by t. That is, p = zeros(N_t+1) start_index = 6*24/dt stop_index = 15*24/dt p[start_index:stop_index] = 0.005"
231,649,0.365,North Sea Region Climate Change Assessment,"day 1) when persistent seasonal stratiï¬cation starts (left column) and ends (middle column), and the total number of stratiï¬ed days (right column). Grey shaded areas are well mixed throughout the year (Holt et al. 2010)"
383,150,0.364,Elements of Risk Analysis with Applications in R,"2. Then from this detrended series, we estimate the raw seasonal factors. Each seasonal factor is the mean over all seasonal factors for that season. In our example, we have 12 seasonal factors, as the season repeats every 12 observations (Why? Because we observe 12 values in each year). So in our example for January, we have the following estimated raw seasonal factor value: all t belonging to month j Dt Fj = (3.10) nyears where for January we have j = 1,... December has j = 12, nyears is the number of years for which we have data."
175,1037,0.351,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","the end of the last period of each growing season. In this case some mechanism is needed to determine the beneï¬ts obtained from a series of allocations over time, as will be presented in the next chapter."
231,336,0.346,North Sea Region Climate Change Assessment,Fig. 3.13 Modelled timing (Julian day) of spring stratiï¬cation (when the surface-bottom temperature difference ï¬rst exceeds 0.5 Â°C for at least three days; solid line) and spring bloom (dashed line) between
383,242,0.34,Elements of Risk Analysis with Applications in R,"# -----------------------------------------------------------# Goal : Fit an additive and a multipli cative HW model to 3 # 2 years of Overseas visits data . 4 # -----------------------------------------------------------5 library ( forecast ) 6 # Data : Overseas Visits to UK data 7 mydata <- read . csv ( "" Risk -2021 - Data VisitsT oUK . csv "" ) 8 head ( mydata ,10) # shows the first 10 rows of data , for a quick check 9 # extract years 1980 and 1981 only , and put them into a ts object ."
225,251,0.333,"Spanish Economic Growth, 1850â2015","9.2 Splicing National Accounts Through Interpolation A straightforward procedure would be, then, splicing the all benchmark series available by accepting the levels directly computed for each benchmark year and distributing the gap between each pair of adjacent benchmark series at their overlapping year at either a constant rate over"
383,247,0.332,Elements of Risk Analysis with Applications in R,"Next, we use the multiplicative Holt-Winters smoothing model that we fit into the first two years of data, and we will do the forecasts for the next three years, in R. We find the best parameter values, using R. And we create the Figure 3.24. # R Code # -----------------------------------------------------------3 # Goal : Do use m ultiplic ative HW model of first two years of Overseas visits data , 4 # and then construct the prediction intervals . 5 # -----------------------------------------------------------6 # 7 # Fit HW multip licativ e model again , let R choose the best parameters . 8 # calculate the forecast for the next 36 months . 9 # Ask for prediction intervals : 50% and 95%. 10 library ( forecast ) 11 fit . hw . visits <- hw ( mydata . visits . ts , seasonal = "" mul tiplica tive "" ,h =36 , level = c (50 ,95) ) 12 # Print the forecasts on the computer screen in Console 13 summary ( fit . hw . visits . multi plicati ve ) 14 # plot the model and forecasts . 15 plot ( fit . hw . visits , xlab = "" Year "" , ylab = "" Overseas visitors to UK [ thousands ] "" , ylim = c (0 ,2500) ) 16 # Next , finally ( ! ) , we create a fan plot 17 fit . hw . visits <- hw ( mydata . visits . ts , seasonal = "" mul tiplica tive "" ,h =36 , fan = TRUE ) 18 plot ( fit . hw . visits , xlab = "" Year "" , ylab = "" Overseas visitors to UK [ thousands ] "" , ylim = c (0 ,2500) )"
345,182,0.326,"Big Data in Context : Legal, Social and Technological insights","References Budzinski O, Michler AF (2017) Subprime-Krise. In: Gabler Wirtschaftslexikon, Springer Gabler Verlag (ed). http://wirtschaftslexikon.gabler.de/Archiv/72525/subprime-krise-v9.html. Accessed 4 Apr 2017 Ehmann E (2014) In: Simitis S (ed) Bundesdatenschutzgesetz, vol 8. Baden-Baden, Nomos Hoeren T (ed) (2014) Big data und Recht. C. H. Beck, Munich Jandt S (2015) Big data und die Zukunft des Scoring. Kommunikation und Recht 18(1):6â8 Landesbeauftragte fÃ¼r Datenschutz und Informationsfreiheit Nordrhein-Westfalen (LDI NRW) (2012) AuskunfteienâFragen und Antworten (FAQ). https://www.ldi.nrw.de/mainmenu_ Datenschutz/submenu_Datenschutzrecht/Inhalt/Auskunfteien/Inhalt/Auskunfteien/Auskunfteien_ -_Haeuï¬g_gestellte_Frage.pdf. Accessed 4 Apr 2017 Laney D (2001) 3D data management: controlling data volume, velocity and variety. http://blogs. gartner.com/doug-laney/ï¬les/2012/01/ad949-3D-Data-Management-Controlling-DataVolume-Velocity-and-Variety.pdf. Accessed 4 Apr 2017 Mansmann U (2014) Gut Gemeint. Wie die Schufa Verbraucher bewertet. cât (10):80â81. http://www. heise.de/ct/ausgabe/2014-10-Wie-die-Schufa-Verbraucher-bewertet-2172377.html. Accessed 4 Apr 2017 Morozov E (2013) BonitÃ¤t Ã¼bers Handy. http://www.faz.net/aktuell/feuilleton/silicon-demokratie/ kolumne-silicon-demokratie-bonitaet-uebers-handy-12060602.html#Drucken. Accessed 4 Apr Rieger F (2012) Kredit auf Daten. http://www.faz.net/aktuell/feuilleton/schufa-facebook-kreditauf-daten-11779657.html. Accessed 4 Apr 2017 Schmucker J (2013) Facebook kommunalâKann das deutsche (Datenschutz-) Recht mit dem Wunsch nach Kommunikations- und Informationsfreiheitfreiheit in Einklang gebracht werden? Deutsche Verwaltungspraxis 64(8):319â324. http://www.dvp-digital.de/ï¬leadmin/pdf/ Zeitschriftenausgaben/DVP_Zeitschrift_2013-08.pdf. Accessed 4 Apr 2017 Schulzki-Haddouti C (2014) ZÃ¼gelloses Scoring. Kaum Kontrolle Ã¼ber Bewertung der KreditwÃ¼rdigkeit. cât (21):38â39. http://www.heise.de/ct/ausgabe/2014-21-Kaum-Kontrolleueber-Bewertung-der-Kreditwuerdigkeit-2393099.html. Accessed 4 Apr 2017 Steinebach M, Winter C, Halvani O, SchÃ¤fer M, Yannikos Y (Fraunhofer-Institut fÃ¼r Sichere Informationstechnologie SIT) (2015) Begleitpapier BÃ¼rgerdialog. Chancen durch Big Data und die Frage des PrivatsphÃ¤reschutzes. https://www.sit.fraunhofer.de/ï¬leadmin/dokumente/studien_ und_technical_reports/Big-Data-Studie2015_FraunhoferSIT.pdf. Accessed 4 Apr 2017 UnabhÃ¤ngiges Landeszentrum fÃ¼r Datenschutz Schleswig-Holstein (ULD), GP Forschungsgruppe (2014) Scoring nach der Datenschutz-Novelle 2009 und neue Entwicklungen. Abschlussbericht. http://www.bmi.bund.de/SharedDocs/Downloads/DE/Nachrichten/Kurzmeldungen/studiescoring.pdf?__blob=publicationFile. Accessed 4 Apr 2017"
383,244,0.326,Elements of Risk Analysis with Applications in R,"Next, we do a goodness-of-fit analysis of the additive HW model of the 2 years of Overseas visits data. # R Code # -----------------------------------------------------------3 # Goal : To do a goodness - of - fit analysis of the additive HW model for first 4 # 2 years of Overseas visits data . 5 # Use the HW model with additive seasonality . 6 # -----------------------------------------------------------7 # Plot of resid vs time"
217,89,0.322,Finite Difference Computing With Pdes : a Modern Software Approach,Series expansions in sympy have the inconvenient O() term that prevents further calculations with the series. We can use the removeO() command to get rid of the O() term: >>> w_tilde_series = w_tilde_series.removeO() >>> w_tilde_series dt**2*w**3/24 + w
52,116,0.321,WeiÃbuch Gelenkersatz : Versorgungssituation Bei Endoprothetischen HÃ¼ft- Und Knieoperationen in Deutschland,"Die Darstellung der Inanspruchnahme und der QualitÃ¤tsaspekte endoprothetischer Eingriffe in Deutschland stÃ¼tzt sich auf zahlreiche Gutachten und Berichte sowie verschiedene Datenquellen. In den Gutachten werden drei Datenquellen herangezogen: 1. Daten gemÃ¤Ã Â§ 21 des Gesetzes Ã¼ber die Entgelte fÃ¼r voll- und teilstationÃ¤re Krankenhausleistungen (Krankenhausentgeltgesetz), 2. Routinedaten einzelner gesetzlicher Krankenkassen, 3. Daten des Statistischen Bundesamtes Ã¼ber die HÃ¤ufigkeit aller gemeldeten Operationen- und ProzedurenschlÃ¼ssel (OPS-Statistik)."
231,648,0.318,North Sea Region Climate Change Assessment,"Fig. 6.17 Simulated mean timing of seasonal stratiï¬cation for present day (RCM-P 1961â1990, upper), future climate (RCM-F 2070â2098, middle) and the difference between them (i.e. projected change from POLCOMS, lower). The graphic shows day of the year (1 January is"
356,377,0.316,Re-engineering the Uptake of ICT in Schools,"Duration This element contains information about an interval in time. P[yY][mM][dD][T[hH][mM][s1[.s2]S]] where: y = number of years (integer, >0) m = number of months (integer, >0) d = number of days (integer, >0) h = number of hours (integer, >0) n = number of minutes (integer, >0) s1 = number of seconds (integer, >0; or integer >=0 if s2 > 0) s2 = fraction of seconds (integer, >0) See http://www.w3.org/TR/xmlschema-2/#isoformats. The character literal designators âPâ, âYâ, âMâ, âDâ, âTâ, âHâ, âMâ, âSâ must appear if the corresponding nonzero value is present. If the value of years, months, days, hours, minutes or seconds is zero, the value and corresponding designation (e.g., âMâ) may be omitted, but at least one designator and value must always be present. The designator âPâ is always present. The designator âTâ shall be omitted if all of the time (hours/min/s) are zero."
383,123,0.307,Elements of Risk Analysis with Applications in R,"TABLE 3.1: Monthly visitors time series for January 1980 - December 1981. Example. Overseas visits. (continues) In the next, we will consider the same monthly time series, but now from January 1980 till December 2020, hence 41 years. The Figure 3.2 reveals some interesting features: â¢ During December and January months, the visits are the lowest; during August, the visits are the highest; this is called a seasonal pattern. â¢ The extent of these seasonal differences (August visits minus January visits) is roughly the same, except it seems that these differences are somewhat larger in the last three years. â¢ There is a general increasing trend except for the decreasing trend from January 2000 to December 2001. Why could this be? â¢ There is a drop in visits from January 2008 to January 2009; this coincides with the financial worldwide crisis. There is a constant trend from January 2009 to December 2012, possibly still due financial crisis. Then there is a constant trend from January 2016 to December 2019. This is then followed"
101,249,0.305,Finite Difference Computing with Exponential Decay Models (Volume 110.0),"4.3 Compound Interest and Inflation Say the annual interest rate is r percent and that the bank adds the interest once a year to your investment. If un is the investment in year n, the investment in year unC1 grows to r n u : unC1 D un C In reality, the interest rate is added every day. We therefore introduce a parameter m for the number of periods per year when the interest is added. If n counts the periods, we have the fundamental model for compound interest: unC1 D un C"
231,151,0.303,North Sea Region Climate Change Assessment,"annual precipitation derived from the HOAPS satellite data and four reanalyses: ERA-Interim, ERA-40, NCEP/CFSR and MERRA. Precipitation amount shows considerable intra-annual (see a selection of months in the E-Supplement Fig. S1.5.8) and interannual variability. For the reference period October 1978 to August 1987, February and/or April were the driest months. February 1986 was unusually dry with widespread means of less than 10 mm, even in coastal areas. Some areas were even rainless (e.g. in the region of De Bilt). Because February 1985 and 1986 received well below-average precipitation, the February mean for the period 1979â1987 was about 10 mm less than for the period 1971â2000 and about half the average for the period 1988â2008. The difference was greatest in the Shetland Islands. At Lerwick, the February mean was 62 mm for the period 1979â1987, 108 mm for 1971â2000 and 144 mm for 1988â2008. The HOAPS data show May as the driest month with average precipitation amounts of less than 25 mm in an extended area east and north of the British Islands. Spring is the driest season except in the western English Channel and the region around the Shetland Islands. Precipitation amount is mostly highest in autumn. The increase in precipitation amount in autumn is caused by the increase in low-pressure activity and convective rains due to the destabilisation of"
231,268,0.303,North Sea Region Climate Change Assessment,"Fig. 2.15 Linear trends in the precipitation fraction due to very wet days (R95pTOT) in winter over the periods 1951â1978 (left) and 1979â 2012 (right). Blue circles denote a trend towards wetter conditions, while orange and red circles denote a trend towards drier conditions,"
255,284,0.303,Railway Ecology,00:00-01:00 01:00-02:00 02:00-03:00 03:00-04:00 04:00-05:00 05:00-06:00 06:00-07:00 07:00-08:00 08:00-09:00 09:00-10:00 10:00-11:00 11:00-12:00 12:00-13:00 13:00-14:00 14:00-15:00 15:00-16:00 16:00-17:00 17:00-18:00 18:00-19:00 19:00-20:00 20:00-21:00 21:00-22:00 22:00-23:00 23:00-24:00
80,533,0.302,Innovations in Quantitative Risk Management (Volume 99.0),"in the covariance structure, which the estimate is able to capture only if one restricts the used history.5 Let r denote the (T Ã N ) matrix containing weekly returns, then the sample Ë S is determined by covariance matrix  ËS ="
124,494,0.3,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"References Belnap, N. 1992. Branching space-time. Synthese 92: 385â434. Belnap, N., M. Perloff, and M. Xu. 2001. Facing the Future. Oxford: Oxford University Press. Carnap, R. 1958. Introduction to symbolic logic and its applications. New York: Dover (translation of EinfÃ¼hrung in die symbolische Logik. Wien: Julius Springer 1954). Crossley, J.L., and I.M. Humberstone. 1977. The logic of âActuallyâ. Reports on Mathematical Logic 8: 11â29. Goranko, V., A. Montanari, and G. Sciavicco. 2004. A road map of interval temporal logics and duration calculi. Journal of Applied Non-Classical Logics 14(1/2004): 1â48. Gusfield, D. 2007. Algorithms on strings, trees, and sequences, 10th ed. Cambridge: Cambridge University Press. Kienzle, B. 2007. Die Bestimmung des Janus. TÃ¼bingen: Mohr Siebeck. Kripke, S. 1980. Naming and necessity. Oxford: Blackwell. McTaggart, J.M.E. 1908. The unreality of time. Mind 18: 457â484. MÃ¼ller, T., and Strobach, N. (2011). A letter on the present state of affairs. Prior, indeterminism and relativity 40 years later. Synthese. Preprint online at Springerlink: doi:10.1007/s11229-0119939-z. MÃ¼ller, T. (2011). Branching space-times, general relativity, the Hausdorff property, and modal consistency. Preprint: http://philsci-archive.pitt.edu/8577/1/bst_hausdorff20apr11.pdf. Prior, A. 1967. Past, Present and Future. Oxford: Oxford University Press. Putnam, H. 1967. Time and physical geometry. Journal of Philosophy 64: 240â247. Quine, W.V.O. 1951. On what there is. In: From a logical point of view, 1â19. Cambridge/Mass: Harvard University Press. Quine, W.V.O. 1953. On a so-called paradox. Mind 62: 65â67. Schopenhauer, A. 1818. Die Welt als Wille und Vorstellung I [The World as Will and Representation volume I]. In: Arthur Schopenhauers Werke in fÃ¼nf BÃ¤nden, ed. Ludger LÃ¼tkehaus, ZÃ¼rich: Haffman 1988. Schopenhauer, A. 1839. Preisschrift ueber die Freiheit des menschlichen Willens. Volume 3 of the same edition, 359â458. Strobach, N. 1998. The moment of change - A systematic history in the philosophy of space and time. Dordrecht: Kluwer. Strobach, N. 2007a. Alternativen in der Raumzeit - eine Studie zur philosophischen Anwendung multimodaler Aussagenlogiken. Berlin: Logos. Strobach, N. 2007b. Fooling around with tenses. Studies in the History and Philosophy of Modern Physics 38(3): 653â672. Strobach, N. 2010. Zellen in der Logik des Lebens. In: Logos, Freie Zeitschrift fÃ¼r wissenschaftliche Philosophie. 2 (2010), 2â51. http://fzwp.de/0002/urn:nbn:de:0265--00022. Strobach, N. 2011. Die Persistenz der biologischen Arten. Provisorische Ãberlegungen zu einer biologischen Interpretation zeitlogischer Strukturen. In: Persistenz - IndexikalitÃ¤t - Zeiterfahrung, ed. P. Schmechtig and G. SchÃ¶nrich, 371â403. Heusenstamm: Ontos. Strobach, N. (forthcoming). 2013. Aristoteles und die Konstanz der Arten. Forthcoming in the proceedings volume to a conference on Aristotleâs biology at Kassel University in 2009, ed. G. Heinemann and R. Timme. Freiburg: Alber. Thomason, R. 1970. Indeterminist time and truth-value gaps. Theoria 36: 264â281. Woodger, J.H. 1937. The Axiomatic Method in Biology. With Appendices by Alfred Tarski and W. F. Floyd. Cambridge: Cambridge University Press."
390,578,0.299,The Hidden Language of Computer Hardware and SW,"ur lives are full of repetition. We count the days through the natural rhythms of the rotation of the earth, the revolution of the moon around the earth, and of the earth around the sun. Each day is different, but our lives are often structured by standard routines that are similar from day to day. In a sense, repetition is also the essence of computing. Nobody needs a computer to add two numbers together. (Letâs hope not, anyway!) But adding a thousand or a million numbers together? Thatâs a job for a computer. This relationship of computing and repetition was obvious early on. In Ada Lovelaceâs famous 1843 discussion of Charles Babbageâs Analytical Engine, she wrote: Both for brevity and for distinctness, a recurring group is called a cycle. A cycle of operations, then, must be understood to signify any set of operations which is repeated more than once. It is equally a cycle, whether it be repeated twice only, or an indefinite number of times; for it is the fact of a repetition occurring at all that constitutes it such. In many cases of analysis there is a recurring group of one or more cycles; that is, a cycle of a cycle, or a cycle of cycles."
159,74,0.297,"BildungsverlÃ¤ufe Von Der Einschulung Bis in Den Ersten Arbeitsmarkt : Theoretische AnsÃ¤tze, Empirische Befunde Und Beispiele",80-81 81-82 82-83 83-84 84-85 85-86 86-87 87-88 88-89 89-90 90-91 91-92 92-93 93-94 94-95 95-96 96-97 97-98 98-99 99-00 00-01 01-02 02-03 03-04 04-05 05-06 06-07 07-08 08-09 09-10 10-11 11-12 12-13 13-14 14-15
217,426,0.294,Finite Difference Computing With Pdes : a Modern Software Approach,"We can also easily make a Taylor series expansion in the discretization parameter >>> import sympy as sym >>> C, p = sym.symbols(âC pâ) >>> # Compute the 7 first terms around p=0 with no O() term >>> rs = r(C, p).series(p, 0, 7).removeO() >>> rs p**6*(5*C**6/112 - C**4/16 + 13*C**2/720 - 1/5040) + p**4*(3*C**4/40 - C**2/12 + 1/120) + p**2*(C**2/6 - 1/6) + 1 >>> # Pick out the leading order term, but drop the constant 1 >>> rs_error_leading_order = (rs - 1).extract_leading_order(p) >>> rs_error_leading_order p**2*(C**2/6 - 1/6) >>> # Turn the series expansion into a Python function >>> rs_pyfunc = lambdify([C, p], rs, modules=ânumpyâ) >>> # Check: rs_pyfunc is exact (=1) for C=1 >>> rs_pyfunc(1, 0.1)"
383,146,0.29,Elements of Risk Analysis with Applications in R,"long-term change in the trend of time series (see Figure 3.2). Such change is not a random error, rather it is a change in trend caused by an external force, in this case, the government intervention. Such a change can be modelled by adding suitable covariates into the time series model. Example. Four examples of time series. Using the time series in Figure 3.6 describe the trend, seasonality and cyclic components. Solution. Possible answers: â¢ The monthly housing sales (top left) show strong seasonality within each year. There is no apparent overall increase or decrease in the data. There is some strong cyclic behaviour: the first full cycle lasts 7 years (1975-1962), and another full cycle is seen to last 9 years (1962-1991). â¢ The US treasury bill contracts (top right) show results from the Chicago market for 100 consecutive trading days in 1981. Here there is no seasonality, but an obvious downward trend. Possibly, if we had a much longer series, we would see that this downward trend is actually part of a long cycle, but when viewed over only 100 days it appears to be a trend. â¢ The Australian quarterly electricity production (bottom left) shows a strong increasing trend, with strong seasonality. There is no evidence of any cyclic behaviour here. Also, we see that the size of the seasonal fluctuations increases with the increasing trend. â¢ The daily change in the Google closing stock price (bottom right) has no trend, seasonality or cyclic behaviour. There are random fluctuations which do not appear to be very predictable and no strong patterns that would help with developing a forecasting model. How do we estimate the components of the time series? There are various tools to quantitatively estimate and evaluate the components of time series, commonly called: Time Series Decomposition tools. The general idea is this: first, we will estimate the trend, mt , then we will estimate the seasonal component, st , and then the remainder (i.e. the errors, or residuals). Note here, that the cyclic and trend component are estimated as one component. Estimating the trend component. A simple method to estimate the trend mt at time t is to take the average of Yt and of its neighbouring values, a so-called moving average. This then poses a question of how many neighbours do we use, and if we use a simple average or a weighted average. Another way to estimate a trend is by using a linear or nonlinear regression model. Example. Overseas visits. (continues) We will be estimating the trend via the moving average in the Overseas visits data."
390,323,0.289,The Hidden Language of Computer Hardware and SW,The time required for one cycle is called the period of the oscillator. Letâs assume that weâre looking at a particular oscillator that has a period of 0.02 seconds. The horizontal axis can be labeled in seconds beginning from some arbitrary time denoted as 0:
48,2,0.288,Time Predictions : Understanding and Avoiding Unrealism in Project Planning and Everyday Life (Volume 5.0),"Dear reader, Our aim with the series Simula SpringerBriefs on Computing is to provide compact introductions to selected ï¬elds of computing. Although the topic of the present volume is important within computing, the authors take a broader approach and draw on research from psychology, forecasting, management science, and software engineering when summarizing knowledge about how to make realistic time predictions. The book is suitable for students, researchers, professionals, and others interested in a concise introduction to the science of time predictions. Entering a new ï¬eld of research can be quite demanding for graduate students, postdocs, and experienced researchers alike: the process often involves reading hundreds of papers, and the methods, results, and notation styles used often vary considerably, which makes for a time-consuming and potentially frustrating experience. The briefs in this series are meant to ease the process by introducing and explaining important concepts and theories in a relatively narrow ï¬eld, and by posing critical questions on the fundamentals of that ï¬eld. A typical brief in this series should be around 100 pages and should be well suited as material for a research seminar in a well-deï¬ned and limited area of computing. We have decided to publish all items in this series under the SpringerOpen framework, as this will allow authors to use the series to publish an initial version of their manuscript that could subsequently evolve into a full-scale book on a broader theme. Since the briefs are freely available online, the authors will not receive any direct income from the sales; however, remuneration is provided for"
175,851,0.288,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","Ã°7:33Ã where the functions ut(St, St+1) and dt(St, St+1) have been determined using Eqs. 7.30â7.32. As the total number of periods remaining, n, increases, the solution of this dynamic programming model will converge to a steady or stationary state. The best ï¬nal storage volume St+1 given an initial storage volume St will likely differ for each within-year period or season t, but for a given season t it will be the same in successive years. In addition, for each storage volume St, streamflow, Qt, and within-year period t the difference between Ftn Ã¾ T Ã°St ; Qt Ã and Ftn Ã°St ; Qt Ã will be the same constant regardless of the storage volume St, flow Qt and period t. This constant is the optimal, in this case minimum, annual value of the objective function, Eq. 7.24. There could be additional limits imposed on storage variables and release variables, such as for"
385,155,0.287,Advanced R,"second version works because lapply can be given the name of a function instead of the function itself: if you read the source of lapply(), youâll see the ï¬rst line uses match.fun() to ï¬nd functions given their names. A more useful application is to combine lapply() or sapply() with subsetting: x <- list(1:3, 4:9, 10:12) sapply(x, ""["", 2) #> [1] 2 5 11 # equivalent to sapply(x, function(x) x[2]) #> [1] 2 5 11"
175,736,0.287,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications",(summer) in year y are denoted by S1y and S2y. The reservoirâs winter operating policy is to store as much of the winterâs inflow Q1y as possible. The winter release R1y is determined by the rule < S1y Ã¾ Q1y  K R1y Â¼ Rmin S1y Ã¾ Q1y
137,393,0.286,Comparative Perspectives on Work-Life Balance and Gender Equality : Fathers on Leave Alone,"Only two fathers had a different set-up : Friday off for one, and a variable day for the other. Due to the large number of female employees who take Wednesday off in order to take care of their child (ren)."
385,57,0.285,Advanced R,"and names of columns must match. Use plyr::rbind.fill() to combine data frames that donât have the same columns. Itâs a common mistake to try and create a data frame by cbind()ing vectors together. This doesnât work because cbind() will create a matrix unless one of the arguments is already a data frame. Instead use data.frame() directly: bad <- data.frame(cbind(a = 1:2, b = c(""a"", ""b""))) str(bad) #> 'data.frame': 2 obs. of 2 variables: #> $ a: Factor w/ 2 levels ""1"",""2"": 1 2 #> $ b: Factor w/ 2 levels ""a"",""b"": 1 2 good <- data.frame(a = 1:2, b = c(""a"", ""b""), stringsAsFactors = FALSE) str(good) #> 'data.frame': 2 obs. of 2 variables: #> $ a: int 1 2 #> $ b: chr ""a"" ""b"""
294,275,0.284,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"Note that we must multiply the t value by 24 because t is measured in hours, not days. In the differential equation system, pS.t/ must be replaced by p.t/S.t/, and in this case we get a differential equation system with a term that is discontinuous. This is usually quite a challenge in mathematics, but as long as we solve the equations numerically in a program, a discontinuous coefficient is easy to treat. There are two ways to implement the discontinuous coefficient p.t/: through a function and through an array. The function approach is perhaps the easiest: def p(t): return 0.005 if (6*24 <= t <= 15*24) else 0"
231,230,0.283,North Sea Region Climate Change Assessment,"Fig. 2.4 Time series of mean seasonal wind speed derived from NCEP/NCAR reanalysis over the North Sea (blue) and the NAO index (black) for winter (DJFM) 1948â2014 recalculated and updated as in Siegismund and Schrum (2001), by F. Schenk. The positive trend (red"
383,124,0.282,Elements of Risk Analysis with Applications in R,"FIGURE 3.2: Overseas monthly visits to UK Jan 1980 to Dec 2020. The visits are in the thousands. by a drop from December 2019 to March 2020, which coincides with the Covid-19 pandemic and the closing of flights. â¢ We also see some random fluctuations. â¢ Hence, when preparing the forecasts from this series, we would need to take into account the seasonal pattern, the trend, as well as the random fluctuations. What would the tourism institutions like to know from these visitsâ time series? They may be interested in having answers to the following questions: 1. What is the expected number of visits each month from April-December 2020? 2. What are the chances that from April to December 2020, the total number of visits will be between 2500 and 3500 thousand? 3. What are the chances that in the next three months, there will be at least 3000 visits per month? Next, assume it is December 1981, and we have the visit data from January 1980 - December 1981. In which of the two following scenarios should we feel more confident to do a forecast? â¢ Scenario 1: We are asked to use the data to predict the number of visits for January 1982."
172,118,0.28,Twitter Als Basis Wissenschaftlicher Studien : Eine Bewertung GÃ¤ngiger Erhebungs- Und Analysemethoden Der Twitter-Forschung,"MongoDB stellt fÃ¼r das sogenannte Preprocessing eine Vielzahl an MÃ¶glichkeiten zur Manipulation und Analyse von Daten zur VerfÃ¼gung. Diese eignen sich jedoch eher zur allgemeinen Strukturierung der Daten, als fÃ¼r die Textmanipulation oder detailliertere Analysen. So kÃ¶nnen DatensÃ¤tze gefiltert oder gruppiert werden, wÃ¤hrend eine inhaltliche Verarbeitung von Texten (z.B. Stoppwort-Filter, Tokenisierung) nur auf Umwegen Ã¼ber andere Programme oder Skripte mÃ¶glich ist. Ein wichtiges Toolkit fÃ¼r die Verarbeitung auf Datensatz-Ebene sind die Methoden zur Datenaggregation, welche drei wesentliche Funktionen beinhalten: Abfragemethoden zur Aggregation, das Aggregation Framework sowie das MapReduce-Verfahren. Die drei Methoden unterscheiden sich im Allgemeinen vor allem in ihrer Performance und ihrem Funktionsumfang. WÃ¤hrend die Abfragemethoden einen geringen Funktionsumfang, aber eine hohe Leistung hinsichtlich Verarbeitungsgeschwindigkeit vorweisen, bietet MapReduce eine sehr hohe FunktionalitÃ¤t (auch fÃ¼r Analysen) mit jedoch eingeschrÃ¤nkter Performance. Einen Mittelweg zwischen Leistung und FunktionalitÃ¤t geht das Aggregation Framework."
241,410,0.278,Second Assessment of Climate Change for the Baltic Sea Basin,"Fig. 6.1 Snow cover anomalies in the Russian part of Baltic Sea basin. a Snow cover days. b Snow depth in March. c Snow water equivalent in March. Dots show anomalies relative to the long-term average for 1938â2008. Dotted lines show linear trends (ROSHYDROMET 2008; Kitaev et al. 2007, 2010)"
231,320,0.277,North Sea Region Climate Change Assessment,"generally show also in heat content (HjÃ¸llo et al. 2009; Meyer et al. 2011) and in all seasons (Fig. 3.4), despite winter-spring variability exceeding summer-autumn variability. The increase in North Sea heat content between 1985 and 2007 was about 0.8 Ã 1020 J, much less than the seasonal range (about 5 Ã 1020 J) and comparable with interannual variability (HjÃ¸llo et al. 2009). Despite an inherent anomaly adjustment time-scale of just a few months (Fig. 3.5 and Meyer et al. 2011), the longer-term decline in SST from the 1940s to 1980s and subsequent marked rise to the early 2000s are widely reported. The basis is in observations, for example those shown by McQuatters-Gollop et al. (2007 using HADISST v1.1; see Fig. 3.6 and E-Supplement Table S3.1), Kirby et al. (2007), Holt et al. (2012, including satellite SST data, Fig. 3.7) and multi-decadal hindcasts, such as those of Meyer et al. (2011) and Holt et al. (2012). Particular features noted"
286,35,0.276,Corporate Data Quality : Voraussetzung Erfolgreicher GeschÃ¤ftsmodelle,"Digitale GeschÃ¤ftsmodelle und das Internet der Dienste basieren auf der Ressource Daten. DatenqualitÃ¤t ist damit fÃ¼r Unternehmen kein âHygienefaktorâ mehr oder gar Selbstzweck von Stabsabteilungen, sondern ist kritisch fÃ¼r die Operational Excellence. DatenqualitÃ¤t ist definiert als ein MaÃ fÃ¼r die Eignung der Daten fÃ¼r bestimmte Anforderungen in GeschÃ¤ftsprozessen, in denen sie verwendet werden (Otto et al. 2011). Im Folgenden wird âDatenmanagementâ stets unter besonderer BerÃ¼cksichtigung des DatenqualitÃ¤tsmanagements behandelt. Zu den wichtigsten Treibern fÃ¼r das qualitÃ¤tsorientierte Datenmanagement gehÃ¶ren:"
126,327,0.276,River Basin Development and Human Rights in Eastern Africa : A Policy Crossroads,"Fishing seasons contrast with pastoral ones, with fundamentally different factors for livelihood. Monthly periods indicated are approximate and fluctuate with environmental conditions. Much variation in villagersâ use of these terms also exists in the northern and central regions"
231,544,0.274,North Sea Region Climate Change Assessment,"consider a particular amount of daily precipitation. Sillmann et al. (2013), for instance, analysed future change in the number of days with at least 10 mm precipitation and projected an increase in western, central and northern Europe, ranging from about two additional days (RCP2.6) to about six additional days (RCP8.5) at the end of the 21st century. In contrast to Sillmann et al. (2013), who based their analysis on data covering the entire year, KNMI (2014) distinguished between winter and summer and used different thresholds for the two seasons, 10 mm in winter and 20 mm in summer. In winter, KNMI (2014) found more days with at least 10 mm precipitation in the Netherlands, with increases of 14â24 % for the two scenarios with moderate future warming and 30â60 % for the two scenarios with strong future warming. For each of the two rates of future warming the strongest increases are associated with a strong influence of circulation change (i.e. a more predominantly westerly"
32,47,0.274,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","sometimes miss some price changes or repeatedly count the same price changes (see bottom of Fig. 2.1). This is good if we are interested in the most recent price signals and in financial markets this is the case. It also reflects the dynamical nature of the time series, as the inter-arrival times may vary from day to day or between equities no new Ä±t needs to be defined, it will always use only the most recent information in both the source and the target time series. The most significant shortcoming is that this TE assumes there is no information being carried by the inter-arrival time interval and it is not clear that some of the theoretical foundations on which the original TE is based necessarily hold, from this point of view this method of calculating the TE is currently only a heuristic and the results presented here are for the moment qualitative in nature."
291,86,0.274,Visualizing Mortality Dynamics in the Lexis Diagram (Volume 44.0),"9.1 Decomposing Seasonal Data The majority of deaths in most countries can be attributed to causes that feature a distinct seasonal pattern. Figure 9.1 depicts the relative monthly frequencies of nine selected causes of death in the United States for women and men combined for the years 1959â2014. The reported number of counts in parentheses in the title of each panel is the actual number of deaths. To control for varying lengths of months, the monthly columns in each histogram have been adjusted for a uniform length (30 days). The horizontal reference lines denote the expected value of a uniform distribution (=1/12). The typical distribution follows a sinusoidal pattern with highest mortality in winter and relatively few cases in the summer. Primarily, those are circulatory diseases (e.g., heart diseases, cerebrovascular diseases)âas shown in the first row of Fig. 9.1âand respiratory diseases such as chronic obstructive pulmonary disease (âCOPDâ), pneumonia or influenza (Eurowinter Group 1997, 2000; Mackenbach et al. 1992; Kunst et al. 1990; Rau 2007; Yen et al. 2000; Seretakis et al. 1997), which are displayed in three horizontal panels in the middle of Fig. 9.1. If diseases, and ultimately, mortality occur seasonally, it has been argued that âan environmental factor has to be considered in the etiology of that diseaseâ (Marrero 1983, p. 275).1 The main environmental factor to trigger higher mortality during winter for circulatory diseases and respiratory diseasesâthe rows on top of"
172,99,0.273,Twitter Als Basis Wissenschaftlicher Studien : Eine Bewertung GÃ¤ngiger Erhebungs- Und Analysemethoden Der Twitter-Forschung,"kann. Nach Abschluss des Schreibprozesses besteht keine MÃ¶glichkeit fÃ¼r einen simultanen Zugriff auf die Daten durch mehrere Personen. Zudem kann die zugreifende Person nur einen Prozess (Suchen, Kopieren, Ãberschreiben etc.) gleichzeitig durchfÃ¼hren. Funktionen einer komplexeren Datenverwaltung, wie ein Umstrukturieren, Aggregieren oder Filtern von Daten ist, wenn Ã¼berhaupt, nur eingeschrÃ¤nkt mÃ¶glich. HierfÃ¼r eignen sich besonders Datenbanksysteme, die das nÃ¤chste Kapitel betrachtet."
390,213,0.272,The Hidden Language of Computer Hardware and SW,9 A B C D 9 A B C D 9 A B C D E 9 A B C D E 9 A B C D E F 10 9 A B C D E F 10 11 9 A B C D E F 10 11 12 9 A B C D E F 10 11 12 13 9 A B C D E F 10 11 12 13 14 A B C D E F 10 11 12 13 14 15 B C D E F 10 11 12 13 14 15 16 C D E F 10 11 12 13 14 15 16 17 D E F 10 11 12 13 14 15 16 17 18 E F 10 11 12 13 14 15 16 17 18 19 F 10 11 12 13 14 15 16 17 18 19 1A 10 11 12 13 14 15 16 17 18 19 1A 1B 11 12 13 14 15 16 17 18 19 1A 1B 1C
385,404,0.272,Advanced R,"This is slow because each time you extend the vector, R has to copy all of the existing elements. Section 17.7 discusses this problem in more depth. Instead, itâs much better to create the space youâll need for the output and then ï¬ll it in. This is easiest with the second form: res <- numeric(length(xs)) for (i in seq_along(xs)) { res[i] <- sqrt(xs[i])"
326,888,0.271,"Autonomes Fahren : Technische, Rechtliche Und Gesellschaftliche Aspekte","24.4.1 GrundsÃ¤tze FÃ¼r jegliche Art persÃ¶nlicher Daten, die erhoben oder erfasst und aus dem Einflussbereich der betroffenen Person heraus Ã¼bermittelt werden, muss es eine klare BegrÃ¼ndung geben, die die einschlÃ¤gigen DatenschutzgrundsÃ¤tze und -anforderungen berÃ¼cksichtigt. Die DatenschutzgrundsÃ¤tze und -anforderungen hÃ¤ngen von der jeweiligen nationalen, regionalen und gelegentlich auch branchenspezifischen Gesetzgebung ab, sodass eine vollstÃ¤ndige Analyse hier unmÃ¶glich wÃ¤re. Erfreulicherweise existiert seit 2011 die internationale Norm ISO/IEC 29100 âPrivacy Frameworkâ [8], die elf DatenschutzgrundsÃ¤tze enthÃ¤lt. Diese GrundsÃ¤tze wurden aus DatenschutzgrundsÃ¤tzen abgeleitet, die in den Jahren und Jahrzehnten zuvor von Staaten, LÃ¤ndern und internationalen Organisationen (z. B. der OECD und der EU) in ihren jeweiligen Regulierungen entwickelt worden waren. Die Editoren der"
137,508,0.271,Comparative Perspectives on Work-Life Balance and Gender Equality : Fathers on Leave Alone,Fig. 13.2 Leave period taken by fathers and mothers (2012) (Source: Ministry HLW (2013) Office Survey Table 18(1) & 18(2) (Recategorised). Note: Diagonal lines connect the borders of the same sets of categories between fathers and mothers)
175,338,0.271,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","(A more deï¬nitive test of whether or not a steady-state policy has been reached will be discussed later.) A steady-state policy will occur if the inflows, Qt, and objectives, TSDt (St, Rt, St+1), remain the same for speciï¬c within-year periods from year to year. This steady-state policy is independent of the assumption that the operation will end at some point. To ï¬nd the steady-state operating policy for this example problem, assume the operation ends in some distant year at the end of season 4 (the right-hand side nodes in Fig. 4.17). At the end of this season the number of remaining seasons, n, equals 0. The values of the remaining minimum sums of weighted squared deviations, Ft Ã°St ; Qt Ã associated with each state (St, Qt), i.e., each node, equal 0. Since for this problem there is no future. Now we can begin the process of ï¬nding the best releases Rt in each successive season t, moving backward to the beginning of stage t = 4, then stage t = 3, then to t = 2, and then to t = 1, and then to t = 4 of the preceding year, and so on, each move to the left increasing the number of remaining seasons n by one. At each stage, or season t, for each discrete state (St, Qt) we can compute the release Rt or equivalently the ï¬nal storage volume St+1, that minimizes Fnt Ã°St ; Qt Ã Â¼ MinimumfTSDt Ã°St ; Rt ; St Ã¾ 1 Ã Ã¾ Ftn1 Ã¾ 1 Ã°St Ã¾ 1 ; Qt Ã¾ 1 Ãg for all 0  St  20"
315,256,0.269,Situating Children of Migrants Across Borders and origins : a Methodological Overview,"In the following, we introduce some advantages and limitations of our method that emerged during the implementation of this procedure. At the end of the section, we discuss possible future applications of the method. Several scholars have argued that the use of a history calendar, as compared to a standard biographical questionnaire, improves the validity and quantity of the data collected (Barbeiro and Spini 2015; Belli 2007; Belli and Callegaro 2009; Glasner and Van der Vaart 2009; Martyn and Martin 2003). The calendar is a flexible tool that allows the development of sequential (in the same column or domain of life) and parallel (by comparing the data in different dimensions of life) retrieval strategies (Belli and Callegaro 2009: 35). In addition to the advantages related to the use of a history calendar, there are specific advantages in combining it with narrative biographical interviews. First, during the interview, both the interviewer and the interviewee can compare the data on the calendar to improve the quality of the data. If the interviewee remarks on a difference, he or she can correct what he or she said or correct the date of an event or period on the calendar. If the interviewer observes some differences, then he or she can point it out and ask for more precise information. Sometimes, during the in-depth biographical interview, the interviewee remembered and added some life events on the calendar (particularly for the last column about subjective life events). Second, the graphical representation of the respondentâs life trajectory proves to be very useful at the moment of the narrative interview. The interviewer can ask questions about a specific dimension and life period and concretely point to it on the calendar with a finger. The interviewee can situate the question on his or her own life course and easily understand the question. Alternatively, it is the interviewee who tells his or her own story, also while answering to a question, while pointing at the relevant event or year on the calendar with a finger.. Third, the calendar may make possible the narration of a difficult life event (parental death, illness or negative critical events) that would otherwise be left aside in the narrative interview. In fact, it is sometimes easier for the interviewee to write"
172,82,0.269,Twitter Als Basis Wissenschaftlicher Studien : Eine Bewertung GÃ¤ngiger Erhebungs- Und Analysemethoden Der Twitter-Forschung,"ZusÃ¤tzlich zu der manuellen Datenerhebung Ã¼ber die Twitter APIs stehen spezialisierte Online-Dienstleister zur VerfÃ¼gung. Die bekanntesten sind Gnip19 und DataSift20. Im April 2015 gab jedoch Datasift bekannt, dass das Unternehmen ab August 2015 den Zugriff auf Twitter-Daten verliert (Halstead, 2015). In Zukunft soll Gnip der einzige Anbieter fÃ¼r Twitter-Daten sein, womit sich Twitter die alleinige Kontrolle Ã¼ber die kommerzielle Verwertung seiner Daten schafft (Hofer-Shall, 2015). Auf Basis einer monatlichen GebÃ¼hr (mehrere tausend USD) kÃ¶nnen unter anderem reprÃ¤sentative Stichproben (z.B. das 10%-Sample Decahose von Gnip) oder historische Tweets bis zu einem Stichtag angefordert werden. Das verfÃ¼gbare Datenset ist somit hinsichtlich der Informationstiefe sehr ausfÃ¼hrlich21, weist jedoch im Normalfall in der Breite nicht den Datenumfang auf, der theoretisch mit gebÃ¼ndelten Abfragen Ã¼ber beide APIs abgerufen werden kÃ¶nnte. Gnip beschrÃ¤nkt sich hier, aufgrund der groÃen Datenmenge, auf die gÃ¤ngigsten Metadaten (Gaffney & Puschmann, 2014, S. 58). Statt einem Request Ã¼ber einer der APIs mÃ¼ssen hier spezifische Datenanfragen an den Dienstleister gestellt werden, der dann â je nach Datentyp â die relevanten Daten einmalig oder regelmÃ¤Ãig Ã¼bermittelt. Die Daten sind bereits strukturiert und kÃ¶nnen beispielweise als Datenbank-Import oder mit vorher extrahierten, relevanten Werten versendet werden. Neben Gnip steht eine grÃ¶Ãere Zahl kostenloser oder gebÃ¼hrenpflichtiger Programme oder Dienste zur VerfÃ¼gung, die das Sammeln von Tweets vereinfachen, jedoch teilweise nur eingeschrÃ¤nkte Funktionen beinhalten. Als beispielhafte Auswahl dieser Dienste stellt diese Arbeit die zwei gÃ¤ngigen Online-Dienste Twitonomy und Tweet Archivist vor. Twitonomy (Diginomy Pty Ltd., 2015) erlaubt in seiner kostenlosen Version unter anderem das Sammeln von Tweets, ein Aggregieren von Tweets eines Nutzers sowie grundlegende Analysen Ã¼ber dessen Twitter-Verhalten, Interaktion und Vernetzung. Die kostenpflichtige Variante beinhaltet detailliertere Informationen und eine Funktion zum Speichern beziehungsweise Exportieren von Tweets und Analysen als PDF oder Excel-Sheet. Twitonomy ist sehr einstiegsfreundlich und gibt mit geringem Aufwand viele aufschlussreiche und Ã¼bersichtlich dargestellte Informationen. Ebenso ist der Export der Daten intuitiv. Dennoch eignet sich der Dienst nur fÃ¼r die Einarbeitung in die Twitter-Forschung, oder â bei geringem Informationsbedarf â fÃ¼r sehr grundlegende Analysen. Die DatenverfÃ¼gbarkeit ist Gnip wurde 2014 von Twitter Ã¼bernommen. www.gnip.com www.datasift.com Bei Gnip sind alle Tweets seit dem Tag der Freischaltung des Online-Dienstes (21.05.2006) gespeichert."
204,271,0.268,Iutam : a Short History,Years 1978-86 2005-14 1998-14 1957-63 1966-73 1984-86 2009-12 1982-08 1950-96 1950-57 1982-86 1948-52 2012-14 1948-50 1996-99 1970-12 1948-53 1988-97 1990-00 1992-11 1954-59 1954-75 1960-99 1992-94 1993-99 1984-95 1952-56 1976-02 1994-14 1996-99 1989-91 2000-14 1971-91 1982-86 1974-80 1964-83 1960-61 2001-02 1984-11
32,46,0.268,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","â¢ Using a fixed interval in which the price at the beginning is compared with the price at the end of the interval conflates signals that may occur before or after another signal but arrives during the same binning interval, thereby mixing future and past events in the measured relationships between bins. â¢ Similarly, multiple price changes within Ä±t may net to zero change and so some price signals are missed. â¢ As bin sizes get smaller they are less statistically reliable as fewer events occur within each bin, equally as bin sizes get larger there are fewer bins per day, thereby also reducing the statistical reliability. â¢ Over the period of a single day, for each bin size the number of total bins is: Ä±t D 30 min: 13 bins/day, Ä±t D 1 min: 390 bins/day, whereas the raw data may have 50â5000+ price changes in a day. The proposed heuristic for the TE introduced above addresses some of these shortcomings but not without introducing some other issues. First, it will always condition out the most recent price change information in the target equity (Boeing in Fig. 2.1) and so uses every bit of relevant information in the target time series. It also uses the most recent price change from the source time series, however it will"
48,114,0.267,Time Predictions : Understanding and Avoiding Unrealism in Project Planning and Everyday Life (Volume 5.0),"5.7 The Time Unit Effect Do you feel that 365 days is longer than one year? Most people seem to feel that way. The likelihood of starting a diet is, for example, higher when the diet program is framed as a one-year plan rather than a 365-day plan [35]. If you find this effect amusing, an even more remarkable and frightening result was reported in a study on judges [36]. Active trial judges were given hypothetical cases and asked to decide what would be the appropriate length of the prison sentences for the offenders. One group of judges was asked to give sentences in months and the other group was asked to give sentences in years. The average length of the sentences, when given in years, was 9.7 years, whereas the average length of the sentences for the same crimes, when given in months, corresponded to 5.5 years (66 months). So, if you happen to commit a crime, you should really hope that the judge gives your sentence in months, or, perhaps even better, in weeks or days, rather than in years. If we feel that 365 days or 12 months is longer than one year, we should also think that it is possible to complete more work in the same time frame when the prediction"
32,39,0.265,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","where cov.; / is the covariance, X and Y are standard deviations and rtk is calculated over a finite historical window of length k where in order to calculate the dynamics of rtk this window is allowed to slide over the data, updating rtk as t progresses. A key issue with data that arrives at irregular or stochastic time intervals and rtk is desired is what counts as a co-occurrence at time t of new data. The most common method is to bin the data into equally separated time intervals of length Ä±t and if two observations xt and yt occur in the interval Åt Ä±t ; tÂ then xt and yt are said to co-occur at time t, this approach is used for the correlations calculated in this article. Throughout the change in the log price is the stochastic event of interest: if at time t the price is pt and at time t0 it changes to pt0 then the stochastic observable is xt0 D log. pt0 / log. pt / [5], the increment t0 t may be fixed in which case it is labelled Ä±t or may dynamically vary, more on this below."
204,279,0.262,Iutam : a Short History,"Years 1978-87 1964-74 2001-08 1976-10 1988-00 1984-92 1995-01 1950-56 2011-14 1979-83, 88-14 1952-53 2009-14 1994-14 1999-13 1992-01 1996-97 2001-14 2007-14 1997-02 1969-75 1978-86 1992-95 1952-53 1992-93 1976-83 1982-87,92-98,0103,09-14 1988-93 1978-86 1984-89 1952-56 1990-2006 1964-65 1978-80 1948-50 1948-51 1982-87 1975-77 1954-75 2012-14"
291,90,0.262,Visualizing Mortality Dynamics in the Lexis Diagram (Volume 44.0),"The patterns observed in the three panels at the bottom of Fig. 9.1 deviate from the ones for circulatory and respiratory diseases above. Motor vehicle accidents do not peak in winter but around July and August. Many people assume that the reason for the peak in all-cause mortality is due to suicides in winter. The middle panel at the bottom of Fig. 9.1 illustrates why this assumption is wrong for three reasons: (1) The seasonal pattern is less pronounced for suicide than for other causes. (2) If one can speak of a seasonal pattern at all, the peak occurs definitely not during winter. (3) The 30,000 observed deaths are less than 1.5% of all deaths; not enough to shape the pattern for all causes. Lung cancer, whose impact on mortality in the United States was discussed in previous chapters, isâlike many malignant neoplasmsâan example of no or only negligible seasonality. Figure 9.1 displays an aggregated picture of monthly deaths. In our analysis we want to investigate, however, whether the seasonal pattern for selected causes of death differs by age as well as whether the seasonal pattern changed over calendar time. The multiplicative model2 suggested by Eilers et al. (2008) to decompose seasonal data allows such an analysis. The model is, at its core, another application of smoothing data via P-splines (Eilers and Marx 1996) as in Chap. 5. It is rather flexible since it allows the estimation not only of counts but also of rates. Exposures are then included as log offsets if the latter is desired, similar to Camardaâs approach (2012, 2015) employed in Chaps. 5, 6, and 7. We use the model in its most simple form: The model is estimating counts assuming an annual unimodal pattern in the data. Not allowing for bimodal patterns or even higher frequencies should not induce any problems in our analysis since the causes in which we are interested in feature clear patterns with one peak and one trough (see Fig. 9.1). We model the expected value of death counts y over age a and time t, ta D E.yta /, to be Poisson distributed using a log-link function log .ta / D vta C fta cos .!t/ C gta sin .!t/ with ! D 2=p, where p is the period. In our case of monthly values p D 12. Further technical details are given in Eilers et al. (2008). The estimation yields three smooth matrices/surfaces, vta for the trend as well as the smooth cosine and sine surfaces fta and gta . The trend surface captures any major changes in the overall pattern that could be caused by varying population sizes, survival improvements, competing risks . . . . We are mainly not interested in this trend surface nor in the the actual sine and cosine surfaces. The two latter surfaces allow us, however, to obtain an estimate for the amplitude and the phase over age and time via simple trigonometric functions. The latter denotes the location of the annual peak of the death counts and is expressed in the difference in days from the 1st of January; i.e., a value of 30 corresponds to late January whereas -30 indicates that mortality is highest in the beginning of December."
217,68,0.262,Finite Difference Computing With Pdes : a Modern Software Approach,"1.3.4 Using a Line-by-Line Ascii Plotter Plotting functions vertically, line by line, in the terminal window using ascii characters only is a simple, fast, and convenient visualization technique for long time series. Note that the time axis then is positive downwards on the screen, so we can let the solution be visualized âforeverâ. The tool scitools.avplotter.Plotter makes it easy to create such plots: def visualize_front_ascii(u, t, I, w, fps=10): Plot u and the exact solution vs t line by line in a terminal window (only using ascii characters). Makes it easy to plot very long time series. from scitools.avplotter import Plotter import time from math import pi P = 2*pi/w umin = 1.2*u.min(); umax = -umin p = Plotter(ymin=umin, ymax=umax, width=60, symbols=â+oâ) for n in range(len(u)): print p.plot(t[n], u[n], I*cos(w*t[n])), \ â%.1fâ % (t[n]/P) time.sleep(1/float(fps))"
175,668,0.261,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","are 12 months in a year, there would be 12 transition matrices, the elements of which could be denoted as ptij . Each deï¬nes the probability of a streamflow ptj Ã¾ 1 Ã°yÃ in month t + 1, given a streamflow pti Ã°yÃ in month t. The steady-state stationary probability vectors for each month can be found by the procedure outlined above, except that now all 12 matrices are used to calculate all 12 steady-state probability vectors. However, once the steady-state vector p is found for one month, the others are easily computed using Eq. 6.121 with t replacing y."
385,738,0.261,Advanced R,1. What are faster alternatives to lm? Which are speciï¬cally designed to work with larger datasets? 2. What package implements a version of match() thatâs faster for repeated lookups? How much faster is it? 3. List four functions (not just those in base R) that convert a string into a date time object. What are their strengths and weaknesses? 4. How many diï¬erent ways can you compute a 1d density estimate in R? 5. Which packages provide the ability to compute a rolling mean? 6. What are the alternatives to optim()?
14,108,0.26,Contested Childhoods : Growing Up in Migrancy,"The names of all study participants have been changed to ensure their anonymity. Many people in South Sudan, especially in the rural areas, do not mark their chronological age. Asylum applications often list January 1st as the applicantâs birthday. This is a random date assigned to all of those who cannot indicate their actual date of birth when ï¬rst processed by the UN Refugee Agency (UNHCR), whose staff must also estimate peopleâs age."
383,151,0.26,Elements of Risk Analysis with Applications in R,"FIGURE 3.7: Overseas monthly visits to UK January 1980 to December 2019 together with estimated trend via moving averages technique. 3. Then we use the raw seasonal factors to get the values of the corrected seasonal factors: sj = Fj â FÌ (3.11) where FÌ is the mean over all Fj , j = 1, . . . , J. Note that J = 4 for quarterly data, and J=12 for monthly data. Estimating the noise (the remainder) component. Once we have"
390,374,0.26,The Hidden Language of Computer Hardware and SW,"Chapter Eighteen With a 24-hour clock, the hour begins at 0 and goes to 23, but English-speaking countries generally use a 12-hour clock, and that introduces a problem. The hour has two digits, like seconds and minutes, but the hours donât start at zero. By convention, noon or midnight is an hour of 12, and then the next hour is 1. Letâs ignore that problem for a moment. Letâs assume that with a 12-hour clock, the hours go 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, and then back to 0, and that the time 00:00:00 is what we call midnight or noon. But thereâs still another idiosyncrasy with the hours: With seconds and minutes, clearing the low digits and clearing the high digits are independent of each other. The low digit must be cleared when it reaches 1010 (decimal 10), and the high digit must be cleared when it reaches 110 (decimal 6). With the hours, the low digit must also be cleared when it reaches 1010. Thatâs the transition from 9:59:59 to 10:00:00. But both digits must be cleared when the high digit is 1 and the low digit is 0010. Thatâs the transition from 11:59:59 to midnight or noon, which weâll be temporarily representing as 00:00:00. This means that the low and high digits of the hour must be considered together. The five flip-flops here show how the two digits can be cleared under two different conditions:"
135,498,0.26,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 4,"Algorithm 1 Å  ÃRENIE - AKTIVÃCIE(v, e, c â 0) Require: ZacÌiatocÌnÃ½ v Require: AktivacÌnÃº energiu e > 0. Require: Doteraz naakumulovanÃº energiu c na vrcholoch grafu. 1: cv â cv + e 2: e â e/S TUPE NÌ - VRCHOLU(v) 3: if e > Î¸ then for all vrcholy t takÃ©, Å¾e z v do t existuje hrana do c â Å  ÃRENIE - AKTIVÃCIE(t, e , c) end for 7: end if 8: return c"
222,326,0.259,Ecosystem Services For Well-Being in Deltas : integrated Assessment For Policy Analysis,"Month Fig. 11.2 Mean monthly temperature (Â°C) (upper) and precipitation (mm/day) (lower) over the GBM sub-region (shown in Fig. 11.1) for the period of 1981â2000 for the RCM 17 member ensemble range (shaded), three selected RCM ensemble members (Q0, Q8 and Q16), and the relevant observational datasets. Dashed lines indicate observations (Caesar et al. 2015âReproduced by permission of The Royal Society of Chemistry)"
241,315,0.259,Second Assessment of Climate Change for the Baltic Sea Basin,"Fig. 4.15 Fractional change in the frequency of very low daily minima in winter surface temperature. The fractional change in occurrence of below-10th percentile daily minimum temperature events is plotted between the winters 1964/1965â1968/1969 and 1990/1991â1994/1995. The 10th percentile is deï¬ned over the former period for each ensemble and at each grid point. In order that the existing dataset could be used, percentiles are calculated over 1961â1990. All daily values for the model were pooled together over the DecemberâFebruary (DJF) period to calculate both percentile thresholds and changes in frequency (Scaife et al. 2008)"
225,331,0.258,"Spanish Economic Growth, 1850â2015",7.987 8.092 8.199 8.309 8.443 8.611 8.798 8.990 9.176 9.360 9.362 9.318 9.254 9.110 8.968 9.002 9.048 9.163 9.308 9.461 9.619 9.786 10.004 10.244 10.424 10.459 10.500 10.567 10.645 10.782 10.947 11.109 11.319 11.184 11.059 11.245 11.437 11.635 (continued)
286,29,0.257,Corporate Data Quality : Voraussetzung Erfolgreicher GeschÃ¤ftsmodelle,"Voraussetzung fÃ¼r den Erfolg von Industrie 4.0 in einzelnen Unternehmen sowie Ã¼ber Supply Chains hinweg ist ein leistungsfÃ¤higes Datenmanagement, das folgende Anforderungen erfÃ¼llt: â¢ Beherrschung der Datenvolumina: Das Datenmanagement im Unternehmen muss in der Lage sein, die Massen an Daten zu verarbeiten und sinnvoll auszuwerten (Wrobel et al. 2014). â¢ Dezentrale Datenverarbeitung: Wenn Maschinen, BehÃ¤lter, FrachtstÃ¼cke usw. âintelligentâ werden, bedeutet dies, dass sie Datenverarbeitungsaufgaben selbstÃ¤ndig Ã¼bernehmen. Funktionen der Datenanalyse, der Datenaggregation und Datenbereitstellung finden also nicht mehr zentral in Enterprise Resource Planning (ERP)-Systemen und Data-Warehousing-Systemen statt, sondern lokal vor Ort. Ein Netzwerk von dezentralen intelligenten GerÃ¤ten ergÃ¤nzt die zentrale Datenverarbeitung der Unternehmen (Aggarwal et al. 2013). â¢ Festlegung von Datenstandards: Zeit-, Kosten- und QualitÃ¤tsvorteile durch den Einsatz cyber-physischer Systeme und der automatische Datenaustausch lassen sich nur dann realisieren, wenn sich fÃ¼r die Datenbeschreibungen und den Datenaustausch Standards etablieren. Diese Standards mÃ¼ssen mindestens innerbetrieblich, besser jedoch Ã¼ber ganze Supply Chains hinweg gelten (Otto et al. 2014). So entwickelt die MobiVoc-Initiative beispielsweise ein Datenvokabular fÃ¼r neue MobilitÃ¤tslÃ¶sungen6."
335,253,0.256,"Open Source Systems : Towards Robust Practices 13Th Ifip Wg 2.13 international Conference, Oss 2017, Buenos Aires, Argentina, May 22-23, 2017, Proceedings",Fig. 3. The release cycle of each library by boxplot. The target libraries are sorted by clustering (broken lines) from C1 to C6. The top figure shows the clustering number and the median of the release cycle days.
195,54,0.256,OdporÃºÄanie pre softvÃ©rovÃ½ch inÅ¾inierov,"Krok 7: VÃ½poÄet presnosti, pokrytia a F1-Å¡tatistiky ZÃ¡vereÄnÃ¡ ÄasÅ¥ skriptu poÄÃ­ta presnosÅ¥ (precision), pokrytie (recall) a F1-Å¡tatistiku (F-measure) pre vÅ¡etky modely a ukladÃ¡ ich do premennej results. > getPrecision <- function(x) as.numeric(unname(x$byClass[3])) > getRecall <- function(x) as.numeric(unname(x$byClass[1])) > getFmeasure <- function(x, y) 2 * ((x * y)/(x + y)) > n.row = length(pred.values.split) > results <- NULL > results <- dataFrame( + colClasses = c(Model = ""character"", Precision = ""double"","
43,337,0.256,Objektovo orientovanÃ© programovanie v C++,"int main() int volba, ceny[4]={3,20,37,52}; cout<<""Vyberte si typ listku:""<<endl; cout<<""1. 24-hodinovy""<<endl; cout<<""2. 30-dnovy""<<endl; cout<<""3. 60-dnovy""<<endl; cout<<""4. 90-dnovy""<<endl; cin>>volba; time t cas=time(NULL); tm *l cas=localtime(&cas); datum dnes(l cas->tm mday,l cas->tm mon+1,l cas->tm year+1900); cout<<""Listok plati do ""; if (volba==1) (++dnes).Vypis(); else if (volba==2) (dnes+30).Vypis(); else if (volba==3) (dnes+60).Vypis(); else if (volba==4) (dnes+90).Vypis(); cout<<""Cena: ""<<ceny[volba-1]<<"" EUR""<<endl;"
204,285,0.255,Iutam : a Short History,"Years 1948-66 1957-71 2012-14 1992-93 1979-83 1988-90, 94-95 1987-89 1989-95 1976-79 1988-00,2011-14 1969-74 1994-97 1973-74 1957-63 2009-14 1971-94 1996-97 2009-14 1981-83 1952-75 1954-62 1964-02 1974-83 1968-70 1952-53 1984-96 1967-82 1950-85 1997-01 2003-14 1948-74 1987-03 1984-93 1994-01 1990-03 2009-14 1950-51 1968-72 2013-14 1960-65 1987-13"
167,167,0.255,The Interconnected Arctic â UArctic Congress 2016,"Fig. 9.4 Seasonal interconnection between SAT and three climate indices Table 9.2 Sensitivity of three climate indices to the cases with polar air outbreaks obtained from the retrospective data (Stalnov n.d.) Month, year December, 1978 October, 1979 March, 1980 January, 1982 November, 1984 February, 1985 February, 1986 January, 2012 January, 2016"
289,714,0.254,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","The novelty of our program logic is to allow non-trivial reasoning about relaxed accesses. Unlike release/acquire accesses, relaxed accesses do not induce synchronisation between threads, so the usual approach of program logics, which relies on ownership transfer, does not apply. Therefore, in addition to reasoning about ownership transfer like a standard separation logic, our logic supports reasoning about relaxed accesses by collecting information about what reads have been observed, and in which order. When combined with information about which writes have been performed, we can deduce that certain executions are impossible. For concreteness, we consider a minimal âWHILEâ programming language with expressions, e â Expr, and statements, s â Stm, whose syntax is given in Fig. 1. Besides local register assignments, statements also include memory reads with relaxed or acquire mode, and memory writes with relaxed or release mode."
172,100,0.253,Twitter Als Basis Wissenschaftlicher Studien : Eine Bewertung GÃ¤ngiger Erhebungs- Und Analysemethoden Der Twitter-Forschung,"FÃ¼r die Speicherung von Tweets stehen grundsÃ¤tzlich alle Arten von Datenbanken zur VerfÃ¼gung. Diese untergliedern sich in mehrere Datenbanksysteme, wie unter anderem in hierarchische, relationale und NoSQL-Datenbanken, und unterscheiden sich teils grundlegend in ihrer Datenstruktur (siehe Abbildung 10)."
385,576,0.253,Advanced R,"This uses the alist() function which simply captures all its arguments. This function is the same as pryr::dots(). Pryr also provides pryr::named_dots(), which, by using deparsed expressions as default names, ensures that all arguments are named (just like data.frame())."
291,32,0.252,Visualizing Mortality Dynamics in the Lexis Diagram (Volume 44.0),"Any dynamics in vital events such as births and deaths involve change over calendar time, age, and/or cohort. The so-called Lexis diagram represents the ideal canvas to illustrate such dynamics. The Lexis Diagram as we use it today consists of a Cartesian coordinate system where calendar time (âperiodâ) is depicted on the xaxis and age on the y-axis (see Fig. 2.1 on page 6).1 We added horizontal and vertical reference lines to facilitate orientation. Birth cohorts move in such a diagram along the 45Ä± line since a person is 1 year later 1 year older. Expressed differently: The current age of a person can be calculated if we subtract the birth date from the current calendar date. We used the example of three eminent demographers of the twentieth century in Fig. 2.1 to illustrate this relationship: William Brass, Ansley Coale, and Nathan Keyfitz. To be able to follow the cohorts on the 45Ä± line, we made sure in Fig. 2.1âas well as in all other figures in this monographâthat the aspect ratio maps the length of one calendar year to exactly one age year. Of course, we are not restricted to depict individuals on the Lexis plane. The standard approach is, indeed, to use population level data. It is obvious that we can not draw lines for every individual in that case. Colors are used instead to indicate the same value for the chosen statistic. While most figures in the remaining chapters show (smoothed) age-specific mortality or its time derivative, we opted to illustrate the basic approach of Lexis surface maps by depicting the population size of the"
32,343,0.251,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","Fig. 15.1 Typical time series for Tokyo (green), Aomori (red), and Kumamoto (blue). (a) Data from April 1999 to December 2014. The numbers and bars at the top denote the year and its range, respectively. (b) The same data in 2007"
286,219,0.25,Corporate Data Quality : Voraussetzung Erfolgreicher GeschÃ¤ftsmodelle,"â¢ Automatisierte digitale DatenqualitÃ¤tsprÃ¼fungen durch GeschÃ¤ftsregeln mÃ¼ssen mit halbmanuellen, physischen PrÃ¼fungen (hier mithilfe des CubiScan-GerÃ¤ts) kombiniert werden. Die Korrektheit der Daten wird so durch PrÃ¼fung am Realweltobjekt sichergestellt. â¢ Workflows und Bereitstellungsbereiche (Staging Areas) vor der DatenÃ¼bernahme ins ERP-System verhindern die Erfassung fehlerhafter Daten und sichern die DatenqualitÃ¤t im Produktivsystem. â¢ Die Ãbertragung von bewÃ¤hrten QualitÃ¤tsmanagementsystemen wie Six Sigma auf das DatenqualitÃ¤tsmanagement erleichtert die Umsetzung und erhÃ¶ht die Akzeptanz."
204,260,0.249,Iutam : a Short History,"Years 1984-11 1969-72 1976-01 1948-63 2013-14 1952-53 1973-77 1963-72 2003-14 1958-60 1981-86 1971-72 1981-02, 04-12 1976-14 2000-01 1995-07 2013-14 2004-14 1996-01 1968-87 1977-80 1991-06 1958-65 1972-78 1950-53 1963-81 1972-75 2012-14 1948-80 1991-02 1957-71 2009-14 2009-14 1948-65 1954-83 1996-03 1978-79 1984-90 1996-06"
286,322,0.249,Corporate Data Quality : Voraussetzung Erfolgreicher GeschÃ¤ftsmodelle,"Ein MaÃ fÃ¼r die Eignung der Daten fÃ¼r bestimmte Anforderungen in GeschÃ¤ftsprozessen, in denen sie verwendet werden. DatenqualitÃ¤t ist ein mehrdimensionales, kontextabhÃ¤ngiges Konzept, das nicht mit einem einzigen Merkmal, sondern anhand verschiedener DatenqualitÃ¤tsdimensionen beschrieben und gemessen werden kann."
294,366,0.249,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"x = linspace(0, 4, 10001) y = f(x) root = None # Initialization for i in range(len(x)-1): if y[i]*y[i+1] < 0: root = x[i] - (x[i+1] - x[i])/(y[i+1] - y[i])*y[i] break # Jump out of loop if root is None: print âCould not find any root in [%g, %g]â % (x[0], x[-1]) else: print âFind (the first) root as x=%gâ % root"
159,67,0.249,"BildungsverlÃ¤ufe Von Der Einschulung Bis in Den Ersten Arbeitsmarkt : Theoretische AnsÃ¤tze, Empirische Befunde Und Beispiele",Im Folgenden werden sowohl Analysen bildungsstatistischer Daten auf der Basis von Daten pro Schuljahr als auch Analysen von Schullaufbahnen auf der Grundlage von Individualdaten anhand von Beispielen dokumentiert. Die Beispiele werden aus der Perspektive der SonderpÃ¤dagogik ausgewÃ¤hlt und stehen in Bezug zu den schulischen Selektionsmassnahmen.
286,115,0.248,Corporate Data Quality : Voraussetzung Erfolgreicher GeschÃ¤ftsmodelle,Das Feld âLoBâ darf nicht leer sein Das Feld âDatumâ muss mit dem korrekten Format befÃ¼llt sein Der Wert im Feld âPrÃ¤mieâ darf nicht negativ sein Das Datum im Feld âBooked Dateâ darf nicht mehr als 40 Tage nach dem Datum im Feld âBound Dateâ liegen (Indikator ProzessqualitÃ¤t)
286,163,0.248,Corporate Data Quality : Voraussetzung Erfolgreicher GeschÃ¤ftsmodelle,"Boschs Ordnungsrahmen fÃ¼r das Stammdatenmanagement unterscheidet zudem vier Teilfunktionen (siehe Abb. 2.18): â¢ Das FÃ¼hrungssystem (Governance) legt die Richtlinien fÃ¼r die Bewirtschaftung jeder Stammdatenklasse fest und Ã¼berwacht die Umsetzung und den Erfolg des Datenmanagements fÃ¼r diese Datenklassen. â¢ Die Datenbereitstellung (Provisioning) hat mehrere Funktionen: Sie entwirft erstens die Organisation des Datenmanagements (inkl. der Rollenzuordnung) und zweitens die Datenerfassungs- und Datenpflegeprozesse. Drittens ist sie fÃ¼r die Entwicklung eines konzeptionellen Stammdatenmodells zustÃ¤ndig und definiert viertens die Anwendungssysteme, in denen die Daten fÃ¼hrend bewirtschaftet werden. â¢ Die Datennutzung umfasst die Verwendung der Daten in den GeschÃ¤ftsprozessen. Aufgrund der KomplexitÃ¤t der Aufbauorganisation und der Vernetzung der Sparten nutzen verschiedene GeschÃ¤ftsprozesse in unterschiedlichen Sparten dieselben Daten. â¢ Die Funktion âKonzepte und Projekteâ ist fÃ¼r die Weiterentwicklung des Datenmanagements sowie die Anpassung der Daten Ã¼ber ihren Lebenszyklus hinweg verantwortlich. Da sich die GeschÃ¤ftsprozesse Ã¤ndern, wandeln sich auch die Anforderungen an die Daten. Beispielsweise wird die Handelsregisternummer im Lieferantenstamm aufgrund gesetzlicher Vorgaben von einem Kann- zu einem Muss-Feld. Die Stammdatenarchitektur (im Folgenden kurz Datenarchitektur genannt) regelt den Zugriff, die Verteilung und den Fluss der Stammdaten mit dem Ziel, hohe DatenqualitÃ¤t sicherzustellen (DAMA 2009). Sie umfasst auch ein konzeptionelles Stammdatenmodell und die Applikationslandschaft. Der Entwurf einer angemessenen Datenarchitektur war somit eine der wichtigsten Gestaltungsaufgaben bei Bosch, um die erwÃ¤hnte Konzernrichtlinie umzusetzen. Die Anforderungen an diese Aufgabe und die erwogenen GestaltungsmÃ¶glichkeiten stehen im Mittelpunkt dieser Fallstudie. Aufgrund des wachsenden Vernetzungsgrads des Unternehmens und der daraus resultierenden Vielzahl an Beziehungen zwischen Datenbereitstellung und Datennutzung stand Bosch vor einer Art âQuadratur des Kreisesâ beim Entwurf der Datenarchitekturen fÃ¼r die einzelnen Stammdatenklassen. Zwar verfolgte das Unternehmen grundsÃ¤tzlich das Ziel, seine Architekturen zu standardisieren, andererseits mussten aber spartenspezifische Anforderungen berÃ¼cksichtigt werden, die keine vollstÃ¤ndige Standardisierung erlaubten. Exemplarische Herausforderungen waren: â¢ Die Migration auf eine einzige Standardarchitektur wÃ¤re in vielen FÃ¤llen zu kostspielig und zu riskant â auch aufgrund unterschiedlicher Anwendungssysteme in den einzelnen Sparten. â¢ Der Harmonisierungsbedarf der Daten unterscheidet sich von Stammdatenklasse zu Stammdatenklasse. So ist er z. B. bei Mitarbeiterdaten vergleichsweise hoch, bei Lieferantendaten jedoch gering. â¢ Der Reifegrad im Stammdatenmanagement war Ã¼ber das Unternehmen hinweg uneinheitlich."
241,271,0.248,Second Assessment of Climate Change for the Baltic Sea Basin,"between summer (JJA) and winter (DJF) seasonal temperatures, and cyclonic circulation, and d ice cover and cyclonic circulation. 15-year averages for 1800â1815, 1811â1825, 1826â1840â¦, 1961â1975, 1976â1990, 1986â2000 (Omstedt et al. 2004)"
231,252,0.247,North Sea Region Climate Change Assessment,"(maximum temperature â¥25 Â°C) between 1981â2010 and 1951â1980 (based on E-OBS data). The change in these indices is not spatially consistent (in contrast to the increase in annual averaged temperatureânot shown). All differences are statistically signiï¬cant at the 5 % level using a one-sided Student t-test. The ï¬gure shows that the number of frost days has declined almost everywhere, with the strongest decreases found in the northern and eastern parts of the domain. The number of summer days has also increased almost everywhere, with the smallest increases in Scotland, northern England and Scandinavia and the largest in northern France."
385,403,0.247,Advanced R,"Itâs useful to remember that there are three basic ways to loop over a vector: 1. loop over the elements: for (x in xs) 2. loop over the numeric indices: for (i in seq_along(xs)) 3. loop over the names: for (nm in names(xs)) The ï¬rst form is usually not a good choice for a for loop because it leads to ineï¬cient ways of saving output. With this form itâs very natural to save the output by extending a datastructure, like in this example: xs <- runif(1e3) res <- c() for (x in xs) { # This is slow! res <- c(res, sqrt(x))"
385,495,0.247,Advanced R,"This gives a function that we can easily use with lapply(). However, if something goes wrong with the loop inside lapply(), it can be diï¬cult to tell whatâs going on. The next section will show how we can use FOs to pull back the curtain and look inside."
49,67,0.246,Artificial Intelligence and Cognitive Science IV,"in terms of Euclidean distance. The index of the winning spatial group wt , w â {0, ... , k } is appended to a time ordered list S if wt â  wt â1 . The node ignores repeating states both in training and recognition modes for the reasons explained in Section 3.3. The time ordered list of indices (a sequence of indices) S represents the training data for a Temporal Data Mining algorithm searching for frequent episodes within S. wt represents the state of the receptive field of the node in time t and S represents the recording of the transitions between the states. The Temporal Data Mining algorithm used in this work is described in [7]. It is based on the frequent episode discovery framework [13]. It searches for frequent episodes with variable length. The frequent episodes identified by N are stored in a list E of lists { E0 , ... , E Ne-1 } , where Ne is the number of the identified frequent episodes. The user determines the minimal length of the frequent episode to be stored. It is ensured that the shorter episodes are not contained in the longer episodes because it would create undesired ambiguity. Operation of N in recognition mode is divided into two consecutive stages. First, a novel input RF t is categorized into one of the spatial groups identified in the training process RF t â wt . If wt â  wt â1 , wt is appended to the list BS (the buffer stack) and the oldest item of BS is deleted. Constant length of BS is thus maintained. BS can be seen as a short term memory because it records the recent changes of states of the receptive field of the node. The length of BS is defined by the user. The elements of BS are initialized to -1 at the start of the algorithm. -1 does not appear in the stored frequent episodes therefore BS cannot be found in any of them before it is filled with valid values after start or after reset. Second, in the given time step, the node tries to find which of the frequent episodes stored in E contains BS (in direct and reverse order). The purpose is to recognize whether the sequence of the recent changes in the receptive field has been frequently observed before. The output of N in time t is a binary vector O t . The elements of O t correspond to the stored frequent episodes. If Ei contains BS in the given time step, the i-th element of O t is set to 1 otherwise it is set to 0. If Ei is shorter than BS the corresponding number of older items in BS is ignored and the matching is performed with the shortened buffer stack. There are several conditions modifying the behavior of a node in recognition mode. The node can be active (flag A = 1) or inactive (flag A = 0), with nodes initially starting with A = 1. The conditions are checked in each time step. If wt = wt â1 the counter Tidle is incremented by 1. O t will be equal to O t â1 . If Tidle exceeds a user defined timeout constant Tout, A is set to 0, and the elements of O t are set to 0. The node remains inactive until there is a significant change in its input ( wt â  wt â1 ). If that happens, the node is reset: A is"
291,91,0.246,Visualizing Mortality Dynamics in the Lexis Diagram (Volume 44.0),"9.2 Results Our data and results are displayed in five panels for each selected cause. On the first page for each cause, we show the observed (ârawâ) monthly numbers of death by calendar time and single age (adjusted for a duration of 30 days) in the upper panel. The panel below plots the fit of the model, i.e., the combined pattern of the trend and the sine and the cosine surfaces, which is equivalent to the observed counts minus the (raw) residuals; see, for example, Fig. 9.2 on page 103 for mortality from all causes combined for women. Our main interest is displayed on the second page for each cause. The top panel shows the estimated trend surface vat . In the case of seasonality of all-cause mortality among US women (Fig. 9.3), we can see that the number of deaths from that category increases with age and reaches its âhotspotâ for octogenarians before the numbers of death decline again. As the trend surface plots the seasonally-adjusted density of deaths, the lower number of deaths for nonagenarians are the consequence of less people being alive rather than a decline in the risk of dying. Even without the additional seasonal component, up to 3,500 women died at a single age during a single month. The height of âexcess mortalityâ is depicted by the amplitude in the middle panel. Higher ages correspond not only to higher mortality; the colors and the contour lines suggest that mortality differences between winter and summer also become larger at higher ages. Increasing seasonality with age has already been described by Adolphe Quetelet in 1838 and is typically also found in more contemporary populations (Feinstein 2002; McDowall 1981; Rau and Doblhammer 2003; Rau 2007). Over time we can not really discern a clear trend. It seems rather that deaths for 70-year-old women in the US are about 10% higher during the peak season and about 15% higher for 90-yearold women than on average during a year. If we multiply the seasonal estimate of a given age and calendar time (e.g., 1.1) with the corresponding square of the trend surface (e.g., 1,500 deaths), we obtain the fitted value (e.g., 1,650 deaths) shown in the lower panel on the previous page. When the peak season occurs in a year is illustrated in the lower panel. The colors indicate a value slightly below 30. Hence, deaths occur most often in the end of January, regardless of age or calendar year. The corresponding plots for men are depicted in Figs. 9.4 and 9.5. While male mortality is higher than female mortality at any ageâat least in highly developed countries, the seasonal characteristics are rather similar between the two sexes: The proportion of excess deaths during winter varies between 5% at age 50 and 15% at age 90 with no apparent period effect. Also the part of the year when deaths peak among men occurs at the end of January. Those seasonal mortality similarities between women and men are not only present for all-cause mortality but also for most causes of death. That is why we restricted ourselves to show only the results for women but they apply equally to men. We show the results for men only in the case of motor vehicle accident because much less women die of that cause. The largest subcategory analyzed by us in this chapter is death from heart diseases (see Figs. 9.6 and 9.7, pp. 107â108). Up to 1,300 deaths were recorded at a single age during a single month of a given year. As we can infer from the"
231,555,0.245,North Sea Region Climate Change Assessment,"characterised by more intense wind extremes to the south of 58Â°N, ranging between 0.2 and 0.4 msâ1 over most of the area at the end of the 21st century. The differences between the two realisations over the North Sea are also revealed in the time series of the change in the intensity of wind extremes at different locations in the North Sea for the four different scenario simulations (Fig. 5.6). At the central North Sea location two of the realisations (A1B_2 and B1_2) simulated weaker wind extremes during the entire 21st century, while at the two locations in the German Bight this tendency is only apparent during the ï¬rst half of the 21st century. The other two realisations (A1B_1 and B1_1), on the other hand, simulated stronger wind extremes during the course of the 21st century. The time series also illustrate the marked internal variability at multi-decadal time scales, making it difï¬cult to identify systematic differences between the SRES A1B and B1 scenario simulations at these locations. For individual 30-year periods, however, marked differences between the A1B and B1 scenarios can occur, i.e., the two realisations A1B_1 and B1_1 at the end of the 21st century."
32,378,0.245,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","Fig. 16.6 The cumulative distribution functions of lifetimes of the series that have run in Weekly ShNonen Jump, Weekly ShNonen Magazine, and Weekly ShNonen Sunday. The lists were obtained from the Japanese pages of [28â30] in December 2009. Currently running series are not included in the statistics"
241,549,0.245,Second Assessment of Climate Change for the Baltic Sea Basin,"9.3.2.2 Changes in Seasonal Variability The long-term changes in Baltic Sea sea level have been also found to depend on the season. Sea level displays an annual cycle and is generally higher during winter months and lower during spring (as demonstrated by HÃ¼nicke and Zorita 2008, see Fig. 9.10). Changes in the annual sea level cycle were studied by Ekman and Stigebrandt (1990), Ekman (1999) for the Stockholm sea level record (covering the ninetieth and twentieth centuries). The authors associated the increase in the amplitude of the dominant annual component to changes in winter wind condition. HÃ¼nicke and Zorita (2008) statistically analysed 30 sea level records from PSMSL (covering the twentieth century) and four long historic sea level records (covering the ninetieth and twentieth centuries, including Stockholm), together with climatic datasets, to investigate centennial trends in the amplitude of the annual cycle. They detected a statistically signiï¬cant increase in amplitude (winter-spring sea level) in the annual cycle in almost all sea level records investigated, but of a weaker sign than the decadal variations in the annual cycle. As the magnitude of the trends appeared almost uniform, with the exception of the Skagerrak area, they concluded that the driving mechanism for the trends in the annual cycle is very likely to be of non-regional origin. Testing several hypothesised factors to explain these centennial trends (wind through the SLP ï¬eld, the barometric effect, temperature and precipitation), only precipitation remained a plausible candidate in their analysis. The physical mechanisms responsible for the long-term trend due to changes in precipitation could be related to salinity changes and their effect on changes on water density. This remains to be resolved. Plag and Tsimplis (1999) investigated the spatial and temporal variability in the seasonal cycle of sea level for PSMSL tide gauge records in the North Sea and Baltic Sea."
32,43,0.245,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","between past and future events making causal relationships difficult to establish, so an alternative is proposed that addresses these issues. I define a modified form of TkY!X by first redefining the stochastic time series in order to capture the continuous nature of the price arrival process. With t and t0 2 R > 0 where 0 is taken as the start of trading on any given trading day and fti g and ftj0 g are the finite sequence of times at which the (log) price changes for two different equities during that day. Define the arrival indices of time series of length I and J as fi  Ig 2 N and f j  Jg 2 N. Now there are two finite sequences of price changes on a single trading day d: fX d .ti /g and fY d .tj0 /g. The entropy of fX d .ti /g conditioned on its most recent past value is: H.X d .ti /jX d .ti 1 // D"
294,47,0.244,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"and ending with stop. The expression linspace(0, 1, 1001) creates 1001 coordinates between 0 and 1 (including both 0 and 1). The mathematically inclined reader will notice that 1001 coordinates correspond to 1000 equal-sized intervals in Å0; 1Â and that the coordinates are then given by ti D i=1000 (i D 0; 1; : : : ; 1000). The value returned from linspace (being stored in t) is an array, i.e., a collection of numbers. When we start computing with this collection of numbers in the arithmetic expression v0*t - 0.5*g*t**2, the expression is calculated for every number in t (i.e., every ti for i D 0; 1; : : : ; 1000), yielding a similar collection of 1001 numbers in the result y. That is, y is also an array. This technique of computing all numbers âin one chunkâ is referred to as vectorization. When it can be used, it is very handy, since both the amount of code and computation time is reduced compared to writing a corresponding for or while loop (Chapter 2) for doing the same thing. The plotting commands are simple: 1. plot(t, y) means plotting all the y coordinates versus all the t coordinates 2. xlabel(ât (s)â) places the text t (s) on the x axis 3. ylabel(ây (m)â) places the text y (m) on the y axis At this stage, you are strongly encouraged to do Exercise 1.4. It builds on the example above, but is much simpler both with respect to the mathematics and the amount of numbers involved."
172,110,0.244,Twitter Als Basis Wissenschaftlicher Studien : Eine Bewertung GÃ¤ngiger Erhebungs- Und Analysemethoden Der Twitter-Forschung,"Beide Arten der Datensicherung haben Vor- und Nachteile. Die Speicherung in Einzeldateien ist einfach auszufÃ¼hren und bedarf keiner speziellen System-Konfiguration. Die erzeugten Dateien sind direkt einlesbar und kÃ¶nnen ohne Umwandlung weiterverarbeitet oder weitergegeben werden. Es bedarf keiner Installation weiterer Software. Jedoch wird die Dateistruktur gerade beim Sammeln groÃer Datenmengen schnell unÃ¼bersichtlich, da groÃe Datenmengen systembedingt auf mehrere Dateien verteilt werden mÃ¼ssen. FÃ¼r eine vollstÃ¤ndige Analyse mÃ¼ssten diese Datenfragmente vorher wieder zusammengefasst werden. Des Weiteren besteht ein hohes Risiko des Datenverlustes: Es sind regelmÃ¤Ãige Backups der Dateien nÃ¶tig, welche aber nur bei bereits vollstÃ¤ndig beschriebenen Dateien mÃ¶glich sind. Sollte ein Systemfehler beim Schreibprozess auftreten, ist die momentan beschriebene Datei unter UmstÃ¤nden verloren. Somit eignet sich das Speichern in"
80,29,0.244,Innovations in Quantitative Risk Management (Volume 99.0),"We point out that the order of time/confidence/horizon arguments in the VaR and ES definitions is different in the Stochastic Holding Period case. This is to stress the different setting with respect to the fixed holding period case. We have immediately the following: Proposition 2 (VaR and ES for SHP independent of returns in deterministic calendar time) Assume that H is independent of the log returns of X in deterministic calendar time. Using the tower property of conditional expectation it is immediate to prove that such a case VaR H,t,c obeys the following equation:"
172,22,0.243,Twitter Als Basis Wissenschaftlicher Studien : Eine Bewertung GÃ¤ngiger Erhebungs- Und Analysemethoden Der Twitter-Forschung,"Streaming API (gesampelte) Daten in Echtzeit ab. Neben diesen beiden populÃ¤ren, da kostenlosen, Methoden der Datenerhebung auf Twitter, gibt es noch eine Vielzahl von Drittanbietern, die Twitter-Daten gebÃ¼hrenpflichtig oder gratis zur VerfÃ¼gung stellen. Conover et al. (2011) erhielten Zugriff auf die sogenannte Gardenhose4 wogegen Wang et al. (2012) Daten vom Dienstleister Gnip kauften und Dusch et al. (2015) sowie Larsson und Moe (2012) Online-Dienste zur Datenerfassung nutzen. Hinsichtlich der Auswertung von Twitter-Daten ergibt sich ein Ã¤hnlich differenziertes Bild: Twitter-bezogene Studien fokussieren sich nicht nur auf reine Inhaltsanalysen, sondern beziehen sich (zusÃ¤tzlich) auch auf Befragungen oder Experimente. Dennoch ist die Inhaltsanalyse nach einer Meta-Studie von Williams, Terras und Warwick (2013) eine dominierende Methode, was sich auch auf das umfangreiche Datenangebot durch Twitter zurÃ¼ckfÃ¼hren lÃ¤sst. Die Nutzung der bereits vorhandenen, leicht zugÃ¤nglichen, stark strukturierten und ausfÃ¼hrlichen Daten ist einfacher und schneller als die DurchfÃ¼hrung von Befragungen oder Experimenten. Interviews werden hÃ¤ufig nur ergÃ¤nzend durchgefÃ¼hrt, etwa, um Ergebnisse der Inhaltsanalyse durch ermittelte Einstellungen und Verhalten der Nutzer zu erklÃ¤ren. Dennoch fehlen methodische Standards, da die Twitter-Forschung noch sehr jung ist (Bruns & Liang, 2012). Ein weiterer Forschungsschwerpunkt liegt deshalb in der Konzeption neuer Methoden und Algorithmen zur Analyse der Daten (Williams et al., 2013). Einige Forschende entwickeln fÃ¼r ihre Forschungszwecke eigene AnsÃ¤tze beziehungsweise Programme zur Twitter-Analyse. Das eigentliche methodische Vorgehen, insbesondere die Datengewinnung, wird dabei selten detailliert prÃ¤sentiert (Weller, 2014). Trotz der hier dargestellten groÃen Bandbreite an AnsÃ¤tzen und Verfahren der Twitter-Analyse, gibt es kaum wissenschaftliche Arbeiten, die sich mit den Methoden der Datengewinnung und Auswertung befassen. Wenn Ã¼berhaupt, wurden nur einzelne Vorgehensweisen angesprochen. So zeigen Perera, Anand, Subbalakshmi und Chandramouli (2010), wie mit der Programmiersprache Python Twitter-Daten gesammelt und in einem MySQLDatensystem verarbeitet werden kÃ¶nnen. Tugores und Colet (2013) vergleichen fÃ¼r eine MobilitÃ¤tsanalyse zwei Varianten von Datenbanksystemen (SQL und noSQL) im Kontext einer Twitter-Analyse mit Python. Bruns und Liang (2012) prÃ¤sentierten mehrere Programme zum Erfassen und Analysieren von Tweets"
217,839,0.243,Finite Difference Computing With Pdes : a Modern Software Approach,"It means that we in the first equation, involving un0 , insert unNx , and that we in the last equation, involving unC1 Nx insert u0 . Normally, we can do this in the simple way that u_1[0] is updated as u_1[Nx] at the beginning of a new time level. In some schemes we may need unNx C1 and un1 . Periodicity then means that these values are equal to un1 and unNx 1 , respectively. For the upwind scheme, it is sufficient to set u_1[0]=u_1[Nx] at a new time level before computing u[1]. This ensures that u[1] becomes right and at the next time level u[0] at the current time level is correctly updated. For the Leapfrog scheme we must update u[0] and u[Nx] using the scheme: if periodic_bc: i = 0 u[i] = u_2[i] - C*(u_1[i+1] - u_1[Nx-1]) for i in range(1, Nx): u[i] = u_2[i] - C*(u_1[i+1] - u_1[i-1]) if periodic_bc: u[Nx] = u[0]"
385,416,0.243,Advanced R,"Note that the order of arguments is a little diï¬erent: function is the ï¬rst argument for Map() and the second for lapply(). This is equivalent to: stopifnot(length(xs) == length(ws)) out <- vector(""list"", length(xs)) for (i in seq_along(xs)) { out[[i]] <- weighted.mean(xs[[i]], ws[[i]])"
291,98,0.243,Visualizing Mortality Dynamics in the Lexis Diagram (Volume 44.0),"Fig. 9.10 Seasonality of mortality from cerebrovascular diseases in the United States, 1959â2014, men, raw counts (adjusted for length of month) and fitted model (Data source: Human Mortality Database)"
228,366,0.243,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"References 1. Adya, M.: Corrections to rule-based forecasting: findings from a replication. Int. J. Forecast. 16(1), 125â127 (2000). http://www.sciencedirect.com/science/article/pii/ S0169207099000345 2. Adya, M., Armstrong, J.S., Collopy, F., Kennedy, M.: An application of rule-based forecasting to a situation lacking domain knowledge. Int. J. Forecast. 16(4), 477â484 (2000) 3. Adya, M., Collopy, F., Armstrong, J.S., Kennedy, M.: Automatic identification of time series features for rule-based forecasting. Int. J. Forecast. 17(2), 143â157 (2001)"
101,321,0.242,Finite Difference Computing with Exponential Decay Models (Volume 110.0),"Suppose ui .t/ is stored in an array u. The mean and the standard deviation of the fortune is most efficiently computed by using two accumulation arrays, sum_u and sum_u2, and performing sum_u += u and sum_u2 += u**2 after every experiment. This technique avoids storing all the ui .t/ time series for computing the statistics. Filename: random_interest. Exercise 4.20: Simulate a population in a changing environment We shall study a population modeled by (4.3) where the environment, represented by r and f , undergoes changes with time. a) Assume that there is a sudden drop (increase) in the birth (death) rate at time t D tr , because of limited nutrition or food supply: t < tr ; r.t/ D %  A; t  tr : This drop in population growth is compensated by a sudden net immigration at time tf > tr : 0; t < tf ; f .t/ D f0 ; t  ta :"
228,358,0.242,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"11.2 Application of OFN Notation for the Fuzzy Observation of NASDAQ Composite Data from November this year for the NASDAQ Composite are presented in Table 11.1. Quotations are given in a widely used format for this type of time series. Subsequent letters of the alphabet represent values for consecutive trading days. Figure 11.2 shows an OHLC (open, high, low, close) chart of the Nasdaq Composite index for one month. The graph shows the following attributes for each of the daily quotations: opening, closing, highest, and lowest value. These attributes, along with the change parameter are shown in Table 11.1. In addition, decrease in quotation is"
286,111,0.242,Corporate Data Quality : Voraussetzung Erfolgreicher GeschÃ¤ftsmodelle,"Das Projekt wurde in drei Phasen zwischen 2011 und 2013 umgesetzt. Die erste Phase umfasste die Definition und Zuordnung von DatenqualitÃ¤tsrollen zu Mitarbeitern sowie die Definition eines unternehmensweiten VerstÃ¤ndnisses des DatenqualitÃ¤tsbegriffs. Die zweite Phase analysierte die Ist-Situation auf Seiten der âDatenproduzentenâ, um zu verstehen, wo Daten in welchen Systemen in welchen Prozessschritten erzeugt werden und was im Anschluss daran mit ihnen geschieht. In der dritten Phase legte das Data-Governance-Team ein Regelwerk fÃ¼r die DatenqualitÃ¤t fest, an das sich die Nutzer der Daten zu halten hatten."
175,723,0.242,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","monthly and cross-correlations. Because the model implicitly includes a constant, this means one needs k* = 13n years of data to obtain a unique solution for this critical equation. For n = 3, k* = 39. One could say that with a record length of 40 years, there would be only 1 degree of freedom left in the residual model error variance described by B. That would be unsatisfactory. When flows at many sites or in many seasons are required, the size of the disaggregation model can be reduced by disaggregation of the flows in stages and not attempting to explicitly reproduce every season-to-season correlation by constructing what have been called condensed and contemporaneous models (Lane 1979; Stedinger and Vogel 1984; Gryier and Stedinger 1988; Koutsoyiannis and Manetas 1996). Condensed models do not attempt to reproduce the cross-correlations among all the flow variates at the same site within a year (Lane 1979; Stedinger et al. 1985), whereas contemporaneous models are like the Markov model developed earlier in Sect. 8.5 and are essentially models developed for individual sites whose innovation vectors Vy have the needed cross-correlations to reproduce the cross-correlations of the concurrent flows (Camacho et al. 1985), as was done in Eq. 6.177. Grygier and Stedinger (1991) describe how this can be done for a condensed disaggregation model without generating inconsistencies."
316,105,0.241,Executing Magic in the Modern Era,"HANGMANâS DAY In nineteenth-century America, there was a widespread belief that Friday was an unlucky time to start a new piece of work or to embark on any ventures, because executions regularly took place on that day. In sympathetic association with the fate of the criminal on the gallows, any work begun would never be finished. Friday is obviously widely associated with the crucifixion of Christ, and the notion of it being unlucky was quite widespread. In England, there were folk beliefs (unrelated to executions) about it being ill-starred to be born on a Friday or to turn a mattress on that day, for example. There was a strong belief amongst fisherman that it was an unlucky day to set out for a catch.2 However, the notion of Hangmanâs Friday is distinctly American, due to numerous states making Friday the customary time of the week for executions. In colonial eighteenth-century America, the English Murder Act of 1752 dictated that all convicted murderers had to be hanged within 48 h of sentencing, though Sundays had to be avoided. In the early Republic, Thomas Jefferson proposed, likewise, that the condemned be executed on the second day following conviction.3 So how the Friday tradition became so engrained in American capital punishment custom is intriguing."
390,579,0.241,The Hidden Language of Computer Hardware and SW,"In modern terminology, these cycles are often called loops. What she calls a cycle of a cycle is now called a nested loop. The CPU that Iâve been building over the past several chapters seems deficient in that regard. At the end of the previous chapter, I showed you a"
259,254,0.241,The Little Book of Semaphores,"shared = Shared () fs = [ consume ]*2 + [ produce ]*2 threads = [ Thread ( loop , shared , f ) for f in fs ] for thread in threads : thread . join () The capacity is 10 cokes, and that the machine is initially half full. So the shared variable cokes is 5. The program creates 4 threads, two producers and two consumers. They both run loop, but producers invoke produce and consumers invoke consume. These functions make unsynchronized access to a shared variable, which is a no-no. Each time through the loop, producers and consumers sleep for a duration chosen from an exponential distribution with mean mu. Since there are two producers and two consumers, two cokes get added to the machine per second, on average, and two get removed. So on average the number of cokes is constant, but in the short run in can vary quite widely. If you run the program for a while, you will probably see the value of cokes dip below zero, or climb above 10. Of course, neither of these should happen."
167,164,0.24,The Interconnected Arctic â UArctic Congress 2016,"process is cumulative, and as a consequence the year 2012 had the lowest ice extent since regular satellite observations started in 1979. In June the situation is opposite, and currently the strength and number of polar air outbreaks of the first type is increasing. It should be pointed out that in summer the second type of polar air outbreaks vanishes in terms of lower than normal temperatures because of the high influence of solar radiation."
175,345,0.24,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","Assuming the inflows that were used to derive this policy actually occurred each year, we can simulate the derived sequential steady-state policy to ï¬nd the storage volumes and releases that would occur in each period, year after year, once a repetitive steady-state condition were reached. This is done in Table 4.21 for an arbitrary initial storage volume of 20 in season t = 1. You can try other initial conditions to verify that it requires only 2 years at most to reach a repetitive steady-state policy. As shown in Table 4.21, if the inflows were repetitive and the optimal policy was followed, the initial storage volumes and releases would"
381,325,0.239,The Dynamics of Opportunity in America : Evidence and Perspectives,"Near-Term Wage Issues This review of wage and compensation trends would be incomplete without a look at a wage issue that has been generating intense interest in the near-term economy and presents a good example of the role of economic slack in nominal wage trends. Though as of this writing the current economic expansion is over five years old, wage growth, not accounting for inflation, has been flat at around 2 % and unresponsive to what tightening has occurred in the labor market. This persistent lack of responsiveness of wage trends to growth has caught the attention of the Federal Reserve as well as the broader media.8 Because, until recently, consumer prices have also been growing around 2 %, the media have often framed the issue of stagnant real earnings as the recoveryâs missing ingredient. In order to be careful not to âcherry pickâ any one wage or compensation series to examine this dynamic, Figure 6.6 plots the first principal component of five different wage and compensation series.9 This technique is commonly used to summarize numerous data series in a way that pulls out their common signal, in this case, yearly changes in nominal growth since the early 1980s. The five series are: â¢ Employment cost index: hourly compensation See Janet Yellen 2014: http://www.federalreserve.gov/newsevents/speech/yellen20140822a.htm and, for a media account, Leonhardt 2015: http://www.nytimes.com/2015/01/18/upshot/drivingthe-obama-tax-plan-the-great-wage-slowdown.html?abt=0002&abg=1. By âcherry picking,â I mean that given these âhigh frequencyâ quarterly data, analysts can sometimes find one series that makes their particular case as far as whether wage growth is speeding up, slowing down, or neutral. I wanted to avoid that possibility, so I combined these quarterly series."
166,323,0.239,Surveying Human Vulnerabilities across the Life Course (Volume 3.0),6 Is It Gold or Just Glittering? So far we have made the case for implementing LHCs in research on vulnerability. However all that glitters is not gold and the LHC method has also a number of drawbacks that researchers must consider. Time and costs are problems of the LHC technique (Belli 1998) especially when the calendar has open answers that must be manually entered into a database and eventually coded. However the use of new technologies like tablets may overcome part of these problems and drastically reduce the costs (e.g. Glasner 2011). More problematic are the administration time and the relative difficulty of the task. Both can easily increase for complex and atypical trajectories but vulnerable populations are the most likely to have complex atypical trajectories that do not follow normative transitions. Thus difficulties in filling LHCs could add to other problematic aspects of surveying vulnerable populations in particular those with low personal resources such as low socio-economic status or low cultural capital. Though the concept of life history calendars is becoming more popular thanks to its implantation on recreational platforms (e.g. Facebook Timelines) filling a calendarbased questionnaire is a cognitively demanding operation. On the one side we showed that LHCs are suitable to study vulnerability processes from different perspectives considering vulnerability as the interrelation between diffusion accumulation and interpretation processes. By allowing for integrated analytical approaches the LHC can stimulate researchers towards complex and multifaceted understandings of vulnerability interconnecting subjective evaluations with factual episodes. On the other side specific segments of the society especially those at the margin that may be more at risk of experiencing prolonged vulnerability may encounter more difficulties in filling calendar-based questionnaires. Thus researchers should take extreme care in designing questionnaires for potentially vulnerable populations.
101,58,0.238,Finite Difference Computing with Exponential Decay Models (Volume 110.0),"1.2.1 Computer Language: Python Any programming language can be used to generate the unC1 values from the formula above. However, in this document we shall mainly make use of Python. There are several good reasons for this choice:  Python has a very clean, readable syntax (often known as âexecutable pseudocodeâ).  Python code is very similar to MATLAB code (and MATLAB has a particularly widespread use for scientific computing).  Python is a full-fledged, very powerful programming language.  Python is similar to C++, but is much simpler to work with and results in more reliable code.  Python has a rich set of modules for scientific computing, and its popularity in scientific computing is rapidly growing.  Python was made for being combined with compiled languages (C, C++, Fortran), so that existing numerical software can be reused, and thereby easing high computational performance with new implementations.  Python has extensive support for administrative tasks needed when doing largescale computational investigations.  Python has extensive support for graphics (visualization, user interfaces, web applications). Learning Python is easy. Many newcomers to the language will probably learn enough from the forthcoming examples to perform their own computer experiments. The examples start with simple Python code and gradually make use of more powerful constructs as we proceed. Unless it is inconvenient for the problem at hand, our Python code is made as close as possible to MATLAB code for easy transition between the two languages. The coming programming examples assumes familiarity with variables, for loops, lists, arrays, functions, positional arguments, and keyword (named) arguments. A background in basic MATLAB programming is often enough to understand Python examples. Readers who feel the Python examples are too hard to follow will benefit from reading a tutorial, e.g.,"
289,1453,0.238,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","is important, since we are interested in whether b occurs infinitely often. We thus duplicate f , and replace the program with the following program Pdup : ({fb = if c then (event a; fa ) else (event b; fb ), fa = if c then (event a; fa ) else (event b; fb )}, fb ). For checking InfTraces(P0 ) â© L = â, it is now sufficient to check that fb is recursively called infinitely often. We can thus obtain the following HES: ((fb =Î½ (c â afa ) â§ (Â¬c â bfb );"
383,269,0.238,Elements of Risk Analysis with Applications in R,"Next, we will do simple exponential smoothing again, for Central England temperature data again. However, this time we will find and use the optimal value of Î±. We find such optimal value, and for that, we will use the function ses. # R Code # -----------------------------------------------------------3 # Goal : Input the Central England Temperature Data , plot it and then perform SES with optimal alpha . 4 # -----------------------------------------------------------5 # 6 # Next use the function ses from the library forecast . 7 # The function ses is able to choose the most optimal alpha . 8 # What we do is we do not tell ses which alpha to use , and that way 9 # the function ses knows that the optimal alpha needs to be found first . 10 fit . ses . Y <- ses ( Y ) 11 # plot the model and fitted values ( Yhat ) 12 plot ( Time ,Y , ylab = "" Central England temperatures [ Celsia ] "" , xlab"
286,323,0.237,Corporate Data Quality : Voraussetzung Erfolgreicher GeschÃ¤ftsmodelle,"Das angestrebte Level der DatenqualitÃ¤t richtet sich somit nach den Anforderungen in den GeschÃ¤ftsprozessen und -funktionen, die diese Daten nutzen, z. B. dem Einkauf, Vertrieb oder dem Berichtswesen. Eine niedrige DatenqualitÃ¤t schmÃ¤lert den Wert der DatengÃ¼ter im Unternehmen, weil ihre Nutzbarkeit gering ist. Unternehmen sind bestrebt, mit dem DQM eine von der GeschÃ¤ftsstrategie geforderte DatenqualitÃ¤t zu erreichen. DatenqualitÃ¤tsdimen- Die wichtigsten Dimensionen, anhand derer DatenqualitÃ¤t beurteilt sionen werden kann, sind:"
217,1061,0.237,Finite Difference Computing With Pdes : a Modern Software Approach,"# Convergence rates r_R_I = convergence_rates(dt, R_I) print âR integrated in time; r:â, print â â.join([â%.1fâ % r for r in r_R_I]) R = np.array(R) # two-dim. numpy array r_R = [convergence_rates(dt, R[:,n])[-1] for n in range(len(t_coarse))]"
365,157,0.237,Climate Smart Agriculture : Building Resilience To Climate Change,"flow). In order to keep the equation as simple as possible, yet robust, the regression is based on one variable and tested in two basins of very different climatologyâs, topographyâs, land use patterns and annual water supply cycles. An important consideration between the gauge and BWI values is a lagged relationship between water accumulating near the surface and detected downstream at the gauge. The lag between the water input upstream and the detection of changes in flow downtstream is based on numerous empirical observations and theory that flow models are more accurate when they include the prior month(s) due to the time lapse for the water accumulate into the major stem of the river (Demirel et al. 2013). The number of prior months used in the predictions of flows is directly related to the size of the basin, the influence of snow melt and its topography. Therefore, a lagged term is included in Equation 2, where Qm(BWI) is the discharge at a station for month m While n is the number of previous month(s) averaged together with the concurrent month BWI value. Qm ( BWI ) = g ( d )"
231,267,0.237,North Sea Region Climate Change Assessment,"Care should be taken if the precipitation fraction exceeding the 95th percentile (R95pTOT) is determined over a climatological period of several decades, since extremes may have increased disproportionally and thus the shape of the distribution may have changed. For example, an index S95pTOT, using the Weibull shape parameter instead of an explicit estimate of the 95th percentile, can be used (Leander et al. 2014). Northern Europe shows a (signiï¬cant) increase in R95pTOT, but this is far less pronounced for S95pTOT. Since R95pTOT cannot distinguish between a shift in the median of the probability distribution for precipitation and a change in only the tail of the distribution, trends are generally âmore negativeâ for S95pTOT, especially over southern Scandinavia, the Netherlands, Germany and the UK. Zolina et al. (2009) introduced a new index for R95pTOT, making use of a gamma distribution for wet day precipitation amounts and the associated theoretical distribution of the fractional contribution of the wettest days to the seasonal or annual total. The trend results for their new index are similar to R95pTOT. Another way of analysing changes in precipitation is by counting the number of wet days. An example of this is the index CWD (maximum number of consecutive wet days, here deï¬ned as the number of days with precipitation â¥1 mm). Trends in the station records for the period 1951â 2012 are shown in Fig. 2.16, which indicates that most of the stations in the North Sea region show a slight increasing"
320,497,0.237,Managing Protected Areas in Central and Eastern Europe Under Climate Change,"dataset and the HABIT-CHANGE-database 2012). Even though models predict a slight increase in precipitation (about 20 mm/day), the number of dry days (<1 mm/day) per vegetation season is expected to increase by 4â5 days. When combining these models the predicted beginning of the vegetation period will shift from 22nd February (1971â2000) to 27th January (2036â2065). Additionally, the predicted water balance, calculated as the mean precipitation minus the potential evapotranspiration, is expected to decrease slightly in autumn. Global-warming-related effects on biodiversity can be mitigated in the short term by designing an appropriate nature reserve (Botkin et al. 2007). However, the challenges of biodiversity loss are daunting, since biodiversity is decreasing even in protected areas. For this reason, alien invasive species found in the Mediterranean, even if not highly invasive, like A. squamatus, might nevertheless potentially become so. In Fig. 19.3 we can recognise the distribution of A. squamatus in areas adjacent to both protected areas of SecÌovlje Salina and SÌkocjanski Inlet. As previously noted, in some years like in 2008 the abundance of A. squamatus increased (GlasnovicÌ and FisÌer PecÌnikar 2010). Our analysis of temperature showed a highly increased mean annual temperature the previous year 2007 (Fig. 19.4), which resulted in a prolonged vegetation period and must have resulted in high seed production as well. To prevent a potential invasion by A. squamatus plants should be removed before reaching the height of 110 cm to prevent or at least minimise seed production, since smaller plants have more than five times fewer flower heads (Fig. 19.2), even though the number of seeds in a single flower head is more or less the same. Calculation show that plants smaller than 110 cm could bear from about 700 to 8,700 seeds, while plants taller than 110 cm produce at least five times more seeds from about 47,000 to 70,000 seeds. Obviously, it is necessary to constrain eventual accumulation of seeds in the soil seed bank and to prevent plants from forming a reservoir for future invasions if appropriate conditions might recur."
383,272,0.237,Elements of Risk Analysis with Applications in R,"In what follows, there are further R Lab questions for you to work on. Solutions are not provided here. They are provided upon request. 4. [Purpose: to practice forecasting.] You are asked to do forecasting for Central England Temperature data. Do all analyses in R. (a) Using R, and suitable criteria compare SES and Holtâs models. (b) For the model that is better, do the goodness-of-fit analysis. (c) Do the forecast for 6-time points ahead. Do you trust your forecasts? Explain. 5. [Purpose: to practice forecasting.] You are asked to analyse securities yield time series data (Risk-2021-Yield-Data.csv). Do all analyses in R. (a) Use R to model the data in three ways: simple exponential smoothing, Holts, and Holt-Winters. Which is the best in terms of RMSE? (b) Generate the autocorrelation (correlogram) for the Yield time series. (c) Generate the autocorrelation (correlogram) for the errors of each of the three models. (d) Look at the plotted autocorrelation and judge if there is evidence against the model. Based on autocorrelation which model is the best? Based on autocorrelation can we trust the predictions of the model? (e) Fit a Holt-Winters model to the data. (f) Generate a one-step-ahead prediction and prediction intervals for that prediction. (g) State your assumptions upon which the interval is based. (h) If the assumptions are not satisfied, can we trust the prediction intervals?"
291,96,0.237,Visualizing Mortality Dynamics in the Lexis Diagram (Volume 44.0),"Fig. 9.8 Seasonality of mortality from cerebrovascular diseases in the United States, 1959â2014, women, raw counts (adjusted for length of month) and fitted model (Data source: Human Mortality Database)"
77,323,0.236,A Life Course Perspective on Health Trajectories and Transitions (Volume 4.0),"Here, health improves (by 1 unit) as an individual gets older (by 1 year), improves for everyone as time passes, and is better amongst people who were born later. Substituting Period with Age C Cohort, we get Health D .2  Age/ C .2  Cohort/"
383,228,0.235,Elements of Risk Analysis with Applications in R,"In risk analysis, time series methods can be used to analyse historical data on various risk factors, such as market trends, economic indicators, or weather patterns. This information can be used to (A) develop predictive models that estimate the likelihood of future events or (B) identify risk factors, which can be used to inform risk management strategies. Time series have a vast number of resources. In our chapter, we used several resources, which we list here: 1. For further understanding of applied time series and R, we recommend this online book on time series: Rob J Hyndman and George Athanasopoulosâs book âForecasting: Principles and Practiceâ, available at https://otexts.com/fpp2/ The book presents the statistical theory on basic to intermediate level. 2. For further understanding of applied time series and more examples in R, we recommend the book âTime Series Analysis and Its Applications. With R Examples. Fourth Edition.â by Shumway and Stoffer [52]. The book presents a statistical theory on an intermediate to advanced level. It has Chapter 2 on Time Series Regression and Exploratory Data Analysis, where they introduce multiple linear regression and then give attention to more topics: exploratory data analysis for preprocessing of nonstationary time series (for example, trend removal), the concept of differencing and the backshift operator, variance stabilisation, as well as nonparametric smoothing of time series. 3. For a quick way to start with R and then run time series analysis in R, we recommend this online book by Avril Coghlan [15]. 4. We focused on the frequentist approach of using time series analysis for risk and uncertainty estimation. There are Bayesian extensions which are also popular and more flexible than the frequentistsâ methods. They are, however, more advanced and outside the scope of this chapter. For an interested reader, we recommend [52]. 5. To learn about how to identify risk factors, we recommend the book by Hanck, Arnold, Gerber and Schmelzer [29]. It is accessible at https://www.econometrics-with-r.org/14-ittsraf.html, and it shows the R code as well. It has Chapter 14 devoted to time series, as well as further time series topics such as estimation of dynamic causal effects, vector autoregression, and cointegration. 6. To get a wider knowledge of the visualisation of time series, we recommend Shurkhovetskyy et al. (2017) âData Abstraction for Visualizing Large Time Seriesâ [53]."
311,2112,0.235,The Physics of the B Factories,Yield (events) â3.9 Â± 1.6 Â± 1.7 â0.2 Â± 2.8 Â± 0.9 â2.9 Â± 3.4 Â± 2.4 3.6 Â± 4.3 Â± 1.3 8 Â± 34 Â± 8 20 Â± 15 Â± 4 â3 Â± 11 Â± 3 9.3 Â± 7.3 Â± 2.8 â3.7 Â± 2.9 Â± 3.3 â1.3 Â± 2.8 Â± 1.1 â4.3 Â± 1.8 Â± 0.6 3.2 Â± 3.8 Â± 1.2 â5.7 Â± 5.8 Â± 2.0 4.8 Â± 5.9 Â± 1.2 9.1 Â± 6.0 Â± 2.8 3.4 Â± 6.4 Â± 3.5 4.0 Â± 6.5 Â± 2.8 11.1 Â± 5.0 Â± 2.5 â0.7 Â± 2.9 Â± 0.9 6.2 Â± 4.6 Â± 1.8 4.7 Â± 4.7 Â± 0.5 â3.1 Â± 1.2 Â± 0.5 â5.1 Â± 4.2 Â± 2.0 â5.7 Â± 14. Â± 3.4 0.6 Â± 5.1 Â± 2.7 â0.2 Â± 7.9 Â± 0.6 â2.8 Â± 2.4 Â± 0.2 7.2 Â± 5.4 Â± 1.6 â11.6 Â± 4.0 Â± 3.1 2.3 Â± 7.9 Â± 3.3 â2.3 Â± 5.0 Â± 2.8 â14.0 Â± 8.4 Â± 2.0 â1.5 Â± 4.2 Â± 1.5 â0.0 Â± 2.1 Â± 0.6 10.1 Â± 5.8 Â± 3.5
217,1216,0.234,Finite Difference Computing With Pdes : a Modern Software Approach,"C.9 Exercises Exercise C.1: Explore computational efficiency of numpy.sum versus built-in Using the task of computing the sum of the first n integers, we want to compare the efficiency of numpy.sum versus Pythonâs built-in function sum. Use IPythonâs %timeit functionality to time these two functions applied to three different arguments: range(n), xrange(n), and arange(n). Filename: sumn. Exercise C.2: Make an improved numpy.savez function The numpy.savez function can save multiple arrays to a zip archive. Unfortunately, if we want to use savez in time-dependent problems and call it multiple times (once per time level), each call leads to a separate zip archive. It is more convenient to have all arrays in one archive, which can be read by numpy.load. Section C.2 provides a recipe for merging all the individual zip archives into one archive. An alternative is to write a new savez function that allows multiple calls and storage into the same archive prior to a final close method to close the archive and make it ready for reading. Implement such an improved savez function as a class Savez. The class should pass the following unit test: def test_Savez(): import tempfile, os tmp = âtmp_testarchiveâ database = Savez(tmp) for i in range(4): array = np.linspace(0, 5+i, 3) kwargs = {âmyarray_%02dâ % i: array} database.savez(**kwargs) database.close() database = np.load(tmp+â.npzâ) expected = { âmyarray_00â: np.array([ 0. , 2.5, 5. ]), âmyarray_01â: np.array([ 0., 3., 6.]) âmyarray_02â: np.array([ 0. , 3.5, 7. ]), âmyarray_03â: np.array([ 0., 4., 8.]), for name in database: computed = database[name] diff = np.abs(expected[name] - computed).max() assert diff < 1E-13 database.close os.remove(tmp+â.npzâ)"
166,303,0.234,Surveying Human Vulnerabilities across the Life Course (Volume 3.0),"In addition to these indirect evaluations some experimental comparisons between calendar instruments and conventional questionnaires showed that adding a timeline to the questionnaire enhanced data quality in comparison to the regular questionnaire procedure (Van der Vaart 2004). Similarly Belli et al. (2001) showed that the calendar produced lower levels of reporting error than conventional question lists concerning the number of residential moves income weeks unemployed and illness but it over-reported history of cohabitation and number of jobs. Why do LHCs help to report retrospective data? According to Conway (1992, 1996) autobiographical memory is a non-linear process that works through different association mechanisms. Events can be recalled through their hierarchy (e.g. from more important to least important) through their sequence (e.g. in chronological order) or in relation to other events and episodes. In addition different individuals may be more comfortable or used to recollecting events in different ways. Thus Belli (1998) argued that the calendar facilitates the use of all three memory retrieval mechanisms as well as facilitating a flexible interaction between the respondent and the interviewer to clarify intended meanings and to reconstruct the past. In line with these assumptions Glasner (2011) compared the number of edits (i.e. corrections) by respondents who answered the same questions in an online conventional biographical questionnaire as compared to an online LHC. Congruently with Belliâs rationale but without the interviewer/respondent interaction Glasner found the number of edits in the calendar mode were twice as high as those in the conventional questionnaire. In other words respondents were more likely to correct and improve the quality of biographical calendar data by respondents than conventional questionnaire data. Consistent with this results from cognitive interviews conducted at the Swiss National Centre of Competence in Research LIVES with respondents aged 35â66 with medium to low levels of education showed that in seven out of ten interviews the calendar structure encouraged respondents to re-edit their answers concerning several life domains (residence cohabitation couple relationships family work and education health other unexpected events). During the cognitive interviews the sequence through which respondents completed a self-administered calendar was recorded by an external observer. Table 1 reports the sequence used by the respondents to fill in the different columns of the calendar (ordered from A D residence to G D other unexpected events). In the most linear sequence (Type 1) three out of ten respondents filled the columns of the calendar in order starting from the first one on the left (residence) then passing to the second (cohabitation) and so on until they reached the last one on the right (column 7 other unexpected events). Type 2 respondents (three out of ten) completed the calendar in sequential order except for the first two columns (residence and cohabitation). They used residence and cohabitation as anchors for recollecting the changes in both domains. Once these two columns were properly completed respondents went through the rest of the calendar in an almost linear manner from columns 3 to 7. Type 3 respondents (four out of ten) completed the task in a less sequential order using the information of certain columns to step back and make changes in the previous data."
43,539,0.234,Objektovo orientovanÃ© programovanie v C++,"â¢ Mesiac nie je z intervalu â¨1, 12â©. â¢ DeÅ nie je z intervalu â¨1, pâ©, kde p je poÄet dnÃ­ zadanÃ©ho mesiaca. #include <iostream> #include <string> #include <sstream> using namespace std; #define neplatny datum 1 class datum private: int den, mesiac, rok; public: datum(string s); datum::datum(string s) int pocet dni[12]={31,28,31,30,31,30,31,31,30,31,30,31}; stringstream ss(s); char c; if (s.size()!=10) throw neplatny datum; if (s[2]!=â.â || s[5]!=â.â) throw neplatny datum; for (int i=0;i<10;i++) if (i==2 || i==5) continue; if (s[i]<48 || s[i]>57) throw neplatny datum; ss>>den>>c>>mesiac>>c>>rok; if (mesiac<1 || mesiac>12) throw neplatny datum; if (den<1 || den>pocet dni[mesiac-1]) throw neplatny datum; int main() string ds; getline(cin,ds); datum D(ds); catch(int vynimka) cout<<""Neplatny datum.""<<endl; return 0;"
383,137,0.234,Elements of Risk Analysis with Applications in R,"Time series is a time-ordered collection of observations. From a probability point of view, time series is a realisation of a collection of T random variables, {Y1 , Y2 , . . . , YT .} Time series may be needed to answer questions such as: â¢ What is the predicted number of visits in January 2004? â¢ What is the expected number of visits in February 2004 given that in January 2004 it was 4,100? â¢ Are the values of past time series independent of each other? Do the observations carry information about the next observations? The first two questions are about the moment (i.e. the expected value) of the time series at some time point, and the last question is about the correlation of the individual components of the time series at various time points. We use small letters to denote the realisations of Y1 , Y2 , . . . , YT , i.e. we write y1 , y2 , . . . , yT . They are also called observations, measured values or recorded values."
34,345,0.234,"Habitats and Biota of the Gulf of Mexico: Before the Deepwater Horizon Oil Spill: Volume 2: Fish Resources, Fisheries, Sea Turtles, Avian Resources, Marine Mammals, Diseases and Mortalities","or early fall. The gestation period is about 9â10 months. The fecundity is about 3â6 pups, with pupping months in May and June. The reproductive periodicity in the Gulf of Mexico is annual, while the periodicity is considered biennial in the South Atlantic. The annual survival rates are 0.72 for age-1 sharks and 0.76â0.83 for adults. Their main prey includes fish, squid, shrimp, and other invertebrates."
383,252,0.233,Elements of Risk Analysis with Applications in R,"# R Code # -----------------------------------------------------------3 # Goal : To plot the fitted linear model together with Kings time series data . 4 # -----------------------------------------------------------5 # 6 plot ( Time , y , ylab = "" Age when King died [ years ] "" , xlab = "" Order of being on the throne "" , type = "" l "" ) 8 y . predict . lin <9 kings . lin $ coefficients [[1]]+ kings . lin $ coefficients [[2]] * 10 # Since Time is already ordered , we can superimpose the fitted ( predicted ) values : 11 lines ( Time , y . predict . lin , col = "" blue "" , lwd =2) 12 # We calculate the residuals 13 resid . lin <-y - y . predict . lin 14 # Next , we plot the residuals vs time 15 plot ( Time , resid . lin , type = "" l "" , ylim = c ( -40 ,40) , ylab = "" Residuals in model with linear term "" )"
271,531,0.233,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"Well, sometimes you have to be [at your workplace] on another or a second day, so then I have to make sure that this same day I donât have any appointments in the afternoon and say, â[IP02], you have to pick up the childrenâ or something, so this is very much about communication. Mm, well good, we have [figured out] this already to some extent with the joint calendar and so on, thatâs already a lot, yes. These organizational tricks, all these organizational tricks are of course already [something]; many others donât do that (â¦). Yes, here come all our cool IT tools. (â¦) Well, we have a Google calendar, so really online, which we have on our smartphones and our computers that weâre mostly using at work. (â¦) There is my calendar in which I have my private appointments or everything in one, my private appointments, and work appointments. [IP02] can see all of this and reversed, I can also see that. That means, if a colleague asks me, âCan we make an appointment for 17 July?â I say, âOops, [IP02] has a meeting [at work] that day, thatâs going to be difficult in case one of the kids is sick or something.â That means, I can already consider this and I donât have to write an email or call first in order to ask whether I can schedule an appointment for this day or not. (P1: 938â968)"
283,765,0.233,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications",776216 18130188 5550332508 1251282702264 166071600559137 13047136918828740 629048543890724216 19087130695796615088 372099690249351071112 4739291519495550245228 39973673337590380474086 225696677727188690570184 860241108921860741947676
241,581,0.233,Second Assessment of Climate Change for the Baltic Sea Basin,"9.5.3.2 Spatio-Temporal Variations Several observation sites reveal short-term (weekly) features in the wave activity that reappear regularly, for example, a relatively calm period at the end of December and beginning of January in the northern Baltic Sea (Soomere et al. 2011). These features are probably site-speciï¬c and only persist for a few decades (Soomere et al. 2012). The extensive seasonal variation in wind speed (Mietus 1998) causes substantial variability (by a factor of two in coastal areas and up to three in offshore regions) in wave height at monthly scales (Schmager et al. 2008; Soomere and RÃ¤Ã¤met 2011) (Fig. 9.17). The calmest months are April to July and the windiest October to January (see also Chap. 4). The most extensive interannual and decadal variations in wave properties exist in the visually observed wave data (Fig. 9.18). The appearance and spatial coherence of such variations has undergone major change. Short-term (interannual and up to 3 years) variability in annual mean wave height displays a consistent pattern (with a typical spatial scale of more than 500 km) from the southern Baltic Proper to the eastern Gulf of Finland from the mid-1950s to the mid-1980s. The coherence is lost in the mid-1980s (Soomere"
383,270,0.232,Elements of Risk Analysis with Applications in R,"Elements of Risk Analysis with Applications in R = "" Day "" , type = "" l "" , main = "" Central England temperatures in 2004 lines ( Time , fit . ses . Y $ fitted , lty =2) legend ( dt1 ,21 , legend = c ( "" Time Series "" , "" SES alpha =0.6 "" ) , col = c ( "" black "" ,"" black "" ) , lty = c (1 ,2) , lwd = c (1 ,1) , cex =0.8)"
283,541,0.232,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","0, 2, 7, 10 0, 3, 4, 9 0, 4, 6, 7, 8, 9 0, 6, 7, 8 0, 3, 4, 5, 7, 9 0, 1, 3, 5, 7, 8 3, 6, 9, 10 0, 1, 3, 5 4, 8 0, 3, 6, 7, 8, 10 0, 1, 2, 4, 6, 7 0, 2, 3, 5, 6, 10 0, 1, 3, 5, 8, 10 1, 6, 7, 9 1, 2, 7, 8 0, 2, 6, 10 0, 2, 4, 5, 7, 8 5, 8 1, 5, 8, 10 0, 1, 3, 5, 6, 9 0, 2, 4, 6, 8, 9 0, 1, 2, 3, 4, 8 115 0, 2, 3, 4, 5, 6, 9, 10 0, 1, 4, 6, 9, 10"
286,57,0.232,Corporate Data Quality : Voraussetzung Erfolgreicher GeschÃ¤ftsmodelle,"Automatisierung: Die Datenvolumina, ihre Vielfalt und ihre Ãnderungsrate steigen. Um der daraus resultierenden KomplexitÃ¤t Herr zu werden, mÃ¼ssen Unternehmen Datenverarbeitungsaufgaben (z. B. die Anlage von Daten, ihre QualitÃ¤tsmessung, ihre Ãnderung und Bereitstellung) so weit wie mÃ¶glich automatisieren, z. B. Ã¼ber Workflows oder GeschÃ¤ftsregeln. LimitatioFlexibilisierung und Verteilung: Datenarchitekturen definieren nen zentraler ein unternehmensweit einheitliches Modell der Konzerndaten Datenarchitekturen (das Konzerndatenmodell) und bestimmen auÃerdem die Datenverteilungs- und Datenhaltungsarchitektur. Sie haben traditionell den Nachteil, dass sie nur mit hohem bÃ¼rokratischem Aufwand erstellt werden kÃ¶nnen und selten aktuell gehalten werden. Moderne Datenarchitekturen mÃ¼ssen hinreichend flexibel an neue Anforderungen angepasst werden kÃ¶nnen und sowohl klassische unternehmensinterne als auch externe Datenobjekte enthalten. Die Herausforderung besteht darin, diese FlexibilitÃ¤t zu ermÃ¶glichen, aber gleichzeitig fÃ¼r die KerngeschÃ¤ftsobjekte weiterhin unternehmensweit maÃgebend zu sein. Semantische Einheitlichkeit: Konzerndaten als unternehmensweite StammIntegration daten mÃ¼ssen eindeutig identifiziert und einheitlich verwendet werden. Das DatenqualitÃ¤tsmanagement muss dafÃ¼r die Konzerndaten integrieren. GrundsÃ¤tzlich stehen zwei Architekturvarianten dafÃ¼r zur VerfÃ¼gung: Entweder werden die Daten in einem System zusammengefÃ¼hrt oder die Daten verbleiben in verschiedenen Systemen und die Systeme werden Ã¼ber Schnittstellen und Datenaustausch miteinander verbunden. HeterogenitÃ¤t der Datentypen: Im Kontext von âBig Dataâ Trennung âstrukwird hÃ¤ufig von âstrukturiertenâ und âunstrukturierten Datenâ turierteâ und gesprochen, um die wachsende HeterogenitÃ¤t der vorkomâunstrukturierteâ menden Datenarten zu beschreiben, mit denen Unternehmen Daten umgehen mÃ¼ssen. Damit ist gemeint, dass neben Daten, die in ERP-Systemen in relationalen Datenbanken abgelegt sind, zunehmend auch Daten wie Videos und Bilder sowie unkonventionelle externe Daten, z. B. Ã¼ber Internetnutzung oder Social Media-KanÃ¤le, verwendet werden. Diese kÃ¶nnen wertvolle Erkenntnisse Ã¼ber den Markt und Konsumentenvorlieben liefern. Solche âunstrukturiertenâ Daten brauchen aber neue Werkzeuge zur Datenanalyse und werden Ã¼blicherweise nicht in relationalen Datenbanken gespeichert. Datenschutz Gerade multinationale Unternehmen sind mit unterschiedlichen Datenschutzvorgaben konfrontiert. Das DatenqualitÃ¤tsmanagement muss sicherstellen, dass diese Regeln eingehalten werden. Problemquellen hierfÃ¼r sind, dass Richtlinien oft nicht transparent sind, sie sich hÃ¤ufig Ã¤ndern (was ebenfalls unbekannt ist) und niemand genau weiÃ, inwieweit sie in den Systemen umgesetzt sind. Daher wird Datenschutz von vielen Unternehmen eher als Behinderung angesehen denn als OpportunitÃ¤t."
385,862,0.232,Advanced R,"An STL vector is very similar to an R vector, except that it grows efï¬ciently. This makes vectors appropriate to use when you donât know in advance how big the output will be. Vectors are templated, which means that you need to specify the type of object the vector will contain when you create it: vector<int>, vector<bool>, vector<double>, vector<String>. You can access individual elements of a vector using the standard [] notation, and you can add a new element to the end of the vector using .push_back(). If you have some idea in advance how big the vector will be, you can use .reserve() to allocate suï¬cient storage. The following code implements run length encoding (rle()). It produces two vectors of output: a vector of values, and a vector lengths giving how many times each element is repeated. It works by looping through the input vector x comparing each value to the previous: if itâs the same, then it increments the last value in lengths; if itâs diï¬erent, it adds the value to the end of values, and sets the corresponding length to 1. #include <Rcpp.h> using namespace Rcpp; // [[Rcpp::export]] List rleC(NumericVector x) { std::vector<int> lengths; std::vector<double> values; // Initialise first value int i = 0; double prev = x[0];"
231,153,0.231,North Sea Region Climate Change Assessment,"1.5.5.1 Sunshine Duration Sunshine duration depends on latitude and daytime cloud cover conditions. Measurements of sunshine duration are not included in the observational routine of the VOS fleet and statistics are not derived from measurements on light-vessels. Annual cycles for some land stations in the North Sea region are shown in Fig. 1.30. Because there is a gradient in sunshine duration from land to sea, their representativeness for sea areas is limited. Change in daylength throughout the year shows a predictable pattern with sunshine duration in the North Sea region at a maximum in summer and a minimum in winter. On average May and July are the sunniest months with about 170 h of sunlight in Lerwick, 250 h in Helgoland and 290 h in Skagen Fyr (adjusted, not shown) for the period 1971â2000. Sunshine duration is lowest in"
172,132,0.231,Twitter Als Basis Wissenschaftlicher Studien : Eine Bewertung GÃ¤ngiger Erhebungs- Und Analysemethoden Der Twitter-Forschung,"fragen und Befehle, sondern funktionieren auch problemlos bei sehr groÃen Datensets. Allerdings ist hier die Befehls- und Prozessstruktur deutlich komplizierter und unÃ¼bersichtlicher. FÃ¼r MapReduce benÃ¶tigt man zudem Kenntnisse in der Programmiersprache JavaScript. Im Vergleich zum Aggregation Framework ist MapReduce fÃ¼r sehr groÃe Daten deutlich besser geeignet, da es keine Begrenzung hinsichtlich der zu verarbeitenden Datenmenge gibt. Allerdings ist bei dieser Methode auch der Leistungsbedarf (CPU-Auslastung) hÃ¶her bei vergleichsweise geringerer Geschwindigkeit (Marturana, 2015). Tabelle 8 fasst die Vor- und Nachteile noch einmal zusammen. Tabelle 8: Vergleich der Aggregations-Methoden von MongoDB ABFRAGEN"
172,91,0.231,Twitter Als Basis Wissenschaftlicher Studien : Eine Bewertung GÃ¤ngiger Erhebungs- Und Analysemethoden Der Twitter-Forschung,"Die wohl einfachste Methode zum Speichern von Tweets ist die Verwendung von einzelnen Dateien, die die Tweets deserialisiert im JSON-Format oder bereits formatiert, gefiltert und umstrukturiert beinhalten. FÃ¼r den Export von Twitter-Daten eignen sich TXT- oder JSON-Dateien, die Tweets im ursprÃ¼nglichen JSON-Format umfassen, oder CSV-Dateien, die bereits selektierte Werte beinhalten. Python bietet die MÃ¶glichkeit, Tweets sowohl im JSON-Format abzuspeichern, als auch umstrukturiert im CSV-Format. WÃ¤hrend die benÃ¶tigten Programmteile zum Lesen der Daten bereits in Python integriert sind, muss jedoch ein zusÃ¤tzlicher Konverter zur Ausgabe strukturierter JSON-Daten installiert werden, um DatenstrÃ¶me nahezu direkt in Textdateien zu schreiben. Es mÃ¼ssen lediglich die im BinÃ¤r-Code vorliegenden JSON-Daten (BSON) aus der Twitter-API in ein strukturiertes JSON-Format deserialisiert werden. Dies erfolgt in der Regel mit einem j s onp ick le .en cod e-Befehl. Die Codierung erfolgt schrittweise je Datensatz. Der Auszug in Listing 9 zeigt ein einfaches Verfahren zum Abspeichern von Tweets im JSON-Format. In diesem Beispiel sollten Tweets mit dem Begriff âTatortâ nachtrÃ¤glich Ã¼ber die REST APIs gesammelt werden. Nach der Definition des Datei-Namens und Formates wurde im Skript das Vorgehen bei neuen Tweets vorgegeben. Jeder neue Tweet wird im JSON-Format als neue Zeile dem Dokument angefÃ¼gt, sodass jede Zeile einem Tweet entspricht. Jeder Schreibvorgang erhÃ¶ht den ZÃ¤hler um 1. Bei Erreichen des definierten Maximalwerts stoppt die Schleife den Speicherprozess."
172,153,0.231,Twitter Als Basis Wissenschaftlicher Studien : Eine Bewertung GÃ¤ngiger Erhebungs- Und Analysemethoden Der Twitter-Forschung,"5.4 DatenverfÃ¼gbarkeit Neben der mangelnden ReprÃ¤sentativitÃ¤t von Twitter-Daten ist auch deren eingeschrÃ¤nkte VerfÃ¼gbarkeit problematisch: Je nach Datenquelle kÃ¶nnen entweder zukÃ¼nftige (Streaming API) oder vergangene (REST APIs) Tweets kostenlos abgefragt werden. Zudem gibt es noch eine Vielzahl von Drittanbietern, die ebenfalls unterschiedliche EinschrÃ¤nkungen hinsichtlich Zeitraum und Umfang haben. Letztlich wÃ¤re eine VollstÃ¤ndigkeit der Daten nur bei Zugriff auf die Firehose beziehungsweise bei Erwerb von Datenpaketen bei Gnip gewÃ¤hrleistet. Zudem sind historische Tweets nach 6 bis 9 Tagen nur noch Ã¼ber einzelne Abfragen (Anhand der User- oder Tweet-ID) abrufbar. Bei sehr aufsehenerregenden Ereignissen, wie beispielsweise dem Finale der FIFA FuÃball-WM der MÃ¤nner, ist eine umfassende Erhebung von Tweets mit Hilfe der Streaming API nicht mÃ¶glich, da sehr schnell das Bandbreiten-Limit von einem Prozent Ã¼berschritten wird. Da die Suchergebnisse Search API nicht vollstÃ¤ndig und zeitlich begrenzt sind, ist es fÃ¼r derartige Ereignisse sinnvoll, Daten nachtrÃ¤glich zu kaufen. Diese EinschrÃ¤nkungen behindern nicht nur die Forschung an sich, sondern erschweren auch eine Reproduzierbarkeit der Ergebnisse, da nicht gewÃ¤hrleistet ist, dass jede Abfrage mit gleichen Parametern identische Daten erzeugt (Gaffney & Puschmann, 2014, S. 65). Die UnvollstÃ¤ndigkeit der Daten erzeugt noch ein weiteres Problem: Ãhnlich wie bei der Analyse von wertenden Aussagen ist auch bei rein quantitativer Betrachtung der Kontext fÃ¼r eine Einordnung der Daten notwendig. Ohne das Wissen Ã¼ber das absolute Twitter-Volumen kÃ¶nnen, wie bereits in Kapitel 4.1.4 erwÃ¤hnt, nur eingeschrÃ¤nkt Aussagen Ã¼ber die Bedeutung absoluter Zahlen getÃ¤tigt werden."
294,17,0.231,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"arguments to other functions, there is good support for interfacing C, C++ and Fortran code (i.e., a Python program may use code written in other languages), and functions explicitly written for scalar input often work fine (without modification) also with vector input. Another important thing, is that Python is available for free. It can be downloaded from the Internet and will run on most platforms. Readers who want to expand their scientific programming skills beyond the introductory level of the present exposition, are encouraged to study the book A Primer on Scientific Programming with Python [13]. This comprehensive book is as suitable for beginners as for professional programmers, and teaches the art of programming through a huge collection of dedicated examples. This book is considered the primary reference, and a natural extension, of the programming matters in the present book. Some computer science terms"
59,507,0.23,KognitÃ­vna veda v kontexte informatickÃ½ch vied,4 44 5 4 74 8 5 05 1 5 25 3 5 45 5 5 65 7 5 85 96 06 1 6 26 36 46 5 BCBDBE 6 66 7 BFBGBH 6 86 97 0 B I BJ 7 17 27 3 BKBLBM 7 47 57 6 7 77 87 9 BRBSBTBU 8 08 18 2 8 38 48 58 6 BWBXBYBZCA 8 78 88 99 0 CBCCCDCE 9 19 CHC IC JCKCLCM 29 39 49 59 69 79 89 1 9 0 00110120130140 5 CNCOCPCQ CRCSCTCUCV1 016017018011 9 111 0 1 11 CWCXCYCZDA1 2 3 11 4 5 6 11 DDDEDF 7 8 11192102112122132142152162172 8 DHD11 I JDKDL DPDQDRDSDTDUDV 1 21DZ 031131EC 33 4 DWDXDY
283,692,0.23,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications",1 + x 3 + x 6 + x 12 + x 17 + x 24 + x 27 + x 34 + x 39 + x 45 + x 48 1+x 9 +x 11 +x 18 +x 21 +x 22 +x 25 +x 27 +x 36 +x 37 +x 42 +x 44 +x 45 +x 50 +x 54 1 + x + x 2 + x 4 + x 8 + x 11 + x 16 + x 22 + x 32 + x 44 + x 59 + x 64 + x 88 1+x +x 2 +x 4 +x 8 +x 16 +x 32 +x 55 +x 59 +x 64 +x 91 +x 93 +x 109 +x 110 +x 118 1 + x 2 + x 10 + x 18 + x 29 + x 32 + x 33 + x 49 + x 50 + x 54 + x 58 + x 65 + x 74 + x 76 + x 78 + x 86 + x 87 + x 88 + x 92 + x 93 + x 95 1 + x 5 + x 10 + x 20 + x 29 + x 31 + x 33 + x 39 + x 40 + x 58 + x 62 + x 66 + x 78 + x 79 + x 80 + x 83 + x 103 + x 105 + x 115 + x 116 + x 121 + x 124 1+x 13 +x 16 +x 18 +x 22 +x 26 +x 39 +x 42 +x 45 +x 46 +x 49 +x 57 +x 65 +x 68 + x 70 +x 78 +x 80 +x 90 +x 91 +x 92 +x 96 +x 97 +x 102 +x 103 +x 105 +x 108 +x 111
228,50,0.23,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"B (x) = Î¼ Aâ B (x; 6, 8, 11), which is shown in Fig. 1.7c. In a similar way, using (1.43) the results of the subtraction are obtained: theinterval B Î± = [3Î± â 5, â2Î±] and the membership function Î¼ Aâ"
291,137,0.23,Visualizing Mortality Dynamics in the Lexis Diagram (Volume 44.0),"â¢ minage: An integer which denotes the youngest ages to be included, set by default to 50. â¢ maxage: An integer which denotes the oldest ages to be included, set by default to 100. â¢ minyear: An integer which denotes the earliest calendar year to be included, set by default to 1950. â¢ maxyear: An integer which denotes the latest calendar year to be included, set by default to 2011. The function returns a matrix that contains the combined number of deaths or exposures for a given combination of calendar year and age. Row names denote ages from minage to maxage; column names denote calendar years from minyear to maxyear."
59,672,0.23,KognitÃ­vna veda v kontexte informatickÃ½ch vied,"3 6 8 2,9 4 1 5 4 3 1 2,8,9 2,6,7 5 9 7,8,9 7,8,9 1,4,8 1,6,8 4,6,7 1,6,9 3 5 6,8 6,8 1,2,3 2,3,4 1,2,8 1,5 2,6,8 7 9 6 1,4,8 2 5,6x 9 7 1 6 8 5,4 3,4,6 x 2,5 9 3 1,2,6 2 1,6 4 5,6,7 3 1 6 2,8 2,6x 4 7 5 9"
372,1416,0.23,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"and the phase goes through one sinusoidal oscillation in a sidereal day. Suppose that the source is circumpolar, i.e., above the horizon for 24 h. From continuous observation of ! over a full day, the 2"" crossings of ! can be tracked so that there are no phase ambiguities. The average value of the geometric term of Eq. (12.4) is zero, so that !in can be estimated and removed. When the source transits the local"
48,128,0.23,Time Predictions : Understanding and Avoiding Unrealism in Project Planning and Everyday Life (Volume 5.0),"Table 6.1 Find two pX predictions based on past time prediction accuracy Observation Corresponding pX pX value when the prediction is 10 hours In 2 out of 10 times, we spent p20 90% of the predicted time 90% or less of the predicted usage time usage (10% or more (0.9 Ã 10 hours = 9 hours) underrun) In 8 out 10 times, we spent 150% or less of the predicted time usage (50% or less overrun)"
385,795,0.23,Advanced R,"code. Sometimes, however, we may want to measure incremental change. One way to do this is to use memory proï¬ling to capture usage every few milliseconds. This functionality is provided by utils::Rprof() but it doesnât provide a very useful display of the results. Instead weâll use the lineprof (https://github.com/hadley/lineprof) package. It is powered by Rprof(), but displays the results in a more informative manner. To demonstrate lineprof, weâre going to explore a bare-bones implementation of read.delim() with only three arguments:"
231,355,0.229,North Sea Region Climate Change Assessment,"Fig. 3.17 Annual mean high water and linear trend (in m) for the period 1843â2012 at Cuxhaven, Germany (lower) and annual 99th percentile of the approximately twice-daily high-tide water levels at Cuxhaven after subtraction of the linear trend in the annual mean levels (upper); an 11-year running mean is also shown in the upper panel (redrawn and updated after von Storch and Reichardt 1997)"
383,15,0.229,Elements of Risk Analysis with Applications in R,"discuss relevant extensions and more advanced topics - related to data science and artificial intelligence - and to recommend books and papers for the eager reader. End-of-chapter feature R lab is used to give the reader the R code for all solved examples from the chapter, as well as the full output from R. We do not give any R code inside the chapters so that the reader can read the chapter undisturbed by R code explanations. The R lab section also contains further exercises to be solved using R. Their solutions are not provided in the book but can be provided upon request to instructors. End-of-chapter feature Exercises gives the reader an opportunity to practice the topics. The use of a calculator is often needed, but R is not needed. Their solutions are not provided in the book but can be provided upon request to instructors."
394,560,0.229,Biotechnologies for Plant Mutation Breeding : Protocols,iAdB96_1 iADB96_2 iADB96_3 iADB96_4 iADB96_5 iADB96_6 iADB96_7 iADB96_8 iADB96_9 iADB96_10 iADB96_11 iADB96_12 iADB96_13 iADB96_14 iADB96_15 iADB96_16 iADB96_17 iADB96_18 iADB96_19 iADB96_20 iADB96_21 iADB96_22 iADB96_23 iADB96_24 iADB96_25
377,240,0.228,Berufliche Passagen Im Lebenslauf : Berufsbildungs- Und Transitionsforschung in Der Schweiz,"Anmerkungen: Die Daten stammen aus der Schuldatenbank des Kantons Genf. Wechsel des Berufes oder der Sektion innerhalb eines Bildungsganges wurden nicht gezÃ¤hlt. Die Jugendlichen ohne Ausbildung in Genf haben entweder den Kanton verlassen (MobilitÃ¤t zwischen Kantonen oder international) oder ihre Ausbildung temporÃ¤r oder defi nitiv abgebrochen. Umorientierungen aus der beruflichen Bildung heraus finden in Richtung allgemeinbildender, privater oder spezialisierter Ausbildungen statt."
294,128,0.228,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"Note the ways of writing the different operations here. Using append() will always increase the list at the end. If you like, you may create an empty list as x = [] before you enter a loop which appends element by element. If you need to know the length of the list, you get the number of elements from len(x), which in our case is 5, after appending 3.14 above. This function is handy if you want to traverse all list elements by index, since range(len(x)) gives you all legal indices. Note that there are many more operations on lists possible than shown here. Previously, we saw how a for loop may run over array elements. When we want to do the same with a list in Python, we may do it as this little example shows, x = [âhelloâ, 4, 3.14, 6] for e in x: print âx element: â, e print âThis was all the elements in the list xâ"
283,546,0.228,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","4, 6, 7, 8 0, 2, 5, 6, 9, 10 1, 2, 5, 9 1, 2, 3, 7 0, 3, 4, 5, 6, 7, 8, 10 2, 3, 4, 5 0, 4, 6, 7, 9, 10 0, 1, 4, 9 1, 2, 4, 5, 6, 7, 9, 10 0, 3, 4, 5, 8, 10 4, 7, 8, 9 5, 10 4, 5, 6, 7, 9, 10 2, 5, 9, 10 0, 1, 2, 6, 7, 8, 9, 10 0, 1, 2, 6, 7, 10 1, 2, 4, 5, 6, 9 0, 1, 4, 5, 7, 8 0, 2, 3, 4, 5, 9 0, 2, 4, 5, 6, 8 2, 3, 7, 9 0, 1, 4, 6, 7, 8, 9, 10 3, 7, 9, 10 0, 1, 2, 7, 8, 9"
385,420,0.228,Advanced R,"What if you need a for loop replacement that doesnât exist in base R? You can often create your own by recognising common looping structures and implementing your own wrapper. For example, you might be interested in smoothing your data using a rolling (or running) mean function: rollmean <- function(x, n) { out <- rep(NA, length(x)) offset <- trunc(n / 2) for (i in (offset + 1):(length(x) - n + offset - 1)) {"
231,240,0.228,North Sea Region Climate Change Assessment,"2.3.2.4 Northern Alps and Central Europe Although not directly linked to the North Sea wind climate, observations from further south are also useful to help understand variations in large-scale atmospheric circulation and give indications about a northward displacement in storm tracks. For Vienna, the number of gale days (above 8 Bft, 17.2 m sâ1) show a clear decrease for the period 1872â 1992 (Matulla et al. 2007), however based on nonhomogenised observations. This is corroborated by signiï¬cant negative trends in the number of days exceeding 7, 8 and 9 Bft (>13.9, 17.2 and 20.8 m sâ1, respectively) in northern Switzerland for the period 1894â1994 (Schiesser et al. 1997). The duration of strong winds (>7 Bft) also shows a negative trend for ZÃ¼rich for 1871â1991 except in winter (BrÃ¶nnimann et al. 2012). Negative trends are also found for central Europe (Matulla et al. 2007; Wang et al. 2011). In contrast, Stucki et al. (2014) found no clear trends, but large interdecadal variability, over Switzerland."
385,859,0.228,Advanced R,"The key points are: â¢ We step through two iterators (input and output) simultaneously. â¢ We can assign into an dereferenced iterator (out_it) to change the values in out. â¢ upper_bound() returns an iterator. If we wanted the value of the upper_bound() we could dereference it; to ï¬gure out its location, we use the distance() function. â¢ Small note: if we want this function to be as fast as findInterval() in R (which uses handwritten C code), we need to compute the calls to .begin() and .end() once and save the results. This is easy, but it distracts from this example so it has been omitted. Making this change yields a function thatâs slightly faster than Râs findInterval() function, but is about 1/10 of the code. Itâs generally better to use algorithms from the STL than hand rolled loops. In Eï¬ective STL, Scott Meyers gives three reasons: eï¬ciency, correctness, and maintainability. Algorithms from the STL are written by C++ experts to be extremely eï¬cient, and they have been around for a long time so they are well tested. Using standard algorithms also makes the intent of your code more clear, helping to make it more readable and more maintainable."
109,10,0.228,Digitalisierung : Bildung | Technik | innovation,"2019 wird der mobile Datenverkehr in Deutschland monatlich ein Volumen von 259,8 Petabyte erreichen â das entspricht etwa 272 Millionen Gigabyte. Datensicherheit ist fÃ¼r insgesamt 78 Prozent der Unternehmen bei der Nutzung von Cloud Computing ein Risikofaktor. 64.426 FÃ¤lle von CyberkriminalitÃ¤t zÃ¤hlte das BKA 2013 in Deutschland. Die Datenmenge, die im Jahr 2020 weltweit erstellt, vervielfÃ¤ltigt und konsumiert wird, wird auf etwa 40 Zettabytes geschÃ¤tzt. Das Speichervolumen eines menschlichen Gehirns wird auf 2,5 Petabyte geschÃ¤tzt. Nur 44 Prozent des Webtraffics gehen direkt auf menschliche AktivitÃ¤ten zurÃ¼ck."
365,284,0.227,Climate Smart Agriculture : Building Resilience To Climate Change,"Notes: Summary statistics correspond to a balanced panel of 644 counties for the 1981â2014 period. Weather variables are aggregated between April and September of each year. For reference, 100 bu./acre of maize are roughly equivalent to 6.3 t/ha"
383,174,0.227,Elements of Risk Analysis with Applications in R,"FIGURE 3.14: Central England daily temperatures time series data. Solution. In the next, we, therefore, assume the Daily Temperature time series data follow no trend (i.e. we assume a constant trend), no seasonality and no cyclic pattern. Our goal is to estimate the values at times T + 1, ..., T + h, i.e. the values yt+1 , ..., yt+h . We will denote such estimates as yÌt+1|T , ..., yÌT +h|T , and we call them the forecasts. Note, that the absolute simplest method for the forecast (at h = 1, 2, . . .) yÌT +h|T = yT (3.20)"
157,235,0.227,"The Economy as a Complex Spatial System : Macro, Meso and Micro Perspectives","Time is discrete (t = 0, 1, ...); each period t is divided into sub-periods (Ï = 0, 1, ...). There is a set of n banks with a typical element bi , where (i = 1, ..., n). Table 1 shows the general form of a bank balance sheet: Table 1. Bank balance sheet Assets"
139,322,0.227,Programming for Computations - MATLAB/Octave (Volume 14.0),"of time steps per period and compute the associated time steps and end time of the simulation (T), given a number of periods to simulate: import odespy # Define the ODE system # uâ = v # vâ = -omega**2*u def f(sol, t, omega=2): u, v = sol return [v, -omega**2*u] # Set and compute problem dependent parameters omega = 2 X_0 = 1 number_of_periods = 40 time_intervals_per_period = 20 from numpy import pi, linspace, cos P = 2*pi/omega # length of one period dt = P/time_intervals_per_period # time step T = number_of_periods*P # final simulation time # Create Odespy solver object odespy_method = odespy.RK2 solver = odespy_method(f, f_args=[omega]) # The initial condition for the system is collected in a list solver.set_initial_condition([X_0, 0]) # Compute the desired time points where we want the solution N_t = int(round(T/dt)) # no of time intervals time_points = linspace(0, T, N_t+1) # Solve the ODE problem sol, t = solver.solve(time_points) # Note: sol contains both displacement and velocity # Extract original variables u = sol[:,0] v = sol[:,1]"
385,491,0.227,Advanced R,"Thatâs harder to follow because (e.g.) the argument of dot_every() is far away from its call. This is sometimes called the Dagwood sandwich (http://en.wikipedia.org/wiki/Dagwood_sandwich) problem: you have too much ï¬lling (too many long arguments) between your slices of bread (parentheses). Iâve also tried to give the FOs descriptive names: delay by 1 (second), (print a) dot every 10 (invocations). The more clearly the function names used in your code express your intent, the easier it will be for others (including future you) to read and understand the code."
385,374,0.227,Advanced R,"Calling a function from a list is straightforward. You extract it then call x <- runif(1e5) system.time(compute_mean$base(x)) user system elapsed 0.001 0.000 0.001 system.time(compute_mean[[2]](x)) user system elapsed system.time(compute_mean[[""manual""]](x)) user system elapsed 0.053 0.003 0.055"
385,739,0.227,Advanced R,"The easiest way to make a function faster is to let it do less work. One way to do that is use a function tailored to a more speciï¬c type of input or ouput, or a more speciï¬c problem. For example: â¢ rowSums(), colSums(), rowMeans(), and colMeans() are faster than equivalent invocations that use apply() because they are vectorised (the topic of the next section). â¢ vapply() is faster than sapply() because it pre-speciï¬es the output type. â¢ If you want to see if a vector contains a single value, any(x == 10) is much faster than 10 %in% x. This is because testing equality is simpler than testing inclusion in a set. Having this knowledge at your ï¬ngertips requires knowing that alternative functions exist: you need to have a good vocabulary. Start with Chapter 4, and expand your vocab by regularly reading R code. Good places to read code are the R-help mailing list (https://stat.ethz. ch/mailman/listinfo/r-help) and stackoverï¬ow (http://stackoverflow. com/questions/tagged/r). Some functions coerce their inputs into a speciï¬c type. If your input is not the right type, the function has to do extra work. Instead, look for a function that works with your data as it is, or consider changing the way you store your data. The most common example of this problem is using apply() on a data frame. apply() always turns its input into a matrix. Not only is this error prone (because a data frame is more general than a matrix), it is also slower. Other functions will do less work if you give them more information about the problem. Itâs always worthwhile to carefully read the documentation and experiment with diï¬erent arguments. Some examples that Iâve discovered in the past include: â¢ read.csv(): specify known column types with colClasses. â¢ factor(): specify known levels with levels."
283,515,0.227,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","0, 4, 9 0, 1, 2, 4, 6 0, 1, 2, 4, 6, 7, 9 1, 2, 4, 5, 6, 9, 10 0, 2, 5, 7, 8 2, 5, 10 6, 8, 9 0, 1, 3, 4, 5, 6, 10 507 0, 1, 2, 3, 4, 5, 6, 7, 10 0, 6, 7 1, 5, 6, 8, 10 522 1, 2, 3, 4, 5, 6, 7, 9, 10 0, 1, 2, 6, 9 0, 2, 3, 5, 9 3, 4, 5, 6, 7 2, 4, 6, 7, 8, 9, 10 1, 6, 7, 9, 10 3, 5, 6, 7, 9 1, 2, 3, 5, 8, 9, 10 2, 3, 6 0, 3, 4, 7, 10"
289,2010,0.227,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","Even though the original equations were non-overlapping, we suddenly obtained an abstraction with two overlapping patterns. Slind observed a similar problem [38, Sect. 3.3.2] in his algorithm. Therefore, he only permits uniform equations, as defined by Wadler [36, Sect. 5.5]. Here, we can give a formal characterization of our requirements as a computable function on pairs of patterns: fun pat compat :: term â term â bool where pat compat (t1 $ t2 ) (u1 $ u2 ) â pat compat t1 u1 â§ (t1 = u1 â pat compat t2 u2 ) pat compat t u â (overlapping t u â t = u)"
283,518,0.227,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","0, 1, 3, 5, 9 1, 2, 4, 8, 9 0, 9, 10 4, 6, 7, 9, 10 0, 1, 5, 6, 8 0, 2, 9 1, 4, 5 0, 3, 5, 9, 10 1, 3, 4, 9, 10 1, 2, 3, 4, 7 0, 2, 3, 4, 6, 7, 9 2, 6, 9 1, 3, 4, 6, 7, 8, 9 1, 4, 10 540 0, 2, 3, 4, 6, 7, 8, 9, 10 0, 1, 2, 8, 9 0, 2, 3, 4, 5, 6, 10 0, 4, 8, 9, 10 2, 4, 8 4, 5, 6, 8, 10 0, 2, 4, 5, 6, 8, 10"
283,563,0.226,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","1, 5, 8, 9 3, 8, 9, 10 610 0, 1, 2, 3, 4, 6, 7, 10 0, 1, 9, 10 0, 2, 3, 5, 6, 7 0, 5, 7, 8 630 0, 1, 2, 3, 5, 6, 8, 9 0, 1, 4, 6, 7, 10 0, 3, 4, 5 1, 6 0, 1, 2, 3, 5, 6 1, 5, 6, 9 660 2, 3, 4, 5, 6, 7, 8, 9 2, 3, 6, 7, 8, 9 0, 1, 2, 5, 8, 9 0, 1, 3, 4, 7, 8 0, 1, 5, 7, 9, 10 0, 1, 2, 4, 7, 9 3, 5, 9, 10 695 0, 2, 3, 4, 5, 6, 7, 8 3, 5, 6, 10 3, 4, 5, 7, 8, 9 1, 2, 3, 4, 5, 7"
283,507,0.226,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","0, 3, 4, 6, 9 1, 3, 4, 5, 6, 9, 10 0, 3, 4, 5, 6, 9, 10 1, 2, 3, 7, 8, 9, 10 2, 5, 6, 8, 9 0, 2, 3, 5, 6, 8, 9 3, 5, 7, 9, 10 0, 2, 3, 4, 5 1, 3, 4, 6, 8 0, 1, 4, 5, 7 1, 2, 3 1, 2, 7, 9, 10 0, 3, 4, 8, 9 1, 3, 5 4, 7, 8 1, 2, 3, 6, 8 1, 2, 4, 6, 7 4, 5, 6, 7, 10 1, 3, 5, 6, 7, 9, 10 1, 5, 9 0, 1, 4, 6, 7, 9, 10 2, 4, 7 354 0, 1, 2, 3, 5, 6, 7, 9, 10"
217,178,0.226,Finite Difference Computing With Pdes : a Modern Software Approach,"1.10.6 Visualization The functions for visualizations differ significantly from those in the undamped case in the vib_undamped.py program because, in the present general case, we do not have an exact solution to include in the plots. Moreover, we have no good estimate of the periods of the oscillations as there will be one period determined by the system parameters, essentially the approximate frequency s 0 .0/=m for linear s and small damping, and one period dictated by F .t/ in case the excitation is periodic. This is, however, nothing that the program can depend on or make use of. Therefore, the user has to specify T and the window width to get a plot that moves with the graph and shows the most recent parts of it in long time simulations. The vib.py code contains several functions for analyzing the time series signal and for visualizing the solutions."
383,161,0.226,Elements of Risk Analysis with Applications in R,"FIGURE 3.9: ACF function for monthly overseas visits data for January 1980- July 1980, hence 7 months only. The 95% confidence bands for autocorrelation are also showed (see the two blue dashed vertical lines). This plot is also called autocorrelogram. In R the above calculations can be done via a built-in function called acf (see the Section R Lab, at the end of this chapter, for more details). This creates the following Figure 3.9. Note that the plot also shows the autocorrelation for time t = 0, and this must be equal to 1, which will always be out of the confidence band, but this will be ignored in our interpretation, as such autocorrelation is not being tested. Note that all other 6 values of autocorrelation are within the 95% confidence band. Thus we conclude that there is no evidence of autocorrelation in the first 7 values of overseas visits data. Example. Overseas visits. (continues) Next, we consider data from January 1980 to December 1981, and also data from January 1980 till March 2020. We now have more data and we are asked the same question: are data autocorrelated? Solution. Now, we will use all the data to plot the autocorrelation function. The following Figure 3.10 was done in R, of which details are in Section 1. Are there any significant correlations? Are there any patterns in the auto-correlation function? If we use all 40 years of Overseas visits data (Figure 3.10, the plot on the"
326,881,0.226,"Autonomes Fahren : Technische, Rechtliche Und Gesellschaftliche Aspekte","24.2.4 Konsequenzen der Weitergabe von Daten an Dritte Daten, die an Dritte jenseits der DomÃ¤nen von Autoeignern oder -fahrern transferiert werden, ermÃ¶glichen es diesen Dritten, ihre Interessen zu verfolgen. Diese Interessen kÃ¶nnen mit den Interessen der sogenannten Datensubjekte, die durch die Daten identifiziert werden (in diesem Fall sind das typischerweise Autofahrer oder -eigner) im Konflikt stehen. In diesem Abschnitt werden Beispiele fÃ¼r die folgenden Drittparteien diskutiert: Fahrzeughersteller, Versicherungsdienstleister, Flottenbetreiber, staatlich autorisierte Parteien, Peer-ad-hoc-Netzwerke, z. B. andere Verkehrsteilnehmer oder andere autonome Fahrzeuge, und Verkehrszentralen. Die Reihenfolge der Unterabschnitte folgt der ansteigenden KomplexitÃ¤t im Setting der Drittparteien."
383,22,0.226,Elements of Risk Analysis with Applications in R,"Monthly visitors time series for January 1980 - December 1981. Lagged time series. . . . . . . . . . . . . . . . . . . . . . . . . Autocorrelation. . . . . . . . . . . . . . . . . . . . . . . . . . Calculations of autocorrelation at lag 2. . . . . . . . . . . . . Weights for four values of Î± in simple exponential smoothing. Daily temperatures data, with SES and SSE. . . . . . . . . . Kings life span data, summary for two regression models. . . Overseas visits 1980-1981, summary for HW model. . . . . ."
335,329,0.226,"Open Source Systems : Towards Robust Practices 13Th Ifip Wg 2.13 international Conference, Oss 2017, Buenos Aires, Argentina, May 22-23, 2017, Proceedings","OpenStack keeps refining its release management process but always committed to a six-month release cycle. Each release cycle encompasses: planning (1 month), implementation (3 months), and integration where most pre-release critical bugs should be fixed (2 months). During the earlier release phase, the âcodingâ efforts are much driven by discussion and specifications, while in a later release phase (i.e., stabilization of release candidates) the development turns into the bug-fixing mode (as reported in other open source projects [5,19,23]). At each release, developers start by implementing the discussed and/or specified key features while, by the end of the release, there is a peak of bug-fixing activities. To sum up, each release cycle starts in a specification and discussion driven way and ends in a bug-tracker oriented way. The âplanning stageâ is at the start of a cycle, just after the previous release. After a period of much stress to make the quality of the previous release acceptable, the community steps back and focus on what should be done for the next release. This phase usually lasts four weeks and runs in parallel with the OpenStack Design Summit on the third week (in a mixture of virtual and face-toface collaboration). The community discusses among peers while gathering feedback and comments. In most cases, specification documents are proposed via an infrastructure system11 that should precisely describe what should be done. Contributors may propose new specs at any moment in the cycle, not just during the planning stage. However doing so during the planning stage is preferred, so that contributors can benefit from the Design Summit discussion and the elected Project Team Leads (PTLs) can include those features into their cycle roadmap. Once a specification is approved by the corresponding project leadership, implementation is tracked in a blueprint12 , where a priority is set and a target milestone is defined, communicating when in the cycle the feature is likely to go live â At this stage, the process reflects the principles of agile methods. The âimplementation stageâ is when contributors actually write the code (or produce documentation, test cases among other software-related artifacts) mapping the defined blueprints. This phase is characterized by milestone iterations (once again a characteristic of agile software development methods). Once developers perceive their work as ready to be proposed for merging into the master"
286,74,0.225,Corporate Data Quality : Voraussetzung Erfolgreicher GeschÃ¤ftsmodelle,"Abbildung 1.13 visualisiert diese Beziehungen. Diese Darstellung der Datenorganisation richtet sich nach dem relationalen Datenmodell. Im idealen, semantisch eindeutigen Fall gibt es eine 1:1-Beziehung zwischen der Datenwelt und der realen Welt, d. h. ein Datenobjekt bildet genau ein GeschÃ¤ftsobjekt ab. In der RealitÃ¤t gibt es jedoch oft mehrere Datenobjekte parallel, die dasselbe GeschÃ¤ftsobjekt reprÃ¤sentieren. In diesem Fall ist das DatenqualitÃ¤tsmanagement gefordert, Richtlinien, Prozesse und Systeme zu etablieren, die erlauben, das fÃ¼r einen bestimmten Kontext ârichtigeâ Datenobjekt zu identifizieren und fÃ¼r die Nutzung in GeschÃ¤ftsprozessen bereitzustellen."
383,28,0.225,Elements of Risk Analysis with Applications in R,"Confidence interval Prediction interval h-step forecast when using all data up to time T Q1 , .., Q4 Quarters of a year Covariance Correlation between measurements from time t and s Autocorrelation at lag k. Estimate of the correlation between measurements that are k time steps apart Exploratory data analysis Autocorrelation function Simple Exponential Smoothing Holtâs Exponential Smoothing Holt-Winterâs Exponential Smoothing Yt , Xt Random variable measured at time t State of a random variable at time t Probability distribution at time 0 One-step probability transition matrix Probability of transiting from i to j in k steps Coeficient of variation Value at Risk Expected Monetary Value"
259,159,0.225,The Little Book of Semaphores,"This problem is from William Stallingsâs Operating Systems [11], but he attributes it to John Trono of St. Michaelâs College in Vermont. Stand Claus sleeps in his shop at the North Pole and can only be awakened by either (1) all nine reindeer being back from their vacation in the South Pacific, or (2) some of the elves having difficulty making toys; to allow Santa to get some sleep, the elves can only wake him when three of them have problems. When three elves are having their problems solved, any other elves wishing to visit Santa must wait for those elves to return. If Santa wakes up to find three elves waiting at his shopâs door, along with the last reindeer having come back from the tropics, Santa has decided that the elves can wait until after Christmas, because it is more important to get his sleigh ready. (It is assumed that the reindeer do not want to leave the tropics, and therefore they stay there until the last possible moment.) The last reindeer to arrive must get Santa while the others wait in a warming hut before being harnessed to the sleigh. Here are some addition specifications: â¢ After the ninth reindeer arrives, Santa must invoke prepareSleigh, and then all nine reindeer must invoke getHitched. â¢ After the third elf arrives, Santa must invoke helpElves. Concurrently, all three elves should invoke getHelp. â¢ All three elves must invoke getHelp before any additional elves enter (increment the elf counter). Santa should run in a loop so he can help many sets of elves. We can assume that there are exactly 9 reindeer, but there may be any number of elves."
257,133,0.225,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","Likewise, a rescheduling function is an â-rescheduler iff it does not distinguish message pools and heaps that are â-indistinguishable and only returns â-schedulers: â-level (Î¨ ) â âsf , M1 , M2 , Ï1 , Ï2 . â-level (Ï1 (Î¨ (sf , M1 , Ï1 ))) â§ (M1 â¼âÎ M2 â§ Ï1 â¼âÎ Ï2 â Î¨ (sf , M1 , Ï1 ) = Î¨ (sf , M2 , Ï2 )) where Ï1 is a projection to the first component of the triple. Definition 1 (Security). A thread pool T satisfies non-interference at attacker level âA and security typing Î iff all fully-reduced executions from âA -related initial heaps (starting with empty message pools) reduce to âA -related terminal heaps, for all âA -level schedulers sf and reschedulers Î¨ : âÏ1 , Ï2 , Ïâ²1 , Ïâ²2 â Heap. âM1â² , M2â² â MPool. âS, S1â² , S2â² â S. âT1â² , T2â² . âsf , sf â²1 , sf â²2 . âA -level (sf ) â§ âA -level (Î¨ ) â§ Ï1 â¼âÎA Ï2 â§ final(T1â² ) â§ final(T2â² ) â§ sf , S, T, Î»ch.0, Ï1 âââÎ¨ sf â²1 , S1â² , T1â² , M1â² , Ïâ²1 â§ sf , S, T, Î»ch.0, Ï2 âââÎ¨ sf â²2 , S2â² , T2â² , M2â² , Ïâ²2 â M1â² â¼âÎA M2â² â§ Ïâ²1 â¼âÎA Ïâ²2"
270,1,0.225,The Huawei and Snowden Questions : Can Electronic Equipment From Untrusted Vendors Be Verified? Can An Untrusted Vendor Build Trust into Electronic Equipment? (Volume 4.0),"Dear reader, Our aim with the series Simula SpringerBriefs on Computing is to provide compact introductions to selected ï¬elds of computing. Entering a new ï¬eld of research can be quite demanding for graduate students, postdocs and experienced researchers alike: the process often involves reading hundreds of papers, and the methods, results and notation styles used often vary considerably, which makes for a time-consuming and potentially frustrating experience. The briefs in this series are meant to ease the process by introducing and explaining important concepts and theories in a relatively narrow ï¬eld, and by posing critical questions on the fundamentals of that ï¬eld. A typical brief in this series should be around 100 pages and should be well suited as material for a research seminar in a well-deï¬ned and limited area of computing. We have decided to publish all items in this series under the SpringerOpen framework, as this will allow authors to use the series to publish an initial version of their manuscript that could subsequently evolve into a full-scale book on a broader theme. Since the briefs are freely available online, the authors will not receive any direct income from the sales; however, remuneration is provided for every completed manuscript. Briefs are written on the basis of an invitation from a member of the editorial board. Suggestions for possible topics are most welcome and can be sent to aslak@simula.no. Springer Heidelberg, Germany January 2016"
283,509,0.225,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","0, 1, 2, 5, 7, 9, 10 1, 2, 4, 9, 10 1, 6, 8, 9, 10 0, 1, 2, 4, 5, 6, 8 5, 6, 9 2, 3, 6, 7, 10 0, 2, 6, 9, 10 0, 5, 8, 9, 10 0, 1, 2, 5, 6, 9, 10 0, 3, 4, 6, 7, 8, 9 0, 2, 5, 6, 7, 8, 10 0, 1, 2, 4, 5, 8, 9 1, 2, 3, 8, 9 3, 4, 5, 7, 8 2, 7, 8 0, 1, 3, 5, 8 3, 5, 10 2, 3, 4, 7, 10 0, 4, 6, 7, 8, 9, 10 0, 1, 3, 4, 6, 7, 10 456 0, 1, 2, 4, 6, 7, 8, 9, 10 0, 2, 4, 5, 8, 9, 10"
165,562,0.225,New Methods for Measuring and Analyzing Segregation,"YW â YB = Î£ pw i â RRTi â Î£ pb i â RRTi = ( Q + P 2 ) â ( Q 2 ) = Q 2 + P 2 = ( Q + P ) 2 = 0.5 . In light of these points, Expression (C.10) can now be understood as follows. The values of P/2 and Q in the term ( Î£ pw i â RRTi â P 2 ) / Q rescale the value of the core term Î£ pw i â RRTi used in computing YW in YW â YB to map its position in the logical range of 0.5 to ( Q + P 2 ) onto the correct position in the logical range of 0.5 to 1.0 for the parallel core term Î£ pw i â RRBi used in computing G. Similarly, the values of Q/2 and P in the term ( Î£ pb i â RRTi â Q 2 ) / P rescale the core term Î£ pb i â RRTi used in computing YB in YW â YB to map its position in the logical range of Q/2 to 0.5 onto the correct position in the logical range of 0.0 to 0.5 for the parallel core term Î£ pb i â RRWi used in computing G. Expression (C.11) can be interpreted in a similar way. P/2 and Q in the term ( Q â Î£ pw i â RRBi â P 2 ) / Q rescale the core term Î£ pw i â RRBi used in computing G to map its position in the logical range of 0.5 to 1.0 on to the correct position in the logical range of 0.5 to Q + P 2 for the core term Î£ pw i â RRTi used in computing YW â YB . Similarly, Q/2 and P in the term ( P â Î£ pb i â RRWi â Q 2 ) / P rescale the core term Î£ pb i â RRWi used in computing G to map its position in the logical range of 0.0 to 0.5 onto the correct position in the logical range of Q/2 to 0.5 for the core term Î£ pb i â RRTi used in computing YW â YB ."
294,116,0.225,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"This call makes range return the integers from (and including) start, up to (but excluding!) stop, in steps of step. Note here that stop-1 is the last integer included. With range, the previous example would rather read for i in range(3, 8, 1): print i**2"
385,348,0.224,Advanced R,"The key idea is function composition. Take two simple functions, one which does something to every column and one which ï¬xes missing values, and combine them to ï¬x missing values in every column. Writing"
291,47,0.224,Visualizing Mortality Dynamics in the Lexis Diagram (Volume 44.0),"3.1.3 SEER Cancer Register Data 1973â2011 The Surveillance, Epidemiology, and End Results (SEER) Program of the National Cancer Institute of the United States allows researchers access to longitudinal data on the individual level about the incidence of cancer and includes also information about the survival of patients. The data coverageâthe SEER data start in 1973âand the large size of data, combined with the ease of access, make the SEER data an ideal instrument for the analysis of cancer survival by age over calendar time. We were using data that were released in April 2014 with a follow-up cutoff date of December 31, 2011 (Surveillance, Epidemiology, and End Results (SEER) Program 2014). The SEER data do not cover all cancer diagnoses of the United States. It is a collection of data from several registries. With the exception of Seattle (Puget Sound) and Metropolitan Atlanta that started in 1974 and 1975, respectively, we only used registers that covered the whole time span from 1973 until the end of 2011. Although we use less data than we could have, we thought that a heterogeneous set of registers would have induced problems for the analysis over time. The registers included in our analysis were: San Francisco-Oakland SMSA, Connecticut, Metropolitan Detroit, Hawaii, Iowa, New Mexico, Utah as well as Seattle and Metropolitan Atlanta. In our analysis of cancer survival in Chap. 10, starting on page 123, we selected five cancer sites: Breast cancer; cancer of the lung and bronchus; cancer of the colon, rectum, and anus; pancreatic cancer; prostate cancer. As shown in Table 3.3, those five cancer sites constitute about 55% of all cancer diagnoses for women as well as for men out of the 4.5 million cases recorded during our observation period. The largest categories are by far breast cancer for women (30.44%) and prostate cancer for men (25.79%). The absolute and relative frequencies of the other cancer sites as well as their respective ICD codes can be inspected from Table 3.3. While ICD-8 was in use at the beginning of the observation period in 1973 and cancer cases are typically coded by the ICD-O standard, all ICD codes were converted to ICD-10 by SEER."
283,540,0.224,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","0, 4, 5, 8 1, 2, 3, 4, 5, 6, 7, 8 1, 2, 5, 6, 7, 8 0, 1, 4, 7, 8, 10 0, 2, 3, 6, 7, 10 0, 4, 6, 8, 9, 10 0, 1, 3, 6, 8, 10 2, 4, 8, 9 44 1, 2, 3, 4, 5, 6, 7, 10 2, 4, 5, 9 2, 3, 4, 6, 7, 8 0, 1, 2, 3, 4, 6 0, 3, 7, 9 6, 8 2, 4, 5, 6, 7, 10 0, 1, 4, 8, 9, 10 2, 5, 8, 10 1, 2, 4, 5, 6, 8 1, 8, 9, 10 3, 4, 5, 6, 7, 8 104 0, 1, 3, 4, 5, 6, 7, 9 2, 6, 7, 8 1, 3, 5, 10 119 0, 1, 2, 4, 6, 7, 8, 9"
105,57,0.224,"Computer and information Sciences : 31St international Symposium, Iscis 2016, KrakÃ³w, Poland, October 27â28, 2016, Proceedings (Volume 659.0)","References 1. Adriaensen, S., Ochoa, G., NoweÌ, A.: A benchmark set extension and comparative study for the hyflex framework. In: IEEE Congress on Evolutionary Computation, CEC 2015, 25â28 May 2015, Sendai, Japan, pp. 784â791 (2015) 2. Alkan, A., OÌzcan, E.: Memetic algorithms for timetabling. In: The 2003 Congress on Evolutionary Computation, CEC 2003, vol. 3, pp. 1796â1802. IEEE (2003) 3. Burke, E.K., Gendreau, M., Hyde, M., Kendall, G., McCollum, B., Ochoa, G., Parkes, A.J., Petrovic, S.: The cross-domain heuristic search challenge â an international research competition. In: Coello, C.A.C. (ed.) LION 2011. LNCS, vol. 6683, pp. 631â634. Springer, Heidelberg (2011) 4. Burke, E.K., Gendreau, M., Hyde, M., Kendall, G., Ochoa, G., OÌzcan, E., Qu, R.: Hyper-heuristics: a survey of the state of the art. J. Oper. Res. Soc. 64(12), 1695â1724 (2013) 5. Cowling, P.I., Kendall, G., Soubeiga, E.: A hyperheuristic approach to scheduling a sales summit. In: Burke, E., Erben, W. (eds.) PATAT 2000. LNCS, vol. 2079, p. 176. Springer, Heidelberg (2001) 6. Gendreau, M., Potvin, J.Y.: Metaheuristics in combinatorial optimization. Ann. Oper. Res. 140(1), 189â213 (2005) 7. GuÌmuÌsÌ§, D.B., OÌzcan, E., Atkin, J.: An investigation of tuning a memetic algorithm for cross-domain search. In: 2016 IEEE Congress on Evolutionary Computation (CEC). IEEE (2016) 8. Ishibuchi, H., Kaige, S.: Implementation of simple multiobjective memetic algorithms and its applications to knapsack problems. Int. J. Hybrid Intell. Syst. 1(1), 22â35 (2004) 9. Kheiri, A., OÌzcan, E.: An iterated multi-stage selection hyper-heuristic. Eur. J. Oper. Res. 250(1), 77â90 (2015) 10. Krasnogor, N., Smith, J., et al.: A memetic algorithm with self-adaptive local search: TsP as a case study. In: GECCO, pp. 987â994 (2000) 11. Merz, P., Freisleben, B.: A comparison of memetic algorithms, tabu search, and ant colonies for the quadratic assignment problem. In: Proceedings of the 1999 Congress on Evolutionary Computation, CEC 1999, vol. 3, pp. 2063â2070 (1999)"
385,399,0.224,Advanced R,"From this code, you can see that lapply() is a wrapper for a common for loop pattern: create a container for output, apply f() to each component of a list, and ï¬ll the container with the results. All other for loop functionals are variations on this theme: they simply use diï¬erent types of input or output. lapply() makes it easier to work with lists by eliminating much of the"
385,338,0.224,Advanced R,"However, the function is not robust to unusual inputs. Look at the following results, decide which ones are incorrect, and modify col_means() to be more robust. (Hint: there are two function calls in col_means() that are particularly prone to problems.) col_means(mtcars) col_means(mtcars[, 0]) col_means(mtcars[0, ]) col_means(mtcars[, ""mpg"", drop = F]) col_means(1:10) col_means(as.matrix(mtcars)) col_means(as.list(mtcars)) mtcars2 <- mtcars mtcars2[-1] <- lapply(mtcars2[-1], as.character) col_means(mtcars2)"
34,1045,0.224,"Habitats and Biota of the Gulf of Mexico: Before the Deepwater Horizon Oil Spill: Volume 2: Fish Resources, Fisheries, Sea Turtles, Avian Resources, Marine Mammals, Diseases and Mortalities","Niven and Butcher (2011) examined the status and trends of birds wintering along the U.S. northern Gulf of Mexico using the Audubon Christmas Counts from 1965 to 2011. Methods are described in the Methods section above. Their initial goal was to examine trends in light of the Deepwater Horizon oil spill, but there was not enough time between the spill and the counts to reflect the effects, if any, from the oil spill. To be on the conservative side, in Table 12.17 only the species with a significant decline of more than 2 % per year, and the species with a significant increase of over 2 %, are listed."
175,346,0.224,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","begin to repeat themselves once a steady-state condition has been reached. Once reached, the storage volumes and releases will be the same each year (since the inflows are the same). These storage volumes are denoted as a blue line on the rule curve shown in Fig. 4.18. The annual total squared deviations will also be the same each year. As seen in Table 4.21, this annual minimum weighted sum of squared deviations for this example equals 186. This is what would be observed if the inflows assumed for this analysis repeated themselves. Note from Tables 4.12, 4.13, 4.14, 4.15 and 4.16, 4.17, 4.18, 4.19 that once the steady-state"
166,306,0.224,Surveying Human Vulnerabilities across the Life Course (Volume 3.0),3 Memory Bias as a Hidden Resource for Understanding Life Histories So far we have discussed how LHCs help to reduce the probability of memory biases and misreporting of dates. A provocative alternative approach is not to consider errors as a problem for data analysis but as an important source of information in their own right. According to CouppiÃ© and DemaziÃ¨re (1995) memory errors are not distributed at random but depend on specific social variables that may be connected
283,152,0.224,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","0, 5, 7, 9, 35 5, 9, 21, 35, 49, 63 5, 7, 9, 21, 35 0, 5, 7, 9, 21, 35 5, 7, 9, 21, 35, 49 0, 5, 7, 9, 21, 35, 49 0, 5, 7, 9, 21, 35, 63 5, 7, 9, 21, 35, 49, 63 0, 5, 7, 9, 21, 35, 49, 63 1, 3, 9, 49 0, 1, 3, 9, 49 0, 1, 5, 21 1, 3, 9, 21, 49 1, 3, 9, 35 0, 1, 3, 9, 35 1, 3, 9, 35, 49 0, 1, 3, 9, 35, 49 0, 1, 3, 9, 21, 35 1, 3, 9, 21, 35, 49 1, 3, 7, 9, 35 0, 1, 3, 7, 9, 35 1, 3, 9, 21, 35, 49, 63 n = 151, m(x) = 166761, NC = 212 1, 5, 15, 37"
217,141,0.223,Finite Difference Computing With Pdes : a Modern Software Approach,"Another function, demo, visualizes the difference between the Euler-Cromer scheme and the scheme (1.7) for the second-oder ODE, arising from the mismatch in the first time level. Using Odespy The Euler-Cromer method is also available in the Odespy package. The important thing to remember, when using this implementation, is that we must order the unknowns as v and u, so the u vector at each time level consists of the velocity v as first component and the displacement u as second component: # Define ODE def f(u, t, w=1): v, u = u return [-w**2*u, v] # Initialize solver I = 1 w = 2*np.pi import odespy solver = odespy.EulerCromer(f, f_kwargs={âwâ: w}) solver.set_initial_condition([0, I]) # Compute time mesh P = 2*np.pi/w # duration of one period dt = P/timesteps_per_period Nt = num_periods*timesteps_per_period T = Nt*dt import numpy as np t_mesh = np.linspace(0, T, Nt+1) # Solve ODE u, t = solver.solve(t_mesh) u = u[:,1] # Extract displacement"
