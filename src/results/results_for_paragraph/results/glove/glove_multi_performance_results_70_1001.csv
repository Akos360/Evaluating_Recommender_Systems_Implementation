book_index,paragraph_index,similarity_score,recommended_book,recommended_text
50,116,0.993,"Seeing Ourselves Through Technology : How We Use Selfies, Blogs and Wearable Devices To See and Shape Ourselves","traces carries with it more uncertainty and subjectivity than our English data. We do not take the traces of a person (footsteps in the snow, steps measured by a Fitbit) as being the same as the person herself. Or as Drucker (2011) puts it, Rendering observation (the act of creating a statistical, empirical, or subjective account or image) as if it were the same as the phenomena observed collapses the critical distance between the phenomenal world and its interpretation, undoing the basis of interpretation on which humanistic knowledge production is based."
192,425,0.992,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"Above the bar (on the manifest level) the discourse of the analyst first of all addresses issues of object choice (a in the upper-left position). Why do researchers focus on and respond to this particular object of research? As a rule, object choice is a matter of âdisplacementâ (Verschiebung). Instead of on the object of desire as such, researchers focus on âsomething elseâ, something which seems more neutral, but which is nonetheless somehow connected with the object a (the object of desire) which may suddenly reveal itself, coming into view as an alluring substitute. In Carmen, the archaeologist initially focusses his cupido sciendi on archaeological"
305,8,0.992,Quantum Computing for Everyone,"All computations involve inputting data, manipulating it according to certain rules, and then outputting the final answer. For classical computations, the bit is the basic unit of data. For quantum computations, this unit is the quantum bitâusually shortened to qubit. A classical bit corresponds to one of two alternatives. Anything that can be in exactly one of two states can represent a bit. Later we will see various examples, which include the truth or falsity of a logical statement, a switch being in the on or off position, and even the presence or absence of a billiard ball. A qubit, like a bit, includes these two alternatives, butâquite unlike a bitâit can also be in a combination of these two states. What does this mean? What exactly is a combination of two states, and what are physical objects that can represent qubits? What is the quantum computation analog to the switch? A qubit can be represented by the spin of an electron or the polarization of a photon. This, though true, does not seem particularly helpful as spins of electrons and polarizations of photons are not things that most of us have knowledge about, let alone experience with. Letâs start with a basic introduction to describe spin and polarization. To do this we describe the foundational experiment performed by Otto Stern and Walther Gerlach on the spin of silver atoms. In 1922, Niels Bohrâs planetary model described the current understanding of atoms. In this model an atom consisted of a positive nucleus orbited by negative electrons. These orbits were circular and were constrained to certain radii. The innermost orbit could contain at most two electrons. Once this was filled, electrons would start filling the next level, where at most eight electrons could be held. Silver atoms have 47 electrons. Two of"
192,60,0.992,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"researchers may become the victims of science as well: tormented, craving subjects (Mayer, Cantor, etc.: $), who suffer from experiences of crisis, comparable to how Oedipus fell victim to a political crisis in mythic times, but at the same time adding something to the oedipal scenario (Lacan 1966, p. 870), because for Lacan the subject of science introduces a new type of subjectivity (S2), as we have seen. The âsubjective dramaâ of scientific progress can be represented by the formula S2 â $ which points to the subversion of the impassive, rational agent (S2) and the resurgence of the divided, tormented subject ($). Whereas the ancient worldview concurred with the concept of the Platonic sphere, and the modern worldview developed on the basis of the Cartesian coordinate system, the question inevitably emerges what mathematical concept represents the basic topology (the basic spatiality and subjectivity) of the current era, which began in 1900? For Lacan, the basic topological structure which exemplifies contemporary scientific subjectivity is the Moebius ring. Psychoanalysis is not depth psychology, he argues, and the unconscious is not a hidden animalistic or archetypal depth of an allegedly rational conscious subject. Rather, the unconscious is the reverse side of consciousness. In the case of a Moebius ring, although there is only one surface, there is always a reverse side, a blind spot, a missing part. But once we get there, the opposite side is lost to us again, for there is always a reverse (1962â 1963/2004, p. 161). We will never reach a position of absolute knowledge, and the gap between knowledge and truth cannot be sutured. The subject of science is constituted by this split, this rupture between (partial) knowledge and (unattainable) truth (1961â1962, p. 48, p. 87, p. 189, p. 199) and this creative failure or impotence (dÃ©faillance, âÏ) fuels rather than discourages the will to know (1968â1969/2006, p. 275)."
124,8,0.991,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"2.1 Branching Time (BT) It is a perennial question of philosophy whether the future is open, what that question means, and what a positive or a negative answer to it would signify for us. The question has arisen in many different contextsâin science, metaphysics, theology, philosophy of language, philosophy of science, and in logic. The logical issue is not so much to provide an answer to the question about the openness of the future, nor primarily about its meaning and significance, but about the proper formal modeling of an open future: How can time and possibility be represented in a unified way? Thus clarified, the logical question of the open future is first and foremost one of providing a useful formal framework within which the philosophical issue of multiple future possibilities can be discussed. In the light of twentieth century developments in modal and temporal logic, that logical question is one about a specific kind of possibility arising out of the interaction of time and modality. That kind of possibility may be called historical possibility or, in the terminology that Belnap favors, real possibility. A formal framework for real possibility must combine in a unified way a representation of past and future, as in temporal logic (tense logic), and of possibility and necessity, as in modal logic. That combination is not just interesting from a logical point of viewâit is also of broader philosophical significance. To mention one salient example, the interaction of time and modality reflects the loss of possibilities over time that seems central to our commonsense idea of agency. Working on his project of tense logic, Arthur Prior devoted his first book-length study to the topic of Time and modality (Prior 1957). A leading idea was that temporal possibility should somehow be grounded in truth at some future time, where time is depicted as linearly ordered. In 1958, Saul Kripke suggested a different formal framework, making use of partial orderings of moments. His exchange with Prior is documented in Ploug and ÃhrstrÃ¸m (2012). The leading idea, which Prior took up and developed in his later book, Past, present and future (Prior 1967), was that the openness of the future should be modeled via a tree of histories (or chronicles) branching into the future. In terms of the partial ordering of moments m, a history h is a maximal chain (a maximal linearly ordered subset) in the orderingâgraphically, one complete branch of the tree, representing a complete possible course of events from the beginning till the end of time (see Fig. 1). If the future is not open, all possible moments are linearly ordered, and there is just one history; if the future is open, however, the possible moments form a partial ordering in which there are multiple histories. In that case, we can say that there are incompatible possibilities for the same clock time (or for the same instant, i), which lie on different histories. Tomorrow, as Aristotleâs famous example goes, there could be a sea-battle, or there could be none, and nothing yet decides between these two future possibilities."
360,126,0.991,Compositionality and Concepts in Linguistics and Psychology,"7.1 Objectivist Two-Tiered Theories Some apparently two-tiered theories are actually theories within an Objectivist framework. Kaplan (1989), for instance, distinguishes âcontext of utteranceâ and âcircumstance of utteranceâ, as well as character and content. A way of putting his distinctions is that the character of an utterance is âthe meaning without taking into account the context of utteranceâ or perhaps âthe meaning in the languageâ. In any particular context of utterance, the values of the various pronouns, demonstratives, times, etc., that occur in the utterance are fixed, and then the evaluation of the character at that context will yield a content. Now using this content, we can evaluate it at a particular âcircumstanceâ (maybe a particular possible world, such as the actual one) and the result will be a truth value. Kaplanâs motivation for the distinction between character and content involves the interpretation of pronouns and other indexicals (perhaps extended to predicates and the like), and not, generally speaking, the Subjectivist-oriented concerns about the structure of concepts and how they lead to beliefs, desires, and other features that guide behaviour. So this picture plainly has both tiers working on the Objectivist side of meaning: once the values of the contextual variables are determined, the resulting semantic content is pretty much what one would expect from an Objectivistâa function on a possible world or âcircumstanceâ to a truth condition. The semantic value of the character of a sentence is a function whose value at any context is the propositional content at that context. That is, a function that takes contexts as arguments and returns content, content being a truth conditional statement, namely a function from contexts and circumstances to some truth value. (See WesterstÃ¥hl (2012) for how to make Kaplanâs theory be compositional.) Another theory that comes from the Objectivist side is Fregeâs, although perhaps the undeveloped Subjectivist side perhaps could be developed in some nonObjectivist way. Fregeâs semantic theory of meaning (Bedeutung) and sense (Sinn) contains a subtheory which has certain explanatory powers relevant to a two-tiered account, even though it has its roots in an Objectivist framework.35 In the objective, physical realm are the usual members of the physical universe of ordinary (and scientifically discoverable) objects. In the objective, abstract realm are not only such items as numbers, but also concepts and functionsâtaken to be the referents of predicates, relations, and operations on objects. Singular terms of language mean (that is, bedeuten or denote) the ordinary objects and the number-like items. Monadic predicates mean the concepts, relational expressions mean the relations, and function terms mean the functionsâall members of the abstract realm. A person can graspâa technical termâthese items in the abstract realm and thereby consider and possibly understand them (in the ordinary senses of these terms). But this is always done under some mode of presentation of the thing thus grasped. Frege insists on this way of putting how objects are grasped under a mode of presentation because of his realization that one needs somehow to distinguish cog35 Well, taking Fregeâs view of the âabstractâ, non-physical, non-mental, âthird-realmâ repository"
192,407,0.991,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"While university discourse gives the floor to the qualified, replaceable subject (the academic expert in the upper-left position), in the discourse of the analyst the qualified person (S2) is literally called into question, by someone who is âqualifiedâ, but not in the usual (university discourse kind of) way (Lacan 1966, p. 794). Psychoanalysis is not a science, but rather a practice in which the subjects of science are questioned, not as carriers of knowledge (S2), but as craving and deflecting subjects ($). In comparison to the discourse of the hysteric, however, rather than simply allowing the craving/deflecting subject to take the floor so as to challenge and provoke the establishment (as happens in The Fiction Factory for instance), the craving subjects are questioned, and the question basically is: what is their desire, what is the object a (the object of desire) that is driving, haunting and addressing them, what is it that they want? The dynamical interaction between a and $ (rather than"
235,281,0.991,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","that âthe measurement creates the outcome which is indeterminate before.â But this is a rather trivial statement expressing the fact that the outside environment with its supposedly huge number of degrees of freedom, in particular also the measurement device, has contributed to the outcome. The authorâs impression is that Bohr and his followers may never have understood the true reason for value indefiniteness: the scarcity and constancy of information encoded into the quantum state; and the entanglement across the Heisenberg cut between object and measurement device. This scarcity also shows up in âstaticâ KochenâSpecker type theorems [6, 314, 401] expressing the fact that only a single maximal observable or context is defined at any time."
8,406,0.991,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","18.3 From Distinguishable Hadrons to SBM The beginning of a new idea in physics often seems to hang on a very fine thread: was anything lost when âThermodynamics of Distinguishable Particlesâ remained unpublished? And what would Hagedorn do after withdrawing his first limitingtemperature paper? My discussion of the matter with Hagedorn suggests that his vision at the time of how limiting temperature could be justified evolved very rapidly. Presenting his final insight was what interested Hagedorn and motivated his work. Therefore, he opted to work on the more complete theoretical model. While the withdrawal of the old, and the preparation of an entirely new paper seemed to be the right path to properly represent the evolving scientific understanding, todayâs perspective is different. In particular the insight that the appearance of a large number of different hadronic states allows to effectively side-step the quantum physics nature of particles within statistical physics became essentially invisible in the ensuing work. Few scientists realize that this is a key property in the SBM, and the fundamental cause allowing the energy content to increase without an increase in temperature, as Hagedorn explains in the withdrawal note, see also the end of Sects. 17.2 and 19.1. The loss of relevance of quantum physics in hot hadronic matter is the scientific fact that we lost sight of after âDistinguishable Particlesâ was withdrawn. To the best of my knowledge the dense, strongly interacting hadronic gas is the only physical system where this happens. Normally, the greater the density of particles, the greater the role of quantum physics. After surfacing briefly in Hagedornâs withdrawn âThermodynamics of Distinguishable Particlesâ paper, this finding faded from view. This indeed was a new idea in physics hanging on a very fine thread which ripped. On the other hand, the Hagedorn limiting temperature lived on. Within a span of only 90 days between the withdrawal of his manuscript, and the date of his new CERN-TH preprint, Hagedorn formulated the Statistical Bootstrap Model. Its salient feature is that the exponential mass spectrum arises from the principle that"
305,94,0.991,Quantum Computing for Everyone,"The easiest way to remember this is by the following construction. ï£® ï£®b0 ï£¹ ï£¹ ï£®a0 b0 ï£¹ ï£¯a0 ï£¯ ï£º ï£º ï£¯ ï£® ï£¹ ï£® ï£¹ ï£¯ ï£° b1 ï£» ï£º ï£¯ a0 b1 ï£º ï£¯a ï£º ï£¯b ï£º ï£¯ ï£®b0 ï£¹ ï£º ï£¯ a1b0 ï£º ï£° 1ï£» ï£° 1ï£» ï£¯ 1 ï£¯ ï£ºï£º ï£¯ ï£° ï£° b1 ï£» ï£» ï£° a1b1 ï£» Notice also that the subscripts follow the standard binary ordering: 00, 01, 10, 11. How Do You Entangle Qubits? This book is about the mathematics that underlies quantum computing. It is not about how to physically create a quantum computer. We are not going to spend much time on the details of physical experiments, but the question of how physicists create entangled particles is such an important one that we will briefly address it. We can represent entangled qubits by either entangled photons or electrons. Though we often say the particles are entangled, what we really mean is that the vector describing their states, a tensor in ï2 â ï2 , is entangled. The actual particles are separate and, as we have just noted, can be very far apart. That said, the question remains: How do you go about creating a pair of particles whose state vector is entangled? First, we look at how physical experiments create entangled particles. Then we look at how quantum gates create entangled qubits. The most commonly used method at this time involves photons. The process is called spontaneous parametric down-conversion. A laser beam sends photons through a special crystal. Most of the photons just pass through, but some photons split into two. Energy and momentum must be conservedâthe total energy and momentum of the two resulting photons must equal the energy and momentum of the initial photon. The conservation laws guarantee that the state describing the polarization of the two photons is entangled. In the universe, electrons are often entangled. At the start of the book we described Stern and Gerlachâs experiment on silver atoms. Recall that"
235,24,0.991,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","A typical example for a successful application of Descartesâ fifth and thirteenth rule is the method of separation of variables for solving differential equations [204]. For instance, SchrÃ¶dinger, by his own account [450] with the help of Weyl, obtained the complete solutions of the SchrÃ¶dinger equation for the hydrogen atom by separating the angular from the radial parts, solving them individually, and finally multiplying the separate solutions. So it seems that more fundamental microphysical theories should always be preferred over phenomenological ones. Yet, good arguments exist that this is not always a viable strategy. Anderson, for instance, points out [13] that âthe ability to reduce everything to simple fundamental laws does not imply the ability to start from those laws and reconstruct the universe. . . . The constructionist hypothesis breaks down when confronted with the twin difficulties of scale and complexity. The behaviour of large and complex aggregates of elementary particles, it turns out, is not to be understood in terms of a simple extrapolation of the properties of a few particles. Instead, at each level of complexity entirely new properties appear, and the understanding of the new behaviours requires research which I think is as fundamental in its nature as any other.â One pointy statement of Maxwell was related to his treatment of gas dynamics, in particular by taking only the mean values of quantities involved, as well as his implicit assumption that the distribution of velocities of gas molecules is continuous [234, p. 422]: âBut I carefully abstain from asking the molecules which enter where they last started from. I only count them and register their mean velocities, avoiding all personal enquiries which would only get me into trouble.â Pattee argues that a hierarchy theory with at least two levels of description might be necessary to represent these conundra [384, p. 117]: âThis is the same conceptual problem that has troubled physicists for so long with respect to irreversibility. How can a dynamical system governed deterministically by time-symmetric equations of motion exhibit irreversible behaviour? And of course there is the same conceptual difficulty in the old problem of free will: How can we be governed by inexorable natural laws and still choose to do whatever we wish? These questions appear paradoxical only in the context of single-level descriptions. If we assume one dynamical law of motion that is time reversible, then there is no way that elaborating more and more complex systems will produce irreversibility under this single dynamical description. I strongly suspect that this simple fact is at the root of the measurement problem in quantum theory, in which the reversible dynamical laws cannot be used to describe the measurement process. This argument is also very closely related to the logicianâs argument that any description of the truth of a symbolic statement must be in a richer metalanguage (i.e., more alternatives) than the language in which the proposition itself is stated.â StÃ¶ltzner and Thirring [489, 493, 529], in discussing Heisenbergâs Urgleichung, which today is often referred to as Theory of Everything [34], at the top level of a âpyramid of laws,â suggest three theses related to a âbreakdownâ to lower, phenomenologic, levels: â(i) The laws of any lower level . . . are not completely determined by the laws of the upper level though they do not contradict them. However, what looks like a fundamental fact at some level may seem purely accidental when looked at"
8,305,0.991,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","16.3 Is the Question About the âFinal Building Blockâ Meaningless? There is the final question that remains: suppose, that everything were correct; there is an infinite number and an exponential mass spectrum of new types of particles and a corresponding limiting temperatureâwhat does that have to do with the here presented end situation, which nevertheless does not mean an end? Here we enter into a theoretical construction wherein one abstracts a general rule from a limited number of experimental data, which is then tentatively postulated as a universal principle. This introduces us to the usual practical circumstance of theoretical physics: we have a model whose other properties are analytically derived using established methods of mathematics and the assumptions that generally apply to the already known laws of nature. In this way we obtain experimentally testable predictions as derived from known or later verifiable behavior. Agreement of these predictions with the facts is necessary, but not sufficient, to ensure that the theoretical model is correct. This applies especially to the model I will now describe. In order to introduce the model in words, I will characterize the situation far less exactly than the technical tools of theoretical physics would allow me to do this. I proceed in this way as I seek at all cost to avoid technical jargon. In a high-energy collision new material particles are copiously produced (events with a multiplicity of a hundred or more have been observed). In our terminology, these particles emerge from the collision-produced boiling primordial matter. In a certain and physically quite precise sense they were all contained in this piece of boiling primal matter. Taking one of these newly generated particles under the microscope (which is not easy: lifespan '10 23 s), we observe that it behaves itself as boiling primordial matter; namely it can decay further into many particles. The greater its mass, the greater is this tendency. Such a particle with a large mass thus has a dual nature: on the one hand, it can be used as an âelementary particleâ contributing to radiative energy equilibrium, on the other hand it can itself create other âelementary particlesâ which contribute to the radiative energy equilibrium. Seen from this perspective, none of these produced particle types can be viewed as an elementary particle, given that other particles can emanate from any of the produced particles, which are again no more elementary since each can be simultaneously created out of the other, and in this way all these particles have undetermined building block composition. Nothing in this picture changes if one day quarks should be confirmed as the primordial building blocks. In our approach they would play a preferential role, being the stuff from which âeverything is built.â As an aside, it is the virtue of our approach that the statement âcomposed ofâ does not characterize the number and the character of the fundamental building blocks. The composition and nature of the source of produced particles can remain cloaked in mystery; it can remain undetermined. The model aims to overcome the limited number of presently known types of particles by continuing the observed behavior of the mass spectrum at low mass to higher mass, (where we experimentally know nothing yet). Once this is"
275,538,0.991,Foundations of Trusted Autonomy,"is a surprising deviation from this knowledge and belief.8 In short, what the creative machine does is perform an action that, relative to the knowledge, beliefs, desires, and expectations of the agents composing its audience, is a surprise.9 We refer to this generic, underlying form of creativity as theory-of-mind-creativity. Our terminology reflects that for one agent to have a âtheory of mindâ of another agent is for the first agent to have beliefs (etc.) about the beliefs of another agent. An early, if not the first, use of the phrase âtheory of mindâ in this sense can be found in [39] â but there the discussion is non-computational, based as it is on experimental psychology, entirely separate from AI. Early modeling of a classic theory-of-mind experiment in psychology, using the tools of logicist AI, can be found in [3]. For a presentation of an approach to achieving literary creativity specifically by performing actions that manipulate the intensional attitudes of readers, including actions that specifically violate what readers believe is going to happen, see [23]."
84,72,0.991,Eye Tracking Methodology,"stage dictionary units then process weakened and unweakened messages. These units contain variable thresholds tuned to importance, relevance, and context. Treisman thus brought together the complementary models of attentional unit or selective filter (the âwhereâ), and expectation (the âwhatâ). Up to this point, even though Treisman provided a convincing theory of visual attention, a key problem remained, referred to as the scene integration problem. The scene integration problem poses the following question: even though we may view the visual scene through something like a selective filter, which is limited in its scope, how is it that we can piece together in our minds a fairly coherent scene of the entire visual field? For example, when looking at a group of people in a room such as in a classroom or at a party, even though it is impossible to gain a detailed view of everyoneâs face at the same time, nevertheless it is possible to assemble a mental picture of where people are located. Our brains are capable of putting together this mental picture even though the selective filter of vision prevents us from physically doing so in one glance. Another well-known example of the scene integration problem is the Kanizsa (1976) illusion, exemplified in Fig. 1.1, named after the person who invented it. Inspecting Fig. 1.1, you will see the edges of a triangle, even though the triangle is defined only by the notches in the disks. How this triangle is integrated by the brain is not yet fully understood. That is, although it is known that the scene is inspected piecemeal as evidenced by the movement of the eyes, it is not clear how the âbig pictureâ is assembled, or integrated, in the mind. This is the crux of the scene integration problem. In one view, offered by the Gestalt psychologists, it is hypothesized that recognition of the entire scene is performed by a parallel one-step process. To examine this hypothesis, a visualization of how a person views such an image (or any other) is particularly helpful. This is the motivation for recording and visualizing a viewerâs eye movements. Even though the investigation of eye movements dates back to 1907 (Dodge 1907),1 a clear depiction of eye movements would not be available until 1967 (see Chap. 5 for a survey of eye tracking techniques). This early eye movement visualization, discussed below, shows the importance of eye movement recording not only for its expressive power of depicting oneâs visual scanning characteristics, but also for its influence on theories of visual attention and perception."
305,18,0.991,Quantum Computing for Everyone,"each time we do it the initial conditions vary slightly. These slight variations can change the outcome from heads to tails and vice versa. There is no real randomness in classical mechanics, just what is often called sensitive dependence to initial conditionsâa small change in the input can get amplified and produce an entirely different outcome. The underlying idea concerning randomness in quantum mechanics is different. The randomness is true randomness. The sequence NSSNNNSS â¦ that we obtained from measuring spin in two directions is considered to be truly random, as we shall see. The sequence of coin tosses, HTTHHHTT â¦ appears random, but the classical laws of physics are deterministic and this apparent randomness would disappear if we could make our measurements with infinite accuracy. At this stage it is natural to question this. Einstein certainly did not like this interpretation, famously saying that God does not play dice. Couldnât there be a deeper theory? If we knew more information about the initial configurations of our electrons, couldnât it be the case that the final results would no longer be random but completely determined? Couldnât there be hidden variablesâonce we know the values of these variables, the apparent randomness disappears? In what follows we will present the mathematical theory in which true randomness is used. Later we will return to these questions. We will describe a clever experiment to distinguish between the hidden variable and the true randomness hypotheses. This experiment has been performed several times. The outcomes have always shown that the randomness is real and that there is no simple hidden variable theory that can eliminate it. We started this chapter by saying that a qubit can be represented by the spin of an electron or the polarization of a photon. We will show how the models for spin and polarization are related. Photons and Polarization It is often said that we are not aware of the strange quantum phenomena because they only occur at incredibly small scales and are not apparent at the scales of our everyday life. There is some truth to this, but there is an experiment that is completely analogous to measuring spin of electrons that can be performed with very little apparatus. It concerns polarized light."
8,166,0.991,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Abstract In this contribution I recall how people working in the late 1960s on the dual resonance model came to the surprising discovery of a Hagedorn-like spectrum, and why they should not have been surprised. I will then turn to discussing the Hagedorn spectrum from a string theory viewpoint (which adds a huge degeneracy to the exponential spectrum). Finally, I will discuss how all this can be reinterpreted in the new incarnation of string theory through the properties of quantum black holes."
249,278,0.991,Advances in Proof-Theoretic Semantics (Volume 43.0),"I hold the first alternative is better, but I have not an a priori argument; I will argue for my thesis by considering what seems a very plausible tactics and explaining why, in my opinion, it is not viable. The tactics is based on the idea of translating one logic into the other, analogously to the case of the translation of a language into another. As a matter of fact, there are several so-called âtranslationsâ, both of classical logic/mathematics into intuitionistic logic/mathematicsâthe so-called negative translations (by Kolmogorov, GÃ¶del, Gentzen, Kuroda and others); and of intuitionistic logic/mathematics into extensions of classical logic/mathematics (Shapiro, Horsten, Artemov). I shall not enter here into a detailed discussion of this tactics. I want only to stress an obvious fact: that the so-called âtranslationsâ are not translations at all. A translation, in general, must be correct, and it is correct if it is meaning-preserving, i.e. if, for every expression E of L (the language to be translated), its translation Tr(E) into L â² has the same meaning as E (whatever meaning is). But there is no reason to believe that the âtranslationsâ mentioned above are meaning-preserving. Consider for instance the BHK clause for implication; Shapiro himself admits that the notion of âtransformations of proofsâ cannot be captured in the language of Epistemic Arithmetic, and SmorynÌski has observed that the âtranslationâ of intuitionistic logic into epistemic logic Â«does not capture the full flavor of talk about methodsÂ» (p. 1497).37 To make another example, Kurodaâs negative translation is based on a simple idea: that intuitionistic double negation is a sort of âequivalentâ of classical truth; this is surely true if one aims at a faithful âimmersionâ of classical logic into intuitionistic logic (i.e. at a representation preserving theoremhood), but not if one aims at a genuine translation, for the classical truth of Î±, expressed by its occurrence within any formula, is something very different from the existence of an obstacle in principle to our being able to deny that Î±, expressed by Â¬Â¬Î±. Moreover, there seems to be a conceptual reason for the impossibility of a genuine translation of one logic into another: on the one hand, a translation is correct only if it is meaning-preserving; on the other hand, classical logic explains the meaning of the logical constants in terms of a notion (bivalent truth) the intuitionist considers unintelligible or illegitimate, and also the converse is true (the classicist finds mysterious the intuitionistic notion of general method or effective function): so it seems unlikely that one of them finds in his own language an expression with the same meaning of an expression of the otherâs language."
124,569,0.991,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"model. A proposition is true at a moment m according to a denotation assignment in a model when it is true at moment m in the history hm of that moment according to that assignment. Two moments of time m and mâ are coinstantaneous when they belong to the same instant. Coinstantaneous moments are on the same horizontal line in each treelike frame. One can analyze historic necessity by quantifying over coinstantaneous moments. The proposition that P is then necessary (in symbols P) is true at a moment according to a model when P is true at all coinstantaneous moments according to all histories in that model. The notion of historic necessity is stronger than that of settled truth. The represented fact is then not only established but inevitable. According to traditional philosophy there are no inevitable actions and intentions. Moreover the possible causes and effects so to speak of actions of any agent at a moment are limited to those which are possible outcomes of the way the world has been up to that moment. As Belnap and Perloff (1992) pointed out, in order to explicate historical relevance we must consider coinstantaneous moments having the same past. Such moments are called alternative moments. Thus m1 and m2 are alternative moments in the last figure. Logical or universal necessity is stronger than historic necessity. The proposition that P is universally necessary (in symbols: P) is true in a circumstance according to a model when P is true in all possible circumstances in that model. In that case the fact represented is always inevitable. A proposition P is obviously tautological according to a model when it is true in every possible circumstance according to any possible denotation assignment. The notion of obvious tautologyhood is the strongest modal notion. The represented fact is then analytically inevitable subjectively as well as objectively."
213,338,0.991,Collider Physics Within The Standard Model : a Primer,"masses. Not only do the W and the Z have large masses, but the large splitting of, for example, the tâb doublet shows that even a global weak SU.2/ is not at all respected by the fermion spectrum. This is a clear signal of spontaneous symmetry breaking and the implementation of spontaneous symmetry breaking in a gauge theory is via the Higgs mechanism. The big questions are about the nature and the properties of the Higgs particle(s). The search for the Higgs boson and for possible new physics that could accompany it was the main goal of the LHC from the start. On the Higgs the LHC should answer the following questions: do some Higgs particles exist? And if so, which ones: a single doublet, more doublets, additional singlets? SM Higgs or SUSY Higgses? Fundamental or composite (of fermions, of WW, or other)? Pseudo-Goldstone bosons of an enlarged symmetry? A manifestation of large extra dimensions (fifth component of a gauge boson, an effect of orbifolding or of boundary conditions, or other)? Or some combination of the above, or something so far unthought of? By now we have a candidate Higgs boson that really looks like the simplest realization of the Higgs mechanism, as described by the minimal SM Higgs. In the following we first consider the a priori expectations for the Higgs sector and then the profile of the Higgs candidate discovered at the LHC."
124,528,0.99,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"What this suggests is introducing a richer modal language for basic STIT, referring also to the two stages: ânowâ and ânextâ. This motivates the NEXT-STIT of Broersen (2011), and we will also encounter this setting in the DEL-style logic of Sect. 6. But right now, we continue in a semantic mode with worlds viewed intuitively as transitions. Likewise, in order to compare STIT with PDL, we must also clarify the intuitive interpretation of PDL-style models. In particular, there are two broad views in the literature. One is that of transition models as abstract processes or machines, the other as unraveled temporal executions. On the process view, worlds are states in a process, and the relations indicate possible transitions. On this view, the model is a sort of automaton, perhaps in a very compact form, where many different transition relations can go from one state to the same next state. By contrast, the second view of PDL-models is one of unraveled temporal execution. Intuitively, once a process starts working, it produces a temporal universe of executions, being histories of successive admissible actions (cf. Clarke et al. 2000; Clarke and Emerson 1981 for this view). For the usual modal languages of action, the difference between the two views does not matter, since the execution tree is just a bisimilar unraveling of the process. And vice versa, we can think of a process as a sort of bisimulation-contracted essence of what can happen in the execution tree. But in our present setting, comparing with STIT seems to favor the temporal execution view.6 We therefore continue with the temporal view, where for simplicity, all event labels are taken to be unique.7 Like with the above basic STIT, we will not take the full temporal models here, but just the snapshots of a one-step action. A PDL action scenario is a set of labeled transitions from some initial state s, each leading to a different successor state. This can be viewed as an obvious special âone-shotâ case of the earlier-mentioned transition models."
311,1907,0.99,The Physics of the B Factories,"18.4.7.2 Invisible Final States of the Î¥ (1S) Motivation - low-mass dark matter The nature of dark matter is one of the great modern physics puzzles. Assuming dark matter is composed of at least one species of particle, the properties of this particle have not been measured (e.g. mass). If the mass of dark matter is small (< mbb ), then there is the possibility of detecting it using rare processes involving undetectable (invisible) final states. One of these, Î¥ â invisible, was motived by work by McElrath (2005), where it was suggested that a new interaction that couples Standard Model particles to dark matter particles could mediate the decay of the Î¥ . Based on the interaction cross-section required to achieve the âfreeze-outâ of dark matter annihilations in the early universe (a process that is required to explain the significant remnant of dark matter in todayâs universe), it was estimated that the branching fractions for Î¥ â (Î³+) invisible (the dominant decay mechanism depended on the spin of the dark matter constituent) could be as high as 0.41% â easily measured at the B-factories with even a modest sample of Î¥ mesons. The BABAR and Belle collaborations have both searched for invisible final states of Î¥ (1S) decay (Tajima, 2007; Aubert, 2009b; del Amo Sanchez, 2011j). Both collaborations produced results in the search for purely invisible final states, Î¥ â invisible while the BABAR collaboration also produced results for radiative invisible final states,"
275,537,0.99,Foundations of Trusted Autonomy,"the correct answer to all such puzzles as the famous âwise-man puzzleâ (an old-century, classic presentation of which is provided in [27]). The puzzle is treated in the standard finitary case in [12]. The infinite case is analyzed in [2]; here, the authors operate essentially as ideal observers. For a detailed case of a human operating as an ideal observer with respect to a problem designed by [25] to be much harder than traditional wise-man problems, see the proof of the solution in [13]. 6 The âarbitraryâ here is important. ShadowProver is perfectly able to solve particular Turingundecidable (provability) problems. It may be helpful to some readers to point out that any reasonable formalization of Simonâs [41] concept of bounded rationality will entail boundedness we invoke here. For an extension and implementation of Simonâs concept, under the umbrella of cognitive calculi like De CEC , see [30]. 7 For example, attempts have been made to imbue a computing machine with the ability to match (or at least approximate) the creativity of GÃ¶del, in proving his famous first incompleteness theorem. See [34]."
78,357,0.99,The Onlife Manifesto : Being Human in a Hyperconnected Era,"Omniscience and omnipotence are deemed to be postures from which anything, including the realization of any utopia, is possible, provided sufficient knowledge and control would be available. In an omniscience-omnipotence utopiaâs worldview, relationships create no surprise, as a relationship is deemed to be a causal one. In that perspective, the totality of the meaning lies in the cause. There is no room for meaning in an effect. An effect is soluble in its cause. An effect is not even an end. It is literally a non-event, since the event is all included in the cause. The omnipotence/omniscience utopia echoes the mortality perspective set out above. It closes down the opening to beginnings and is antinomic to thaumazein, as what deserves wonder, in the omniscience-omnipotence utopiaâs worldview, is onlyâ¦ omniscience and omnipotence! The perspective of natality counters the omniscience-omnipotence utopia without falling into the drawbacks of nihilism, because it encapsulates the confidence in recurrent beginnings. Plurality is the second element of this alternative to an omniscience-omnipotence utopia. Indeed, as we have seen above, the key features of plurality are that each entity engaged in the relationship is (i) equal (all on the same ground), (ii) singular (each who is unique) and (iii) partly hidden to him or herself (the reflective charac14"
117,398,0.99,Care in Healthcare : Reflections On Theory and Practice,"Phenomenology and Lived Space Lifeworld Phenomenology refers to a philosophical attitude towards the world. âPhenomenology is the study of human experience and of the way things present themselves to us in and through such experienceâ (Sokolowski 2000). In contrast to Western scientific thinking, phenomenology aims to bring together polarities such as mind-body, subject-object, individualsocial and feelings-thoughts. The hyphens signify intertwining rather than separation (Finlay 2011). This is why Merleau-Ponty describes phenomenology as a science of ambiguity. There is always ambiguity and, in a sense, indeterminacy, âprecisely because we are not capable of disembodied reflection upon our activities, but are involved in an intentional arc that absorbs both our body and our mindâ (Merleau-Ponty 1962). Heidegger describes the impossibility of being disconnected from the world by his concept of being-in-the-world. The lifeworldâLebensweltâ is the central phenomenological focus and portrays this lived wholeness and inseparability. It denotes a meaningful whole that is both shared and experienced by individuals from their own unique perspective (Heidegger 1998). The issue of the lifeworld should be understood against the background of the advent of modern science; before then, people simply thought that the world we live in was the only world there was (Sokolowski 2000). The project of phenomenology as started with Edmund Husserl (1859â1938) was to show that the exact, mathematical sciences are founded on the lifeworld and that they are transformations of the experience people directly have of things in the world. Husserl attempted to call to mind that the lifeworld (â¦) is always already there, existing in advance for us, the âgroundâ of all praxis whether theoretical or extra theoretical. The world is"
303,119,0.99,Multiculturalism and Conflict Reconciliation in the Asia-Pacific,"constructed by interactions with others while also providing the space for such interactions. This space is, in a sense, not a subject, however, because it only accepts and permits these interactions to take place. Therefore, it is a place (Nishida, 1949). This is, in a sense, a double subjectivity, which consists of a constructed and an encompassing subject. These two are contradictory, but are integrated simultaneously. This contradiction is absolute rather than relative, because this contradiction involves the selfâs opposition to itself. There is nothing in this place prior to the interactions, and thus Nishida sometimes describes it as the âplace of absolute nothingness,â which is in sharp contrast to ârelative nothingness,â which is indeed antonymous to âbeing.â In this sense, a place of nothingness is based on the concept of the absolute nothingness, and in fact Nishida later used a different concept to refer to the same idea, namely, âabsolute contradictory self-identityâ (Nishida, 1965). It is possible to say that Nishidaâs concept of the âplace of nothingnessâ is the key to understanding the political meaning of World Englishes and the tribute system of China. Both are inclusive towards others and have a blurred center. Nevertheless, they function as systems with coherence and continuity. They transform themselves into something new in a continuous manner. In this sense, using Englishes as a communicative device for comprehending contemporary world affairs is equivalent to saying that IR is a place that is inclusive towards different narratives and the discourses of others. Introducing the concept of the âplace of nothingnessâ and the tribute system of China into our intellectual activities is more of a thought experiment than the provision of a concrete policy program. It is suggestive, however, in considering the future paradigm of research methodology. In order to transform IR into a more diverse and democratic discipline, we have to ready ourselves for the forthcoming changes that will presumably take place at the peripheries. Rather than turn down arguments and theories of non-Western traditions mainly because of their âimperfectâ English quality or logical inconsistency, we have to focus more on elements, whether intentional or coincidental, generated by new and unfamiliar forms of representation. IR will otherwise become one of those means of unification and standardization at the world scale that, according to Hannah Arendt, are a typical feature of the disappearance of the public and totalitarianism. Thus, we can conclude here that it is not others who need to be transformed into something new, but those who are working on contemporary world affairs."
124,180,0.99,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"3 The Enigmatic Present In the indeterminist philosophy which the logic of branching time is intended to capture, we consider the past settled, but the future unsettled: there is only one path back, but there are many forward. The present, however, is somewhat enigmatically situated on the cusp between these two: is it settled, like the past, or unsettled, like the future? (Often âunsettlingâ, to be sure, but unsettled?). The technical issue associated with this query is this: should the valuation V in models for branching time assign values to atomic sentences at moments or should it instead assign such values at moment/history pairs? We might call these the static and dynamic views of moments, respectively. When we consider moments to be analogs of possible worlds, each associated with a maximal consistent set of basic facts and their consequences, we expect the atomic formulas to be true or false at a moment, even though more complex formulas involving tense operators must be evaluated at moment/history pairs. This is the view that a moment is a state through which some histories pass, and that the atomic formulas of the language are purely stative and should therefore be determinately true or false at any given state. On such a static view, the valuation should assign a truth value to each atomic formula at each moment, rather than at each point of evaluation. This has the odd result that some formulas get truth values at moments, but most only get truth values at moment/history pairs. We then have an odd contrast between points of valuation (moments) and points of evaluation (moment/history pairs), and a correspondingly odd contrast in treatment between atomic and non-atomic sentences. If, on the other hand, we hold that formulasâall formulasâhave truth values only with respect to a point of evaluationâa moment/history pairâthen in the absence of further constraints the valuation is free to assign different values to the same atomic formula at the same moment, but along different histories. Then, for example, âthe cat is aliveâ can be true at m/ h 1 and yet âthe cat is deadâ be true at m/ h 2 . If the cat in question is Schroedingerâs, suffering from that malaise known as quantum uncertainty, this might seem a desirable feature of the system. Similarly, âI choose c1 â and âI choose c2 â could be true at the same momentâthe moment of choiceâbut along distinct histories. On this view, then, atomic formulas do not (or perhaps do not all) express states. They are (some of them, at least)"
360,443,0.99,Compositionality and Concepts in Linguistics and Psychology,"substances and these accidents or features are seen to inhere in the substance. This contrasts markedly from most modern views, in which concepts consist of properties. That is, the concept is made of or just is the set of properties. Thus, the properties are more basic than the concept, which has several interesting consequences. One is that if this general view were true, one would expect that categorization would be a matter of identifying the properties of a thing and then using those properties to determine whether this thing is in the category. Indeed, as a general description, this is basically how categorization is understood in most modern theories. However, this implies that the properties should be identiï¬ed prior to the category. Thus, before one grasps that the thing they are looking at is a bird, they should already have grasped that they are seeing a beak and feathers and wings, etc. Note that it is not that they are seeing visual information that can be interpreted in these ways, but literally that they are grasping those conceptual features. A second implication of this approach is that there should be a clear and simple linkage from the extraction of simple visual features to simple conceptual features to the concept. In short, we should have no discontinuity between research on object recognition and research on concepts and categorization. Yet, research on object recognition rarely uses conceptual features, while research on concepts and categorization assumes conceptual features and rarely even posits the exact connection to the simple visual (or, for that matter, other sensory) features. Finally, it is important to understand that the A-T view of concepts and properties is embedded within a very complicated and intricate understanding of not only the mind, but of the physical world. Thus, for example, the A-T way of thinking about essences, properties, and concepts derives not from what is convenient for philosophy of mind, but from prior metaphysical commitments about the physical world and how the mind is seen to function (fairly reliably) as a part of that world. That is to say, the A-T approach to accounting for physical change through terms like actuality/potentiality and form/matter serves as the basis for A-T cognitive theory, and this cognitive theory is, in turn, a further speciï¬cation of those basic metaphysical commitments. An immediate and obvious beneï¬t of this approach, say advocates of A-T cognitive theory, is that it is completely consistent across differing levels of cognitive and metaphysical explanation, from how intellectual concept formation occurs, to other sorts of cognition, to an explanation of the constitution of physical things (including humans), and even to the processes of change as such (e.g., Feser 2014) because it is all part of a general and consistent view of the world. What is surprising about this is how well the philosophy of mind inherent in the A-T approach ï¬ts modern evidence about the mind. (Spalding and GagnÃ© 2013), for example, discuss how the A-T approach ï¬ts well with a wide variety of evidence drawn from modern psychological research on concepts, including evidence for prototypes and exemplars, as well as work on generics and essentialism, the relation between human and non-human animalsâ abilities to deal with categories, and other recent research topics in concepts. We should also note here that (Prasada and Dillinghamâs 2009) notion of K-properties (i.e., properties that are indicative of a kind of thing) is explicitly Aristotelian, though they do not explicitly make use of"
124,497,0.99,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"1 Ersatzism of Belnapian Elegance The aim of this postscript is to present a theory of possible ancestry that emulates the elegance of Nuel Belnapâs theory of branching space-time (BST), and in particular of the modal side of BST 1 . To bring out what is particularly elegant about it, let us have a look at how other theories of modality model possibilities. It is most common to model a possibility as a possible world, viewing the collection of all possible worlds as modal space, which is structured by the relation of accessibility that holds between worlds. Modal primitivism takes a possible world as a given object, irreducible to anything else. Modal ersatzism reduces each possible world to a construction from objects of a different kind, typically to a maximally consistent set of sentences or to a maximally coherent collection of states of affairs.2 On this basis, both primitivism and ersatzism of the typical kind form modal space by knitting together their respective modal components by adding the relation of accessibility to the collection of possible worlds. BST, in contrast, gives a picture of much more cohesion and unity. Far from constructing modal space by knitting together possibilities, which (for typical ersatzism) are themselves the result of pasting together some of a plurality of modal atoms, it carves possibilities out from a single pre-existing structure, Our World. This is so because what corresponds naturally to possible worlds in the BST framework are histories, and these are just subsets of Our World that are defined by recourse to the inner structure of Our World (its ordering relation) alone. So BST, though of course also a case of modal ersatzism, is ersatzism of an untypically elegant kind. The theory of possible ancestry sketched in Strobachâs âIn Retrospectâ is like primitivism and like typical ersatzism insofar it also knits together possibilities to form modal space. Hence we will here dub it âTPAknit â. What we want to achieve in this sequel to Strobachâs paper is to find a theory of possible ancestry such that there is some obvious one-to-one correspondence of some of its elements to the possibilities of TPAknit , but which is closer to Belnapâs BST insofar as it is based on carving out possibilities rather than knitting them together. (To make this contrast explicit, we will sometimes call our theory of possible ancestry âTPAcarve â, but usually we will stick to the shorter âTPAâ.) The results will not quite be Belnap-style BST structures, but nearly so, and they will give a flexible framework for biological modeling. 1 Belnap (1992). 2 In our use, the term âersatzismâ is meant only as a neutral description of one kind of metaphysical"
82,290,0.99,Fading Foundations : Probability and The Regress Problem,"6.1 The No Starting Point Objection In the previous chapter we discussed the main pragmatic argument against justification by infinite chains, known as the finite mind objection. Perhaps even more serious, however, are the conceptual objections. They aim to show that even creatures with an infinite lifespan or with a mind that can handle infinitely long or complex chains will run into problems, because the very idea of justification is at odds with a chain of infinite length: conceptual arguments . . . appeal . . . to the incompatibility of the concept of epistemic justification and infinite series of support.1"
8,966,0.99,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","of energy into matter. In an as yet unknown way, this could lead to a better understanding of the stability of matter and conversion of matter into energy. â¢ While as noted above, the mass of quarks is believed to originate in the Higgs field, the mass of nucleons, a âbagâ of three confined quarks is about 40 times larger than the sum of constituent quark masses. Nucleons dominate the mass of matter by a factor 2000. For this reason, the origin of the mass of matter is recognized to be caused by the confinement of quarks, compressed to a relatively small, hadron volumeâthis confinement mass effect dominates the Higgs effect by a large factor [8, 9]. Therefore, the vacuum structure which causes confinement of color is responsible for the inertia of matter. We can hope to learn how to use this deep insight in the future. â¢ In the standard model there are three families of particles which duplicate in essence all their properties, except for their mass-generating interaction with the Higgs field. They are thus distinguished only by three different sets of elementary particle masses. At present we do not have a good explanation why this is so. There have been few experiments possible to study this situation since in experiments involving elementary particle collisions, we deal with a few if not only one pair of newly created second, or third family at a time. A new situation arises in the QGP formed in relativistic heavy ion collisions. QGP includes a large number of particles from the second family: the strange quarks, and in fact also, the yet heavier charmed quarks; and from third family at the LHC also bottom quarks. The new ability to study a large number of these second and third generation particles present together in a different vacuum structure of QGP could help answer the riddle about the meaning and origin of the three particle families. âCouldâ means that a proposal has not emerged on how to approach this fundamental question."
235,22,0.99,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","law is therefore a statistical, not a mathematical, truth, for it depends on the fact that the bodies we deal with consist of millions of molecules, and that we never can get hold of single molecules.â Another, ironic example is the (incorrect) physical âproofâ that âall nonzero natural numbers are primes,â graphically depicted in Fig. 1.1. This sarcastic anecdote should emphasize the epistemic incompleteness and transitivity of all of our constructions, suspended âin free thought;â and, in particular, the preliminarity of scientific findings."
223,253,0.99,Knowledge and Action (Volume 9.0),"Interaction of Systems Now that we have described the basic operating principles of the RIMâs two systems, it is necessary to show how they function together. The very structures of the two systems contain a first important point relevant to their interaction: the impulsive system is always effortlessly active, whereas the reflective system may also be inactive. The implication is that the reflective system, when it does operate, does so in parallel with the impulsive system, not in place of it. That is, reflective processing always occurs with parallel impulsive processing. It is also clear that the concepts that are transformed in working-memory space in the reflective system do not come from nowhere but from the long-term store of the impulsive system. As the systems cannot interact when the reflective system is disengaged, it is adequate to examine how they interact from the beginning to the end of a reflective operation. When a reflective operation begins, perceptual input will already have activated several associative elements. For example, when thinking about what to have for lunch, a person may already have seen what is on offer in the cafeteria, a selection that will activate whatever associations that person has with the given meal options, but other perceptual data in the attentional focus (the presentation of the"
227,17,0.99,Problem Solving in Mathematics Education,"Hence, the fruitless efforts of the initiation phase are only seemingly so. They not only set up the aforementioned tension responsible for the emotional release at the time of illumination, but also create the conditions necessary for the process to enter into the incubation phase. Illumination is the manifestation of a bridging that occurs between the unconscious mind and the conscious mind (PoincarÃ© 1952), a coming to (conscious) mind of an idea or solution. What brings the idea forward to consciousness is unclear, however. There are theories of the aesthetic qualities of the idea, effective surprise/shock of recognition, fluency of processing, or breaking functional ï¬xedness. For reasons of brevity I will only expand on the ï¬rst of these. PoincarÃ© proposed that ideas that were stimulated during initiation remained stimulated during incubation. However, freed from the constraints of conscious thought and deliberate calculation, these ideas would begin to come together in rapid and random unions so that âtheir mutual impacts may produce new combinationsâ (PoincarÃ© 1952). These new combinations, or ideas, would then be evaluated for viability using an aesthetic sieve, which allows through to the conscious mind only the âright combinationsâ (PoincarÃ© 1952). It is important to note, however, that good or aesthetic does not necessarily mean correct. Correctness is evaluated during the veriï¬cation stage. The purpose of veriï¬cation is not only to check for correctness. It is also a method by which the solver re-engages with the problem at the level of details. That is, during the unconscious work the problem is engaged with at the level of ideas and concepts. During veriï¬cation the solver can examine these ideas in closer details. PoincarÃ© succinctly describes both of these purposes. As for the calculations, themselves, they must be made in the second period of conscious work, that which follows the inspiration, that in which one veriï¬es the results of this inspiration and deduces their consequences. (PoincarÃ© 1952, p. 62)"
124,568,0.99,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"A maximal chain h of moments of time is called a history. It represents a possible course of history of our world. Some histories have a first and a last moment. According to these histories the world has a beginning and an end. As Belnap et al. (2001) pointed out, each possible circumstance is a pair of a moment m and of a history h to which that moment belongs. Thanks to histories temporal logic can analyze important modal notions like settled truth and historic necessity. Certain propositions are true at a moment according to all histories. Their truth is then settled at that moment no matter how the world continues. So are past propositions and propositions attributing propositional attitudes to agents. Whoever desires something at a moment desires that thing at that moment no matter what happens later. Contrary to the past, the future is open. The world can continue in various ways after indeterminist moments. Thus the truth of future propositions is not settled at such moments. It depends on which historical continuation of that moment is under consideration. When there are different possible historic continuations of a moment, its actual future continuation is not then determined. However, as Occam7 pointed out, if the world continues after a moment, it will continue in a unique way. The actual historic continuation of each non final moment will be unique even if it is still undetermined at that very moment. Indeterminism cannot prevent that uniqueness. Human agents who persist in an indeterminist world, have expectations and make plans. According to phenomenology and philosophy of mind, human agents who are directed by virtue of their intentionality towards things and facts of the world, are intrinsically oriented at each moment of their active life towards the real continuation of the world. We all ignore how the world will continue but we are intrinsically oriented at each moment towards the real continuation of that moment. So we always distinguish conceptually that real continuation from other possible continuations whenever we act or think in the world. Whoever attempts at a moment to achieve a future objective, intends to achieve that objective in the real continuation of that moment. Whoever foresees or wishes to have a future grandchild, foresees or wishes that grandchild in the real future. So in my approach both the moment and the historic continuation of the moment are to be considered in order to evaluate our actions and attitudes oriented towards the future. Consequently8 our elementary illocutions and propositional attitudes at each moment have or will have a certain satisfaction value even if that satisfaction value is still then undetermined when they have a future propositional content. In order to keep a present promise and execute a present intention to give things later, an agent must give later these things in the real continuation of the world. Other possible historic continuations do not matter.9 According to my temporal logic every moment m has then a proper history hm in each model. Whenever a moment m is the final moment of a history h, that history h is its proper history hm . All moments that belong to the proper history of an indeterminist moment have of course the same proper history in each 7 See Prior (1967). 8 See my paper âTowards a Formal Pragmatics of Discourseâ (Vanderveken 2013). 9 Belnap N., M. Perloff and Ming Xu who reject the idea that each moment of utterance has a proper"
192,63,0.99,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"In order to discern the philosophemes and imperatives (S1) which structure scientific discourse (S2), we must step outside of the normal scientific discursive mode and analyse scientific discourse from an oblique perspective (Zwart 2017c). The idea that there are multiple types of discourse was already apparent in Freudâs preface to Dora, as we have seen, where Freud at a certain point steps outside the psychoanalytical mode of writing (the case history mode) to comment on his own work from the perspective of academic, professional discourse, as a universitytrained expert (S2). Likewise, in order to discern the basic dialectical structure of scientific discourse, we must move from the discourse of the professional expert (the scientific researcher: S2) to psychoanalytical discourse. In order to come to terms with phenomena of integrity and misconduct in science, we must step outside normal science discourse, adopting an oblique perspective, a psychoanalytic stance. The axis of attention takes a quarter turn. Instead of on the objects of research (molecules, elementary particles, historical archives, artworks, election polls, and so on) we assess research practices from a slightly tilted, oblique perspective. Instead of on the object-pole (molecules, microbes, model organisms, etc.), the focus is rather on the subject-object interaction: on the researcher (the research team) at work, on the interrelations between experimenters and their targets, âobserving the observerâ, as Bachelard (1938/1949, p. 13) once phrased it, following the discourse of academic experts with evenly-posed attention (âgleichschwebende Aufmerksamkeitâ; Freud 1912/1943), and from a critical angle: a position which is comparable to how psychoanalysts keep track of the analysandâs discursive flow. At a certain point, somewhere in the stream of discourse, a specific metaphor, concept or confusion may light up, triggering our attention, catching the philosophical or psychoanalytical ear, so that a shift towards a more active, Socratic mode of listening is indicated, prompting questions and dialogue. The intentio obliqua has a long history which goes back to medieval scholasticism. Thomas Aquinas already stated that, whereas human understanding is predominantly directed towards external reality, critical reflection on human understanding requires a change of perspective, an intentio obliqua (Schmidt 1966). By opting for an oblique perspective, a diagnostics of contemporary knowledge can be achieved: a critical assessment of the way contemporary research allows nature or social reality to emerge. This means that, rather than in protons, mitochondria, microbes, ethnic prejudices or political preferences, philosophers are interested in the Î»ÏÎ³Î¿Ïâdimension: the words or signifiers that are actually used to bring such items to the fore. Bachelard once argued that, in terms of competence, philosophers have but one: âthe competence of readingâ (1948, p. 6). Not only in the sense that they are"
363,196,0.99,History and Cultural Memory in Neo-Victorian Fiction,"period and our own, the specificity with which Lucy foresees us evokes the logic of the ghost; suggests the disruption of linear time, the aberrant presence of the Victorian in our culture. In a photographic reversal, that is, Lucy is able to see us. For Jacob, it is as though âunbidden, he had glimpsed Lucy in another realmâ (218). The distance between past and present is elided. Indeed, both novels have been criticised in reviews for psychological anachronism. For Susan Elderkin, Lucy is too modern, âeerily ahead of her timeâ and the portrait of the period is unconvincing: âreferences to Dickens and pink bonnets come as a surpriseâ (Elderkin, 2004). For Ion Martea, the novel âfails to bring the insight into the period the author had intended to deliverâ (Martea, undated review). Humphreys, too, has been criticised for creating an anachronistic heroine. Andrea Barrett observes that Annie âreshapes Isabelleâs thinking about proper representations of womenâ and, therefore, Annie âcan seem both too good to be true and anachronistic, her character shaped by class and gender issues that belong to our time and not hersâ (Barrett, 2001). For Elderkin, Martea and Barrett these anachronisms represent chronological and historiographical confusion and a serious failing in an historical novel. However, I would argue that the use of anachronism contributes to the sense that the Victorian past continues to exist in uncanny forms today; it suggests its absent presence. The sense of linear disruption is consolidated in Afterimage through the immediacy of its present tense. It suggests a kind of afterlife, or, more properly as Humphreysâ title suggests, an âafterimageâ, a picture that continues to be visible, in altered form, after its original has vanished. It is memory, seeking to hold onto the transient and, in the process, transfiguring it. These novels proceed as memory does for Benjamin, in fragments, connecting events according to âresemblancesâ or âcorrespondencesâ (Benjamin, 1968a: 211). They offer themselves as aberrant repetitions of the Victorian period, suggesting that what is important is not that the past is accurately known, or fully understood or made sense of, but that it is remembered in fractured form, as shards of memory. Jones suggests that âwords and images do not have achieved reparation within them, what they have are the gestures towards reparationâ ( Jones, 2006). Like Lucyâs notebook of âSpecial Things Seenâ, the historical novel becomes, here, a kind of shrine, a âkind of honouring attentionâ (200). Thus, a scene in Sixty Lights in which Lucy sees a gaslit street in London âremade in a quivery film of lightâ provides a model for thinking about memory and historical recollection in the novel. Moreover, it provides a model for thinking about Jonesâ project as an historical novelist or,"
192,119,0.99,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"To be able to play this role and probe this fatal dynamics, the archaeologist must leave his field (S2 = academic expertise pushed back in the lower-left position). He is no longer a frustrated archaeologist, but rather an analyst hearing a confession, exploring the subjectâs psychic past. In contrast to normal confessions, as Freud phrases it in The Question of Lay Analysis, Don JosÃ© not only confesses everything he knows, but also what he does not know.5 During the analytical session, he explores his unfathomable fascination and obsession for his object a, embodied by Carmen. And the by-product of this exercise is an important ethical insight, an important truth (S1), namely that Carmen is not lawless at all. Her world is not a moral vacuum, where all normativity is suspended or eliminated, far from it. After shifting from the archaeology of the Roman past to the archaeology of the present (psychoanalysis) and in the context of a psychoanalytical retrospect it becomes clear that the moral topology of Carmenâs world is pre-structured by the collision between two irreconcilable forms of normativity, namely the human law of societal legislations and regulations versus another, unwritten, enigmatic law, the âlaw of Egyptâ (the divine Law, overruling the human law; S1 emerging in the lower-right position), a form of normativity to which Carmen is extremely sensitive, due to her âupbringingâ, according to Don JosÃ©:"
124,580,0.99,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"content must represent a fact that exist at that moment or will exist in the world in its real historic continuation. As I already said, agents live in an indeterminist world. Their future is open. At each moment where they think and act, they ignore how the world will continue. However, their attitudes and actions are always directed by virtue of their intentionality toward the real historic continuation. Whenever parents refer to their next child they refer to their next child in the real future. In order that a present attitude or illocution directed at the future be satisfied, it is not enough that things will be at a posterior moment as the agent now represents them. They must be so later in the real future. So the satisfaction of propositional attitudes and elementary illocutionary acts of an agent at a moment requires the truth at that very moment of their propositional content. The notion of satisfaction is a generalization of the notion of actual truth14 that takes into account the direction of fit of attitudes and illocutions. The relation of fit or of correspondence is symmetrical: if a proposition fits the world then the world fits that proposition. However there is more to the notion of satisfaction than to that of actual truth because one must consider the direction of fit from which the correspondence must be achieved between the mind and the world in the analysis of satisfaction of attitudes, just as one must consider the direction of fit from which the correspondence must be achieved between language and the world in the analysis of satisfaction of illocutions. There are four possible directions of fit between ideas and things, just as there are four possible directions of fit between words and things. Just as assertive illocutions have the language-to-world direction of fit, cognitive attitudes have the mind-to-world direction of fit. They are satisfied when their propositional content fits the world. In that case the agentâs ideas15 must correspond to things as they are then in the world. On the contrary, volitive attitudes have the opposite world-to-mind direction of fit just as commissive and directive illocutions have the opposite world-to-language direction of fit. They are satisfied only if the world fits their propositional content. In that case represented things in the world must correspond to the agentâs ideas. Each direction of fit between mind and the world determines which side is at fault in case of dissatisfaction. In the cognitive and assertive cases, the agent is at fault in the case of dissatisfaction. So when the agent realizes that there is no correspondence between his or her ideas and represented, that agent immediately changes his or her beliefs and is ready to revise his or her assertions. This is why the truth and falsehood predicates apply so well to satisfied cognitive attitudes and assertive illocutions. A belief and an assertion at a moment are satisfied when they are then true and unsatisfied when they are then false. Satisfaction and dissatisfaction amount to actual truth and actual falsehood in the case of cognitive attitudes and assertive illocutions. However, the truth predicates do not apply at all to volitive"
192,56,0.99,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"According to Platonic cosmology, the world was basically a sphere, an immense macrocosmic mirror, providing an imaginary model for the ideal human polis and its ruling elite. But this imaginary spherical phantasm was derided by Aristophanes who, in Symposium (Plato 1925/1996), argues that, if the ideal world (ÎºÏÏÎ¼Î¿Ï) is spherical, primordial humans must have been spherical (i.e. egg-shaped) as well. It is in this text, Lacan argues (1960â1961/2001, p. 81), that the term Spaltung (Î´Î¹ÎµÏÏÎ¯ÏÎ¸Î·Î¼ÎµÎ½) occurs for the first time: in Aristophanesâ parable, explaining how human integrity was once deliberately demolished by Zeus, namely by splitting or slicing early humans in two (like boiled eggs that are spliced with the help of a hair), so that we (their descendants) are still frantically searching for our lost âother halfâ: the lost part of what we once were (Plato 1925/1996, 189Eâ191C). I will come back to this parable later in this book, because what is at stake in this story is human integrity. Integrity literally means wholeness (integritas) in Latin (Zwart 2000a). The parable basically claims that although human beings once upon a time were godlike creature (in their original position), we have become divided subjects long ago ($), marked by an irrevocable loss of integrity, an ancestral, original flaw if you like. Civilisation is a project which aims to rehabilitate the subject, not by restoring the primordial egg-like shape of course, but by initiating a process of working through, of coming to terms with and compensating for the loss, both individually and collectively (Zwart 2017a). An important exemplification of restored integrity is the ancient figure of the Master (in Lacanian algebra: S1), an authoritative voice, someone who has seen the truth. His views and theories reflect (and are mirrored by) the macrocosm. His thinking (Î»ÏÎ³Î¿Ï) corresponds with the logic (Î»ÏÎ³Î¿Ï) which pervades the universe as such; his thoughts, his intellect corresponds with being as such. Via adequate thinking (resulting in a theory which reflects and corresponds with the cosmos) forms of anxiety and discontent which torment ordinary human beings are overcome, whilst integrity is restored. As a rational being, the Master participates in the spirit (Î½Î¿á¿¦Ï)"
124,17,0.99,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"a single history is however typically Hausdorff. This makes good sense given indeterminism: If different possibilities exist for the same position in space-time, the corresponding possible point events may be topologically inseparable in the full indeterministic model. â¢ These topological observations are linked to the question whether BST can be viewed as a space-time theory. Earman (2008) has asked a pointed question about the tenability of BST as a space-time theory, sharply criticizing McCallâs (1994) version of BST and raising doubts about Belnapâs framework. His main challenge is to clarify the meaning of non-Hausdorffness that occurs in BST, since in space-time theories this is a highly unwelcome feature. Some recent literature, including Tomasz Placekâs contribution to this volume, has clarified the situation considerably, highlighting the difference between branching within a space-time, which indeed has unwelcome effects well known to general relativists, and the BST notion of branching histories, in which the histories are individually nonbranching space-times. The connection between BST and general relativity is only beginning to be made, and a revision of Belnapâs prior choice principle may be in order to move the two theories closer to each other. (Technically, the issue is that the prior choice principle typically leads to a violation of local Euclidicity, which is, however, presupposed even for generalized, non-Hausdorff manifolds.) Apart from Placekâs contribution, see also Sect. 6 of the contribution by Pleitz and Strobach, and MÃ¼ller (2013b). â¢ Another area of physics that may be able to interact fruitfully with the BST framework is quantum mechanics. As BST incorporates both indeterminism and spacelike separation, it seems to be especially well suited for clarifying the issue of space-like correlations in multi-particle quantum systems, pointed out in a famous"
235,37,0.99,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","transformed into an intrinsic observation mode by âbundlingâ or âwrapping upâ the object with the observer, thereby also including the interface; see Fig. 1.5 for a graphical depiction. Nesting can be iterated ad infinitum (or rather, ad nauseam), like a Russian doll of arbitrary depth, to put forward the idea that somebodyâs observer-cut-object conceptualization can be another agentâs object. This can go on forever; until such time as one is convinced that, from the point of view of nesting, measurement is purely conventional; and suspended in a never-ending sequence of observer-cut-object layers of description. The thrust of nesting lies in the fact that it demonstrates quite clearly that extrinsic observers are purely fictional and illusory, although they may fapp exist. Moreover, irreversibility can only fapp emerge if the observer and the object are subject to uniform reversible motion. Strictly speaking, irreversibility is (provable) impossible for uniformly one-to-one evolutions. This (yet not fapp) eliminates the principle possibility for âirreversible measurementâ in quantum mechanics. Of course, it is still possible to obtain strict irreversibility through the addition of some many-to-one process, such as nonlinear evolution: for instance, the function f (x) = x 2 maps both x and âx into the same value."
223,93,0.99,Knowledge and Action (Volume 9.0),"thinking, and by some misinterpretation of the original concept of rationality in this context. VarrÃ³ (2010) noted a similar misunderstanding with respect to the concept of discourse between critical realist thinkers and discourse theorists. In the de-essentialized and dynamic, but nevertheless highly structuralist and imprisoning, interpretation of discourses and practices reminiscent of early Foucault (1972) and the practice turn (Schatzki et al., 2001; ViÅ¡ÅovskÃ½, 2009), there seems little space for rationality or reason in the traditional modernist sense. In this framework, politicsâand thus also spatial politicsâseems to be defined primarily as authority and power and seems to deal only with the effects of power relations and not with the structure of the deliberations that take place in the framework of these relations. As part of the misunderstanding of rationality, rationality is seen only as a foundational universal concept, for it was forwarded by enlightenment at a time when reason had actually been expelled from the view of the human beingâs abilities to deliberate about the world. However, Welsch (1997, 1999) prompted the question of how to differentiate and judge the various systems of meaning and logics, or the various forms of rationality involved, without some all-embracing perspective. Distinctions and judgments based on any one of these types or paradigms of rationality would necessarily misrepresent the others. Welsch suggested that there must be a different type of functioning that underlies human reflective capacity. It is this type of reflection that he reintroduced as reason, enhancing rationalityâor better, enhancing rationalities. In a seminal book written in the Anglo-Saxon tradition, Schrag (1992) took up and endorsed this very specific kind of reason. Both scholars called it transversal reason (Schrag, 1992, p. 148; Welsch, 1997, p. 315). Because transversal reason relates geographic realities and geographic differences to each other, it is crucial for the geographic perspective as well. As the reflexive ability to recognize and clarify the differences as well as the relationships between the various forms of rationality, transversal reason is actually a necessary condition for the theory of plurality and difference. Related to the current situation of plurality and hybridity, this kind of transversal reason is not a new invention but rather a skill that is increasingly used consciously or unconsciously in everyday practice and that is becoming more and more an inner constituent of peopleâs reasoning and life designs. The present age is not one seemingly bereft of rationality but rather one in which reason and rationality are reunited as a mental and reflective activity operating at every step of rational deliberation on discursive articulations. Reason and rationality are not two separate faculties, and in a sense are not faculties at all, but rather signify different layers and functional modes of our reflective activity. âReasonâ refers to the basic mechanism, ârationalityâ to the various concrete, object-directed [or place related] versions of this activity. (Welsch, 1999, Pt. III, sec. 5, par. 2)"
363,104,0.99,History and Cultural Memory in Neo-Victorian Fiction,"would transform itself, in my five- or six-year old mind, into an empty wildernessâ (3). By analogy, even the space that we imagine belongs to ârealityâ is always already mapped. It is unavoidable. The novelâs attempt to represent the unpresentable, an empty reality, devoid of overarching pattern and meaning is therefore undermined or countered by the category of natural history, which naturalises the human condition and makes a continued desire for history, in the face of the problematisation of historical knowledge, a universal and timeless quality. Tomâs installation of natural history as a category of the real suggests that his empty reality does posit Truth, that of nature, in much the same way as Elias cannot avoid her metaphysical idea of âHistoryâ as that which is beyond what we live (the present) and remember (the past). Yet, whereas for Elias, the Truth that constitutes the historical sublime is âopposed to or other to, the materiality of lived historyâ (Elias, 2001: 53) the novel attempts to avoid a metaphysical positing of truth by making it material. Desire is the fundamental truth of human nature, just as the desire to return is the fundamental truth of natural history. Both these truths are firmly housed not in a metaphysical concept, however, but in the materiality of bodies and of nature. Committed to representing the late-twentieth century crisis of historiography, the novel nonetheless posits the continuing value of seeking historical knowledge, even if that knowledge is provisional. For reclaimed land is highly fertile. The Fens yield âfifteen tons of potatoes or nineteen sacks of wheat an acreâ (16). Similarly, the emptiness of reality, the absence of over-arching pattern and meaning, invites inscription, proliferates stories, which produce a multiplicity of meanings. These meanings are not guaranteed by a perspective outside of the stories themselves and, as such, are always open to revision, to new meanings. Indeed history-making is abundantly fertile. As we have seen, stories and things made to happen beget more stories and things made to happen, so that while âhistory itselfâ might be empty, historymaking is a swarming, irresistible fecundity. Often the stories produced in and by history-making are, like civilisation itself, âprecious. An artifice â so easily knocked down â but preciousâ (240). As Decoste observes, this celebration of civilisation does not equate to an Atkinson-like faith in progress, but rather, a belief in process. Civilisation is equated with ceaseless inquiry: for this artifice that keeps the void at bay is, for Crick, itself a process and not a final destination. The civilization that must be husbanded and ceaselessly renewed is, indeed, the operation of inquiry itself,"
235,115,0.99,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","Both von Neumann and SchrÃ¶dinger thought of this as a sort of a zero-sum game, very much like complementary observables: due to the scarcity and fixed amount of information which merely gets permuted during state evolution, one can either have total knowledge of the individual parts; with zero relational knowledge of the correlations and relations among the parts; or conversely one can have total knowledge of the correlation and relations among the parts; but know nothing about the properties of the individual parts. Stated differently, any kind of mixture between the two extremes can be realized for an ensemble of multiple particles or parts: (i) either the properties of the individual parts are totally determined; in this case the relations and correlations among the parts remain indeterminate, (ii) or the relations and correlations among the parts are totally determined; but then the properties of the individual parts remain indeterminate. For classical particles only the first case can be realized. The latter case is a genuine quantum mechanical feature. Everett expressed this by saying that, in general (that is, with the exception of quasi-classical states) [206], âa constituent subsystem cannot be said to be in any single well-defined state, independently of the remainder of the composite system.â The entire state of multiple quanta can be expressed completely in terms of correlations or joint probability distributions [365, 576], or, by another term, relational properties [587, 588], among observables belonging to the subsystems. As pointedly stated by Bennett [287] in quantum physics the possibility exists âthat you have a complete knowledge of the whole without knowing the state of any one part. That a thing can be in a definite state, even though its parts were not. . . . Itâs not a complicated idea but itâs an idea that nobody would ever think of.â SchrÃ¶dinger called such states in German verschrÃ¤nkt, and in English entangled. In the context of multiple particles the formal criterion for entanglement is that an entangled state of multiple particles (an entangled multipartite state) cannot be represented as a product of states of single particles."
192,409,0.99,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"The object a takes the initiative and draws the recipient, the craving subject into action. Pinocchio (object a) deflects the life of Geppetto ($), for instance, while the seductive nature of the âeffect xâ deflected Stapel from his initial course. Psychoanalysis aims to reconstruct this dynamics by following the Fallgeschichte with evenly-poised attention. The psychoanalytic reader is not interested in social psychology (for instance: in prejudice studies) as such, but rather in the question: what exactly is it that is drawing/deflecting the tormented subject into this apparently desperate trajectory.25 What is the difference between experimental social science (the field for which Stapel qualified until he disqualified himself) and psychoanalysis? In his inaugural lecture, Diederik A. Stapel (2001) defined social psychology as âthe science of everyday lifeâ, combining specificity with precision, a title which is reminiscent of one of Freudâs publications, namely The Psychopathology of Everyday Life (Zur Psychopathologie des Alltagslebens 1904/1941). In this book, Freud aims to demonstrate how minor, everyday instances of malfunctioning (forgetting names, memories or foreign words, slips of the tongue, reading errors, etc.) are symptomatic of unconscious complexes and inhibitions. Quite a few examples used by Freud involve ethnic prejudices and anti-Semitism, and this is comparable to social psychology as we have seen. For the prototypical social psychologist, the Mr. Hyde of European Caucasians is an anti-Semite (especially the ones who do not know this about themselves). According to Freud, psychologists such as Wilhelm Wundt cannot really explain specific instances of forgetfulness or mistakes. His laboratory knowledge fails to elucidate concrete examples taken from everyday life. In the case of psychoanalysis, however, whenever a specific mistake occurs, a dialogue will evolve, building on free associations, which is expected to disclose the inconvenient, obfuscated factor. Whereas social psychology aims at discovering general mechanisms,"
249,277,0.99,Advances in Proof-Theoretic Semantics (Volume 43.0),"understood classically, (2) is utterly unacceptable. This situation is far from surprising; on the contrary, it illustrates a general truth reminded above: the classical meaning of the logical constants is deeply different from their intuitionistic meaning. Consider for instance the schema âÎ± or not Î±â: classically understood (i.e., formalized as Î± + âÎ±) it expresses the intuitively true principle that every proposition is either true or false, (intuitively true because our common-sense or pre-theoretic intuitions about the world are predominantly realistic) whereas intuitionistically understood (i.e. formalized as Î± â¨ Â¬Î±) it expresses the intuitively false principle that every proposition is decidable in the sense that there is either a proof or a refutation of it. However, this situation generates a serious problem: the problem whether a rational discussion between a supporter of classical logic and a supporter of intuitionistic logic is possible at all. How is it possible that there is real disagreement or real agreement between them, given that both disagreement and agreement about a principle presuppose that the same meaning is assigned to it by both parties, while, as we have just seen, the meaning of one and the same formula drastically changes across classical and intuitionistic readings? It seems to me that there are at least two alternative strategies to tackle the problem. The first consists in placing the discussion between the two parties before the formalization of the intuitive notions (as the logical constants, the notion of truth, and so on) into a formal language. The discussion, in this case, concerns questions like the following: (i) Which intuitive notions should be formalized? For instance: inclusive or exclusive disjunction? Which notion of implication? Which notion of truth? (ii) Which intuitive notion should be chosen as the key-notion of the theory of meaning, i.e. as the notion in terms of which the meaning of the expressions of the formal language (in particular of the logical constants) is to be characterized? For instance: (bivalent) truth (as the realist claims), or knowability/existence of a proof (as the neo-verificationist claims), or knowledge/actual proof (as the intuitionist claims)? In this case the problem can be solved, provided that each party accepts the intelligibility of the key-notion adopted by the other party; for only in this case a rational discussion is possible: the same intuitive notions are accessible to both parties, and the disagreement concerns the legitimacy, the adequacy, the fruitfulness, etc. of adopting one notion or another as the key-notion. From this standpoint, Brouwerâs idea that such classical notions as bivalent truth or actual infinity are unintelligible is to be abandoned, in favor of a slightly different claim: that those classical notions, precisely because they are intelligible, turn out to be incapable to play the foundational role the classicist gives them. Of course, such a claim should be motivated by a rational argument; which means that a rational discussion would be possible. The second strategy consists in placing the discussion between the two parties after the formalization of the intuitive notions. In this case the problem of course arises, owing to the fact that the choice of different key-notions for the theory of meaning induces differences in the meaning of the logical constants. However, there may be tactics to solve it."
124,507,0.99,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"4 The Question of Embeddability: States, Moments, and Histories Belnap developed BST as a formal framework to model an indeterminist world, making a significant step toward accommodating modern physics by adding resources to model spatial variation to the branching time framework of Kripke and Prior. The intuitive interpretation of our theory of possible ancestry presupposes a similar intuition of indeterminism.17 Hence the question arises of how its ontology can be embedded in a branching time structure. The task is not trivial, because typically neither a structure of possibilia nor any of its restrictions to an admissible set will have the requisite property of having no downward branches. A typical family tree is not a tree in the branching-time sense, and the same goes for any typical structure of possibilia. (If fission were the only means of reproduction, a structure of possibilia need indeed not have downward branches. But they will appear as soon as there can be reproductive acts that require more than one participant.) So, how can we construct a branching time structure from the means at our disposal? Here, the admissible sets clearly play a central role. We will motivate our construction by a look at how the possibilities they model correspond to elements of the branching time structure that is implicit in our intended interpretation. Facing the future, it is obvious that from the possibility modeled by an admissible set there typically will sprout many different historical paths towards later possible situations, depending on which possible children (modeled by direct descendants in terms of âº) of some of the inhabitants of that possibility come into existence. Facing the past, we encounter a small surprise because there may also be a plurality of paths branches leading up to the possibility modeled by an admissible set. E.g., towards the admissible set containing Eve, Adam, and their two sons Abel and Cain, there is one path via the admissible set {Eve, Adam, Abel} and a second path via the admissible (Footnote 16 continued) nature (cf. Sect. 2.5 of âIn Retrospectâ) so that there might be reason to understand postulate U in a metaphysical way. But in this special case there remains a pragmatic reason to group it with other specific requirements, namely its high degree of contentiousness. 17 This will be obvious in the examples of Sect. 5, where we admit, for some given possibility, that from then on things may take one of many different courses: Elizabeth and Peter have these children, but they might have had others, and so on."
375,41,0.99,Musical Haptics,"can be informed by thinking carefully about a neural system that âknowsâ how to harness the mechanics of the body and object dynamics and a physical system that can âcompute in hardwareâ in service of a solution to a motor problem. The human perceptual system is aligned not only to extracting structure from signals (or even pairs of signals) but to extract structure from pairs of signals known to be excitations and responses (inputs and outputs). What the perceptual system extracts in that case is what the psychologist J. J. Gibson refers to as âinvariantsâ [19]. According to Gibson, our perceptual system is oriented not to the sensory field (which he terms the âambient arrayâ) but to the structure in the sensory field, the set of signals which are relevant in the pursuit of a specific goal. For example, in catching a ball, the âsignalâ of relevance is the size of the looming image on the retina and indeed the shape of that image; together these encode both the speed and angle of the approaching ball. Similarly, in controlling a drum roll, the signal of relevance is the rebound from the drumhead which must be sustained at a particular level to ensure an even roll. The important thing to note is that for the skilled player, there is no awareness of the proximal or bodily sensation of the signal. Instead, the external or âdistalâ object is taken to be the signalâs source. In classical control, such a structured signal is represented by its generator or a representation of a system known to generate such a structured signal. Consider for a moment, a musician who experiences a rapid oscillation-like behavior arising from the coupling of his or her own body and an instrument, perhaps the bounce of a bow on a string, or the availability of a rapid re-strike on a piano key due to the function of the repetition lever. Such an experience can generally be evoked again and again by the musician learning to harness such a behavior and develop it into a reliable technique, even if it is not quite reliable at first. The process of evoking the behavior, by timing oneâs muscle actions, would almost certainly have something to do with driving the behavior, even while the behaviorâs dynamics might involve rapid communication of energy between body and instrument as described above. Given that the behavior is invariant to the mechanical properties of body and instrument (insofar that those properties are constant) it seems quite plausible that the musician would develop a kind of internal description or internal model of the dynamics of the behavior. That internal model will likely also include the possibilities for driving the behavior and the associated sensitivities. In his pioneering work on human motor control, Nicolai Bernstein has described how the actions of a blacksmith are planned and executed in combination with knowledge of the dynamics of the hammer, workpiece, and anvil [20]. People who are highly skilled at wielding tools are able to decouple certain components of planned movements, thereby making available multiple âloopsâ or levels of control which they can âtightenâ or âloosenâ at will. In the drumming example cited above, we have seen that players can similarly control the impedance of their hand and arm to control the height of stick bounces (the speed of the drum roll), while independently controlling the overall movement amplitude (the loudness of the drum roll). Interestingly, the concept of an internal model has become very influential in the field of human motor behavior in recent years [21] and model-based control has become an important sub-discipline in control theory. There is therefore much"
213,360,0.989,Collider Physics Within The Standard Model : a Primer,"3.17 Limitations of the Standard Model No signal of new physics has been found, either by direct production of new particles at the LHC, or in the electroweak precision tests, or in flavour physics. Given the success of the SM, why are we not satisfied with this theory? Once the Higgs particle has been found, why donât we declare particle physics closed? The reason is that there are both conceptual problems and phenomenological indications for physics beyond the SM. On the conceptual side the most obvious problems are that quantum gravity is not included in the SM and that the famous hierarchy (or naturalness or fine-tuning) problem remains open. Among the main phenomenological hints for new physics we can list coupling unification, dark matter, neutrino masses (discussed in Sect. 3.7), baryogenesis, and the cosmological vacuum energy. At accelerator experiments, the most plausible departure from the SM is the muon anomalous magnetic moment which, as discussed in Sect. 3.9, shows a deviation by about 3 , but some caution should be applied since a large fraction of the uncertainty is of theoretical origin, in particular that due to the hadronic contribution to lightâlight scattering [245]. The computed evolution with energy of the effective SM gauge couplings clearly points towards the unification of the electroweak and strong forces (GUTs) at scales of energy MGUT  1015 â1016 GeV [315], which are close to the scale of quantum gravity, MPlanck  1019 GeV. The crossing of the three gauge couplings at a single"
271,40,0.989,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"technologically based media of communication, which are all becoming digital. Deep mediatization is by no means homogeneous or linear. It is highly complicated, contradictory and a conflict-driven process. Nevertheless, in the Western hemisphere, deep mediatization takes place across societies as a whole. Yet even when we strive to escape from this all-encompassing contemporary mediatizationâfor example, individuals who refuse to use certain (digital) media in an attempt at âcopingâ (Schimank 2011: 459â462) with being reachable at all times of the day and night, or organizations that introduce email-free holidaysâsuch behaviour merely constitutes what we can call temporary âoases of demediatizationâ, in loose reference to Hartmut Rosa (2013: 87). In this sense, popular self-help literature on âmindfulnessââthe practice of bringing oneâs attention to things occurring in the present moment, beyond any mediated communicationâis less about any durable containment of mediatization: it is rather an expression that deep mediatization includes spaces of self-reflection and controlled escape in order to remain manageable for us as human beings. With respect to these arguments, the concept of deep mediatization is neither an attempt at a closed theory nor a limited theoretical approach. There are various traditions of mediatization research, and such a range is needed because of the complexity of the field.3 However, across these different traditions, we can at a first level understand mediatization as a âsensitising conceptâ (Jensen 2013: 213â217; StrÃ¶mbÃ¤ck and Esser 2014: 4; Lunt and Livingstone 2016: 464); that is, a concept that âgives the user a general sense of reference and guidance in approaching empirical instancesâ and that âmerely suggests directions along which to lookâ (Blumer 1954: 7). This means to look at the overall spread of different media and the related changes in various social domains (Schulz 2014: 58â62). Using the term deep mediatization makes us âsensitiveâ to how far mediatization nowadays progresses into what has been called âmediatized worldsâ (Hepp and Krotz 2014: 6) and a âmediatized way of lifeâ (Vorderer et al. 2015: 259). At a second level, and departing from this, we need further concepts and approaches to describe in detail how the transformation that we relate to the term mediatization actually takes place. While we have a rough estimate of the processes and practices that constitute deep mediatization, we still lack thorough empirical investigations."
249,304,0.989,Advances in Proof-Theoretic Semantics (Volume 43.0),"1 Fregeâs Question Gottlob Frege opened his seminal paper Sinn und Bedeutung [3] by asking what the epistemic difference is between the equations a = a and a = b.1 His proposal to distinguish between sense and denotation (or reference) of a term turned out to be one of the most fruitful conceptual advances in the history of philosophical logic. Modern Possible Worlds Semantics draws on this distinction: the sense of a term refers to the full variety of possible worlds (in the way that we have to consider the denotation of a term in every possible world), while the (Fregean) denotation has to take into account only the actual world. As appealing as this view might be, there are (at least) two problems with it. First, it comes with a concept of rigid designators. Second, it is not applicable to mathematics, because mathematical equations hold equally in every possible world. While many approaches try to attack the problem from a semantic perspective, here we would like to provide a syntactic account, which takes up Fregeâs original question 1 In fact, he doesnât put this directly as a question, but rather states that âa = a holds a priori"
213,292,0.989,Collider Physics Within The Standard Model : a Primer,"Neutrino mixing is important because it could in principle provide new clues for the understanding of the flavour problem. Even more so since neutrino mixing angles show a pattern that is completely different from that of quark mixing: for quarks all mixing angles are small, while for neutrinos two angles are large (one is still compatible with the maximal value) and only the third one is small. In reality, it is frustrating that there has been no real illumination of the problem of flavour. Models can reproduce the data on neutrino mixing in a wide range of dynamical setups that goes from anarchy to discrete flavour symmetries (for reviews and references see, for example, [35, 37, 50â52, 264]), but we have not yet been able to single out a unique and convincing baseline for the understanding of fermion masses and mixings. Despite many interesting ideas and the formulation of many elegant models, the mysteries of the flavour structure of the three generations of fermions have not yet been unveiled."
305,105,0.989,Quantum Computing for Everyone,"We now turn to Einstein and see how he viewed these entangled states. Einstein and Local Realism Gravity provides a good example to explain local realism. Newtonâs law of gravity gives a formula that tells us the strength of the force between two masses. If you plug in the size of the masses, the distance they are apart, and the gravitational constant, the formula gives the size of the attractive force. Newtonâs law transformed physics. It can be used, for example, to show that a planet orbiting a star moves in an elliptical orbit. But though it tells us the value of the force, it does not tell us the mechanism that connects the planet to the sun. Although Newtonâs law of gravitation was useful for calculations, it did not explain how gravity worked. Newton, himself, was concerned about"
249,368,0.989,Advances in Proof-Theoretic Semantics (Volume 43.0),"argumentsâ. My ideas, as should be quite clear from my paper, are very far from Aristotle. They come from categorial proof theory, a mathematical field at the border of category theory and proof theory. Wilfrid seems to think that speaking of priority and primacy means one must be speaking about âwhat can be defined in terms of whatâ. One may reasonably claim that for explaining how an organism functions physiological notions, like for example homoeostasis, have primacy over anatomical notions, like for example parenchyma or stroma. This does not mean that the later notions are definable in terms of the former ones, and not vice versa. In defining anatomical notions concerning organs one may, but need not, rely on physiological notions, but one would equally rely on anatomical notionsâin particular on the notion of organâwhen defining physiological notions. In general, in the order of explanation, it seems indisputable that one may claim precedence for the notions of a science that seeks laws accounting for phenomena over the notions of a taxonomical science (see [1], Sect. 10). Nearer to the field of logic, theoretical linguistics and its notions would have for explaining how language functions precedence, primacy, over descriptive linguistics and its notions. In a different register, in the order of exposition and not the order of explanation, some notions can have for deep and natural reasons precedence over others without this meaning that the later notions are simply definable in terms of the former. In logic, one usually has that in the order of exposition the connectives of propositional logic have primacy, priority, over the quantifiers of predicate logic, without the latter being definable in terms of the former. In the foundations of mathematics, one usually has that in the order of exposition logical notions, the connectives and the quantifiers, together with the axioms concerning them, have priority over the set-theoretical membership relation, together with the axioms concerning it, without the latter being definable in terms of the former, as some authorities still expected a hundred years ago. I will return to matters of primacy towards the end of this note. I argue in [3] and elsewhere that the notion of inference should not be understood as the notion of consequence relation. I donât understand what Wilfrid means by saying before (1.10) that I forgot the codes of inferences. It is quite the opposite. I argue that an inference should not be taken as an ordered pair made of the premise and the conclusion, an ordered pair which is a member of a consequence relation. With inferences we do not have a relation, but a graph in the sense of category theory, which is given by a function assigning to every arrow an ordered pair of objects (some graph-theorists call that a directed graph, and others, following [8], could call it a directed pseudograph). Wilfrid finds after (1.9) that a notation for derivations (does he mean by that the same as I mean by inference or deduction?) in which he draws triangles âhas the advantage that it allows one to write several hypothesesâ. The usual notation in the style of Gentzen with sequents plural on the left can claim the same merit. In the context where we are interested in identity of inferences there is no mathematical loss, and there is a gain in clarity, if we restrict ourselves to the categorial format with a single object as the source of an arrow. Since the premises are finite in number, we can replace a plurality of them by their conjunction, and the absence of them by the propositional constant true. If on the other hand we are interested in the question of"
249,364,0.989,Advances in Proof-Theoretic Semantics (Volume 43.0),"Kostaâs question (10) asks whether (11) is definable from (12), and he expects the answer No. Clearly Kosta is right: (11) is not definable from (12) (and a fortiori not âreducible toâ (12)) for the glaring semantic reason that (11) carries an extra argument. This is not just an accident of Kostaâs formulation. Itâs an essential part of the notion of z being inferable from y that people can perform an act called making an inference from y to z, but it is certainly not part of the notion of consequence that people can make a consequence. And I agree with Kosta that this is a point worth making. I also agree with him that for purposes of the foundations of logic, a psychological analysis of âmaking an inferenceâ is not the right way to go. But then why does Kosta add this comment? This reduction of inference to implication, which squares well with the second dogma of semantics, is indeed the point of view of practically all of the philosophy of logic and language in the twentieth century."
124,496,0.989,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"the (possible) moments and (possible) histories of a branching time structure. We then show how the problem of ontological competition can be solved by adding an incompatibility partition to a structure of possibilia, and conclude with some remarks about how this addition might provide a clue for developing a variant of the theory of branching space-time that can account for the trousers worlds of general relativity."
360,491,0.989,Compositionality and Concepts in Linguistics and Psychology,"sorts of semantic operations. The composition of vectors happens outside of the DRT model, but as the result is also a vector, it can, like the component vectors, be assoââââââ ðð¨ð±). âââââââ ciated with a constant in a DRS, which we will represent as e.g. comp(ð«ðð, âââââ ââââââ âââââ ââââââ In other words, constants of the form ð«ðð, ðð¨ð±, and comp(ð«ðð, ðð¨ð±) are all of the same type. Thus, distributional semantics will give us a relatively concrete algebraic model for simple and complex concepts on which both sorts of concepts are of fundamentally the same nature, much in the way lattice-theoretic structures serve as models for treating atomic entities and pluralities as fundamentally similar types of objects (Link 1983). The next piece we need is a way to exploit nouns and adjectives with such interpretations within DRT, so that referents can be associated with the concepts that nouns and adjectives pick out. Zamparelli used Carlsonâs (1977) realization relation, which we represent here as Realize, aims to do this: This relation holds between an object and a kind just in case the object constitutes an instance of the kind.19 Again following Zamparelli, we assume that the Realize relation is introduced by (possibly abstract) functional morphosyntax that turns a noun into an expression that denotes a set of entities. As a first approximation, then, we can represent a referential expression such as a box as in (8), where u is the discourse referent introduced by the phrase, âââââââ which must satisfy the condition that it is a realization of the concept ðð¨ð±. âââââââ Realize(u, ðð¨ð±) Now consider modification. Prior to the point in the syntax at which the Realize relation is introduced, the composition operations at work will combine vector-denoting expressions; this corresponds to concept composition. We model conceptuallyafforded composition as the result of composing adjective and noun vectors directly into a new vector, corresponding to a complex concept, which can then stand in the Realize relation to a discourse referent, as in (9). ââââââ ðð¨ð±)) âââââââ Realize(u, comp(ð«ðð, The syntactic rules of the language will have to make it clear when this sort of composition can be appealed to and when not; interestingly, studies of the syntax of modification clearly indicate that syntax could, indeed, encode this kind of information (see, e.g. McNally and Boleda 2004 and Bouchard 2005 on adjective ordering constraints of the sort exemplified by relational adjectives). Now let us consider referentially-afforded concept composition. As mentioned in Sect. 3, this is attested only when the referent of the nominal is already familiar in the 19 Carlsonâs ontology also included stages (spatiotemporal slices) of individuals, which could also"
360,293,0.989,Compositionality and Concepts in Linguistics and Psychology,"Another logical step would be to investigate other cases in language where typicality affects reasoning. So far we have seen that understanding both reciprocal sentences and the sentences with conjunction that were investigated in the current chapter, is inseparable from the study of concepts. Another area where we see typicality affecting interpretation, is the area of adjective-noun constructions such as red hair (Lee 2017). For such a construction, the typicality structure of hair appears to interact with the way we interpret the adjective red. Even though the concept red in isolation might have as its most typical instance a focal red, orange-like hues are generally more typical for the concept hair. When the two combine, these typicality preferences interact (for more on these effects see the work by Lee (2017) and Winter (2017)). This interaction is intuitively of a similar nature to the one between a verb concept like pinch and the reciprocal expression each other, as well as the one between verb concepts like sitting and cooking and the logical expression and. It is highly likely that these are not the only areas in which this is the case, thus it is worthwhile for further research to investigate whether a principle like the MTH can function as a general principle of language use."
269,196,0.989,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"the phrase âplaya haterâ serves to conjure up. Letâs face it, sheâd felt the envy of the hater. In her potent analysis of âugly feelingsâ, Sianne Ngai argues that âenvy lacks cultural recognition as a valid mode of publicly recognizing or responding to social disparities, even though it remains the only agonistic emotion defined as having a perceived inequality as its objectâ (Ngai 2005, 128). The temptation is always to tell the envious person to âget over itâ â where âitâ is a designator of the inadequacy of that individualâs psyche â rather than to think through the imbrication of the psychological and the social so as to understand what it is that provokes the desire to inhabit (or to crush) the space of the other. It would be fair to say that FC hasnât âgot over itâ; her envy is still strong. But the introduction â and then circulation â of the figure of the playa hater ended up, over time, having a significant effect on how she herself thought about, and indeed entered into, collaborations with those from other disciplines â as well as how she thought about the spatial logics of the interdisciplinary field in which she was a participant. The diagnostic precision of the phrase âplaya haterâ perhaps allowed her, over time, to move from âfeeling fuzzyâ about how such an appellation had ended up being attached to her, to being able to inhabit an interdisciplinary landscape that she was less likely to construe in such agonistic terms."
306,18,0.989,"Early Geometrical Thinking in the Environment of Patterns, Mosaics and Isometries","During the ï¬rst levelâthe visual levelâconcepts develop on the basis of experiences and conscious observations from reality: students ï¬rst learn to recognize shapes then analyse the properties of the shapes. The visual level is the main step in spatial knowledge. On the visual level, students recognize a ï¬gure as a whole and are able to represent it as a mind vision. Note that Van Hiele (1986) states that âthe levels are situated not in the subject matter, but in the thinking of manâ and Arcavi (2003) suggests that visualization can be considered as a method of âseeing the unseen.â Moreover, Viholainen (2006) states that âvisual thinking is probably the most usual type of informal thinking in mathematics.â At the visual level, therefore, the student: â¢ identiï¬es, compares, and sorts shapes on the basis of their appearance as a whole; â¢ solves problems using general properties and techniques (e.g., overlaying, measuring); â¢ uses informal language; â¢ does not analyse in terms of attributes. Later students see relationships between shapes and make simple deductions. Only after these levels have been attained can they create deductive proofs. Van Hiele did not deem that any of the levels was free from the thinking. In particular, it cannot be assumed that the visual level eliminates action (manipulation) by objects. De Lange (1987), presenting his interpretation of Van Hieleâs"
223,136,0.989,Knowledge and Action (Volume 9.0),"But the palace is also a marvelous movie theater, one projector in each of the basement corners, golden rays carrying the alternative translations from the machine rooms to the screens of the opposite walls: the glass of Marcel Duchampâs La mariÃ©e mise Ã  nu par ses cÃ©libataires, mÃªme (1914) (The Bride Stripped Bare by Her Bachelors, Even); the limestone wall of Platoâs cave; the wood panel of Fra Angelicoâs Annunciation; all found again in the mappa of cartographic reason. And when the projections of the imagined identities hit the sheets of glass, they miraculously change into a set of Peircean signs, no longer the private fantasies of their inventor but communicable bits in an evolving discourse. To be technical, the a becomes the symbol of a, the a = a the icon of a = a, the a = b the index of a = b. But just as the painterâs canvas must be properly prepared for the paint not to crack or run off, so must our minds be indoctrinated to ensure that all that is solid does not melt into air. Three grand institutions have risen to the task: religion (with its belief in the a of shared conventions), art (with itâs a = a striving for perfect resemblance), science (with itâs a = b, the as-if knowledge of provisional truth). Each mode of understanding entrenched within its own self-supporting power structures, rituals, rules, and regulations. If these rituals could be perfectly performed, then the projection lines would strike the screening planes at 90Â° angles, every message going straight back to the cornered restatement it came from, nothing learned in the process. But even though the Saussurean/Lacanian sign is steeped in mimetic desire, the diverse ontologies of Signifier and signified guarantee that this perpetual urge can never be satisfied. Hence the fortunate consequence that no translation can ever be perfect. It follows that in actuality the inclination of the (en)lightening rays is never right on and that the projections, instead of returning to the original identities unchanged, they begin to bounce between the walls. In turn, this slight deflection means that whatever I happen to think, say, and do is never pure and simple but always a nondissolvable blend of religion, art, and science. And suddenly I see where the trigger of tragedy lies: in the purifying spirit of the right angle, in the hatred of the other which is built into the desire of every identity formulation. Hitlerâs Lebensraum, Stalinâs Gulag, the Rwanda genocide, the iconoclastic controversy, Jyllands-Postenâs Mohammed picturesâall of them variations on the same theme. Murderous is our history, murky the connection between knowledge and action. In turn, this analysis explains why for 40 years tragedy has occupied such an important place in my own conception of what it means to be human, indeed why I take it to be the most insightful of all available conceptions of thought-in-action and action-in-thought. The original setting is crucial, for Sophoclesâa Janus-like figure who with one eye was scanning the old, with another was imagining the futureâ lived his long life in the abyss between the mythos of Homer and the logos of Plato. What he then discovered was that the greatest tension of his time lay in the attitudes to predicament, for while the archaic poets had taken a personâs social standing to reflect his or her ability to handle contradiction, the new philosophers defined"
360,161,0.989,Compositionality and Concepts in Linguistics and Psychology,"As various writers in the past have claimed, the former is âinertâ and âis a mere congeriesâ, whereas the latter actually âdoes somethingâ in making an assertion that describes a feature of Kim. Note that a structurally identical puzzle arises if âWordâ is replaced by âConceptâ and âMy assertionâ is replaced by âMy judgmentâ. Both Objectivists and Subjectivists are subject to this puzzle, and they both should give some sort of answer. However, Subjectivists seem not to have thought of a difference between the noun-phrase conceptual combination {SANDY; SWIM } and the sentential conceptual combination {SANDY IS SWIMMING}. Nor has it seemed to occur to them that in order to give a concept-analysis of some noun phrase like a man who is tall, there needs to be a sentential analysis of the embedded relative clause (as well as an analysis of the contribution of âaâ, as mentioned above in Sect. 8.1.2). This puzzle is at least as old as Plato, and has provoked considerable discussion over the centuries. One type of answer might be to postulate a copula-like intermediary, call it âexemplificationâ, with the idea that it is this intermediary that causes the other items to merge into a full sentence. But as one can pretty easily see, this just leads to some sort of infinite regress: for, now there needs to be a further explanation of how it is that âKimâ (or the person corresponding to that name) can combine with this newly postulated object to then be able to combine with ârunsâ (or the object corresponding to that predicate). This is known as Bradleyâs Regress, or The Unity of the Proposition/Sentence problem. Objectivists have proposed various answers to the Unity problem, one of the most popular being some variant on the metaphor of saturated/unsaturated from Frege (1892a). The idea is that singular terms are such that they are designed to designate âindependentâ or âsaturatedâ objects, whereas predicate terms are designed to designate âunfilledâ or âunsaturatedâ items, namely âconceptsâ. (Recall that for Frege and the Objectivists, a concept is a non-mental, abstract entity.) When these two different kinds of entities âmeetâ, the saturated object âfills inâ the unsaturated object, producing a new, saturated objectâa complete sentence (which expresses a âthoughtâ, in Fregean terminology. . . where âthoughtâ is also understood as a non-mental, abstract item). Well, whatever one thinks of such metaphorical accounts (and something akin to this is adopted in Formal Semanticsâ model-theoretic theories), it is at least some sort of explanation, showing that there might be some wiggle room in Objectivist theories that would provide an appropriate description of what is going on. However, it seems that Subjectivist accounts are not in such a good place, at least not if they hold that lexical concepts designate mental items like concepts. For it seems that this would presume there to be a concept of INSTANTIATIONâsort of a mental version of the linguistic exemplification, and now it appears that the Bradley regress can start. Subjectivists of the sort we have been considering will respond by claiming that conceptual combination is how they handle this topic. But it should be noted that conceptual combination in these works is defined for noun-noun and adjective-noun combinations (perhaps also for adverb-verb combinations?). But this wonât work for sentence generation: perhaps the conceptual combination of DOG and"
330,213,0.989,Dynamics of Long-Life Assets : From Technology Adaptation to Upgrading the Business Model,"Many authors have tried to characterize and measure VR-technologies in terms of quality of the experience. It is however an evasive quality and hard to measure in a quantiï¬able way. Gibson for example, who predates Steuer (1992) also talks of presence as the measure (Gibson 1979). In present terminology the word immersion is often used to describe the quality of the VR system. Immersion denotes the quality of the sensory stimuli that the system can produce. It is related, although not directly, to the subjective feeling of âpresenceâ of the user. And logically the greater the quality of the stimuli the higher the probability of achieving a high level of presences. Though as many researchers in the ï¬eld note, presence is highly dependent on the individual and some individuals have a greater capacity to experience presence. Presence can be interpreted as a measure of the extent the user forgets the medium to the beneï¬t of the experience of âbeingâ in the virtual environment (Loomis 1992). Other examples are Loeffler and Anderson (1994) who deï¬nes VR as âa 3D virtual environment that is rendered in real time and controlled by the usersâ. Similarly to Steuer (1992) framework, they include the concepts of vividness (rendering) and interactivity (control). Although it seems to be narrower in the sense that is only alludes to visual stimuli, rendering. There have been attempts at quantifying both immersion and presence. Pausch attempted to quantify the level of immersion in VR (Pausch et al. 1997). Meehan et al. (2002) wrote about physiological measurements of the VR experience by invoking stress on the subjects to grasp the fleeing aspect of presence. The measurements extended to heart rate, skin conductance, and skin temperature to determine the reaction of the test subject and compare to the change in the same measures given a real situation. The logic being that if our reactions to a situation in the virtual environment mimics our reaction to the same situation in the real world, our mind and bodies are likely believing the experience. The topic is debated from a different standpoint by Bowman, who poses the question of how much immersion Table 1 Strengths and weaknesses of 3D visualisation (Teyseyre and Campo 2009) Strengths"
227,36,0.989,Problem Solving in Mathematics Education,"itself. At the same time, however, there is a great reliance on prior knowledge and past experiences. The Gestalt method of problem solving, then, is at the same time very different and very similar to the process of design. Gestalt psychology has not fared well during the evolution of cognitive psychology. Although it honours the work of the unconscious mind it does so at the expense of practicality. If learning is, indeed, entirely based on insight then there is little point in continuing to study learning. âWhen one begins by assuming that the most important cognitive phenomena are inaccessible, there really is not much left to talk aboutâ (Schoenfeld 1985, p. 273). However, of interest here is the Gestalt psychologistsâ claim that focus on problem solving methods creates functional ï¬xedness (Ashcraft 1989). Mason et al. (1982), as well as Perkins (2000) deal with this in their work on getting unstuck."
118,768,0.989,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"animal. As soon as we conjure up the most familiar images of flower buds or young leaves on a tree, or creatures hatching from their eggs, we are reminded that new life is utterly fragile and miraculous, and appeals to us for protection. This is the frame of mindâwonder and humility when witnessing the gift of life, and a sense of responsibility for the well being of all living thingsâthat undergirds scientist citizenship. At the end of Devilâs Tango, in the chapter called âWhat the Light Was Like,â14 Pineda presents us with another sensory exercise to complement the first one. This time we are asked to imagine a scene called up from the authorâs pastâ her memory of gazing at trees bathed in sunlight. Pineda recalls how she was able to comprehend the passage of time by watching how the light moved across a grove. The light embraced in turn each tree and every part of each tree as the earth turned on its axis, a movement normally imperceptible to us yet on that day made perceptible to her through attentiveness to the caressing passage of sunlight over trees. Both of Pinedaâs sensory exercises are telling us to direct our gaze away from outer space toward this beautiful planet that we already inhabit, because without total regard for Earth, we risk destroying it beyond repair. Especially in the episode of remembering how sunlight moved across a grove, Pineda calls attention to the miracle of in/finite space and in/finite time that we are always capable of perceiving in the here and now. These sensory exercises re-inscribe a scientistâs understanding of in/finite space and in/finite time in the language and point of view of a poet. For although space and time are foundational concepts in all fields of inquiry, philosophy, art, science, and social science have different ways of representing and thus comprehending space and time. The sensory images comprising Pinedaâs instructions for imagining the amplitude of 86 planets and thereby re-cognizing our commitment to planet Earth, and the sensory images comprising Pinedaâs instructions for seeing what she saw on that day of sunlight passing over trees, come from the discipline of poetry and exemplify her placement of the poetâs toolbox in the service of the lunchbox. The most prominent example of Pinedaâs poetic language is of course the metaphor âdevilâs tango,â which is used to illuminate the fact that nuclear history records a dance with deathâHomo sapiensâ apparent addiction to nuclear technology no matter how great its known record of devastation and irreversible damage. Poetic language is not something for writers or literature scholars only, but is part and parcel of the language skills needed by a nuclear engineerâby any scientist or technicianâto communicate specialized knowledge to laypersons, by virtue of the fact that poetic language is the primary language through which we comprehend and express the beauty of life and the gift of human being. To be a nuclear engineer without literacy in poetic language is to be like a computer with a voice, able to speak oneâs expert knowledge but devoid of any context of lived life"
124,340,0.989,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"tories are isomorphic to the Minkowski spacetime (see Placek and Belnap (2012)). BST1992 can be used to model quantum experiments with non-local correlations (Placek 2010). Furthermore, a branching reading can be given to the consistent histories formulation of quantum mechanics (see MÃ¼ller (2007)). This bright picture, however, has been marred by a tension between BST1992 and general relativity (GR). There are serious obstacles to accommodating GR in the branching framework, the most important of which, I believe, is a difference in spirit. The great perception of GR is that coordinalization works by patches: this theory permits the assignments of coordinates (elements of Rn ) to subsets (patches) of the totality of events, with the proviso that the patches cover the totality of events. Local coordinalization by patches is to be contrasted with a global coordinalization, as provided by a mapping of a whole spacetime on Rn . Patches, if sufficiently small, have familiar and desirable properties. In essence, they look like subspaces of Minkowski spacetime,2 which in turn permits a definition of a partial ordering on a patch. Typically these nice properties do not transform to a GR spacetime as a whole, however. In contrast, BST1992 does not work in terms of local patches. This theory assumes a partial ordering on its base set, and defines history (aka BST spacetime) as a maximal upward directed subset of the base set. With some extra assumptions added, a BST1992 history can be mapped on Rn . Even if one wants to do coordinalization in a piecemeal way, there is no structure in BST1992 that could play the role of patches. Apart from this difference in spirit, there are technical issues as well: First, the ordering assumed in BST1992 is partial, whereas the natural ordering of a GR spacetime, defined in terms of geodesics, is not necessarily so: it allows for a failure of anti-symmetry. Second, the BST1992 criterion for historicity (or, belonging to one BST spacetime), i.e., being maximally upward directed, flies in the face of some wellstudied GR spacetimes, like the Schwarzschild spacetime or the de Sitter cosmological model. The criterion rules out as well some intuitive, although non-physical, candidates for a spacetime since it implies that for two events x and y to belong to some one spacetime, there should be a âlater witnessâ, that is, some z such that x â©½ z and y â©½ z. Consequently, an open square or an open half-plane Râ Ã R, both with Minkowskian ordering, cannot be BST1992 spacetimes.3 A sought-for generalization of BST1992 should thus modify the criterion for historicity appropriately. (For a discussion as to how one can modify the BST1992 notion of history, see MÃ¼ller (2013).) The first attempt to overcome the tensions between GR and BST1992 is MÃ¼ller (2011). The present chapter continues this work in a somewhat different way, by first generalizing BST1992 appropriately, then defining generalized manifolds on models of generalized BST and, finally, by producing tangent vector spaces. Although the main aim of this chapter is to offer a GR-friendly generalization of BST1992, I begin by addressing an objection to BST1992. As John Norton once 2 Strictly speaking, these are properties of tangent spaces rather than of subsets of events. 3 This ordering â©½ is defined on Rn by putting x â©½ y iff x â©½ y and"
306,63,0.989,"Early Geometrical Thinking in the Environment of Patterns, Mosaics and Isometries","Many geometric activities can take place without words and without symbolism. Visual language is very rich in content so it is possible to use it at very early educational levels. The act of constructing patterns and making tessellation requires a long sequence of elementary acts: observation, ordering, copying, and repeating. Swoboda (2005) showed that drawing a pattern is not merely perceptual copying, but is a deep thinking process, which involves the body and gestures (Marchini and Vighi 2005). Of the domains of knowledge where children must enter, geometry is the one that needs the fullest cognitive activity, as it requires gestures, language, and seeing. It requires the child to construct, reason, and see; each activity is indissoluble from the others (Duval 2005). Arzarello (2004) emphasized the role of body movements and gestures in learning. Gestural expressiveness can be considered a sort of language useful to understand pupilsâ thoughts, taking into account the poor language competencies of children of those ages. Geometry as rhythms and patterns gives a chance to code and de-code rules and formulas at a very low level. The passage from perception through geometrical symbolic representation to verbal mathematical description is very long and often strange for children. Engagement in activities creates a good opportunity to create a space for discussion. Children start to transform these relations into words. By talking, these relations gain the status of existence. They emerge gradually from experience and start to be facts related to the mathematical world. a. Creation of argumentation Example 3 (Work in a small group) Children made a paper âpath,â drew âstones,â and painted them in three different colours, making patterns. The next day, the groups exchanged âpaths.â The teacher covered one stone with a sheet of paper. Children were then asked to say what colour this stone was and give the reason why their opinion was correct.1 Children could use general, abstract argumentation to see âa general in particularâ (Fig. 36) Ola (5), Wiktoria (5), and MichaÅ (4) T: Who knows which colour the covered stone is? MichaÅ: White. T: And why do you think that it is white, MichaÅ?"
8,538,0.989,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","23.1 Introduction Properties of nuclear matter have inspired much of the theoretical work in manybody theory during the last decades. While initially attention was focused on the saturation properties of cold nuclear matter, more recently the advent of high-energy heavy ion accelerators has stimulated work on the high temperature and density domain of the phase diagram. There exist several main lines of approach to this complicated theoretical problem in which substantial simplifications of the actual physical circumstances are supposed. We will not review these approaches here except to say that they can be divided into two categories: (1) the nuclear matter is considered to be a non-interacting ideal gas; or (2) nuclear interactions are considered at the level of classical particle scattering. It is immediately apparent that the interesting features of nuclear matter, such as density isomerism at high temperatures, phase transitions, condensation phenomena, etc., can hardly be discussed in the framework of the ideal gas equations of state. The fact that some kind of agreement of inclusive particle spectra in heavy ion collisions is found between theory and experiment is in fact only indicative that a thermal equilibrium is achieved in a fireball created in the collisions. To find out more about the properties of these fireballs, one has to perform more refined experiments and consider a more elaborate theory. This aim is achieved in a nonthermodynamical way in the approaches that deal with the A1 C A2 many-body problem, in which each particle is followed during the collision; but it becomes virtually impossible to identify the relevant collective motion that is characteristic of phase transitions and critical phenomena. In order to derive the physical properties of hot nuclear matter which are independent of a particular choice of the two-body and multibody interaction, we employ a technique (âbootstrapâ) developed for similar problems in elementary particle physicsâhere, however, sufficiently modified to suit the different physical environment. An additional motivation in this direction is the recent recognition that the understanding of nuclear matter at the saturation point depends very sensitively on the character of the two-body potential at short distances, which is not well defined by two-body reactions. It is possible to view the bootstrap technique only as a convenient way of introducing the relevant physical properties which cannot be so easily defined by the choice of a specific potential, but which globally might even be more important than details of the two-body force. We will concentrate on the gross features of nuclear matter that follow when we incorporate into the description the following aspects of nuclear interactions: (i) conservation of baryon number and clustering of nucleons (i.e., attractive forces leading to many-body clusters with well-defined baryon number); (ii) nucleon (isobar) excitations and internal cluster excitations (i.e., internal degrees of freedom that can absorb part of the energy of the system at finite temperature, thus transforming kinetic energy into mass);"
8,428,0.989,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","By the reinterpretation of our model, we have of course abandoned the point of view of the canonical ensemble of temperature T and have obtained the description of a single system of given energy E. This system must no longer be thought of as being in contact with its surroundingsâwhich would be hard to imagine for a highenergy collisionâbut, thanks to the peculiar behaviour of E.T/, it has a temperature T ! T0 in its own right. As for the value of T0 D . 2 =V/1=3 , it must be chosen such that, at least, the system itself conserves the main features of a thermodynamical system: the particles must be able to interact with each other. As we wish to describe highenergy collisions of strongly interacting particles, where the particles produced will escape radially from the region of interaction, they will cease to interact once their mutual distances become much larger than the range of forces, i.e., the Compton wavelength of the pion. Thus the volume V is to be taken as"
60,28,0.989,Research on Teaching and Learning Probability,"2.2.2 Paradoxes and Counterintuitive Results Probabilistic reasoning is different from reasoning in classical two-valued logic, where a statement is either true or false. Probabilistic reasoning follows different rules than classical logic. A famous example, where the transitivity of preferences does not hold, is Efronâs intransitive dice (Savage 1994), where the second person who selects a die to play always has an advantage in the game (no matter which die their opponent first chooses). Furthermore, the field of probability is replete with intuitive challenges and paradoxes, while misconceptions and fallacies are abundant (Borovcnik and Kapadia 2014b). These counterintuitive results also appear in elementary probability, while in other areas of mathematics counterintuitive results only happen when working with advanced concepts (Batanero 2013; Borovcnik 2011). For example, it is counterintuitive that obtaining a run of four consecutive heads when tossing a fair coin does not affect the probability that the following coin flip will result in heads (i.e., the gamblerâs fallacy)."
17,29,0.989,The Philosophy of Mathematics Education,"This initial tension between Lakatos and Kitcher has never left (up to now) the study of mathematics through its practices. The nature of that tension is anything but new. We have known it in the philosophy of science in the form of the context of discovery versus context of justiï¬cation divide. A justiï¬cation is preferably seen as something independent from the discovery process. In other words, if I need to justify something, I need not wonder about how it is has been found. In the case of mathematics, if such an independence holds, then I only need to look at the ï¬nished proof, the ï¬nal version and wonder whether or not I can justify that this text that claims to be a proof, indeed is a proof. The processes that led to the proof are of no"
235,102,0.989,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","consisting perhaps in passing it through various kinds of sorting apparatus, such as slits and polarimeters, the system being left undisturbed after the preparation.â SchrÃ¶dinger, in his Generalbeichte [452, Footnote 1, p. 845] (general confession) of 1935, pointed out that [539, Sect. 6, p. 328] âActually [[in truth]]âso they sayâ there is intrinsically only awareness, observation, measurement. If through them I have procured at a given moment the best knowledge of the state of the physical object that is possibly attainable in accord with natural laws, then I can turn aside as meaningless any further questioning about the âactual state,â inasmuch as I am convinced that no further observation can extend my knowledge of itâat least, not without an equivalent diminution in some other respect (namely by changing the state, see below).â1 No further justification is given here. A quantum state is thus identified with a maximal co-measurable (or co-preparable) entity. This is based on complementarity: not all conceivable quantum physical properties are co-measurable. (For classical models of complementarity, see, for instance, Mooreâs discrete-valued automaton analogue of the Heisenberg uncertainty principle [373, 446, 499], as well as Wrightâs generalized urn model [578], and partition logics in general [511].) In the Hilbert space formulation of quantum mechanics a state is thus formalized by two entities; some structural elements, and a measure on these elements [520]: (I) equivalently, (i) an orthonormal basis of Hilbert space; (ii) a set of mutually orthogonal projection operators corresponding to an orthonormal basis called context; (iii) a maximal observable, or maximal operator, or maximal transformation whose spectral sum contains the set of mutual orthogonal projection operators from the aforementioned basis; (iv) a maximal Boolean subalgebra [249, 300, 376, 420] of the quantum logic also called a block; (II) as well as a two-valued (0-1) measure (or, used synonymously, valuation, or truth assignment) on all the aforementioned entities, singling out or selecting one of them such that this measure is one on exactly one of them, and zero on all the others."
360,54,0.989,Compositionality and Concepts in Linguistics and Psychology,"object coming into being, or just the parts that are arranged appropriately? Are the (computational) connectionists right in saying that every property of a connectionist system is an emergent property?5 We leave these questions aside at this point. But compositionally of the Ontological variety is not the only notion in the neighbourhood. When discussing linguistic matters, it is far more common, at least in philosophy of language and formal semantics, to see a discussion that presumes there to be an already-given, well-defined system of the sort that Ontological Compositionality has characterizedânamely the syntactic structures of language, treated as a complex object made from atoms (and not as a newly-created whole). The atoms are lexical items and the wholes that are made from them are syntactically-defined larger and larger linguistic units. But if we become interested in some property of this systemâfor instance, in the meanings of the itemsâthe issue then becomes whether there is a way to define this propertyâthe meaningâof all complex items solely in terms of the meanings of the syntactically-given parts of the complex item and their syntactic method of combination. A âyesâ answer to this for every complex member signals that the semantic theory assigns meanings compositionally. One might note that in this kind of compositional theory, there are two structures: the first, antecedently-given one (in the language case, the syntactic structure), and a second structure (in the language case, it would be the structure that âthe meaningsâ manifest) which is a kind of mirror of the first structure. So another way of asking whether a semantic theory is compositional in this sense is to ask whether there is a homomorphic mapping from the syntactic structure to the semantic structure. In the language case, this mapping is called âthe meaning functionâ, which most authors writing about semantic compositionality symbolize by ð. So, X = ð(A) means that (i) A is some element of the first structure [i.e., A is some syntactic item], and (ii) that X is the item in the meaning structure that is paired up with A (i.e., X is the meaning of A). Then this second conception of compositionality asserts that there is a function f such that whenever A is composed (in the sense of Ontological Compositionality) of B, C, D . . . by means of syntactic method R, then ð(A) is f applied to < ð(B), ð(C), ð(D), â¦ ð(R) >. That is: the system is compositional if and only if, there is a function f such that for every (syntactically) complex item A in the syntactic system, its meaning ð(A) is a function of, and only of, the meanings of Aâs syntactic parts, together with the way they are combined. If NP1 and VP1 make up the entirety of S1 and they are combined by rule-R to do so, then ð(S1 ) = f (ð(NP1 ), ð(VP1 ), ð(R)). Underlying this type of compositionality is the slogan: Definition 2 (Functional Compositionality) The ð of a whole is a function of the ðâs of its parts and the ways those parts are combined."
342,39,0.989,Semiotics in Mathematics Education,"The summary of influential semiotic theories conducted in the previous section provided an idea of the impact of these theories in mathematics education. In this section we discuss in more detail the impact that semiotics has had in speciï¬c problems of mathematics teaching and learning. As previously mentioned, two different approaches can be distinguished within semiotics, depending on how signs are conceptualized: a representational one, in which signs are essentially representation devices, and one in which signs are conceptualized as mediating tools (Radford 2014b). There is still a third approachâ a dialectical materialist oneâin which signs and artefacts are a fundamental part of mathematical activity, yet they do not represent knowledge, nor do they mediate it (Radford 2012). This is the approach to signs, artifacts, and material culture in general that is featured in the theory of objectiï¬cation (Radford 2006b, 2008b, 2013b, 2015a, b). Such a conception of signs and artefacts is consubstantial with the conception of the dialectical materialist idea of activity. This conception of activity is very different from usual conceptions that reduce activity to a series of actions that an individual performs in the attainment of his or her goal. The latter line of thinking reduces activity to a functional conception: activity amounts to the deeds and doings of the individuals. Activity in the theory of objectiï¬cation does not merely mean to do something. Activity (TÃ¤tigkeit in German and deyatelânostâ"
363,42,0.989,History and Cultural Memory in Neo-Victorian Fiction,"(ibid.: 154). Memory is not retrieved for âthe time of textuality is not the linear, before-and-after, cause-and-effect time embedded in the logic of the archive but the time of a continuous analeptic and proleptic shapingâ (ibid.: 154). Here, again, memory is inextricably bound to the very fabric of the present as the means by which present and past make and remake each other. And, understood this way, memory resists identifying a singular origin but mimics instead a series of fragments and repetitions. Throughout History and Cultural Memory in Neo-Victorian Fiction I explore the ways in which the reworking of the Victorian period in neo-Victorian novels embody its uncanny repetition, so that the period is both shaped by and shapes our twenty-first century present. The re-presentation of the past entailed is not the assertion of historical Truth but is simply the âweirdness of a ghostâ (Rody, 1995: 104). In a similar vein, Cora Kaplan argues that we should understand âVictorianaâ as âwhat we might call history out of place, something atemporal and almost spooky in its effects, yet busily at work constituting this time â yours and mine â of late Capitalist modernityâ (Kaplan, 2007: 6). As we shall see, a great many contemporary historical fictions that return to the Victorian era are preoccupied with images of ghosts and metaphors of haunting, especially positioning the fictional text as medium of the past.6 The materiality of the ghost is illusory and always already under erasure. The ghost is an evocative metaphor for the past, as âthe nothing-and-yet-not-nothing and the neither-nowhere-nor-not-nowhere that nonetheless leaves a trace in passing and which has such a material effectâ (Wolfreys, 2002: 140). Embedded in the figure of the spectre is indeterminacy and incompletion. As Nick Peim argues, âthe authenticity of the spectre is always questionable â a function of the gap between its partial nature and the full version it claims to representâ (Peim, 2005: 77). The ghost becomes a useful metaphor for charting a position for these novels between the positing of history as Presence, a locus of univocal meaning, and the ironic subversion or negation of the very possibility of historical knowledge. Yet while Rody identifies the ghost with âa fearful claim of the past upon the presentâ (Rody, 1995: 104), a phrase that grants the past agency, the ability to make demands upon the present, I would suggest that the ghost signals rather the uncanny repetition of the past in the present. The ghost speaks with the voice of flesh and spirit, and adopts its look, but, in its very essence, or âinessenceâ, as Jacques Derrida would have it, it is departed. Its very disappearance is held always before it: âThere is something disappeared, departed in"
8,920,0.989,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","nomenological descriptions based on an observed picture of nature. For example, it is difficult to argue that, were the colour symmetry SU(2) and not SU(3), we would still observe the resonance dominance of hadronic spectra and could therefore use the bootstrap model. All present understanding of phases of hadronic matter is based on approximate models, which requires that Table 32.1 be read from left to right. I believe that the description of hadrons in terms of bound quark states on the one hand, and the statistical bootstrap for hadrons on the other hand, have many common properties and are quite complementary. Both the statistical bootstrap and the bag model of quarks are based on quite equivalent phenomenological observations. While it would be most interesting to derive the phenomenological models quantitatively from the accepted fundamental basisâthe Lagrangian quantum field theory of a non-Abelian SU(3) âglueâ gauge field coupled to coloured quarksâwe will have to content ourselves in this report with a qualitative understanding only. Already this will allow us to study the properties of hadronic matter in both aggregate states: the hadronic gas and the state in which individual hadrons have dissolved into the plasma consisting of quarks and of the gauge field quanta, the gluons. It is interesting to follow the path taken by an isolated quark-gluon plasma fireball in the ; T plane, or equivalently in the ; T plane. Several cases are depicted in Fig. 32.2. In the Big Bang expansion, the cooling shown by the dashed line occurs in a Universe in which most of the energy is in the radiation. Hence, the baryon density  is quite small. In normal stellar collapse leading to cold neutron stars, we follow the dash-dotted line parallel to the  axis. The compression is accompanied by little heating."
107,277,0.989,"Symbiotic interaction : 5Th international Workshop, Symbiotic 2016, Padua, Italy, September 29â30, 2016, Revised Selected Papers","There are indeed ways to display that one is taking informations into account without disturbing the ongoing accomplishment of the speakerâs turn. What is striking here, is that the participant is not only displaying this, but sheâs also managing a basic co-presâ ence problem [19]: through a meticulous to and fro eye (and head) movement (L03), she operates the possible actions enabled by turn constructional units organization [21, 35, 45] to keep track of both the participation frame and the delivered information on herself. In other words sheâs considering the robotâs turn as a component of a larger organizational process: we can see here how body posture, eyes movements and speech are entangled, in order to structure an intricate event like âreceiving information about oneselfâ. Moreover, such methods that consist in moving body orientation from the speakerâs âfaceâ to an alternative (imaginary) space where one can accountably think over (i.e. showing a process of thinking), illustrate how treating the robot as a sociointeractional partner could be achieved, in the present of interaction. This phenomenon can be even more intricate. In the following extracts two particiâ pants interact with the robot, P2 is attending to the quiz, P is the one oï¬cially âconnectedâ to the robot:"
223,183,0.989,Knowledge and Action (Volume 9.0),"As aptly as these introductory words by SchÃ¼tz and Simmel summarize my own hypothesis on the presumed phenomenon of non-knowledge,1 I note that it is captured still more precisely by economist Joseph Stiglitzâs (2005) formulation about the âinvisible handâ (p. 133) ostensibly operating in the market place. Asked why the invisible hand is invisible, Stiglitz gave a straightforward answer: because it does not exist. Similarly, I ask in this chapter why non-knowledge is difficult to grasp. And my equally analogous response is: because there is no such thing as non-knowledge. Not wishing to capitulate already at this early point, I concentrate in this chapter on scientific discourses in which participants maintain that something like nonknowledge does exist. The knowledge/non-knowledge dichotomy appears in many discussions on the subject as a kind of performative speech act (Sartori, 1968). However, it recommends only one side of that which it designates, namely, knowledge. I cannot quite sustain my doubt about the existence of not-knowing; from time to time I have to deviate from it and maintain that non-knowledge does exist. At the same time, I draw attention to other terms that are empirically and theoretically more productive than the naked assertion that non-knowledge exists. Finally, I will point to a number of intriguing, but rarely studied topics relating to the question of the societal function or societal treatment of apparently insufficient knowledge. My usage of the term non-knowledge follows the convention in the literature that discusses the absence of knowledge. The term is synonymous with not knowing and has a close affinity but not identity with ignorance. In German the term is Nichtwissen."
249,173,0.989,Advances in Proof-Theoretic Semantics (Volume 43.0),"The GE-rule justified by this (along the same lines as for implication) is then â¢ â¢ C â¢G E which, given the usual â¥E rule and the unnecessary duplication of premisses, can be simplified to So, by this â¢E rule, the premiss of the â¢I rule is deducible, hence â¢ is deducible, hence â¥ is deducible. There is however a weakness (other than just that it leads to inconsistency) in the alleged justification of â¢ as a logical constant: it is a circularity. We follow Martin-LÃ¶f [15, 16] and Dummett [2] in accepting that we understand a proposition when we understand what it means to have a canonical proof of it, i.e. what forms a canonical proof can take. In the case of â¢, there is a circularity: the introduction rule gives us a canonical proof only once we have a proof of â¥ from the assumption of â¢, i.e. have a method for transforming arbitrary proofs of â¢ into proofs of â¥. The reference here to âarbitrary proofs of â¢â is the circularity. There are similar ideas about type formers, and it is instructive to consider another case, an apparent circularity: the formation rule (in [15]) for the type N of natural numbers. That is a type that we understand when we know what its canonical elements are; these are 0 and, when we have an element n of N , the term s(n). The reference back to âan element n of N â looks like a circularity of the same kind; but it is rather differentâwe donât need to grasp all elements of N to construct a canonical element by means of the rule, just one of them, namely n. A formal treatment of this issue has long been available in the type theory literature, e.g. Mendler [18], Luo [14], Coq [1]. We will try to give a simplified version of the ideas. With the convention that propositions are interpreted as types (of their proofs), we take type theory as a generalisation of logic, with ideas and restrictions in the former being applicable to the latter. The simplest recursive case (N ) has just been considered and the recursion explained as harmless (despite Dummettâs reservations expressed as his âcomplexity conditionâ [2]). What about more general definitions?"
359,233,0.989,"Micro-, Meso- and Macro-Dynamics of the Brain","physical parallelism,9 ultimately arriving at the modern understanding of the role of the brain in perception. Second, as we will see, there are direct conceptual parallels between his ideas and many of the more mathematically rigorous modern ideas. Third, Hayekâs work in theoretical psychology is underappreciated, especially given both its breadth and depth. We will see that Hayekâs work provides a conceptual framework that suggests overlap between a number of modern theoretical ideas and AAA (Fig. 8d). With regards to AAA, the main point here is the connection between computation at the single cell level (e.g., as discussed, coincidence-detection, association) and more network-level implications. This link is what Hayek explores. Hayekâs foundational idea is quite straightforward. He posited three orders: (1) the external world (which he called the physical order), (2) the brain (which he called the sensory order), and (3) the mind (which he called the phenomenal order), and he focused his efforts on understanding the relationship between the three. In Hayekâs formulation, the state of the brain has an isomorphic correspondence with that of the mind. The structure of the psychological realm, for Hayek, was relational (e.g., psychological objects are defined relative to other psychological objects), and as such, that structure of relationships that make up the psyche had to be recapitulated in the structure of the neural network and its activity. This strict correspondence contrasts with the correspondence between the outside world and the structure of the brain (and thus the mind), which is imperfect, as shown by the existence of sensory illusions. The problem for Hayek was then to describe how the relational network that is the psyche can be encoded in the structure and activity of a neural network, given the computational properties of single neurons that make up that network. Although this might seem trivial to todayâs standards, it cannot be overstated how important this development was, especially given prevailing ideas at the time. In the end, we will see that Hayekâs solution comes in a form that is in many ways remarkably similar (though missing the details of biophysics and anatomy that remained uncovered until the 1990s) to the ideas of AAA, Integrated Information theory, and Predictive Coding and discuss their connections. For Hayek the main questions were: 1. How can a relational network be encoded in the structure and activity of a neural network?10 2. How are the relations between objects in the outside world learned and encoded (imperfectly) in the neural network of the brain11?"
360,89,0.989,Compositionality and Concepts in Linguistics and Psychology,"In Chomskyâs view, âit is possible that natural language has only syntax and pragmaticsâ (Chomsky 1995, p. 26); that is, only âinternalist computations and performance systems that access themâ; semantic theories are merely âpart of an interface levelâ or âa form of syntaxâ (Chomsky 1992, p. 223). In his view, then, there is no distinction between semantics and pragmatics, and neither forms a part of linguistic theory. So, in the Chomskean take on the matter, semantic topics such as the meaning of lexical items are a matter for theories other than linguistics to determine. It is a matter of the cognitive systems that describe our beliefs about things in the world which would determine what meaning (and reference) is. Without using the term concept in any technical sense, it would seem that Chomsky endorses the claim made in Definition (3) above, about Emergentism, and denies only that this should have anything to do with linguistics. (Of course, when an Emergentist hears this they will be astonished to hear the âonlyâ in such a sentence, since they believe that this is the fundamental starting point for linguistic theory!) Nonetheless, we can see that this form of Evansâ âtraditional linguistic theoriesâ does not embody his âfoolâs paradise of literalismâ. Meanings and such are not at all a part of linguistic theory. However, not every Essentialist agrees with Chomsky on this point. Many believe that every theory should incorporate a linguistic componentâsemanticsâthat yields meanings, in much the same way that most philosophers of language believe there to"
249,363,0.989,Advances in Proof-Theoretic Semantics (Volume 43.0),"My notation doesnât show the f , but if needed one could write an f in the middle of the triangle. Also Kostaâs notation can be written in a line; this is an advantage in text, but possibly a hindrance for writing out pictures of complex derivations. On the other hand my notation has the advantage that it allows one to write several hypotheses, whereas Kostaâs arrow notation allows just one source for the arrow; for my application in (7) above, that would have been a fatal flaw. As all this illustrates, there are some quite subtle relationships between notation and concept, and they are very sensitive to the purpose that the notation will be put to, and the mathematical context in which it will be used. But elsewhere Kosta forgets the variables. For example he asks [4, Â§5]: Can inferences be reduced to consequence relations? So that having an inference from A to B means just that B is a consequence of A. where should the variables go? I suggest that the concept of an inference needs three variables, essentially as in Kostaâs notation (9) for derivations: x is an inference from y to z."
275,618,0.989,Foundations of Trusted Autonomy,"intractable epistemological uncertainty [13, 14, 50]. The black swan anecdote serves to illustrate Knightâs epistemic uncertainty outside an obviously economic or financial setting. Talebâs description of the behaviour of humans in concocting reasons for the event after the fact mirrors Keynesâ conventions describing economic agent behaviour, but does so in a manner that highlights the role in shaping expectations whether economic or otherwise - of a widely discredited position usually known as universal causality in philosophy. Talebâs observation that humans regard events as being much more attributable to determinable causes than they really are has a long history in philosophy as the notion of universal causation. Universal causation maintains that all events are the result of prior events, and this belief connects the construction of Talebâs post hoc explanations for rare events with Keynesâ view that economic agents maintain undue reliance on the veracity of prediction. The post- economic meltdown criticisms of financial firms assuming that the relevant risks are all measurable is a modern manifestation of the same effect. The intuitive appeal of this view lies in that whenever we ask simple questions after the fact about why a particular event occurred, we can obtain a plausible explanation for its occurrence in terms of some causal chain of earlier events. So it would seem on the face of it that every event is caused by something, albeit probabilistically, and hence that every event follows from prior events according to some governing logic. Yet the more deeply we dig, the more ostensibly antecedent sets of conditions look like reasons for deciding to act in a certain way, or, more pertinently, for not acting a certain way, and the less they look like the inevitable causes of an event that we first supposed. In other words, alleged causes are really only epistemic factors that influence the decisions we make from within a problem context, rather than immutable ontological features of an environment into which the agent peers from the outside. Universal causality connects to predictability through causal determinism, which holds that every event is necessitated by some set of prior events. This claim is then the antecedent to so-called âscientificâ determinism, which concludes therefore that the world is basically predictable. To elaborate: âscientificâ determinism alleges that the structure of the world is such that future events can be predicted with precision depending on that of knowledge of the governing laws of the phenomena of interest and the accuracy of the account of past events. It is worth noting that âscientificâ determinism is poorly named, for it is not actually about determinism at all: determinism refers to the absence of arbitrary choices in the application of transition operators, with non-determinism then being the admission of arbitrary choices. Rather, âscientificâ determinism is an assertion about predictability, equivalent to assuming stability and logical completeness. Though perhaps intuitively appealing, the inference from all events having necessary causes to predictability is flatly wrong: it is well established that even fully deterministic systems in which the current state completely determines the transition to a unique subsequent state can be nonetheless savagely unpredictable [20, 22]. Conversely, non-deterministic systems can also be completely predictable. Even if we limit ourselves to completely deterministic sys-"
297,943,0.989,The R Book,"The hardest part of any statistical work is getting started. And one of the hardest things about getting started is choosing the right kind of statistical analysis. The choice depends on the nature of your data and on the particular question you are trying to answer. The key is to understand what kind of response variable you have, and to know the nature of your explanatory variables. The response variable is the thing you are working on: it is the variable whose variation you are attempting to understand. This is the variable that goes on the y axis of the graph. The explanatory variable goes on the x axis of the graph; you are interested in the extent to which variation in the response variable is associated with variation in the explanatory variable. You also need to consider the way that the variables in your analysis measure what they purport to measure. A continuous measurement is a variable such as height or weight that can take any real numbered value. A categorical variable is a factor with two or more levels: sex is a factor with two levels (male and female), and colour might be a factor with seven levels (red, orange, yellow, green, blue, indigo, violet). It is essential, therefore, that you can answer the following questions:"
124,177,0.989,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"2 Newton Versus Einstein In common discourse, we expect to be able to say where we would be at this time if we had taken a different road. Einstein says that strictly speaking, we canât: that there is no such thing as absolute simultaneity. Our common parlance is based on intuitions which are more Newtonian than relativistic, and the Newtonian view includes a linear conception of time, rather than a branching one. One may wonder, however, whether the non-relativistic aspect of Newtonian physics might be separable from its assumption of temporal linearity. Is it reasonable to entertain an account of branching time which makes room for simultaneity between moments on distinct branches, and thus in this respect is Newtonian rather than relativistic? Some systems of branching time logic include an equivalence relation I among moments. In such models the equivalence classes under I are called instants, and"
249,215,0.989,Advances in Proof-Theoretic Semantics (Volume 43.0),"For one thing, â is presumably grounded upon a sort of our epistemic capacity to put symbols in parallel (inside and outside sequents) as discussed above. The epistemic capacity may be so fundamental that it plays fundamental rÃ´les in symbolic reasoning as well as many other cognitive practices; this will lead to a sort of epistemic account of admissibility of â in the principle of categorical harmony. Another âinformationalâ account of it seems possible as well. There are three fundamental questions: What propositions hold? Why do they hold? How do they hold? The first one is about truth and falsity, the second one about proofs, and the last one about the mechanisms of proofs. An answer to the last question must presumably include an account of the way how resources or assumptions for inference are used in proofs, or how relevant inferential information is used in proofs. And â may be seen as a means to address that particular part of the third question. This is the informational account, which has some affinities with the view of linear logic as the logic of resources. Yet another âphysicalâ account may be came up with. In recent developments of categorical quantum mechanics by Abramsky and Coecke (see Abramsky [1] and references therein), the capacities to put things in parallel as well as in sequence play vital rÃ´les in their so-called graphical calculus for quantum mechanics and computation, where parallel composition represents the composition of quantum systems (resp. processes), i.e., the tensor product of Hilbert spaces (resp. morphisms), which is crucial in quantum phenomena involving entanglement, such as the EinsteinPodolsky-Rosen paradox and the violation of the Bell inequality. In general, â lacks diagonals and projections, unlike cartesian Ã, and this corresponds to the No-Cloning and No-Deleting theorems in quantum computation stating that quantum information can neither be copied nor deleted (note that diagonals â : X â X â X copy information X , and projections p : X â Y â X delete information Y ). On the other hand, classical information can be copied and deleted as you like. So, the monoidal feature of â witnesses a crucial border between classical and quantum information. To account for such quantum features of the microscopic world, we do need â in the logic of quantum mechanics, and this would justify to add â to primitive vocabularies. The physical account seems relevant to the well-known question âIs logic empirical?â, which was originally posed in the context of quantum logic, and has been discussed by Quine, Putnam, Dummett, and actually Kripke (see Stairs [24]). The need of multiplicative â in the âtrueâ logic of quantum mechanics is quite a recent issue which has not been addressed in the philosophy community yet, and this may have some consequences to both the traditional question âIs logic empirical?â and the present question âWhy are substructural logical constants are so special?â, as partly argued above. A more detailed analysis of these issues will be given somewhere else. Sambin et al. [19] present a novel method to introduce logical constants by what they call the reflection principle and definitional equalities, some of which are as follows: â¢ Ï â¨ Ï â¢ Î¾ iff Ï â¢ Î¾ and Ï â¢ Î¾ . â¢ Ï, Ï â¢ Î¾ iff Ï â Ï â¢ Î¾ . â¢ Î â¢ Ï â Ï iff Î â¢ (Ï â¢ Ï)."
82,105,0.989,Fading Foundations : Probability and The Regress Problem,"ity space is homogeneous (containing only propositions or beliefs), the application to externalism is a bit more complicated. Within externalism many things can be reasons, so the probability space is rather diverse, containing not only beliefs and propositions, but also perceptions, memories, facts, and so on. This difficulty can however be handled as follows. First we define different spaces: a space of beliefs, a space of perceptions, a space of memories, and so on. Then we define a space which is the Cartesian product of all those spaces. And finally we decide which relation of probabilistic causality we want to focus on. Do we want to focus on perceptions causing beliefs? Or memories causing beliefs? Or beliefs causing desires? Desires causing beliefs? Deciding on the answers to these questions is necessary in order to keep a grip on the heterogenous probability space, but it is just a slight technical complication, and it is not important for the general philosophical point that (2.1) is neutral with respect to both internalism and externalism. 45 Swinburne 2001, Chapters 2, 7, and 8. 46 Goldman 1986, 5-6."
227,33,0.989,Problem Solving in Mathematics Education,"Specializing is the process of getting to know the problem and how it behaves through the examination of special instances of the problem. This process is synonymous with problem solving by design and involves the repeated oscillation between the entry and attack phases of Mason et al. (1982) heuristic. The entry phase is comprised of âgetting startedâ and âgetting involvedâ with the problem by using what is immediately known about it. Attacking the problem involves conjecturing and testing a number of hypotheses in an attempt to gain greater understanding of the problem and to move towards a solution. At some point within this process of oscillating between entry and attack the solver will get stuck, which Mason et al. (1982) refer to as âan honourable and positive state, from which much can be learnedâ (p. 55). The authors dedicate an entire chapter to this state in which they acknowledge that getting stuck occurs long before an awareness of being stuck develops. They proposes that the ï¬rst step to dealing with being stuck is the simple act of writing STUCK! The act of expressing my feelings helps to distance me from my state of being stuck. It frees me from incapacitating emotions and reminds me of actions that I can take. (p. 56)"
315,46,0.989,Situating Children of Migrants Across Borders and origins : a Methodological Overview,"The comparative project as well as the level of analysis chosen thus to a great extent structure the conclusions. There is no right or wrong way to construct a comparison, but it is necessary to be aware of the ways in which certain choices at the inception reflect options concerning the similarities or singularities of the immigrant experience. Green (1994: 14)"
124,308,0.989,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"Young Saul Kripke then continued his letter by explaining that the formula, â¦ p â â¦âp can be verified using Priorâs representation of Diodorean time as discrete sequences, but that this formula can be shown not to be provable in S4. In this way Kripke made an important contribution to the search for an axiomatic system corresponding to the Diodorean notion of modality. This research engaged several researchers in the late 1950s and the early 1960s. (See Prior 1967, p. 176). Even more important was the following passage from Saul Kripkeâs letter in which he suggested how the semantics of S4 could be visualized. Kripkeâs formulation of this very original idea in the letter makes it reasonable to classify the occurrence of this letter as one of the most important events in the history of logic during the twentieth century. Kripke wrote: I have in fact obtained this infinite matrix on the basis of my own investigations on semantical completeness theorems for quantified extensions of S4 (with or without the Barcan axiom). However, I shall present it here from the point of view of your âtensedâ interpretation. (I myself was working with ordinary modal logic.) The matrix seems related to the âindeterminismâ discussed in your last chapters, although it probably cannot be identified with it. Now in an indetermined system, we perhaps should not regard time as a linear series, as you have done. Given the present moment, there are several possibilities for what the next moment may be likeâand for each possible next moment, there are several possibilities for the next moment after that. Thus the situation takes the form, not of a linear sequence, but of a âtreeâ (Fig. 1):"
95,135,0.989,Elements of Robotics,"The Persistent Braitenberg vehicle (Fig. 4.1) has infinite behavior because it continues to move without stopping. A toaster also demonstrates infinite behavior because you can keep toasting slices of bread forever (until you unplug the toaster or run out of bread). The second new concept is nondeterminism. States left and right each have two outgoing transitions, one for reaching the edge of the sector being searched and one for detecting an object. The meaning of nondeterminism is that any of the outgoing transitions may be taken. There are three possibilities: â¢ The object is detected but the search is not at an edge of the sector; in this case, the transition to appr is taken. â¢ The search is at an edge of the sector but an object is not detected; in this case, the transition from left to right or from right to left is taken. â¢ The search is at an edge of the sector exactly when an object is detected; in this case, an arbitrary transition is taken. That is, the robot might approach the object or it might change the direction of the search. The nondeterministic behavior of the third case might cause the robot to fail to approach the object when it is first detected, if this event occurs at the same time as the event of reaching Â±45â¦ . However, after a short period the conditions will be checked again and it is likely that only one of the events will occur."
108,143,0.989,Bordieuan Field Theory as an Instrument for Military Operational Analysis,"Abstract The discussion above demonstrates that it is possible to acquire useful knowledge for military purposes by studying the prelude to the conflict in Bosnia from a field theory perspective. As seen the method here is more the historianâs than one which one would need on a contemporary example. But the case here was to present the use of the theory, not of the methods. As has been said earlier, one can use almost any kind of method when one works with field theory."
244,1000,0.989,"Advancing Human Assessment : The Methodological, Psychological and Policy Contributions of Ets","the day (Spearman 1923; Thurstone 1926) was probably responsible for the lack of acceptance of his perspective.16 Among the problems raised about the sampling perspective as a warrant to score interpretation was that, in principle, it seemed to require the preexistence of a universe of items, so that random samples could be taken from it. Such an idea presupposes some means of defining the universe of items. The resistance to the idea was most vocally expressed by Jane Loevinger (1965), who could not envision how to explicate such universes. Nevertheless, the relevance of sampling in validation was affirmed by Cronbach (1980) and Kane (1982), although not as a sufficient consideration, even though the link back to Tryon was lost along the way. What appears to have been missed in Tryonâs argument is that he intended the universe of items to be isomorphic with a âuniverse of factors, causes, or components determining individual differencesâ (p. 433), which would imply a crossing of content and process in the creation of a universe of items. Such an idea foreshadows notions of validity that would be proposed many decades later, specifically notions related to construct representation (Embretson 1983). Instead, in time, the sampling perspective became synonymous with content validity (Cronbach 1971): âWhether the operations that finally constitute the test correspond to the specified universe is the question of content validityâ (p. 452, emphasis added). The idea of a universe was taken seriously by Cronbach (although using for illustration an example from social psychology, which, interestingly, implies a constructed-response test design): For observation of sociability, the universe specification presumably will define a category of âsocial actsâ to be tallied and a list of situations in which observations are to be made. Each observation ought to have validity as a sample from this universe. (p. 452)"
8,434,0.989,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","increase proportionally to E, nor is the distribution of the observed particle number constant. The way out of this apparent disagreement is again provided by the model itself: the N particles found in a particular case are by no means the final particles observed in photographic emulsions (mainly pions)âspeaking of distinguishable particles, we have to consider them to be anything between a pion and a âfireballâ. Equation (19.29) gives us the probability of just finding N such not further specified objects. It states that all values of N are (for N  N) practically equally likely. The question how probable is it to find N specified particles is quite another one, as we shall see in a moment when we discuss large angle elastic scattering. Presently, we note that, interpreting the N particles as âfireballsâ of unspecified excitation energy, Eq. (19.29) tells us that a âtwo-fireball modelâ would never work,6 as there will be contributions of almost the same weight from 3, 4, 5, . . . , fireballs, a situation similar to that in the multiperipheral model of Amati et al. [12] and in considerations by Wilson [13]. But if this is so, then the number of pions and other final particles observed in experiments should even be larger than N D E=3T0 , since these particles are produced in a chain of decays starting from the first N âfireballsâ and going into smaller and smaller ones. Here the answer is that introducing the masses and a contracted volume will bring that in order: we shall come back to this problem in Sect. 19.4, where it will be shown that N, the number of âfireballsâ, tends to 5 and becomes energy independent for E ! 1. Let us now consider the energy spectrum of our particles. First we treat the case where the question of how many particles we expect and the question of what their energies might be are intimately connected: large angle elastic and exchange scattering. In that case, we have two definite final particles, each with energy E=2, and from Eq. (19.28), we conclude that the probability of finding a particle in the energy level ""Ë D E=2 is given by wË D"
8,840,0.989,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","dense state is lost during the expansion of the fireballs as the hadronic gas rescatters many times while it evolves into the final kinetic and chemical equilibrium state. In order to observe properties of quark-gluon plasma, we must design a thermometer, an isolated degree of freedom weakly coupled to the hadronic matter. Nature has, in principle (but not in practice) provided several such thermometers: leptons and heavy flavours of quarks. We would like to point here to a particular phenomenon perhaps quite uniquely characteristic of quark matter. First we note that, at a given temperature, the quark-gluon plasma will contain an equal number of strange (s) quarks and antistrange (s) quarks, naturally assuming that the hadronic collision time is much too short to allow for light flavour weak interaction conversion to strangeness. Thus, assuming equilibrium in the quark plasma, we find the density of the strange quarks to be (two spins and three colours) D D6"
275,532,0.989,Foundations of Trusted Autonomy,"This summation is supported by countless experiments in which human subjects deploy their ability to spin stories on the spot in support of propositions that are simply and clearly false.3 Whereas Ariely identifies a form of creativity that consists in the generation of narrative, as will soon be seen, we base our formal analysis and constructions upon a less complicated form of creativity that is subsumed by narratological creativity: what we call theory-of-mind creativity. It isnât that we find creativity associated with narrative uninteresting or unworthy of investigation from the perspective of logicist computational cognitive modeling or AI or robotics (on the contrary, we have investigated it with considerable gusto; see e.g. [7]), itâs simply that such things as story generation are fairly narrow in the overall space of creativity (and indeed very narrow in AI), and we seek to cast a wider net with TACU than would be enabled by our use herein of such narrow capability."
306,6,0.989,"Early Geometrical Thinking in the Environment of Patterns, Mosaics and Isometries","Geometry is one of the best areas for a child to enter the world of mathematics. The geometrical world can be opened very early because geometrical knowledge correlates very well with childrenâs natural cognition. All the information gathered by perception has special importance for them. Learning for young children mainly consists of acquiring information by observing the world made up of objects. One of the features of these objects is their shape. Among shapes, there are regular ones. Perceived objects provoke further action. Children often say that a triangle is âvery nice,â which is another way of saying that it is regular (HejnÃ½ 1993). Since any regularity is attractive, it is easy to interest a child with it, and the motivation for any action is then natural. Various activities facilitate the learning of objects, creating intuition on which geometrical concepts are built. This is the main reason why geometrical concepts recognised by perception are closer to childrenâs abilities than arithmetical ones. One of the ways to further explore the world of geometry is to provide proper visual information associated with the possibility of manipulation and experimentation, with room for a childâs own creativity and ingenuity. In a patterns environment, recognizing a geometrical concept is spontaneous and is connected with solving problems in which children are able to clearly deï¬ne the purpose of their"
363,212,0.988,History and Cultural Memory in Neo-Victorian Fiction,"the moment when the nullity of reality rises up against history and takes it back to itself. Indeed, the real is associated, for Decoste, ânot simply with the death of meaning but with more concrete deaths, more material destruction, as wellâ (ibid.: 382). I want to suggest that this attributes to reality a materiality, design and an intention that the novel is not supported by the novel. This can be seen has his playing out, on a smaller scale, the course of narrative history that Decoste observes in the novel on a larger scale. Tending to insist on final answers without appeal, on stable but restricted definitions of the meaning of things, the narrativisation that fuels the making of history carries with it, too, the desire to simplify, finalise, and even brutally exclude. In its fulfilment of such narrative desires, history works to bring forth apocalyptic conflagrations to burn away that which does not conform to the answer sought, the âproperâ end of the storyâ (ibid.: 390). As we have already seen, Tom also provides two explanations, one mythic and the other more realist, for Ernest Atkinsonâs actions in the novel. Focusing on the evocation of a âfairy-tale placeâ, Hanne Tange argues that the description of the Fenland setting does not place the Fens in the wider context of the map of England, and that this is to emphasise the marginality of the locale, claiming âno initial attempt is made to place this landscape on the map of England, for the rest is of little significanceâ. I agree that the Fens are represented as somewhat insulated from a British centre. And, moreover, that Tomâs ultimate relocation to London marks, in some sense, a commitment to national history. However, the introduction, so early in the novel, of the precise geography of the Fens does seem to attempt to place the Fens on the map of England, and this is important to the depiction of âmaking historyâ, in all its resonances, in the novel (see Tange, 2004: 78). Indeed, Brewer and Tillyard assert that Swiftâs novel attempts to be âa universal story of the human conditionâ (Brewer and Tillyard, 1985: 50). My reading of the novel aligns human nature with natural history and therefore identifies it as a category of the real, in opposition to history, stories and making things happen. It therefore counters Decosteâs association of the real with âinhumanityâ, because of the death and destruction he attributes to the real (Decoste, 2002: 395). Cooper, too, notes that Tom âmaps her desirabilityâ in these terms. For Cooper, Mary is âstereotyped and essentialized as a kind of âeternal feminineâ â¦ less a character than a placeholder or conduit for desire: a dioramic sequence of paradises lostâ (Cooper, 1996: 385). See also Pamela Cooperâs argument that âin Ernestâs passion for Helen the trajectory of imperialism is reversed; its expansive energies are redirected towards an interior space of desire, neither subjective nor objective, but abject: the body of the daughterâ (Cooper, 1996: 380). Indeed Brewer and Tillyard argue that the novel traffics in the âcult of naturalnessâ, what they call âthe notion of ânaturalnessâ as somehow both better and more realâ and which âhas become one [of] the most tiresome clichÃ©s of the modern ageâ (Brewer and Tillyard, 1985: 51). The novel certainly does seem to support this reading, although it is countered, as we shall see, by Tomâs own belief in the value of artificial history and the civilisation it supports and makes possible."
266,56,0.988,Societal Implications of Community-Oriented Policing and Technology,"Challenges of Using Crowd Knowledge Sourcing and Recommendations for the Way Forward Whilst the opportunities seem advantageous, questions arise about the likelihood of success and benefit. Building upon insights from Gary Marx we must acknowledge however that no matter how âideal a technical control system may appear in the abstract under ideal laboratory conditions or successful in the short run, the world of application is often much messier and more complicated than the public relations efforts claimâ (Marx 2012). Marx understands that there is rarely a perfect technical solution, or one without issues. The same is true of a technical answer to community problems."
113,156,0.988,Pentecostalism and Witchcraft : Spiritual Warfare in Africa and Melanesia,"CONCLUSION: NDOKI AND DOW JONES My speculative proposal of seeing sorcery as an index stems from my previous interrogations on the âagency of intangiblesâ (Blanes and EspÃ­rito Santo 2013). In that book, we discussed traces and effects of invisible and intangible objects in social life. Here I argue that, beyond the recognition of objects per se, those intangible effects often become shifting and fluctuating âstock market indexesâ, objects of measurement of value. Thus, an index is createdânot necessarily a mathematical construct like in Wall Street, but a multifarious ideological construct that is able to describe and regulate the âspiritual marketâ in modes that may or may not include capitalist modes of economic activity. Therefore, the index necessarily shifts according to who is performing the measurement, and ultimately produces as much regulation as it does deregulation, considering Pentecostalismâs particular and continuous moral demand on the person and community, as is noted by Eriksen and Rio (this volume) in the case of Vanuatu. Thus the point here is that, as an art of invisible agency, ndoki becomes object of what Alfred Gell called, via Peirce, âabductive reasoningâ, or inference. One recognizes the agency and identifies the effects, and thus finds the best plausible explanation for the connection between two events. So in conclusion, this kind of reasoning produces an index, which varies according to the agents who produce it: catholic, prophetic, Pentecostal. But this index emerges as a moral index precisely because it is perceived as describing and affecting economic but also political and spiritual activity."
360,494,0.988,Compositionality and Concepts in Linguistics and Psychology,"6 Conclusions Semantic composition is a dynamic process that cannot be understood without simultaneously considering what we are referring to and the concepts associated with the words we are using. Concepts, and thus the words associated with them, encode significant regularities. At the same time, they are plastic, insofar as we must use a finite vocabulary to describe a potentially infinite variety of situations and generalizations in the world. However, once a word is applied to a referent, that word is grounded in a very specific manner, and the referent can influence the way we understand the word and its associated concept(s) in the context of use. This interplay between our conceptual structure and the world is what motivated the first contribution of this paper, namely to propose that modification works in two ways: It can be conceptually afforded, when the modifier and the head introduce concepts that fit to form a complex concept, and the speaker and the hearer use this fit in their interpretations; or referentially afforded, when the result of combining the modifier with the noun depends on specific properties of the referent. This proposal has an antecedent in Asher (2011), but we have made it more explicit and have proposed a specific analysis combining distributional semantics and DRT. Along the way we hope to have made a case for further exploring distributional representations within semantic theory. They are automatically induced (and thus"
372,841,0.988,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"the unquantized signals to an extent that is most serious for two-level sampling, and the deviation decreases as the number of levels is increased. Correction for this effect requires determination of how the cross-correlation of the quantized data, here designated R, is related to the true cross-correlation, %. To examine the effect of quantization, we consider the effect of a time offset ! on two Gaussian waveforms that are otherwise identical. In the case of two-level sampling, the required relationship is given by the Van Vleck equation [Eq. (8.25)] and is R2 .!/ D"
192,49,0.988,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"Hegel considers Keplerâs sublime laws of heavenly movements as a highlight of human understanding (1830/1970, Â§ 270). His idea that perfect cubes determine the distances between the planets exemplifies his fidelity to reason: his reliance, with absolute confidence, on the presence of reason (logos) in nature, and therefore Keplerâs laws are the most beautiful produced ones by natural science."
289,2074,0.988,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","of y cannot read 1 because there is the path store(y, 1) âââ store(y, 2) ââ load(y) which would contradict the Coherence axiom. In the second execution there is no such path and the load may read 1. It is desirable for our denotation to hide the precise operations inside the block â this lets it relate syntactically distinct blocks. Nonetheless, the history must record hb effects such as those above that are visible to the context. In Execution 1, the Coherence violation is still visible if we only consider context operations, call, ret, and the guarantee G â i.e. the history. In Execution 2, the fact that the read is permitted is likewise visible from examining the history. Thus the guarantee, combined with the local variable post-states, capture the effect of the block on the context without recording the actions inside the block."
253,68,0.988,"Autonomous Driving : Technical, Legal and Social Aspects","The very act of driving conjures a range of strong and very human emotions. Whether it is the feeling of freedom that the mobility of the car provides, the frustration of being stuck in trafï¬c, the panic when realizing a potential collision looms or the joy of an open road with a favorite song on the radio, driving is a human experience. With automated vehicles, however, that experience changesâboth for passengers in the automated car and other road users who must walk or drive alongside it as part of the social experience of trafï¬c. The car ceases to be simply an extension of its human driver and becomes an agent in its own right, navigating through the highways and rules of human society. Given the sometimes uneasy relationship between humans and machines, what will these new interactions between humans and machines look like? Fabian Kroeger sets the stage for this discussion by showing what our cultural heritage reflects about our views of automation. In his chapter, Das automatisierte Fahren im gesellschaftsgeschichtlichen und kulturwissenschaftlichen Kontext, he details the long history of automated vehicle concepts and their treatment in media, beginning with Utopian visions of the beneï¬ts of such technology. His chapter traces the path from this early optimism towards the more cautionary themes found in recent ï¬lm depictions of our automated future. This frames a central question running through the remaining chapters âhow can the challenges of human-machine interaction be overcome to realize the promise of this technology? A key aspect of that interaction is how automated cars will conform to the ethical standards of the human world in which they operate. Patrick Lin opens this topic with an overall discussion of Why Ethics Matters for Autonomous Cars. Even with the best technology imaginable, sometimes crashes will be unavoidable for automated vehicles that share the road with human drivers and programmers must decide what to do when presented with such dilemma situations. As Lin shows, such decisions raise issues of equity, discrimination and unintended consequences that must be thoughtfully considered. Christian Gerdes and Sarah Thornton take the programming aspect of this discussion a step further with Implementable Ethics for Automated Vehicles. Mapping philosophical concepts to engineering concepts, they demonstrate how different approaches to ethical reasoning can be turned into algorithms that make decisions for automated vehicles. The"
328,60,0.988,a Philosophy of Israel Education : a Relational Approach,"program of Israel experience educational programs. Within a relatively short time, it is likely that for the first time since the destruction of the second Temple, the majority of the Jews of the world will have visited Israel. It is increasingly clear that one cannot conceive of an Israel education, which does not include an Israel experience. Indeed, if there is one facet of Israel education that has proven itself, it is that the power of an effective Israel experience is unmatched and irreplaceable.4 It is only logical that Israel education without an experience in Israel at some point is only a partial activity. The power of the actual experience in Israel is related, among other things, to Israel being a totally immersive culture. Embracing real-life experiences and the multiple expressions of Jewishness that exist both in Israel and in Jewish life today is paramount. Modern Israel encompasses the panorama of narratives from the biblical Promised Land to postmodern contemporary society that we discussed in Chap. 2. In Israel, Abraham, Moses, Hosea, Judah the Maccabee, Maimonides, Herzl, Ahad Haâam, Golda Meir, and David Ben-Gurion are not only street names, but also figures who still âliveâ there and their âvoicesâ are heard daily. Indeed, the Israeli experience provides a direct linkage to a rich heritage and a living culture. The power of this meeting is enhanced by being deeply experiential, sensual, and people-centered. There are many ways to see Israelâwith a FodorâsÂ© Guidebook, a camera, the Bible, a prayer book, or the latest edition of The New York Times.5 The person-centered approach obviously implies maximizing the direct encounter and minimizing mediated framings. It aims to facilitate a dialogue between person and place in which both speak to each other. Young travelers come to the Western Wall to speak to it, and the Wall wants to talk to them. The art of enabling encounter is a delicate one; sometimes in order to facilitate a direct interaction, it is necessary to engage in a certain degree of framing (or what is denoted as âpre-conditions of educationâ) so that the encounter can actually happen. There are situations in which a totally unmediated dynamic can sometimes actually prevent, rather than enable, genuine experience. The art of framing is the ability to enable the dialogue to take place, and not to impose a specific landscape on the actual moment. The actual implementation of Israel trips is a fine art, which has proven to be one of the unique achievements of American Jewish education. Indeed, one might well suggest that this aspect has been one of the most professional and sophisticated of contemporary forms of Jewish education."
360,427,0.988,Compositionality and Concepts in Linguistics and Psychology,"2 Background Concepts can often be paraphrased by expressing an underlying implicit relation between the concepts (flu virus is a virus that causes flu). Previous research has found that relational information plays a central role in the processing of combined concepts (for an overview, see GagnÃ© and Spalding 2014a). The interpretation of combined concepts involves selecting the appropriate relational structure, and factors associated with the use of the relations affect the processing of the combined concept (e.g., combined concepts that share a recently used relational structure and a constituent are processed more easily than those that use a different relational structure, and these effects are true of both novel and known combinations; see GagnÃ© and Spalding 2014a). However, such relational interpretations provide only the gist (i.e., a paraphrase) of the combined conceptâs meaning (e.g., that a dog house is a house for dogs) and do not exhaust our knowledge of the combined concept. Where does the rest of the meaning come from? For example, imagine that someone hears, for the ï¬rst time, the phrase dog house and that they know the meaning of dog and house. Now, suppose we ask this learner, do doghouses have doorknobs? There are several interesting points to note. First, the gist interpretation house for a dog, does not, in and of itself, give the answer to the question. Second, the gist is not unrelated or irrelevant to answering the question. The person might reason that because dogs do not have hands, a doorknob would not be likely. Clearly, though, this is an inference based on the gist, not something that is inherently true of the combination of the concepts dog and house. Third, to be more explicit, note that dog and house could also be combined to form a gist something like house that has a dog, and this type of doghouse would be highly likely to have doorknobs. The key point for the current chapter is that answering the question requires an inference about the speciï¬c combination (and gist). Our primary research question concerns how people make judgments about whether or not particular properties are true for a given concept. In terms of empirical evidence, we will focus on property veriï¬cation in combined concepts, which refers to how people determine whether a particular characteristic (e.g., âis heavyâ) is true"
132,169,0.988,AiREAS: Sustainocracy for a Healthy City: Phase 3: Civilian Participation â Including the Global Health Deal Proposition,"In the previous publication on the development of AiREAS and its ï¬rst phase of making visible the invisible, we already went to great length in introducing the human complexity. We introduced the cyclic evolutionary progress pattern that develops between the interaction of the consequences of what we do and the discovery of what we are through backward interpretation. To DO develops our to BE. We also suggested that we are at a point in history when a major psycho-social turnaround is taking place in which we are coming to know what we are and can develop our actions and choices around that wisdom, as shown in the picture above. Only then can our awareness (to BE) start guiding our actions (to DO) (see ï¬gure below). This energetic swap is unique in human history and represents our evolution from a collective perspective. It announces a whole new era in our existence as a self-aware, creative species."
82,206,0.988,Fading Foundations : Probability and The Regress Problem,"3.7 Barbara Bacterium In this chapter we have introduced the concept of a probabilistic regress, that is an epistemic chain of the form q ââ A1 ââ A2 ââ A3 ââ A4 . . . where the arrow is interpreted in terms of probabilistic support. We examined Lewisâs view that such a regress is absurd, since it allegedly implies that the probability of q is zero. According to Lewis, the only way to avoid the absurdity was to stop at a proposition, p, which is certain: q ââ A1 ââ A2 ââ A3 ââ A4 . . . ââ p. We have opposed Lewisâs argument by giving counterexamples, i.e. probabilistic regresses which yield a unique, nonzero probability value for the target. Some of these regresses were based on uniform conditional probabilities, others on nonuniform ones. All our counterexamples were abstract. This is somewhat unfortunate, since a familiar objection to infinite regresses is that they are not concrete and lack practical relevance. The objection becomes even more pressing if one distinguishes (as we did not do here but will do in later chapters) between propositions and beliefs. Propositions are abstract entities, but beliefs are propositional attitudes that people really have. Whereas the idea of an infinite propositional regress might sound not unreasonable, an infinite dox-"
257,294,0.988,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","Game IV (simultaneous with hidden choice). This game is a tuple (D, A, u). However, it is not an ordinary game in the sense that the payoff a mixed strategy profile cannot be defined by averaging the payoff of the corresponding pure strategies. More precisely, the payoff of a mixed profile is defined by averaging on the strategy of the attacker, but not on that of the defender. In fact, when hidden choice is used, there is an additional level of uncertainty in the relation between the observables and the secret from the point of view of the attacker, since he is not sure about which channel is producing those observables. A mixed strategy Î´ for the defender produces a convex combination of channels (the channels associated to the pure strategies) with the same coefficients, and we know from previous sections that the vulnerability is a convex function of the channel, and in general is not linear. In order to define the payoff of a mixed strategy profile (Î´, Î±), we need therefore to consider the channel that the attacker perceives given his limited knowledge. Let us assume that the action that the attacker draws from Î± is a. He does not know the action of the defender, but we can assume that he knows his strategy (each player can derive the optimal strategy of the opponent, under the assumption of common knowledge and rational players). The channel the attacker will see is â¨dâÎ´ Cda , obtaining a corresponding payoff of V [Ï, â¨dâÎ´ Cda ]. By averaging on the strategy of the attacker we obtain U (Î´, Î±) = EaâÎ± V [Ï, â¨dâÎ´ Cda ] = âaâA Î±(a) V [Ï, â¨dâÎ´ Cda ]. From Theorem 7(b) we derive: U (Î´, Î±) = V [Ï, aâÎ± â¨dâÎ´ Cda ] and hence the whole system can be equivalently regarded as channel aâÎ± â¨dâÎ´ Cda . Note that, by Proposition 6(c), the order of the operators is interchangeable, and the system can be equivalently regarded as â¨dâÎ´ aâÎ± Cda . This shows the robustness of this model. From Corollary 8 we derive that U (Î´, Î±) is convex in Î´ and linear in Î·, hence we can compute the Nash equilibrium by the minimax method."
8,331,0.988,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","The rest of the paper discusses when the formula is valid and what corrections are necessary. What interests us here is that this is (to my knowledge) the first time that it was shown quantitatively that thermodynamics might be applied to such a tiny system as a nucleus. The reason is the enormous level density of heavy nuclei at high excitation energy. Note also that the formula was derived for the emission of a single neutron with only a few degrees of freedom (phase space). We conclude with âLesson 4â: Lesson Four (L4). Thermodynamics and/or statistics might be (cautiously!) applied to very small systems, provided these have a very large level density (whatever that means)."
360,430,0.988,Compositionality and Concepts in Linguistics and Psychology,"Indeed, the purpose of using a combined concept rather than just the category is usually to refer to a subcategory that is distinct from other members of the category (e.g., Clark and Berman 1987; Downing 1977). This accounts for why previous research (e.g., Connolly et al.2007, 2011; GagnÃ© and Spalding2014b; Hampton et al. 2011) has found that a property (e.g., has webbed feet) that is true of an unmodiï¬ed concept (e.g., duck) also is seen as being true of a modiï¬ed concept (e.g., baby duck) but less so. This explanation readily accounts for why a property (e.g., has teeth) that is false of an unmodiï¬ed concept (e.g., candle) is judged as being more true of a modiï¬ed concept (e.g., purple candle), as reported in a series of experiments reported by (Spalding and GagnÃ© 2015). Both results (for the true and the false properties) can be explained by participants reasoning that the subcategory is distinct in some way from the category. Interestingly, the reasoning process appears to be more heavily dependent on meta-cognition and on pragmatic factors than on the speciï¬c content of the concepts involved. Consequently, content-free modiï¬ers (i.e., unknown words) also produced a modiï¬cation effect even though they had no content that could be used to assess speciï¬c similarity or dissimilarity with the category. Finally, Spalding and GagnÃ© (2015) argue that if property veriï¬cation for combined concepts involves such predication following reasoning and judgment, it is possible that such processes are also involved in all property judgments for single concepts, not just combined concepts. This, in turn, suggests a very different, more Aristotelian-Thomistic, approach to the relation between property and concept, rather than the kind of implicit âcontainerâ metaphor that is so common in current theories of concepts. In particular, in the A-T approach, concepts are not âmade ofâ properties and they do not âcontainâ properties, and so one does not verify properties by âlooking insideâ the concept. We return to this point in the Discussion."
49,324,0.988,Artificial Intelligence and Cognitive Science IV,"After receiving a reward or a punishment, new decision space Di in a hierarchy of actions is created. This new decision space is then connected to a physiological (or intentional) variable through the motivation link m(Di). Because of this approach, an agent autonomously connects consequences of his behavior with own physiology and learns how to preserve homoeostasis. A set of variables and actions contained in particular decision spaces (and thus also the shape of action hierarchy) is maintained during the agent's life by using four main operations: sub-spacing, behavior associating, variable removing and variable promoting. This approach dramatically reduces the size of decision space which has to be searched by the learning algorithm. It speeds up the learning convergence and enables our agent to learn even in very complex domains."
360,12,0.988,Compositionality and Concepts in Linguistics and Psychology,"the catâ leads to an imaginative construction of a scene in which previous visual and tactile experiences are recruited to ï¬ll out the meaning of the sentence. This process explains how the verb can change depending on the contextâstroking oneâs beard, or stroking a putt in golf capture something of the same meaning (a gentle movement in contact with an object) but at the same time the situation simulated leads to many differences. Barsalouâs point is that to represent the meaning of âstrokeâ as a single lexical symbol STROKE (Agent, Patient) fails to account for the different inferences that are to be drawn from the simulation. Hence the meaning of the word âstrokeâ, for example its propensity to allow us to think true and false thoughts involving acts of stroking, can only be determined relative to its context of use, which triggers complex interactions with the simulated situations. Pelletier argues the case for the more formal approach to semantics and meaning. He provides a ânatural historyâ of different positions held by philosophers of language, linguists of different schools, and cognitive psychologists. He draws attention to a critical question that one should ask about any utterance, namely âwhat was really said?â The difï¬culty that Pelletier points to is one that has long been a contentious issue between philosophers and psychologists. Can the mental representation of a concept be treated as being the âreal meaningâ of a term? Arenât there external aspects of meaning that are not inside the head? If there are, as Pelletier argues (see also Rey 1983), how is this externalist view of semantics connected to the efforts of cognitive scientists to provide a âmicroâ account of how individual meanings arise with particular speakers and hearers and particular contextual situations? How is it possible to integrate the important and valuable insights into how different syntactic forms construct meanings for sentences with the empirical evidence of how speakers themselves construct meanings in context-dependent ways? In the chapter by Hampton, these issues are given a further airing. Hampton attempts to answer the conundrum of how concepts can combine to meet the constraints of compositionality in the case where those concepts are in fact vague or fuzzy prototypes. He outlines the Composite Prototype Model for combining prototype concepts. Concepts are seen as being constituted by frame representations containing features expressed as dimensions and values (e.g. COLOUR = [RED, ORANGE]). When people have to form a conjunction or disjunction of prototype concepts, the model provides an account of the processes by which the two frames are amalgamated into a single composite frame. Along the way some properties of the concepts are lost, while new emergent properties may also be added to the composite, either from knowledge of the world or from the need to resolve incompatibilities between the two concepts. A key result supporting the model is the ï¬nding that, because of the merging of the two sets of prototypical features, noun phrases that appear to express a conjunction (e.g. âsports which are also gamesâ) do not in fact receive a strict conjunctive interpretation when people categorize different exemplars. In a second major section, Hampton explains how this result relates to the distinction between two important theoretical notions linked to prototype theory, namely Typicality and Graded Truth. These two notions are then further differentiated from lack of knowledge or ignorance. Hampton"
232,252,0.988,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"Taking actual objects as our starting point (rather than the social groups who appropriate them) offers a new perspective on representations. In this case, objects are not considered as independent and decontextualized entities, but are observed through the prism of the history of aesthetic representations, which âmakes it possible to think about knowledge, ideologies and the techniques that are implementedâwhich should certainly avoid a linear story formâthe relationships between the object of the representation and its production or receptionâ [21]. The cultural object is a reference to âall concrete objects (books, writings, paintings, photographs, ï¬lms, architecture, sculpture, etc.) resulting from a formal production and intended to produce in those who receive it a symbolic âeffectâ (aesthetic contemplation, subscription to its values, producing a belief, etc.)â [6]. This type of object âis part of a civilizational and historical context and participates in the deï¬nition of the worldview of which it is a partâ [22]. It is disseminated in communities of varying homogeneity and scope. Its influence on social reality is therefore difï¬cult to assess and the researcher must collect external data. They can then compare the cultural object with other sources to determine its function in the public space and its contextual signiï¬cance, or examine its reception by critics. Comparisons, which depend on the availability of sources, should not however be carried out at the expense of the internal analysis of the object. In practice, the object is involved in the formation of social processes, but remains a product whose constituent elements must be carefully studied, as it is the starting point for research. Cultural objects relating to nuclear accidents are very diverse in terms of both form and content. However, they all approach the subject from a dramatized angle, as is shown in the confusion between a nuclear reactor and a nuclear weapon. Objects draw upon radioactive imagery marked by Hiroshima and Nagasaki, the anxiety generated during the Cold War, and the proliferation of weapons of mass destruction. The image of the mushroom cloud is particularly evoked as a representation of the explosion of a nuclear reactor. As an example, the city of Springï¬eld is destroyed in an episode of The Simpsons [23] as a result of the explosion of a nuclear plant, in the same way that Hiroshima was razed to the ground by Little Boy. The similarity with reality highlights the discursive potential of a cultural object, which maintains a dual relationship with social representations.7"
360,292,0.988,Compositionality and Concepts in Linguistics and Psychology,"Despite the fact that we can safely rule out reference shift as an alternative explanation of the results, obviously there are many other factors that are worth further exploration. The correlation that was found in this study was high (r = 0.66, n = 36, p < 0.001), though obviously not perfect. This means that there must be more factors that affect interpretation besides the one tested here. An important next step is to delve deeper into typicality effects for complex predicates. In the current chapter, I report an experiment that indirectly measured one particular typicality measurement with one particular dependent measure, namely the typicality of two simultaneous actions, rated on a scale. One can imagine that in fact the typicality of the opposite event, i.e. two predicates applying to two separate individuals, or perhaps sequentially to one individual, might also affect the interpretation of a plural sentence with those predicates. Moreover, as pointed out by a reviewer, perhaps not only the verb concepts but also the head noun of the sentences play a role. It might be that the compatibility of two predicates is quite different in the context of humans than it is for example in the context of dogs: people can run and scratch their heads simultaneously, but dogs cannot. In order to fully understand the factors that influence sentence interpretation, an intricate combination of typicality measures is necessary. Also, it will be good to correlate rating measures with different kinds of dependent measures such as categorizaton speed or error rate to have a more robust resultâsimilar to the investigations into typicality effects for nouns. However, the fact that even one measure can distinguish different types of verb pairs so clearly, is a promising starting point for this enterprise. Another related issue is the deeper question of how typicality effects come about: What exactly makes a particular instance of a concept typical? A potential candidate factor is that typicality is formed by prior experiences or likelihood of a situation. An anonymous reviewer, however, pointed out example (16). (16) The boys are unicycling and juggling The reviewer claims that despite the fact that we probably rarely see a person simultaneously unicycling and juggling, we still probably interpret the conjunction in sentence (16) intersectively (though of course a full sample of participants would need to be consulted to be sure). Such an example points out that typicality is not simply a matter of frequency, but a far more complex notion that needs to be studied further. The question of what makes something typical does not affect the results described in this chapter per se, but knowing what affects typicality would give them more explanatory power, as pointed out by this reviewer."
49,506,0.988,Artificial Intelligence and Cognitive Science IV,"the Schachter-Singer theory does. According to it, the event causes an arousal for which there must be some reason The Facial-feedback theory describes changes in facial muscles â for example, a smile reflects joy. Ekman studied emotions that corresponded with the facial expression â traveling round the globe he figured out the basic emotions (see Table 1, line 2). The main feature of his set of the basic emotions is the fact that the resulting facial expressions are not subject of culture, but are universal â not depending on geographic places, religions, languages, genders, etc. Perception/ Interpretation"
275,348,0.988,Foundations of Trusted Autonomy,"12.2 Compatible and Incompatible States The conjunction fallacy does not mean that people always judge the probability of a conjunction as higher than each of its conjuncts. That would indeed be counter to probability theory. Just imagine that the choice between (a) and (b) was presented without the story about Linda. Then one would expect everyone with some notion of probability to choose (a) over (b), as confirmed in [35]. But when asked the question after first hearing the story, even people schooled in statistics fall victim to the conjunction fallacy. Why is this? The question generated a host of publications with possible explanations over the last several decades (see [19] for an overview). Among the many kinds of explanations offered, two stand out in particular. One assumes that words such as âandâ and âprobabilityâ are misunderstood by the participants, or at least not understood in their formal interpretation. The other assumes a reasoning bias. A recent overview [30] concludes that the latter has the best support of the two. But this answer begs the question: if there is a reasoning bias, where does that reasoning bias originate? Indeed, we are not satisfied with just knowing there is such a bias, rather we would like to describe how that bias unfolds as a cognitive process. To do so, let us formulate the participantsâ judgements as the outcome of a decision process. The explanations in the literature almost invariably involve two competing states, one in which Linda is a bankteller, and another where she is a feminist. To most participants in the experiment these states are not compatible, and whether bias or reasoning, each can in principle tip the scale in favor of one state or the other. In order to make headway, we have to take a closer look at the notion of compatible and incompatible states. In this presentation we will formulate states in the language of quantum cognition. Especially for the reader who is not already familiar with this approach, we will first recall some concepts from quantum mechanics. One such concept is the (formal) notion of compatible and incompatible states. Incompatibility lies at the heart of Heisenbergâs famous uncertainty principle. It holds that when we are certain about a quantum particleâs momentum, we are necessarily uncertain about its position, and vice versa. Position and momentum are therefore called incompatible states. On the other hand, again given the momentum of that particle, we can still measure its kinetic energy with certainty, momentum and kinetic energy thus being compatible states."
306,38,0.988,"Early Geometrical Thinking in the Environment of Patterns, Mosaics and Isometries","An important concept involved in a painting, starting from its planning to its realization, is the âconcept of space.â The canvas is an empty space that must be organized by placing objects (independent space). In other geometrical situations, the ï¬gures create the space (non-independent space): ââ¦ essentially, or primarily, we think that to the objects (or to the shapes), the space is only a coexistence of themâ (Speranza 1997, p. 130). Usually, the space managed in a painting is a âmicrospace,â namely a space that is manageable with the hands and the eyes (for instance, a sheet of paper). Sometimes it is also a âmesospace,â manageable only with the eyes (i.e., a wall in a room). In a space, the geometrical relations that describe the position of one object relative to another are more important than the ï¬gures as such. In order to demonstrate this, mosaics and tessellations, including those created by Escher, have been used. Although Escherâs mosaics have often been used at higher educational levels, it turns out that children at lower levels can also successfully deal with them (Marchini and Vighi 2009a, b). Working with mosaics indicates that children pose the intuitions of geometrical isometries, but their development requires a conscious educational treatment. In particular, among the isometries, axial symmetry is a very complex topic. Research has documented the difï¬culties observed in its understanding. Piaget and Inhelder (1947) pointed out the individuation of a âvertical axisâ of symmetry in very young pupils. This could be a didactical obstacle, as Brousseau (1983) has shown. Swoboda (2011) highlighted the difference between a static or a dynamic approach to axial symmetry among 4â6 year-old pupils. She designed an experiment on the construction and deconstruction of a pattern using printed tiles. Firstly, the tiles were âequal,â having the same orientation. She then placed a âsymmetrical tileâ in the pattern. She observed that when children were requested to reconstruct the regularity of the pattern, they tried to rotate the new tile instead of turning it over. This indicates that the visual representation of the static relationship between objects is insufï¬cient for a full understanding of isometrics as transformation."
307,369,0.988,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"12.1 Markov Model of a Wild Type Sodium Channel Markov models have turned out to be a powerful tool in representing the physics of the sodium channel and a series of alternatives have been proposed by various authors. Since this is still a very active field of research, it is hard to claim one particular model as the definitive model. We shall therefore focus on a kind of model that has a structure that seems to be more or less agreed upon but, as usual, we attack this problem with simplicity in mind. This also holds true for the way we introduce the effect of a mutation. We start by considering a simple model of the sodium channel, illustrated in Fig. 12.1. The actual functions used in our computations will be given below. However, we should note that the functions will always be chosen such that they satisfy the principle of detailed balance, which, for the model given in Fig. 12.1, means that the following relation holds: kio koc kci D koi kic kco :"
75,116,0.988,"Opening Science : The Evolving Guide on How the Internet is Changing Research, Collaboration and Scholarly Publishing","catered to a small fraction of society. Especially on TV, however, we as communication scholars should be wary of the distorted image of science reflected by conventional mass media. Coverage and content are mostly limited to either the explanation of phenomena in everyday life (ââWhy canât I whip cream with a washing machine?ââ) or supposed success stories (ââScientists have finally found a cure for Cootiesââ). Thereby journalism neither succeeds in depicting the âbig science pictureâ of policy, ethics, and economics holistically, nor the real complexity of a knowledge-creation process authentically, which is everything but linear, being a process in which knowledge is permanently being contradicted or falsified, and is therefore never final. However, the notion of what the essence of science really is could perfectly well be vulgarised through web technologies, in the sense of making the different steps within this process of knowledge-creation transparent, for instance by means of a continuous blog or other messaging or sharing platforms. Yet there are still only very few examples for such formats (see below), and they are particularly sparse in journalism. The tendency to reduce science to supposed success stories is certainly also a result of its mediatisation, i.e. science and science policies reacting and adapting to the mass media logic by which it is increasingly being shaped and framed (Krotz 2007; Fuller 2010; Weingart 2001). This brings us to the second dimension of science communication."
124,130,0.988,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"Abstract Moral luck is the phenomenon that agents are not always held accountable for performance of a choice that under normal circumstances is likely to result in a state that is considered bad, but where due to some unexpected interaction the bad outcome does not obtain. We can also speak of âmoral misfortuneâ in the mirror situation where an agent chooses the good thing but the outcome is bad. This paper studies formalizations of moral and legal luck (and moral and legal misfortune). The three ingredients essential to modelling luck of these two different kinds are (1) indeterminacy of action effects, (2) determination on the part of the acting agent, (3) the possibility of evaluation of acts and/or their outcomes relative to a normative moral or legal code. The first, indeterminacy of action, is modelled by extending stit logic by allowing choices to have a probabilistic effect. The second, deliberateness of action, is modelled by (a) endowing stit operators with the possibility to specify a lower bound on the change of success, and (b) by introducing the notion of attempt as a maximisation of the probability of success. The third, evaluation relative to a moral or legal code, is modelled using Andersonâs reduction of normative truth to logical truth. The conclusion will be that the problems embodied by the phenomenon of moral luck may be introduced by confusing it with legal luck. Formalizations of both forms are given."
363,89,0.988,History and Cultural Memory in Neo-Victorian Fiction,"such diverse experiences as the discovery of Freddie Parrâs body, which âpinioned [Tom] with fearâ so that âhe ceased to be a babeâ (61); sexual discovery, âwhich unlocked for [him] realms of candour and raptureâ (61); and the blow to the head which commits Sarah Atkinson to a waking coma: âhorror. Confusion. Plenty of Here and Nowâ (77). Those moments described as an encounter with the Here and Now are confrontations with the limitations of stories. They are proof that history is a âthin garment easily punctured by a knife blade called Nowâ (36). For, strangely enough, though they are the effects of history-making, these moments are âtense with the present tense ... fraught with the here and nowâ (207). While the Here and Now results from history it is also, conceptually, opposed to it, as in Priceâs rejection of history: âwhat matters is the here and nowâ (60). The vexed nature of this term, its grasping of a present that is continually slipping away, prompts Crick to wonder: âwhat is this much-adduced Here and Now? What is this indefinable zone between what is past and what is to come?â (60). The Here and Now can be understood as what Linda Hutcheon calls âbrute realityâ (Hutcheon, 1988: 155); it is present experience in that fleeting moment before it becomes the past, narrativised and emplotted. Or, as Janik puts it, the Here and Now is âdirect, unmediated, unintellectualized experienceâ ( Janik, 1995: 178. See also Janik, 1989: 84â6). It is an encounter with the limits of our powers to explain. That is, it is that moment when we confront an event which seems inexplicable, for which we have no contextualising, explanatory story. For this reason, the Here and Now is also an aperture where, very briefly, reality, in Tomâs sense of âemptinessâ, is glimpsed.8 This evanescent experience is the closest we come to non-narration, to being âoutsideâ story and meaning. It is the moment when history is made ânonsense by that sensation in the pit of your stomach â¦ the feeling that all is nothingâ (270). It is the reminder that there is no guiding purpose, or over-arching pattern and meaning to history; meaning is (provisionally) provided only by the stories we tell. Thus, an encounter with the Here and Now is always a more or less bloody apocalypse, an encounter with the end; the end of meaning, the end of a particular story which has framed and filled reality. Thus when Price proclaims the end of history just as Tom is confronting the limits of his power to explain Maryâs baby-snatching actions, it prompts Tom to launch himself into the past, to break away from teaching the French Revolution and try to come to terms, instead, with his own sense of the end and to elucidate the moments of crisis in his life which elude meaning and defy explanation. âThere are a thousand million waysâ, he reflects, âin which the world comes to an endâ (155)."
192,384,0.988,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"(Stapel 2012, p. 102). An important ingredient of theatres in general (and of the social psychology theatre in particular) is deception. Research subjects (usually psychology students) must be naÃ¯ve. As soon as subjects are aware of the (objective of) the experiment, this will affect their responses and ruin the results. Ideally, research subjects believe that they know what the research is about, but this actually proves to be an illusion, a distraction; a lure. Like Claudius in Shakespeareâs Hamlet (Act III, scene 2), they are lured into a mousetrap. According to Hamlet, mousetrap is title of the play performed by the actors in Shakespeareâs drama. While research subjects erroneously believe to know in what kind of situation they are becoming involved, they are suddenly exposed to an unexpected stimulus, while their responses are being meticulously monitored (as happened to Claudius as well, cf. Chap. 3), so that these responses may be considered as an indication of a disavowed flaw (racial prejudice). The experimental set-up of Stapelâs fictitious Science experiment at Utrecht train station is a cunning exemplification of the mousetrap theme. The respondents believe that, by filling in a questionnaire, they are participating in a survey, and that the chair is merely placed there for their convenience (filling in questionnaires while standing upright seems rather impractical). Thus, they are unaware of the fact that the row of chairs is actually a mousetrap. By keeping their distance vis-Ã -vis the black male, they betray their obfuscated prejudices. So, in other words, Stapel is social psychologyâs Hamlet, who masterfully inserts a play into a play (the row of chairs representing the mousetrap scene), while Utrecht station is the theatre of life, where a plethora of prejudices are steering our daily behaviour (in subliminal, unconscious ways). In order to come to terms with them, social psychologists must design a device that captures them. The problem is, of course, that Stapelâs âplayâ has never been performed, that his experiment has never been conducted. Like Shakespeareâs Hamlet, it is fiction. Stapel is actually a playwright and his article a form of drama, parodying the vernacular of fashionable science. The respondents (playing the role of Claudius, but in a contemporary setting) were not really put to the test. But this means, dialectically speaking, that the prejudices attributed to the research subjects (in the Science article) actually reflect the prejudices of the researchers themselves. It is the prejudice if the researchers that their research subjects will prove prejudiced (by keeping their distance to the non-Caucasian male). In other words, the prejudices and stereotypes enacted in this paper are actually the (prejudiced, stereotypical) expectations of the researcher (Stapel) himself, projected onto the research subjects. He wants and expects his research subjects to be prejudiced, to be biased, even if (all too often) this proves not to be the case, for otherwise he could simply have conducted the experiment instead of fabricating it. In Lacanian terms, the hidden prejudice, which must be there, proves so difficult to capture because it is the âobject aâ of social psychology. It is that which somehow must be brought to the fore (with the help of mousetraps), an objective which is considered so important that it even legitimises the use of deception. To paraphrase Gospodin Gregg (Chap. 4), social psychology is bent on exposing the Mr. Hyde in us, the evidence of prejudice, the object a, something which somehow remains hidden (first"
249,359,0.988,Advances in Proof-Theoretic Semantics (Volume 43.0),"In the âunderstood but not statedâ derivation on the right, the formula (Ï â Ï) at the top is an axiom, and the discharging step that derives (Ï â Ï) from Ï falls away. A general metarule asserts that for every step , Î± â¢ Î² we have a step , (Ï â Î±) â¢ (Ï â Î²). (This analysis is extraordinarily close to Fregeâs explanation of making and discharging assumptions, though it was given over 800 years before Frege. But as Peter noted at the meeting, Ibn SÄ±ÌnaÌ and Frege had different motivations. In fact Ibn SÄ±ÌnaÌ wanted to understand the real intentions of the person giving the proof, whereas Frege aimed through Begriffsschrift to display the true âlogical weavingâ of informal proofs that begin âLet . . .â [6, pp. 379ff].) Ibn SÄ±ÌnaÌâs position is in effect a claim about what kind of contentful argument is expressed by the natural deduction rules. So itâs directly relevant to how we can read the proof rule of â-introduction as carrying information about the meaning of â. The discussion so far has used only natural deduction proof rules. It would be possible to give a semantics using â¢ as a primitive notion, so that for example we define â§ by (Ï â§ Ï) â¢ Ï, (Ï â§ Ï) â¢ Ï, Ï, Ï â¢ (Ï â§ Ï). (There are well-known variants of this definition.) The difficulty with taking â¢ as primitive is that until we have a definition of â¢, there is going to be no purchase for transfer arguments. In particular we wonât be able even to raise the question whether (8) gives the same information as a truth table for â§, frankly because until â¢ is explained, we donât know what information (8) is giving us. One last point: some kinds of semantics refer to the semantic value of an expression as the âdenotationâ of the expression. This is just a name, no more. It certainly doesnât entail that the semantics treats expressions as proper names of their semantic values. To single out some kinds of semantics as âdenotationalâ is like singling out the semantics that are written in Turkish; the classification is pointless."
117,106,0.988,Care in Healthcare : Reflections On Theory and Practice,"Hermeneutic Autonomy One way to escape the autonomy dilemma is to look for an understanding of individual will-formation that introduces reason and interindividual reflection as guiding factors in this process, without at the same time assuming a definite set of ethical norms that trump individual choice. Many accounts of the self that have been developed in hermeneutic philosophy can be read in this way. Most notably, Paul Ricoeur sees will-formation as a reason-guided process. Reasons are not just given preferences. They always include a judgement about what is to be regarded as good in an inter-individual sense. If a certain course of action is judged as being good, any person in a similar situation ought to be able to follow this judgement, regardless of whether or not they initially have such a preference. The judgement need not be restricted to instrumental goodness, according to which an action is good if it serves as a means to bring about a desired state of affairs. âGoodâ also covers actions of which one assumes that they can be part of what constitutes a ââgood lifeâ with and for others, in just institutionsâ (Ricoeur 1992, p. 172). Therefore, forming oneâs own will is potentially always an interindividual, communicative relation and contains a âdialogical dimensionâ (Ricoeur 1992, p. 180). Recognising and weighing reasons involves taking up the perspective of others. One does not begin decision-making with a fixed set of preferences that just need to be correctly informed in order to lead to what the individual then can regard as a good decision. On the contrary, decision-making starts as an always in principle open search for reasons which are formed by taking up a number of points of view through which one finds oneâs own perspective. If this is a correct model of what it means to form a will, a debate about what is good is a seamless extension of the internal will-forming process. In such a debate, each standpoint functions as a reason that must be weighed and assessed, just as reasons and standpoints are internally assessed in the will-forming process. Hence, the result of a debate can have an influence on the individual will, regardless of the standpoint and preferences that made up this individual will in the first place. If someone is convinced by such a debate and changes their will accordingly, this is essentially identical to the process by which one forms oneâs will oneself."
124,190,0.988,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"From that point of view, distinct branching time models represent genuinely different ways a world might be, each with its own branching structure, its own histories, its own internal possibilities. These pastwards-connected branching time modelsâ Belnapian worlds, as we may call them8 âwill be in some respects much like the possibilities represented by the Kripkean worlds of normal systems of modal logic. They will differ from such worlds in at least two respects, however. First, of course, each Belnapian world has a rich internal structure which Kripkean worlds lack. But also, these Belnapian worlds are isolated from one another in separate models, whereas Kripkean worlds coexist within the same model. Accordingly, we might recognize the possibilities internal to a Belnapian world as real possibilities (relative to that world) and recognize the possibilities represented by the availability of other models as merely nominal possibilitiesâother ways the language might have been given application. We will examine this thought again in Sect. 7. At this level, the problem of trans-world identity might be thought to surface again, but our having noted that there is no special problem of trans-history identity within a branching time structure can be the occasion for reconsidering the role different worlds play. If different Belnapian worlds only appear in different models, i.e. if we impose on models the requirement of historical connection, then perhaps we can profit from some reflection on the general nature of formal models, examining the comparative role of different models, and therefore of different Belnapian worlds. So at the risk of being pedantic, let us review the basic features of models."
51,29,0.988,How Generations Remember,"tions people face (Reulecke 2010: 121).8 Giving meaning to past events is a situational process and not a single act. It is likely that the meaning people give to important events and periods in their biography changes during the course of life. Here it is important to state, as Gardner has, that âthe life course is of course not culturally neutral, nor is it unaffected by particular geographical locations, for these involve very real material conditions and power relationsâ (Gardner 2002: 221). In this book, the power relations concerning the transmission of memories (or the failure of the same) between generations in particular are carved out. The generations I refer to evolved from my interlocutorsâ narratives. These generations are understood as sharing a historical experience that generates a âcommunity of perceptionâ (Olick 1999: 339). A shared past is crucial for a generationâs constitution. Equally important are âcertain interpretative principlesâ and âdiscursive practicesâ (Nugin 2010: 355â356). The narrator takes a central role; individuals are not passive consumers of experiences, but rather play an active role in generating meaning from their experiences. Even if in most cases the generations outlined here correlate with the age of my interlocutors, we need to keep in mind that the boundaries drawn between the generations are not clear-cut and age alone is not always decisive. Consequently the generations should not be considered as homogeneous cohorts, but rather as trends based on generational identification. The notion of âgenerationâ used in this book should thus be understood as a heuristic device (see Borneman 1992). Regardless of shared âdiscursive tacticsâ identified, I did not encounter one âstandardâ narrative representative for each generation,"
148,222,0.988,Anti-fragile ICT Systems (Volume 1.0),"interested in the cortexâs ability to predict, because an anomaly is detected when a prediction is violated. The reader should know that a great deal of information also flows downward in the hierarchies of the cortex. While these feedback connections are crucial to understanding how the brain creates behavior, they do not play an important role in the current version of HTM and will not be discussed here."
249,221,0.988,Advances in Proof-Theoretic Semantics (Volume 43.0),"the two adjoints at once is tonk. The problem of tonk, therefore, lies in confusing two essentially different adjoints as if they represented the same one logical constant. We may thus conclude as follows: â¢ The problem of tonk is the problem of equivocation. The binary truth constant and the binary falsity constant are clearly different logical constants, yet tonk mixes them up, to be absurd. This confusion of essentially different adjoints is at the root of the paradoxicality of tonk. There is no problem at all if we add to a logical system the right adjoint of ââ¥ and the left adjoint of ââ¤ separately, any of which is completely harmless. Unpleasant phenomena only emerge if we add the two adjoints as just a single connective, that is, we make the fallacy of equivocation. Let us think of a slightly different sort of equivocation. As explained above, â§ is right adjoint to diagonal â, and â¨ is left adjoint to it. What if we confuse these two adjoints? By way of experiment, let us define âdisconjunctionâ as the functor that is right adjoint to diagonal, and left adjoint to it at the same time. Of course, a logical system with disconjunction leads to inconsistency (or triviality). Needless to say, the problem of disconjunction is the problem of equivocation: conjunction and disjunction are different, yet disconjunction mixes them up. Then, is the problem of disconjunction precisely the same as the problem of tonk? This would be extensionally true, yet intensionally false. It is true in the sense that both pseudo-logical constants fall into the fallacy of equivocation. Nonetheless, it is false in the sense that the double adjointness condition of disconjunction is stronger than the double adjointness condition of tonk. What precisely makes the difference between tonk and disconjunction? Tonk is a right adjoint of one functor, and at the same time a left adjoint of another functor. In contrast to this, disconjunction is a right and left adjoint of just a single functor. Disconjunction is, so to say, a uniformly doubly adjoint functor, as opposed to the fact that tonk is merely a doubly adjoint functor. The difference between tonk and disconjunction thus lies in uniformity. Hence: â¢ On the ground that uniform double adjointness is in general stronger than double adjointness, we could say that disconjunction is more paradoxical than tonk, endorsing a stronger sort of equivocation. â¢ We thereby lead to the concept of intensional degrees of paradoxicality of logical constants. Degrees concerned here are degrees of uniformity of double adjointness or equivocation. What is then the strongest degree of paradoxicality in terms of adjointness? It is self-adjointness, and it is at the source of Russell-type paradoxical constants. A selfadjoint functor is a functor that is right and left adjoint to itself. This is the strongest form of double adjointness. Now, let us think of a nullary paradoxical connective R defined by the following double-line rule (this sort of paradoxical connectives has been discussed in Schroeder-Heister [20, 22]):"
82,216,0.988,Fading Foundations : Probability and The Regress Problem,"In (4.1) the importance of P(p) has somewhat decreased. It is still the case that it largely determines P(q), but the influence of the conditional probabilities has become greater. In general it is so that, as the chain becomes longer, the support provided by the totality of the conditional probabilities increases, while that given by the foundation decreases. In other words, as m in Am grows larger and larger, a law of diminishing returns come into force: the influence of P(p) on P(q) tapers off with each link, until it finally fades away completely. In the limit that m tends to infinity, all the probabilistic support for q comes from the conditional probabilities together, and none from the ground or foundation. This characteristic, that is essential to a probabilistic regress as we defined it, we call the feature of fading foundations. As we add more and more links to the chain the influence of P(p) tails off, and P(q) draws closer and closer to its final value. The feature of fading foundations can be illustrated by our story about Barbara bacterium in the previous chapter. Recall that q is the proposition âBarbara has trait T â, An is âBarbaraâs ancestor in the nth generation has T â, and p is âBarbaraâs primordial mother has T â. Now imagine that long and extensive empirical research in our laboratory has taught us that the probability that a bacterium has T is 0.99 when her mother has T , and that it is 0.04 when her mother lacks T :"
8,844,0.988,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","What is particularly noteworthy about the s-carrying antibaryons is that they can conventionally only be produced in direct pair production reactions. Up to about Ek;lab =A D 3:5 GeV, this process is very strongly suppressed by energyâmomentum conservation because, for free pp collisions, the threshold is at about 7 GeV. We would thus like to argue that a study of the Î and Î£0 in nuclear collisions for 2 < Ek;lab =A < 4 GeV could shed light on the early stages of the nuclear collisions in which quark matter may be formed. Let us mention here another effect of importance in this context: the production rate of a pair of particles with a conserved quantum number like strangeness will usually be suppressed by the Boltzmann factor e 2m=T , rather than a factor e m=T as is the case in thermomechanical equilibrium (see, for example, the addendum in [8]). As relativistic nuclear collisions are just on the borderline between those two limiting cases, it is important when considering the yield of strange particles to understand the transition between them. We will now show how one can describe these different cases in a unified statistical description [23]. As we have already implicitly discussed [see Eq. (27.13)], the logarithm of the grand partition function Z is a sum over all different particle configurations, e.g., expressed with the help of the mass spectrum. Hence, we can now concentrate in particular on that part of ln Z which is exclusively associated with the strangeness. As the temperatures of interest to us and which allow appreciable strangeness production are at the same time high enough to prevent the strange particles from being thermodynamically degenerate, we can restrict ourselves again to the discussion of Boltzmann statistics only. The contribution to Z of a state with k strange particles is 1 X s Zk D Z .T; V/ ; kÅ  s I"
192,51,0.988,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"Keplerâs experience (of contradiction and frustration) was nonetheless important, because it revealed that, apparently, the awesome Platonic starting point had been naÃ¯ve, inadequate and misguiding. In fact, his frustrating experience enabled Kepler to discover that the orbits of the planets are actually elliptical. This is in agreement with the dialectical insight that experiences of negativity (frustration, contradictions, etc.) are inevitable and progressive. It is only by exposing ourselves (our worldviews) to the real that knowledge production may progress, however painful and offensive such experiences may be. Keplerâs fiasco became the starting point for a process, an epistemic adventure that culminated in space travel. Unfortunately, Lacan adds, although a modern space capsule is basically a laboratory, a device for conducting multiple experiments, Russian and American space programs failed to grasp the opportunity to conduct experiments of a more philosophical (phenomenological) nature. How did Gagarin, Glenn and the other early"
13,94,0.988,Feeling Gender : a Generational and Psychosocial Approach,"but the social patterns to which they speak that is the target (Roseneil 2006: 848). In my case, the whole-sample analysis does not work on the level of individual psychobiographies, but on the level of shared psychological patterns. It is not the defended subjects I am aiming at, but rather the patterns of defence across individual cases, no matter how varied the individual trajectories to this pattern were. Furthermore, the concept of the unconscious I work with is not only or even primarily understood with reference to the dynamically repressed, but rather as a part of the self that organises experience and informs creativity and agency on an emotional level (see Chap. 2). For this reason I relate to my interview transcripts not as persons, but as texts. I regard the interviews as a corpus of text in which I try to find the emotional meaning as an integrated part of what is said, directly or indirectly. Just as narrative perspectives open up âoutwardsâ to social change, they also open up âinwardsâ through conveying the emotional meanings attached to such change (Rudberg and Nielsen 2011). It is possible to work with a psychoanalytic ontology (Hollway and Jefferson 2000; Roseneil 2006) and to employ a context-sensitive textual analysis and a psychologically sensitive reading that acknowledge that an emotional meaning is always present in what is said, without analysing individual unconscious conflicts. From the way the informants talk and the words they use, it is possible to learn something about their personal images of parents and thus to gain some insight into how also emotional aspects of meanings of gender are part of a personâs biography. I have found the psychoanalytic concepts of identification, disidentification, projection, disavowal, idealisation and ambivalence (see Chap. 2) useful as ways to interpret what the informants say about their relationships with others and their own bodies. They may work as âsmall pockets of feelingsâ in the text. What people say about their feelings is sometimes relatively straightforward, while at other times it may be more hidden or ambiguous. The relation between utterance and textual context is an important cue here. If an informant says âI had a good relationship with my motherâ and then proceeds to describe her relationship with her father with much more enthusiasm and detail, then this textual context lends significance to the claim. Another informant could utter exactly the same phrase, but the context of the interview could give it a different meaning"
360,445,0.988,Compositionality and Concepts in Linguistics and Psychology,"The applicability of the A-T view to cognitive science is seen acutely in what we have been discussing throughout this article: property veriï¬cation judgments and subsequent modiï¬cations made to those judgments. To show this applicability, it is perhaps best to begin with a general explanation of how a property veriï¬cation judgment is made (in its strictest sense) on the A-T view (i.e., through what Aristotle and Aquinas call episteme or scientia, respectively), and then present how the same approach might account for the sort of property veriï¬cation results encountered in our experiment. On the A-T view, property veriï¬cation in its strictest sense occurs in virtue of understanding how a given property is explanatorily useful in knowing the nature of the thing in which the property is found. In other words, in the A-T view, to do property veriï¬cation is to verify that a characteristic is, in fact, a property of the thing (i.e., that it is a characteristic that flows from the essence of the thing). To put the matter concretely, we can ask, âHow does one decide whether birds have wings?â but with the understanding that we are actually asking how one decides whether wings is a proper accident (flowing from the essence) of bird. On the A-T view, property veriï¬cation in this strict sense will occur when we understand why the property regularly occurs in the kind of thing of which we are predicating the property. Note that in the A-T view, much of what we âknowâ about the world, even though a reliable guide for action, etc., is not actually known in the strict sense of episteme or scientia. We return to this point below. Now, as we have pointed out above, judgments are not made by looking in the concept to see if there is a property there, but rather through a judgment about the"
82,265,0.988,Fading Foundations : Probability and The Regress Problem,"with its inherent requirement for explicitly planned and acknowledged transits, is an impossibility. And the reason for this lies not in the impossibility of motion, but in the fact that making a journey to somewhere (as distinct from reaching or arriving there) involves deliberation and intentional goal-setting. And since man is a finite being, an infinitude of conscious mental acts is impossible for us. So while that first structural regress is harmless, the second regression of infinitely many consciously performed acts is an impossibility.14"
124,547,0.988,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"forces, updating knowledge from public observation or more private sources is the key topic in dynamic-epistemic logics (van Benthem 2011). It is natural to add epistemic operators of all these sorts to logics of decision and action, and in fact, this is happening in logics of games (cf. van Benthem 2014). Many kinds of knowledge relevant to action scenarios are local, having to do with what agents know temporarily as they make a choice. But more global âprocedural knowledgeâ about the future of the process is essential, too, and then the trees of STIT may lose their grip. If I know something about your space of possible strategies, the informational situation will need âSTIT forestsâ rather than trees to distinguish the alternatives (cf. van Benthem et al. 2009). The same complication arises in genuine multi-agent scenarios. One cannot assume that agents know everything about others, and to cope with this variation, again, models have to be complicated beyond the basic STIT format. Pursuing these matters is beyond the scope of this chaper, but explicit modeling of knowledge seems inescapable in a serious theory of choice and action. For a discussion along these lines, see Pacuit and Simon (2011) for a logical system that merges ideas from STIT and PDL while explicitly representing the agentsâ knowledge. We see it as one virtue of our linking up STIT and PDL that experiences in the latter area can then be enlisted for the former. Our next section will present a case study, of one particular dynamic epistemic logic with added STIT features."
107,13,0.988,"Symbiotic interaction : 5Th international Workshop, Symbiotic 2016, Padua, Italy, September 29â30, 2016, Revised Selected Papers","Of course, a metaphor is able to illuminate some aspects of a domain by similarity with some other domain [12], but not all aspects. Where the biological metaphor stops is in referring to two leaving organisms in partnership, since a symbiotic technology is not credited to share the same needs (and rights) as the human being to which it is joined. http://cordis.europa.eu/fp7/ict/docs/ict-wp2013-10-7-2013-with-cover-issn.pdf."
217,2,0.988,Finite Difference Computing With Pdes : a Modern Software Approach,"understanding of all details involved in the model and the solution method. Everybody nowadays has a laptop and the natural method to attack a 1D heat equation is a simple Python or Matlab program with a difference scheme. The conclusion goes for other fundamental PDEs like the wave equation and Poisson equation as long as the geometry of the domain is a hypercube. The present book contains all the practical information needed to use the finite difference tool in a safe way. Various pedagogical elements are utilized to reach the learning outcomes, and these are commented upon next. Simplify, understand, generalize The bookâs overall pedagogical philosophy is the three-step process of first simplifying the problem to something we can understand in detail, and when that understanding is in place, we can generalize and hopefully address real-world applications with a sound scientific problem-solving approach. For example, in the chapter on a particular family of equations we first simplify the problem in question to a 1D, constant-coefficient equation with simple boundary conditions. We learn how to construct a finite difference method, how to implement it, and how to understand the behavior of the numerical solution. Then we can generalize to higher dimensions, variable coefficients, a source term, and more complicated boundary conditions. The solution of a compound problem is in this way an assembly of elements that are well understood in simpler settings. Constructive mathematics This text favors a constructive approach to mathematics. Instead of a set of definitions followed by popping up a method, we emphasize how to think about the construction of a method. The aim is to obtain a good intuitive understanding of the mathematical methods. The text is written in an easy-to-read style much inspired by the following quote. Some people think that stiff challenges are the best device to induce learning, but I am not one of them. The natural way to learn something is by spending vast amounts of easy, enjoyable time at it. This goes whether you want to speak German, sight-read at the piano, type, or do mathematics. Give me the German storybook for fifth graders that I feel like reading in bed, not Goethe and a dictionary. The latter will bring rapid progress at first, then exhaustion and failure to resolve. The main thing to be said for stiff challenges is that inevitably we will encounter them, so we had better learn to face them boldly. Putting them in the curriculum can help teach us to do so. But for teaching the skill or subject matter itself, they are overrated. [18, p. 86] Lloyd N. Trefethen, Applied Mathematician, 1955-."
124,326,0.988,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"F(1)Î· â§ â¦F(1)(Ï â§ â¦F(1)Î· â§ F(1)(Ï â§ F(1)Ï )) The problem for a Thin Red Line theory in evaluating this proposition is how to understand the embedded occurrences of the F-operator. One way to do this is by using the following branching time structure, which has been enriched with arrows indicating not only a single designated future, but actually a designated future at every branching point in the system (Fig. 2): The example shows that if the model is taken seriously, then there must be a function TRL, which gives the true future for any moment of time, m. More precisely, TRL(m) yields the linear past as well as the true future of m, extended to a maximal set. In this way, TRL(m) will for any moment of time, m, be a chronicle within the branching time system. It is very likely that William of Ockham would have accepted the points made by Belnap and Green regarding embedded tenses. When analysing the features of the Ockhamistic model, it becomes evident that within the model there must be a true future, not only in every actual situation or instant, but also in every possible situation. This was at least realised by Luis de Molina, who worked some centuries after Ockham, but still very much in the same scholastic tradition. Molinaâs special contribution is the idea of (Godâs) middle knowledge, âby which, in virtue of the most profound and inscrutable comprehension of each free will, He saw in His own essence what each such will would do with its innate freedom were it to be placed in this or that or indeed in infinitely many orders of things â even though it would really be able, if it so willed, to do the oppositeâ (quoted from Craig 1988, p. 175). Craig goes on to explain it as follows: ââ¦ whereas by His natural knowledge God knows that, say, Peter when placed in a certain set of circumstances could either betray Christ or not betray Christ, being free to do either under identical circumstances, by His middle knowledge God knows what Peter would do if placed under those circumstancesâ (Craig 1988, p. 175). Craig has argued that such counterfactuals of freedom can be true even if there is nothing to make it true and no grounding of such truth. On the contrary, the truth of counterfactuals of freedom might be taken as"
232,41,0.988,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"1 Introduction The French word dommage (damage) has several translations into English. Two are of particular interest: ï¬rst, it is the partial or total physical destruction of a living being (ranging from physical injury to death) or an object, due to an accident; second, it represents loss, as in the expression âitâs a pityâ or âwhat a pityâ. This polysemy makes it difï¬cult to create an empirical concept in the sense of a class of objects that can be characterized by the elements that compose it. But why even try? Because a deeper understanding of the anthropological meaning of damage can help to identify social dynamics that receive little attention in classical risk prevention studies [1]. In practice, the word appears to designate both the consequences of a disaster, and the losses suffered by humans and non-humans which are serious enough for it to be asserted that the disaster must not happen again. In this case, taking damage and its dynamics into account can help to go beyond the simple issue of loss. D. Pecaud UniversitÃ© de Nantes, Nantes, France D. Pecaud (&) Centre for Research on Risks and Crises (CRC), MINES ParisTech/PSL Research University, Paris, France e-mail: dominique.pecaud@univ-nantes.fr Â© The Author(s) 2017 J. Ahn et al. (eds.), Resilience: A New Paradigm of Nuclear Safety, DOI 10.1007/978-3-319-58768-4_2"
275,354,0.988,Foundations of Trusted Autonomy,"on a matter, requires changing from one point of view to another, and the two points of view can imply incompatibility. In other words, the uncertainty principle entails that it is not possible to be simultaneously decided on both the image and content with respect to assessing trustworthiness. Just like it is not possible to form a joint probability of both the position and momentum of a quantum particle, it is not possible for the human participant to form a joint probability across decisions, e.g., whether they both trust the image and the content of the image. Not being able to form joint probabilities signals the presence of contextuality. A well studied example of contextuality is the curious phenomenon of entanglement: Empirical observations are collected in four measurement settings of a system of two quantum particles such as photons. Each of the four settings yields a pairwise joint probability distribution which models the observations made in that measurement setting. An entangled system is deemed âcontextual"" because it is not possible to combine these four pairwise joint probability distributions into a single probabilistic model such that the four pairwise empirical distributions are marginal distributions of this global model. Even though contextuality manifests within the sub-atomic realm, there is a growing body of research which is exploring whether contextuality manifests in cognition and related areas (e.g., [1, 2, 8, 9, 16, 17]). In the context of our example, contextuality arises because the image and content decision perspectives cannot be meaningfully combined into a single joint distribution."
51,48,0.988,How Generations Remember,"Counter-versions may emerge at the same time as a dominant narrative is told or after years of silence (see Foucault 1977; Gal 2002; Ochs and Capps 1996; Saikia 2004). Within an authoritarian state, such narratives are likely to remain in the private sphere or outside of state control (e.g., in the memories of dissidents). In this context it is tempting to view the dominant discourse as oppressive and negative, and the discourses that contest it as positive and closer to âtruthâ. However, rather than asking about the truth of the official or counter-narratives, the more relevant and significant question, also for this book, is about the relationship between them (see Fentress and Wickham 1992). They are necessarily interrelated, since any counter-narrative always relates to the dominant discourse (see Schramm 2011). Moreover, as will become clear in Chaps. 2 and 3, which deal with the memory politics and historiography of the Yugoslav period and in the present, the status of narratives is not fixed: a counter-narrative can become the dominant narrative manifested in historiography and vice versa. Nevertheless, even if we no longer treat history and memory as antithetical concepts, it does not mean that no distinction between the two should be made. Instead, I suggest that the question of interconnectedness should be explored within the specific ethnographic context. By opting to speak of narratives of the past, I seek to avoid drawing too clear a distinction between memory and history. It is not useful to draw a strict line between memory and history, neither analytically nor ethnographically. With regard to the latter, Birth argues: To ethnographically explore the fluid, interdependent relation between history and memory discards an inflexible bifurcation of the past into âhistoryâ versus âmemory.â This dichotomy plays a role in both the purported objectivity of history and subjectivity of remembering. In this contrast, history becomes contextual, and âmemory,â whether it is collective or individual, becomes a dimension of intersubjective significance. (Birth 2006: 177)"
223,354,0.988,Knowledge and Action (Volume 9.0),"As John Dewey (1934/2005) observed âthe first great consideration is that life goes on in an environment; not merely in it but because of it, through interaction with itâ (p. 12, italics in original). It is puzzling that although organizational scholars may agree with him, they have not agreed on how social and physical space interact. Researchers who consider it problematic that âmost previous research assumes that spatial orderings of things and people are merely part of the backgroundâ (Edenius & Yakhlef, 2007, p. 207) have been exploring space in organizations from different angles. Some authors are very critical of the passive role assigned to space: âTo picture space as a âframeâ or container with no other purpose than to preserve what has been put in it is an error displaying traces of Cartesian philosophyâ (Kornberger & Clegg, 2004, p. 1101). However, there is a risk that analysts attempting to redress the balance sometimes attribute such great powers to space as to anthropomorphize it and thereby relegate its inhabitants to the status of pawns of masterbuilders (e.g., Kornberger & Clegg, 2004). Our contention is that a clear conceptualization of the relationship between physical and social space is critical for understanding the actions people undertake in their present setting and envisage for the future. Our"
270,81,0.988,The Huawei and Snowden Questions : Can Electronic Equipment From Untrusted Vendors Be Verified? Can An Untrusted Vendor Build Trust into Electronic Equipment? (Volume 4.0),"What computers can and cannot do has been a long-standing topic in the foundation of computer science. Some of the pioneers of the field had a strong background in mathematics and, in the early days of computing, worked on the mathematical formulation of the limits of computation. The work led to the notion of decidability. Informally speaking, a question that can be answered by either yes or no is decidable if a computer can compute the correct answer in a finite amount of time. The relation that the notion of decidability has to our problem of vendor trust should be obvious. If the question of whether an executable program performs malicious acts is decidable, we can hope to devise a program to check the code made by an untrusted vendor. If it is known to be undecidable, this conclusion should impact on where to invest our efforts. In this chapter, we review and explain some of the key results on decidability and explain how these results impact the problem of untrusted equipment vendors."
359,239,0.988,"Micro-, Meso- and Macro-Dynamics of the Brain","In the simplest case of classification, two neurons that individually cause the same postsynaptic effect are seen by the network as being equivalent, that is, as being in one class. Thus, the position of these two neurons in the entire system of relationships is the same. Different neurons will in general have varying degrees of overlap in their postsynaptic effects, making it possible to talk about varying degrees of similarity with respect to their position in the system of relations. In this way, Hayek spoke of the postsynaptic activity representing the common attributes of presynaptic impulses that bring about that postsynaptic effect, though he preferred to say that the postsynaptic activity constitutes the attribute, rather than represents it. This was to make the ontological point that these neural systems are what the common attributes actually are and that they do not exist outside of the material actions of the neural network. In other words, the contents of consciousness have a one-to-one correspondence not only with the activity of neurons but also in the structure of the network in which that activity exists. Importantly, this theory differed radically from contemporaneous theories where the qualitative aspects of the mind were somehow attached to the properties of electrical signals themselves. Here, instead, we see the beginnings of an understanding of the psyche that has at its core relations and information: âit is thus the position of the individual impulse or group of impulses in the whole system of connections which gives it its distinctive quality.â (Hayek 1999). Indeed, it is important to point out that there are two separable aspects of this scheme. The first is the (simple) classification of different signals by their differing effects (âto respond differently to different impulsesâ). In this way, if each of a group of cells causes the firing of a postsynaptic cell A, and each of a different group of cells causes the firing of a different cell B, then the network has classified these groups of cells into two distinct classes. This alone, however, does not make up a system of relations, because so far we have only described distinct attributes, A and B, with no real relationship between them. The second aspect is then that of putting those attributes in a relationship with one another. This is where multiple classification comes in. By way of example, this process occurs when a postsynaptic cell requires the concurrent input of any of a member of class A alongside any of a member of class B, or the concurrent input of any member of class C and any member of class D. In such a case, we can say that the postsynaptic cell responds to the relationship between A and B, which is the same relationship as between C and D. These two processes have been put to quantitative work in a modern theory of consciousness, called integrated information theory (IIT), proposed by Giulio Tononi (2008). We will not describe the theory in all of its conceptual and mathematical detail here. For our purposes, it is important to point out the conceptual overlap with Hayekâs ideas of classification, even though the two theories start from a very different set of considerations. The two concepts necessary for Hayekâs scheme to set up a network of relations, that of setting up distinct attributes by virtue of them having distinct postsynaptic effects and that of relating these attributes to each other by virtue of their overlapping (classifying classes) and diverging (being in multiple classes at once) inputs onto postsynaptic cells, can"
78,312,0.988,The Onlife Manifesto : Being Human in a Hyperconnected Era,"perception and cognition of the world are generating an environment that simulates agency. Whereas the International Telecommunications Union spoke of the Internet of Things as âthe offline world going onlineâ (ITU 2005), in some sense the plethora of autonomic decision systems are turning our inanimate environment âOnlifeâ. In this section I will investigate what this means for the public sphere, or even for the traditional private/public divide in itself. I will engage with the notion of the public sphere to inquire whether and how smart environments generate a kind of ânatalityâ here (Arendt 1958): a novelty, a beginning, an empty space to experimentâwith as yet unknown affordances."
49,384,0.988,Artificial Intelligence and Cognitive Science IV,"because we reduce the world on one side, but we also produce something that is not in the world. We produce the substance, we produce pure ideality. But come back to the reduction. However, we can only guess what we throw away when we realize phenomenological reduction? They could be two possibilities: â¢ Either we act so that it will try it and we will imagine that the thing can do without the other properties and therefore these properties are not essential (essence); â¢ Or we have some premonition of what is the nature of thong and on this basis we identify irrelevance properties of thing. Husserl responds that by making this epochÃ© we obtained myself, with his own pure life of consciousness in which and through which all the objective world exists for me - in a way which is right for me. The world for me ... is not nothing but a world that is in such my cogito consciously being for me and having a validity for me. We have the whole world in itself in our consciousness. So the natural world - that world, I am talking about and I can speak about- precedes de facto existence the pure ego and his cogitationes as being on that is earlier.[6 Â§ 56] Our approach is something else. We believe that the constitution of any thing as a whole is always the composition properties of the stimuli that were pulling from sensors of entities, which are already in advance as a singular. They are what the human mind recognizes through operations identity and negativity, and these properties are combined by human thinking (given together) so way that we realize constituted a thing (for example a cup) as a whole. Properties have their basis in existence. Human thinking is only recognized that as a specific feature in that they are assigned to something what was already defined before by operation of identity and of negativity in encapsulated structure of endocepts. Property is what is formed and what stands out in relationships. Without any relationship can not reveal what actually is, can not reveal their essence. The stimulus comes to the brain even different neuronal pathways and they are also in different parts of the brain identified. For example the visual center of the brain is divided into several areas. One of them, for example, identify vertical lines and again from those stimuli that mark the horizontal, in other parts of the brain are different colors, is identified in another movement, etc."
360,458,0.988,Compositionality and Concepts in Linguistics and Psychology,"This distinction builds on the long-standing observation that language mediates between concepts in our mind and the things they refer to in the world (Ogden and Richards 1923, among many others). We take these connections to concepts and to the world to be distinct aspects of language, each of which facilitates a different process of concept composition.2 Take for instance the phrase red box in the examples in (3). In the absence of any context, red, when modifying box (or indeed any noun denoting a physical object), refers to its color, and so we can usually paraphrase (3-a) as âIdentify a box that is red in color and put the relevant scarf inside itâ. However, it may also refer to other properties of the box referent, such as the intended color of its contents, if the discourse context makes the relevant property clear (3-b)."
262,217,0.988,"Reality Lost : Markets of Attention, Misinformation and Manipulation","If a democracy at any given time and place is categorized as either factual or post-factual, you risk losing sense of the diverse social tendencies pulling in several directions all at once and creating a nuanced picture of what is real rather than a simple either/or situation. Societal development is not unambiguous. In order to navigate in a forever changing and messy reality and understand the tendencies and phenomena at play in our time, there is a need for maps with guideposts and beacons to navigate properly. With such beacons it will be possible to gain understanding of a complex and changing world that may form the foundation for further study of the political landscape. The concepts of factual and post-factual democracies are such beacons; they are ideal types. Sociologist Max Weber (1864â1920) introduced the ideal type as a conceptual instrument to compare different singular phenomena (Coser 1977). According to Weber, ideal types are methodological tools to analyze the world, not describe it in detail:"
208,129,0.988,Actors and the Art of Performance,"each experiences his own potency only in collaboration with others, that couplings bring forth life and that the quality of one is dependent upon the quality of the other. But dependency does not, as is often believed, revoke freedom. In creative interplay, dependency is a prerequisite for maximal freedom, for the freedom of play. Actors know, or at least intuit âthat the true site of originality and strength is neither the other nor myself, but our relation itself.â30 âIt is the originality of the relation which must be conqueredâ so that the play can be a success, a felicitous event. That is why the space surrounding actorsâ relationships is neither the ego of one nor the ego of the other, but their cusp, in between the two. It is the hyphen of the open moment that both separates and joins, like the fond gaze that enables both actors to transcend themselves in play(ing) without losing their own individuality. From the paradox of with-out me, a web is spun between them (Greek: hyphe-web, hyphen-together), held by the finest of threads, and when it works, âwhen the relation is original, then the stereotype is shaken, transcended, evacuated, and jealousy, for instance, has no more room in this relation without a site, without topos.â31 Response and responsibility meet. When all senses are penetrated in this way, and oneâs very existence merges with others, doesnât it bring ethics and aesthetics in the closest proximity? Isnât one precondition of the art of ensemble acting a regard for the exposed defenselessness of the other(s) and respect for the face of the other?32 Through this connection, the actors break through, throw off the pretenses and prejudices their past has conditioned them to carry. Regarding one another, they give each other space, create a shared space, one through the other, for the unexpected, the unforeseeable, leading one another. This happens not only during rehearsal, when putting the play together, but also in every staging of the performance. Performative quality always necessitates drawing from the past and anticipating the"
257,258,0.988,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","theory: if one player selects an action deterministically, the other player may exploit this choice and get an advantage. For each player the optimal strategy is to play probabilistically, using a distribution that maximizes his own gain for all possible actions of the adversary. In zero-sum games, in which the gain of one player coincides with the loss of the other, the optimal pair of distributions always exists, and it is called saddle point. It also coincides with the Nash equilibrium, which is defined as the point in which neither of the two players gets any advantage in changing unilaterally his strategy. Motivated by these examples, this paper investigates the two kinds of choice, visible and hidden (to the attacker), in a game-theoretic setting. Looking at them as language operators, we study their algebraic properties, which will help reason about their behavior in games. We consider zero-sum games, in which the gain (for the attacker) is represented by the leakage. While for visible choice it is appropriate to use the âclassicâ game-theoretic framework, for hidden choice we need to adopt the more general framework of the information leakage games proposed in [4]. This happens because, in contrast with standard game theory, in games with hidden choice the utility of a mixed strategy is a convex function of the distribution on the defenderâs pure actions, rather than simply the expected value of their utilities. We will consider both simultaneous gamesâin which each player chooses independentlyâand sequential gamesâin which one player chooses his action first. We aim at comparing all these situations, and at identifying the precise advantage of the hidden choice over the visible one. To measure leakage we use the well-known information-theoretic model. A central notion in this model is that of entropy, but here we use its converse, vulnerability, which represents the magnitude of the threat. In order to derive results as general as possible, we adopt the very comprehensive notion of vulnerability as any convex and continuous function, as used in [5,8]. This notion has been shown [5] to subsume most information measures, including Bayes vulnerability (aka minvulnerability, aka (the converse of) Bayes risk) [10,27], Shannon entropy [26], guessing entropy [22], and g-vulnerability [6]. The main contributions of this paper are: â We present a general framework for reasoning about information leakage in a game-theoretic setting, extending the notion of information leakage games proposed in [4] to both simultaneous and sequential games, with either hidden or visible choice. â We present a rigorous compositional way, using visible and hidden choice operators, for representing adversary and defenderâs actions in information leakage games. In particular, we study the algebraic properties of visible and hidden choice on channels, and compare the two kinds of choice with respect to the capability of reducing leakage, in presence of an adaptive attacker. â We provide a taxonomy of the various scenarios (simultaneous and sequential) showing when randomization is necessary, for either attacker or defender, to achieve optimality. Although it is well-known in information flow that the defenderâs best strategy is usually randomized, only recently it has been"
124,168,0.988,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"Abstract Belnapâs work on stit theory employs an Ockhamist theory of branching time, in which the fundamental possibilia within models are commonly taken to be moments of time, connected into a tree-like branching structure. In the semantics for alethic modal logic, necessity is characterized by quantification over relevant possible worlds within a model, yet Belnap refers to an entire model of branching time as our world, seemingly leaving no room for non-trivial quantification over worlds within a single model. This chapter explores the question how the notion of possible worlds should be understood in relation to an Ockhamist framework, in order to be able to combine an account of alethic modalities with an account of branching time and stit theory. The advantages and drawbacks of several alternative approaches are examined."
8,698,0.988,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","evaporation from some hot object of elementary dimensions. To honour Koppe for this pioneering work, this model was called the âFermi statistical modelâ [6]. The hot dense object became known as a âfireballâ. Very soon it was discovered that a single fireball could not explain the momentum distribution of emitted particles; it was impossible to find, for any given event, a Lorentz frame in which the momentum distribution was isotropic. Indeed, this should not have been expected, since even if a single fireball had formed it would, in general, have a very high spin. Moreover, phase space calculations show that the actual anisotropyâa forward/backward jet in the centre-of-momentum frameâcannot be accounted for by assuming a single, high spin fireball [7]. This is easily understood: the initial state has very definite phase relations between its individual partial waves; a single fireball, even if considered as a statistical sum over spins, cannot reproduce these phase relations. It was then found, with the help of ingeniously chosen variables [8], that two fireballs moving with large opposite velocities in the CM frame were a much better approximation of reality. Adding a third fireball, at rest in CM, substantially improved the picture [9] (and references therein). Of course, the two oppositely moving fireballs need not have the same mass nor the same speed, and the third could also have some velocity in the CM. Therefore, one should rather introduce mass and velocity distributions, but then why have just three fireballs? Why not sometimes one, sometimes two, and sometimes three or even more? Thus, one should also introduce a distribution for the number of fireballs. It seems that, in this way, one obtains so much freedom that one can fit everything. This is not so if some simple model assumptions are made which are based on observation and which are very restrictive. This was done in the thermodynamical model [2], which I shall briefly describe. It was designed to predict inclusive momentum distributions and branching ratios of particles produced in high energy pp collisions, but it was later easily adapted to p, Kp [10], and pA (even heavy nuclei) collisions [11]. Some of the simplifying assumptions may be grossly wrong if extended to the relativistic heavy ion collision. I shall come back to this. The simplifying postulates were [2, 12]: Postulate 1. In high energy collisions of hadrons, collective motions have only components in the direction of the collision axis. It is possible to find a continuum of comoving Lorentz frames (local rest frames) such that a comoving observer will, in his neighbourhood, see only thermal motion. Turbulence is absent. Postulate 2. All the kinetic energy of the incoming particles, which disappears by decelerating hadronic matter, is adiabatically and locally converted into excitation energy (heat). Postulate 1 is illustrated in Figs. 26.1, and 26.2 The first figure images are taken from my CERN lectures in 1971 [12], while the second are from recent theoretical articles on the relativistic heavy ion collision [13, 14]. Figure 26.1 is a picture of the distribution at the moment of impact, while Fig. 26.2 show a time development, on left in a model [13] and on right in a simplified hydrodynamic calculation [14], the"
213,287,0.988,Collider Physics Within The Standard Model : a Primer,"as a consequence of the Majorana nature of neutrinos. In conclusion, neutrino masses are believed to be small because neutrinos are Majorana particles with masses inversely proportional to the large scale M of energy where L non-conservation is induced. This corresponds to an important enlargement of the original minimal SM, where no R was included and L conservation was imposed by hand (but this ansatz would be totally unsatisfactory because L conservation is true âaccidentallyâ only at the renormalizable level, but is violated by non-renormalizable terms like the Weinberg operator and by instanton effects). Actually, L and B non-conservation are necessary if we want to explain baryogenesis and we have Grand Unified Theories (GUTs) in mind. It is interesting that the observed magnitudes of the mass-squared splittings of neutrinos are well compatible with a scale M remarkably close to the GUTpscale, where L non-conservation is indeed naturally expected. In fact, for mÄ  Âm2atm  0:05 eV (see Table 3.1) and mÄ  m2D =M with mD  v  200 GeV, we find M  1015 GeV which indeed is an impressive indication for MGUT . Table 3.1 Fits to neutrino oscillation data from [229] (free fluxes, including short baseline reactor data)"
333,7,0.988,Uses of Technology in Upper Secondary Mathematics Education,"them a permanence that cannot be achieved by physical beings or objects. Indeed, they connect with and express very general features of the human experience of the world. This is why, if we were to read in the newspaper tomorrow morning that the Natural Numbers had been destroyed in a ï¬re, we would smile. We know this is not possible, even though there are many instances of representations of the Natural Numbers in perishable material media. Part of the reason for the more enduring nature of symbolic entities like the Natural Numbers is the very fact that they do not refer directly to speciï¬c objects in the physical or cultural world. That is, the representational and symbolic challenges with which we opened this discussion are also sources of mathematical power. To understand the nature and power of symbolic entities, we can look ï¬rst at how they emerged in human history and then at how they operate in modern discourse."
170,90,0.988,Impact of Information Society Research in the Global South,"2.2 ICT4D Model In spite of these agreements, our disparate backgrounds came into play and eventually led to contentious debates that, upon the prompting of Ballantyne, we conceded to be attendant to the process of convergence and synthesis. The opposing discussions, in fact, generated a model that many will consider coherent yet comprehensive, elegant yet parsimonious. Characteristic of the development discourse that we represented, there was an initial tendency to use the terms model, theory, framework, construct, and concept interchangeably. Indeed, the interfaces between some of these terms were quite significant. As we progressed, however, the differences became more distinct. We began to refer to a framework as a structured set of conceptual boxes wherein we can situate a narrative or approach the study of ICT4D. By theory, we meant an explanation of the causal relationships among the elements that make up the ICT4D phenomenon. By ICT4D model, we meant a visual representation of how elements making up the ICT4D phenomenon interacted. At The Hague meeting, the ICT4D model became the visual representation of the ICT4D framework. The elements are, in effect, ICT4D concepts, and a statement of relationship between two or more of these concepts was considered a construct. Constructs Theoretically, the model takes off from a generally acknowledged construct: Communication can effect developmental changes in societies. This proposition had been tried and tested since the 1950s through the 1970s in the works of Beal et al. (1957), Rogers (1962), and Quebral (1973). In fact, this idea is being actively revived in the current C4D (communication for development) initiative within UN agencies."
360,340,0.988,Compositionality and Concepts in Linguistics and Psychology,"Partial-split situations are quite unacceptable for both sentences (10a) and (10b). Thus, when Dan walks, George writes and Bill and John are doing neither activity, sentence (10a) is quite odd, and similarly for (10b). By contrast, in full-split situations, Poortmanâs experiments show a difference between Dutch sentences similar (10a) and (10b). While 81% of the participants accepted sentences like (10a) in a full-split situation, only 24% accepted (10b) (in Poortmanâs experiment the subjects were definite descriptions, e.g. the boys). Once more, when accounting for this effect we rely on critical typicality points. In (11) below we apply principle (7) to the case of the expression are walking and writing. In this example, we denote WALK& WRITE for the head concept HC, where DIST, the distributivity concept, is the gradable concept GC. (11) Let LM-WW be the set of local typicality maxima for the concept WALK& WRITE . In formula: LM-WW = arg maxx TYPWALK&WRITE (x). The CT point(s) for the complex concept DIST WALK& WRITE is defined by: arg maxxâLM-WW TYPDIST (x). In words: the CT points for the expression are walking and writing are the situations that attain maximal typicality for the gradable concept DIST among the situations that attain maximal typicality for WALK& WRITE. Among the three situations we consider, the full-split and partial-split situations are more typical for WALK& WRITE than the joint situation. Between these two situations, the full-split situation is substantially more typical for the concept DIST, as it contains more people who are engaged in the relevant activities. This means that for sentence (10a), full-split is the CT point among the three situations. By contrast, for sentence (10a), the three situations are equally typical for the head concept WALK& SING. This is because the concepts WALK and SING, unlike WALK and WRITE, are not in any conflict (this kind of contrast is shown in another experiment by Poortman). In this case, for sentence (10b), the critical typicality point is the âjoint situationâ, which is the most typical for the distributivity concept alone. This analysis is summarized in Fig. 6."
264,126,0.988,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"What Is Mathematics, Really? There are many, and many different, valid answers to the Courant-Robbins question âWhat is Mathematics?â A more philosophical one is given by Reuben Hershâs book âWhat is Mathematics, Really?â Hersh (1997), and there are more psychological ones, on the working level. Classics include Jacques Hadamardâs âEssay on the Psychology of Invention in the Mathematical Fieldâ and Henri PoincarÃ©âs essays on methodology; a more recent approach is Devlinâs âIntroduction to Mathematical Thinkingâ Devlin (2012), or Villaniâs book (2012). And there have been many attempts to describe mathematics in encyclopedic form over the last few centuries. Probably the most recent one is the gargantuan âPrinceton Companion to Mathematicsâ, edited by Gowers et al. (2008), which indeed is a âPrinceton Companion to Pure Mathematics.â However, at a time where ZBMath counts more than 100,000 papers and books per year, and 29,953 submissions to the math and math-ph sections of arXiv. org in 2016, it is hopeless to give a compact and simple description of what mathematics really is, even if we had only the âcurrent research disciplineâ in mind. The discussions about the classiï¬cation of mathematics show how difï¬cult it is to cut the science into slices, and it is even debatable whether there is any meaningful way to separate applied research from pure mathematics. Probably the most diplomatic way is to acknowledge that there are âmany mathematics.â Some years ago Tao (2007) gave an open list of mathematics that is/are good for different purposesâfrom âproblem-solving mathematicsâ and âuseful mathematicsâ to âdeï¬nitive mathematicsâ, and wrote: As the above list demonstrates, the concept of mathematical quality is a high-dimensional one, and lacks an obvious canonical total ordering. I believe this is because mathematics is itself complex and high-dimensional, and evolves in unexpected and adaptive ways; each of the above qualities represents a different way in which we as a community improve our understanding and usage of the subject."
385,398,0.988,Advanced R,"The simplest functional is lapply(), which you may already be familiar with. lapply() takes a function, applies it to each element in a list, and returns the results in the form of a list. lapply() is the building block for many other functionals, so itâs important to understand how it works. Hereâs a pictorial representation:"
297,1003,0.988,The R Book,"One of the commonest reasons for a lack of ï¬t is through the existence of outliers in the data. It is important to understand, however, that a point may appear to be an outlier because of misspeciï¬cation of the model, and not because there is anything wrong with the data. It is important to understand that analysis of residuals is a very poor way of looking for inï¬uence. Precisely because a point is highly inï¬uential, it forces the regression line close to it, and hence the inï¬uential point may have a very small residual. Take this circle of data that shows absolutely no relationship between y and x: x <- c(2,3,3,3,4) y <- c(2,3,2,1,2) We want to draw two graphs side by side, and we want them to have the same axis scales: windows(7,4) par(mfrow=c(1,2)) plot(x,y,xlim=c(0,8),ylim=c(0,8)) Obviously, there is no relationship between y and x in the original data. But let us add an outlier at the point (7, 6) using concatenation c and see what happens: x1 <- c(x,7) y1 <- c(y,6) plot(x1,y1,xlim=c(0,8),ylim=c(0,8)) abline(lm(y1~x1),col=""blue"")"
232,263,0.988,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"For the researcher, the value of the cultural object lies in its symbolic speciï¬city. By highlighting what is âbuiltâ, the object creates a synergy between cultural references. Its analysis makes it possible to access social representations in ways that do not involve interviews or surveys. The object has a dual role, notably in terms of structuring day-to-day conversationâobjects feed discursive dynamics and stimulate the production of new representations. Regarding the nuclear industry, the production and circulation of content is a battleground that the groups involved in the controversy must conquer. While the use of military terminology may seem excessive, it is not possible to orient decisions and actions without ï¬rst changing the representations that are associated with them. Therefore, how the sector is represented can, depending on the societal context, become a fundamental issue in drawing up energy policy. Content analysis, however, should not be at the expense of a more aesthetic approach to representations. The iconic and narrative dimension of the object (through the formalization of content), is a major contributor to the constitution of knowledge and understanding the world. Furthermore, the study of forms (in the broadest sense), is useful in identifying which of the elements created by an event lead to rupture or continuity in the representation of reality. In this respect, French documentaries about Fukushima constitute a coherent corpus that can be used to analyze the impact of the disaster on representations."
8,979,0.988,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Strange Antibaryons In regard to strange antibaryon signature: in the 1982 discussion of the possible and forthcoming CERN SPS experiments I said [18], see Sect. 31.4: â. . . we should search for the rise of the abundance of particles like Î, Î, â¦, â¦, and . . . such experiments would uniquely determine the existence of the phase transition to the quark-gluon plasma.. . . Strangeness-based measurements have the advantage that they are based on the observation of a strongly interacting particle (s; sN quark) originating from the hot plasma phase; these are much more abundant than the electromagnetic particles (dileptons or direct photons).â"
322,155,0.988,European Citizenship After Brexit : Freedom of Movement and Rights of Residence,"citizens enjoy their âfundamental statusâ and âcitizenship rightsâ only Solange (so long) as the member state of which they happen to have the nationality does not invoke Article 50. Has Brexit shown this to be the case? Is Union citizenship, after all, nothing but a concession made by the Masters of the Treatises that can be called back at political convenience? Is EU citizenship somehow âmeaninglessâ? A positive implication of adopting the functionalist theory is that answers to these questions are of no consequence for the analysis conducted. It does not matter for the functionalist theory whether the citizenship is ârealâ or a case of rhetoric. The theory is only committed to the fact that citizenship is a status, putative or not. The theory is agnostic towards âthickâ concepts associated with citizenship such as identity and recognition. This is due to the fact that it is not a theory suggesting a conception of citizenship. There are arguably many such conceptions, the pros and cons of which have been debated in great detail over the last decades by citizenship scholars (Mindus 2014). The functionalist theory offers a concept of citizenship, not a conception. The concept of citizenship that the theory develops only requires a very minimal ontological commitment: That status civitatis is, conceptually speaking, a middle-term. As such, it is not true, nor false. As such, it does not âcorrespondâ to any empirical fact. It can, nonetheless, be investigated from an empirical perspective. There does not need to be anything âout thereâ to which the concept corresponds for it to work. What is required of status civitatis is to connect grounds for acquisition and loss with legal positions. This characterisation does not only ï¬t the legal facts. It is fully able to explain the constitutional dimension of citizenship policy. No ontological commitments to peopleâs identity or ability to identify with others are needed for this purpose. The theory is in line with the tradition of Scandinavian legal realism, a corpus of philosophical literature concerning this very point that has too often been neglected by legal scholars to their detriment.1 In keeping with this tradition, there is no need to believe that the legal concepts used somehow exist to provide insightful analysis into how they work. As Alf Ross famously argued in relation to the concept of ownership, it might be the case that it is in reality a meaningless word, a form of rhetoric, without changing the fact that, by looking at how it connects the conditioning facts and the conditional consequences in the law, we can understand perfectly well how the concept works â et pour cause, scrutinise it critically. I submit that the same is true"
360,119,0.988,Compositionality and Concepts in Linguistics and Psychology,"This sort of theory of the meaning of word-concepts is in the âmiddleâ of my dimension, and despite that, I would wish to view it as being Atomistic even though it allows for meanings to form âcircularâ definitional chains due to the large amount of material allowed in the meanings of a word. At the far end of the dimension, maximally different from the Shank or Wierzbicka/Goddard theories, and forming the Wholistic version of these types of theories, are so-called conceptual role theories. Conceptual Role theories can be found in very many versions of Cognitive Linguistics. Like the Langacker version I just outlined, these theories also do not have a small number of primitive concepts, but rather all word-concepts are allowed to appear in the definition or explanation of any other concept. What sets conceptual role theories apart from the Atomistic Langackerian theories, and makes them be Wholistic, is their employment of semantic liaisons. A semantic liaison is an implicative connection that one concept might have to another conceptâtypical liaisons are ones of semantic inclusion (e.g., that being a tiger implies being a feline), contrary semantic entailment (e.g., that being a mammal is incompatible with being cold-blooded), compatibility relations (e.g., that one object can be both a brother to someone and a cousin of someone). More generally, in conceptual role theories all such implicative connections that a concept A has are inherited by any concept that includes concept A as part of its definition. Thus, if concept A has concept B as a part of its definition, then concept A also has all the implicative relations that B embodies. It is usually thought that this means that every concept has all other concepts and their implicative relations as part of its meaning. So, the meaning of a lexical item will be the concept it occasions plus all the âliaisonsâ it has with other concepts. Here a liaison is seen as either some meaning-entailment or (less strictly) some other association that the concept has with other concepts (that is, all the encyclopedic associations that a person might have with respect to the concept plus their liaisons). The meaning then is seen as the collection of all these liaisons. And of course, that is true for all concepts, including for the ones that form the liaisons with the first concept. In other words, meaning now becomes a feature of the entire set or body of concepts, and insofar as one can attribute meaning to any one concept, it is"
49,380,0.988,Artificial Intelligence and Cognitive Science IV,"A simple movement, which is thinking, always creating reflection of being and by this way thinking constitutes things and the world. Constitutional activity of thinking is usually linked to speech, to âlogosâ, but the thinking is movement, not language itself. Thinking is something what enables speech. However, it may break away from the speech and perform reflection being that exists at the level of perception, or on the other side at the level of certainty of the truth about essences that speech is not able to capture. If we want to differ essences and their processes of constitution as follows movement of thought, it is necessary to distinguish a layer of thinking from the layer of beings. If we did not do so, it would be ontological conception in contradiction. That's because it does not allow it to accommodate the multiplicity as a totality and a differentiation of the thinking and what there is another form of existence and how it is possible. It should be borne in mind that even the initial exclusion of thinking can do nothing else but thinking itself. Thinking is not deducible from being, because it would first have to devote itself. Much easier is to accept of independence of thought, which is in addition to being and which carried out the difference in essences. How can we explain in detail the processes of differentiation of entities which is carried out by thinking. Thinking in discourse performs comparisons between current percepts and endocepts, ie, thinking is saying ""that's not it"" or ""that's it."" Basically, these processes are therefore negation and identity. For example, when we want to characterize what is white, then our thinking is saying white is what is not red, what is not blue, what is not black, what is not yellow and so on. And when we are looking on something white then our thinking is saying it is like white. Percepts are constitutes by this way. However, if we do not built own mental picture of the world's, it means endocepts that precedes perception (it is probably before puberty), or if it is a new sensation, the mind does not have its own basis for comparison, we create then the things on comparisons with other sensations. This process is based on negation, because we cannot determine the percept by otherwise. Let's say that in the prepubescent stage of ontogenetic development we determine all things on the level of sense. For example ""salinity"" as something what is ""not bitter"", ""non-acidic,"" ""not sweet""."
78,388,0.988,The Onlife Manifesto : Being Human in a Hyperconnected Era,"This critical accompaniment of ICTs can only take shape in concrete practices of design, use, and implementation, in which human beings can get critically involved in how technologies mediate their existence. A critical use of information technology then becomes an âascetic practiceâ, in which human beings explicitly anticipate technological mediations, and develop creative appropriations of technologies in order to give a desirable shape to these mediations. At the same time, the design of information technology becomes an inherently moral activity, in which designers do not only develop technological artifacts, but also the social impacts that come with it. And policy-making activities regarding the implementation of new technologies then become ways of governing our technologically mediated world. Let me return to one of the examples I gave at the beginning of this contribution in order to elaborate how this critical accompaniment of technologies could be a fruitful form of ethical and political reflection on technology. As indicated above, one of the most salient aspects of Google Glass is its impact on interpersonal relations. The âdoublingâ of the relations between humans and world that it brings about adds a second layer to the communication between people, which remains invisible to the other person. When two people meet, they cannot see which information the other has available about them. Googleâs search engine might reveal private information on the basis of face recognition software, or it might confuse the person with somebody else. Because this parallel information is only available for the person wearing the device, an asymmetry comes about that makes open communication impossible and that radically transforms the character of public space and public life."
264,579,0.988,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"Functions and the Number Line The second presentation, by Hyman Bass (Michigan, United States), considered the problem of conceptualising the domain of a function through an examination of the number line. A robust understanding of the continuous real number line is a central goal of K-12 education, but the extent to which this is achieved is questionable. Bass argued that the roots of this problem rest to a large extent in the early introduction of numbers in grade 1. He discussed a promising way of approaching this due to a theory by Davydov. Bass considered the question of a student who, at the end of high school, can meaningfully hear, âLet f(x) be a function of a real variable x.â The home of this x is the real number continuum R. How did this R, with its rich algebraic and geometric structure, make itself progressively known to ï¬rst-grade student Anne, who can be presumed to know little more than simple cardinal counting? There are two possible narratives that can explain this accomplishment. The construction narrative starts with counting numbers, gradually builds R by accretion of new numbers (negative integers and fractions), and eventually a âhole"
208,126,0.988,Actors and the Art of Performance,"possible futures has always already been violated and at the same time overtaken. This is the museâs view, the turn, the breathturn and the beauty of performative art. It [beauty] steps forward namelessly as a secret: Its mysteries outline the âbareness of form.â [ ... ] It is part of, participates in, the uniqueness of the moment. For this reason it allows, beyond language, solely an imperative of showing: âlook!â or âhear!â28 In light of the aesthetics of contemporary theater, again almost dogmatic in a perverse reversal, we can translate Lucilleâs cry âLong live the King!â as âLong live beauty!â This is not meant to conjure up some preserved yesteryear, to continue along Celanâs lines. We are not paying homage to some ancien rÃ©gime, but rather to a yet-to-come rÃ©gime de lâavenir. âLong live beautyâ is a call to beauty that appears suddenly, a moment of extreme vulnerability and porosity. Beauty as a breathturn, attentive to the big affirmation. Why do you want to be an actor? Perhaps that is why."
249,262,0.988,Advances in Proof-Theoretic Semantics (Volume 43.0),"2.4 Unknown Statements I have said that the assertibility of Â¬(Â¬ K Î± â§Â¬ K Â¬Î±) does not exclude the existence of the fact that neither Î± nor Â¬Î± are known. Can the intuitionist assert the existence of such a fact? I think not, and in this section I shall try to motivate this opinion. Let me observe first that âK Î±â, in all its possible readings, clearly is an empirical statement, not a mathematical one. I have argued elsewhere that the negation of many empirical statements, and in particular of K-statements, cannot be plausibly equated to intuitionistic negation Â¬, and I have proposed that it be equated to Nelsonâs strong negation â¼.26 So, if we add â¼ to the language LIPLK of Intuitionistic Propositional Logic plus the operator K, and we assume for simplicity that all the empirical sentences of LIPLâ¼,K have proofs,27 we must add, to the Definition 2 of the notion of proof of K Î±, a definition of the notion of proof of â¼ K Î±. Here is my proposal: Definition 5 Whenever one is presented with something that is not a proof of Î±, a proof of â¼ K Î± is the observation that what one is presented with is not a proof of Î±. It should be noticed that Dummettâs remarkâthat intuitionistic truth, when it is equated with the actual possession of a proof, does not commute with negationâis certainly correct when it is understood as referring to strong negation. For example, the observation that what one is presented with is not a proof that it is raining is not the same thing as the observation that what one is presented with is a proof that it is not raining. As a consequence, Dummettâs objection to the validity of the (T) schema as a criterion for being a truth operator seems to cause trouble in this case. However, in this case the argument (9) is no longer valid: the second step is an application of contraposition, but contraposition is not valid for strong negation. As a consequence, the fact that strong negation does not commute with truth does not entail the invalidity of the (T) schema. We can therefore conclude that, even when we add to intuitionism strong negation, the knowledge operator K is a truth operator. 25 A discussion of this assumption is beyond the limits of this paper. 26 Reference [24]. 27 In general empirical sentences have (non-conclusive) justifications. A definition of the notion of justification for the sentences of LIPLâ¼,K presupposes a solution of Gettier problems. I have suggested such a definition in [23]."
307,460,0.988,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"In this final chapter we will use the theoretical drugs developed in various chapters above for whole cell simulations. So far we have studied very small parts of a cell. We started by studying the dynamics going on in a single dyad; see Fig. 2.1. The size of one dyad is less than 1/1,000 m3 [3] and we have been concerned with the concentration of calcium ions in this small volume. We have also studied the voltage dynamics in the vicinity of a single ion channel. The size of a single channel is about 1 nm. Now we address what is going on in a whole cell and it is important to realize that, compared to the single dyad and the single ion channel, the whole cell is huge; a normal ventricular cell is about 30,000 m3 [3], or on the order of 30 million times larger than the single dyad. In the analysis of single channels, we have regarded the state of a channel as a stochastic variable. In the whole cell, however, the effect of a huge number of channels is added and the sum can be modeled using deterministic equations. We will still use the same Markov model formalism in terms of reaction schemes to formulate the models, but now we will use the associated master equations (see page 5) to define the open probability of the channel. Thus we need to solve deterministic systems of ordinary differential equations to find the open probability as a function of time. Since the state of the channels will be represented using Markov model reaction schemes, we can study mutations in the same manner as we did for the single channel case. Therefore, we can use the results we derived above regarding optimal theoretical drugs for the single channel case for the whole cell case as well. The reasoning behind this was indicated earlier: If a mathematical model of a cell is constructed by using models of a huge number of single channels and we can repair the function of each single channel, the whole cell will be repaired. In this chapter we will start by introducing a model of the action potential of the whole cell. We will focus on a simplified model that will merely represent the action potential in a qualitatively relevant manner; it will not represent any particular Â© The Author(s) 2016 A. Tveito, G.T. Lines, Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models, Lecture Notes in Computational Science and Engineering 111, DOI 10.1007/978-3-319-30030-6_15"
124,504,0.988,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"First, the trivial example. The basic theory is that special case where for Î² and Ï we insert some tautologies into the general form, i.e., where further constraints are put neither on the structure of possibilia nor on the admissible sets. Next, the example of a direct counterpart of the theory TPAknit , called simply âTPAâ in Sect. 3.2 of âIn Retrospectâ. Its axiom TPA 1 corresponds to the framework delivered already by our basic theory, but it does its work on a more specific structure of possibilia, which can be obtained by putting the conjunction of all the BTA postulates from Sect. 2.2 of âIn Retrospectâ in the place of Î². The possible strengthened theories discussed by Strobach in Sect. 3.2 can be obtained by adding U as a further conjunct of Î² and adding TPA 2 or TPA 2* as a further conjunct of Ï. Thus we have found a natural one-to-one correspondence between the possibilities of TPAcarve and the possibilities of TPAknit (which our basic theory could not yet deliver): Each one of the pairs âDP , <P â§ of TPAknit corresponds to an admissible set of the present specific variant of TPAcarve , because, when the domain of TPAcarve is taken to be a superset of the union of all the DP , each relation <P need only be taken as the restriction of the relation âº of TPAcarve to the admissible set corresponding to DP , and everything will fall into place nicely. As our next family of examples, we have some specific theories that share the following characteristic with the above direct counterpart of the theory TPAknit : Each one of the possibilities they deliver satisfies all the postulates of the non-modal basic theory of ancestry (BTA) of Sect. 2.2 of âIn Retrospectâ. We have constructed the direct counterpart by putting all the BTA postulates into the slot held open by âÎ²â in the general form of a theory of possible ancestry. It is interesting to see what happens when we move some of them over to the slot âÏâ, that is, when we understand them not as global but as local requirements. The results are impressive in the case of constraints of cardinality, where it arguably makes much biological sense, too. Using the cardinality constraints11 as conjuncts not of Î² but of Ï will allow the structure of possibilia to have an infinite domain because it moves the requirements of finiteness into the admissible sets. For example, any possible being in each possibility has only finitely many direct descendants, but it may nevertheless stand in the relation of direct possible ancestry to infinitely many possible beings. Here is a reason why we should not demand that some ancestor has only finitely many possible descendants. While a living being cannot actually reproduce infinitely often within a given amount of time and cannot actually leave infinitely many direct descendants, it may well do so possibly in the following sense: While no infinite branching within the same alternative is possible, the same parents may have an infinity of different possible children. (The more strongly we understand the metaphysical principle of the necessity of origin, the more plausible this gets. For according to a very strict reading of that principle, even offspring from the same sperm and egg would be a different individual if both had met a second earlier, or half a second, or a quarter of a second etc. Maybe this reading of the principle is too strict to be credible.12 Still, 11 BTA postulates C2, C3, C6, and C7. 12 So thinks Pleitz; Strobach likes the strict reading. So here the two authors disagree."
271,387,0.988,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"concerning religious organizations can be better understood by the different foci. Even ifâand the examples at the beginning show this quite wellâsocial media are an integral part of the organizationâs media ensemble within a translocal, globalized scope (a very âdeepâ degree of mediatization), within the much more local scope of the parishes, the organizations can show resistance to media change at the same time (a much âless deepâ degree of mediatization). And furthermore, it seems that for both scopes there are different requirements for integrating digital media on the part of the religious authorities. New possibilities for media use are not always chosen. Therefore, while the term mediatization grasps manifold interrelations between the change of media and communication on the one hand, and the changes in culture and society on the other, we also should take contrary movements into consideration when analyzing this process (cf. Hepp and RÃ¶ser 2014: 165). By distinguishing a translocal and local scope, on the one hand and different forms of authority constructions on the other, we are able to critically analyze that the organization of the Catholic Church transforms in a more differentiated way when it comes to media: as other research shows, there are, for example, tendencies of âbrandingâ and âprofessionalizationâ on a translocal level. Locally, however, we can observe quite different processes: a reluctant appropriation of the organizationâs media ensemble by the local authorities even though mediarelated communication is considered as necessary for translocal authority figures such as the pope or bishops. Regarding the religious organization, we can observe a tension between a âdeepâ degree of mediatization in regard to a translocal scope in contrast to a âless deepâ degree of mediatization within a local scope. An interesting follow-up question and part of our own future research is how such different degrees of mediatization produce different tempi of transformations: a slower transformation process on a local level in contrast to a faster transformation process on a translocal level, the tensions that emerge and the negotiations that have to be madeânot only between translocal and local, but between different localities worldwide."
363,43,0.988,History and Cultural Memory in Neo-Victorian Fiction,"the apparition itself as reapparition of the departedâ (Derrida, 1994: 6). These texts exploit the ghostliness of textuality to foreground the non-presence of history, its disappearance, and to suggest that its meaning is fleeting, or flickering; in fact, more than this, its meaning only exists as it is created, and recreated afresh. In this sense it is like the text itself, the meaning of which is configured and reconfigured with each reading, and by each reader. Similarly, the past is configured and reconfigured, and attributed different and multifarious meanings in each act of historical recall. Julian Wolfreys writes: ârecognizing the signs of haunting it must be concluded that whether one speaks of the experience of reading or the experience of the materiality of history, one witnesses and responds to ghostsâ (Wolfreys, 2002: 11). Indeed, he explores the ways in which texts, because of our tendency to anthropomorphise them, can themselves be considered ghostly, âare neither dead nor alive, yet they hover at the very limits between living and dyingâ (ibid.: xxii). This ghostliness is part of what separates the historical novel from the objectives, and assumed objectivity of history, and aligns it with the functions of memory: âTo the extent that memory âreincarnatesâ, âresurrectsâ, âre-cyclesâ, and makes the past âreappearâ and live again in the present, it cannot perform historically since it refuses to keep the past in the past, to draw the line, as it were, that is constitutive of the modern enterprise of historiographyâ (Spiegal, 162). The contemporary proliferation of historical fictions, and their commercial success, registers a persistent desire for cultural memory. Stemming from this continuing desire for stories about the past, historical fiction might extend and elaborate our versions of the past, offering different ways of seeing it, without asserting finality or Truth. In the earliest identification of the neo-Victorian subgenre, Dana Shiller observes, the âneo-Victorian novel â¦ attest[s] to the unflagging desire for knowledge of the past, a desire not extinguished by doubts as to how accessible it really isâ (Shiller, 1997: 557). This could be reformulated as an unflagging desire for historical recollection, the act of remembrance, which is privileged over historical knowledge itself. In her discussion of collecting in Susan Sontagâs historical novel The Volcano Lover, Julie C. Hayes suggests that it is desire that ensures the past will continue to be interpreted, that its stories will continue to be told. Indeed, far from erasing historical difference and distance, she argues that the collectorâs desire for the object, his or her passion and the resulting fear of its loss, âassures [the objectâs] status as unique, as having belonged to a specific, punctual place and time. The pastiche-collection is thus not so much critical or ironic, as paradoxical and complex, less bent on unmasking"
118,343,0.988,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"geographically global (a malicious human act), imperceptible in time either manifesting very quickly (on the Internet) or very slowly (high level radioactive waste disposal), or irreversible (release of radioactivity due to a core-melt accident). We are like the driver of a modern automobile, cruising along on the Interstate (in a linear world), and now suddenly, we are faced with âblack iceâ! The impacts we have described above lead to unprecedented ethical issues as reflected in the three questions above. Moreover, questions such as: âWhat constitutes an acceptable risk and why?â take on new meaning in the face of challenges to the ecology of life. There is a growing belief, as noted by Donald Rumsfeldâs quote above, that not only is the future unknown, it is unknowable. Moreover, because these complex ethical issues are arising so much faster than ever before, and because there has been little time to develop normative processes for decisionmaking, there is even greater ambiguity. The unknown-unknown looms large in the domain of Risk as feelings. What we are pointing to, for lack of a better description, is a Cultural Risk Analysis. This would entail making explicit the implicit cultural conditioning of individuals, and organizations/institutions, and their relationship to the society in which they abide. Such a Cultural Risk Analysis would illuminate cases where the underlying societal culture runs counter to the demands of safety culture, such as for nuclear power. If aspects of the societal culture are left implicit, they just donât underlie the safety culture, they will undermine it. If made explicit, it becomes possible for the safety culture to be designed and constructed in a way that accounts for, accommodates or even overcomes the conflicts between the two cultures. Such a Cultural Risk Analysis would then require an analysis of cultural conditioning, much the same way we analyze the machine. This would mean understanding how underlying assumptions, values, and beliefs come from culturally defined sources and not âobjective factsâ.20 However, there is one major difference; people are âcomplexâ emotional, mental, physical and spiritual human beings. Humans are not âcomplicatedâ machines and so are not amenable to a linear reductionist approach. Human beings have emergent properties, namely feelings and thoughts that do not reside in any one part of the body. Humans may respond differently to the same stimulus on any given day. And there are no âclosed formâ analytical solutions to describe human behavior; it is, for the most part subjective. Coincidentally with the development of these new complex technologies, there has been growing empirical evidence that in the realm of human decision-making, the emotional precedes the cognitive [29], and that motivation and intention derive from the unconsciousâ emotive and subconscious-mental [30]. These findings have found their way into such fields as Behavioral Economics [31] and Risk Perception [32], among others (An extensive literature review can be found in [33]). And a number of consulting companies have developed analytical methods in an attempt to quantify the âRisk Cultureâ of Business Organizations. In this case, the focus is on comparing the âselfinterestâ of the individual employees versus the corporate interest. 20 By âobjective factsâ, I mean empirical observation and data. Evolution and Global warming"
124,545,0.988,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"Now this is not just a technical move: âprofile gapsâ encode something interesting, namely correlations between behavior of agents. In a general game model, if player i changes her move, then the only available profiles for this may now be ones where some other player j has changed his move as well. Game theorists have studied correlations extensively: cf. (Aumann 1987; Brandenburger and Friedenberg 2008). But the same notion has come up in logic, since correlations provide âinformation channelsâ where the behavior of one agent can carry information about that of another (Barwise and Seligman 1997). And more recently, generalized forms of such dependencies have become the focus of attention in âdependence logicsâ (VÃ¤Ã¤nÃ¤nen 2007). In other words, independence may be costly, and the Product Axiom that seemed the pride of STIT may eventually stand in the way, being just an extreme case of a more sophisticated theory of agent behavior.19 In the rest of this chapter, we look at extensions of the current framework with features that seem essential to rational agency, and that have been the subject of study in dynamic logics."
275,51,0.988,Foundations of Trusted Autonomy,"The difference to AIÎ¼ defined in (2.2) is that the true environment Î¼ has been replaced with the universal distribution M in (2.3). A full expansion can be found in [28] (p. 143). While AIÎ¼ is optimal when knowing the true environment Î¼, AIXI is able to learn essentially any environment through interaction. Due to Solomonoffâs result (Theorem 1) the distribution M will converge to the true environment Î¼ almost regardless of what the true environment Î¼ is. And once M has converged to Î¼, the behaviour of AIXI will converge to the behaviour of the optimal agent AIÎ¼ which perfectly knows the environment. Formal results on AIXIâs performance can be found in [28, 38, 46]. Put a different way, AIXI arrives to the world with essentially no knowledge or preconception of what it is going to encounter. However, AIXI quickly makes up for its lack of knowledge with a powerful learning ability, which means that it will soon figure out how the environment works. From the beginning and throughout its âlifeâ, AIXI acts optimally according to its growing knowledge, and as soon as this knowledge state is sufficiently complete, AIXI acts as well as any agent that knew everything about the environment from the start. Based on these observations (described in much greater technical detail by [28]), we would like to make the claim that AIXI defines the optimal behaviour in any computable, unknown environment. Trusting AIXI. The AIXI formula is a precise description of the optimal behaviour in an unknown world. It thus offers designers of practical agents a target to aim for (Sect. 2.4). Meanwhile, it also enables safety researchers to engage in formal investigations of the consequences of this behaviour (Sects. 2.5 and 2.6). Having a good understanding of the behaviour and consequences an autonomous system strives towards, is essential for us being able to trust the system."
192,234,0.988,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"Arrowsmith addresses a basic divide running through biomedicine, a clash between two (incommensurable) deontologies, two integrity regimes as it were: the principles of medical ethics on the one hand and the demands of experimental research on the other. Although biomedicine is allegedly motivated by the objective to promote well-being (enhancing the effectiveness of clinical practice), Arrowsmith emphasises that there is another, rather disruptive impulse at work as well: a violent will to control life, endangering rather than protecting the well-being, not only of research animals, but also of patients and, eventually, of biomedical researchers (such as Martin Arrowsmith) themselves. They must choose between two options, both of which are presented as morally unsatisfactory: on the one hand medical practice, portrayed as fundamentally insincere (permeated by mauvaise foi, to use the Sartrean term), on the other hand the methodology of randomised trials, depicted as inconsiderate and ruthless. After a series of fiascos, Martinâs âsolutionâ is simplification and escape (flight instead of fight). This raises the question whether this nihilistic portrayal of the moral dichotomy is inevitable. Dialectically, the relationship between medical practice and basic research may perhaps be seen in a different light. In Arrowsmith, the claim is made that, as soon as the principles of biomedical science (M1) are applied to practical situations, multiple conflicts and contradictions emerge (M2), but a more sustainable and satisfactory outcome would be the awareness that, eventually, the one cannot really function without the other (âM3). In splendid isolation, pure science becomes thin and empty, so that the plea for âpureâ research may actually be an immunisation strategy, a mechanism of defence (the beautiful soul position). Moreover, it is precisely in the confrontation with real-life situations that the relentless drive towards control, fuelling the quest for knowledge, is brought to the fore. In other words, application and extrapolation are necessary experiences to discover what science really is about. The experience of working through the conflict is then seen as a precondition for self-understanding. But this outcome is not easily achieved. The chronic tension is there for real, as indicated when we read the novel in terms of the discourse of the analyst:"
8,782,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","The theoretical techniques required for the description of the two phases are quite different: in the case of hadronic gas, a strongly attractive interaction has to be accounted for, which leads to the formation of the numerous hadronic resonancesâ which are in fact bound states of several (anti) quarks. if this is really the case, then our intuition demands that at sufficiently high particle (baryon) density the individuality of such a bound state will be lost. In relativistic physics in particular, meson production at high temperatures might already lead to such a transition at moderate baryon density. As is currently believed, the quarkâquark interaction is of moderate strength, allowing a perturbative treatment of the quark-gluon plasma as relativistic Fermi and Bose gases. As this is a very well studied technique to be found in several reviews [2], we shall present the relevant results for the relativistic Fermi gas and restrict the discussion to the interesting phenomenological consequences. Thus the theoretical part of this report will be devoted mainly to the strongly interacting phase of hadronic gas. We will also describe some experimental consequences for relativistic nuclear collisions such as particle temperatures, i.e., mean transverse momenta and entropy. As we will deal with relativistic particles throughout this work, a suitable generalization of standard thermodynamics is necessary, and we follow the way described by Touschek [3]. Not only is it the most elegant, but it is also by simple physical arguments the only physical generalization of the concepts of thermodynamics to relativistic particle kinematics. Our notation is such that â D c D k D 1. The inverse temperature Ë and volume V are generalized to become four-vectors: E ! p D .p0 ; p/ D mu ;"
360,472,0.987,Compositionality and Concepts in Linguistics and Psychology,"3 A Dual System for Semantic Composition 3.1 Conceptually Versus Referentially Afforded Composition We begin with a very programmatic proposal concerning two ways in which the construction of meaning can be mediated. Our proposal is based on the following assumption. Assumption The construction of meaning draws on connections we make between linguistic expressions and our conceptual structure, on the one hand, and the world, on the other. This assumption is of course familiar from traditional semiotic models and also resonates with the âdual contentâ model recently proposed in Del Pinal (2015), which provides (p. 44ff.) a useful overview of the different ways in which language, conceptual structure, and the world have been related to each other both in the philosophy of language and cognitive psychology literature. The assumption also underlies the classic Fregean model that distinguishes sense (Sinn), which Frege suggests forms part of the âcommon treasure of thoughts that [humanity] transmits from one generation to anotherâ, and reference (Bedeutung) (Frege 1892, p. 297 ). However, in modern formal semantics in the Montagovian tradition, despite its Fregean roots, conceptual structure has largely been set aside. In this latter tradition, Fregean sense has largely been substituted for the notion of intension, modeled non-psychologistically as, for example, a function from possible worlds to truth values. We recover the classically Fregean notion of sense as including conceptual-like information and propose that both conceptual and referential aspects of meaning play a role in composition. Specifically, we can think of them as affording concept combination in different ways. Our use of the term affordance is based on Chemeroâs (2003) development of the notion, originally due to Gibson (1979); it is also inspired in Rietveldâs (2008) extension of the notion to higher cognition. Chemero defines affordance as a relation between features of situations and abilities of organisms, and argues that to perceive an affordance is to recognize that the feature in question facilitates an action by the organism. The classic example is a mug with a handle: If a person who has never seen a mug gets to interact with it, it is very likely that she will grab it by the handle. The mug, by its shape, affords the grabbing-by-the-handle action on the part of the person. Our extension of this idea to the case of language is very simple. We take the connection to concepts, on the one hand, and to the world, on the other, to be distinct features of language, each of which facilitatesâthat is, affordsâa distinct composition process. If we posit that language users have access to both of these features and the corresponding processes that they facilitate, the tension we observed between default and highly context dependent interpretations in Sect. 2 disappears."
28,279,0.987,A History of Self-Harm in Britain,"To return to specifics, we see that self-cutting is both a residual and a newly emergent category. It is understood â gradually and unevenly â as a method of affective self-regulation rather than social communication. This opens the way to neurological explanations of the behaviour. This happens because neurological explanations focus upon the individualâs nervous system as a privileged site of understanding. The reason that it is only a small step from âindividual tensionâ to âneurochemistryâ is that both approaches, or concepts, take the individual at their starting points. A communicative attempt, in contrast, focuses upon a social situation in which various people are embedded. However, even this contrast has recently become unstable, as there is work that investigates the âneurology of social cognitionâ as well as sociological work on the discipline of neuroscience (upon which this book has drawn).11 However, the point stands that internal emotional turmoil maps much more easily onto neurological understandings than does psychosocial communication. Although the neurochemistry of complex behaviour is widely acknowledged to be in its infancy (a claim also made in the mid-1970s), there are a number of guiding principles that underwrite these perspectives. Regardless of the particular system or neurotransmitter that is implicated, these sorts of studies are all based around the assumption that neurochemistry is at the root of the behaviour, and operates prior to culture, and is indeed, outside culture. As Hilary and Stephen Rose argue with respect to molecular biology: âAgain and again the molecular biologists leading the sequencing [of] the human genome [between 1990 and 2013] claimed that the completed genome would constitute human identityâ. They add that âThe neurosciences have not been left behind; their claims to explain selfhood, love and consciousness as located in certain brain regions ... have been articulated in a string of popular booksâ.12 Given the audacity and ambition of these claims, it is unsurprising that neuroscience and neurobiology are increasingly utilised to investigate the (comparatively modestsounding) self-cutting-as-tension-release in order to reveal its neurological basis. Michael Simpson is among the first to speculate upon a biological basis for the behaviour of self-cutting in 1976, but he is notably cautious in ascribing the behaviour any secure biological basis.13 In 2001, Fiona Gardner (a psychoanalytically trained therapist) writes in a cautious and equivocal vein about âself-harmâ:"
278,1,0.987,Taking Stock of Industrial Ecology,"Industrial ecology has come of age: it has its own journal, its own society and, in this volume, a first full retrospective â âtaking stockâ of its first quarter of a century as a scientific field. It is a remarkable achievement. To speak of society as having an industrial ecology would barely have been understood, as little as three decades ago. Early in the 1990s, I submitted an article for a newspaper with the phrase industrial ecology in it, only to have the editor send it back to me, corrected to industrial âeconomyâ. At the time, we all knew exactly what the industrial economy was (or thought we did), but clearly industrial âecologyâ could only be a typo. When I explained that it was not, the editor deemed it best to remove the term, because âno one would understand itâ. Two decades later industrial ecology is a clearer concept than industrial economy. The former even has its own Wikipedia entry; the latter, strangely, does not. Language is a curious commodity. Its malleability appears sometimes to be almost infinite. Meanings change and mutate over time, as intellectual territory is created and destroyed. We can respond to this linguistic contortionism in several distinct ways: at least two of them are wrong. One of the wrong ways is to suppose that the meanings embedded in terms are not just fixed but rightly so. We can spend a lot of time and energy defending the territory that language creates: defile my meanings at your peril; they are part of my identity and protect my legitimacy in the world. This is a subtly disguised variant on G. E. Mooreâs (1903) naturalistic fallacy: what is, is what ought to be; and woe betide offenders. The best way to avoid such an error is not to be too attached to the precision of language. Alternatively we can celebrate the loss of meaning in late, postmodern, advanced consumer capitalism, where nothing is any longer sacred, and definition counts for naught. Accepting this fluidity of meaning, it is all too easy to allow ourselves to float above rigour and define away contestation. In fact, a cynical variation on this theme is to deliberately employ such tactics to create your own territory. Academics throughout the ages have fallen foul of this. Let us put two unfamiliar words together and build a career from it. Come on; it is easy: epigenetic precognition, categorical"
82,75,0.987,Fading Foundations : Probability and The Regress Problem,"justification relation primarily a logical relation, as is stated by Richard Feldman and Earl Conee?12 Or should we follow David Armstrong and Alvin Goldman and hold that it is ultimately causal?13 Which answer one gives to Q1 and Q2 depends largely on whether one takes an internalist or an externalistic view of epistemic justification. Our intuitive understanding of epistemic justification, which Kant would have called âconfused and undeterminedâ, revolves around two aspects that philosophers of all times have struggled to amalgamate.14 On the one hand, justification has to do with the way the world is: it would be inappropriate to call our beliefs justified without requiring that they represent, at least remotely, how things actually are. On the other hand, justification applies to the way the world appears to us: it would be awkward to call my beliefs unjustified if I have reasoned impeccably towards a conclusion which, through some freakish turn of fate, happens to be false. The fact that externalists tend to stress the former, world-centred aspect of justification, while internalists emphasize the latter, agent-centred aspect, is reflected in their answers to Q1 and Q2. What do internalists and externalists say about the ontological status of the relata in âA j justifies Ai â? Concerning Ai , the thing justified, there seems to be not much disagreement. In the case at hand, both factions assert that Ai is a proposition, or a belief in a proposition.15 But what about the ontological status of the justifier, A j ? Here the answer depends on which of the many different versions of internalism or externalism we are talking about. It also depends on whether A j is regarded as something that is itself inferred or as suffices to make a proposition a reason for anotherâ (Williams 2014, 237). As will become clear later in this chapter, we agree that a relation of entailment or of conditional probability is not sufficient for saying that one proposition is a reason for another. However, if Williams is implying that such a relation is not necessary either, then we part company. In general, Williamsâ approach to the epistemic regress problem is inspired by the later Wittgenstein and by ordinary language philosophers like Austin, and as such tends to eschew a more formal or theoretical approach, like the one that we pursue in this book. 12 Feldman and Conee 1985; Conee and Feldman 2004. 13 Armstrong 1973; Goldman 1967. 14 Verworren und unbestimmt â see Kantâs treatise âEnquiry concerning the clarity of the principles of natural theology and ethicsâ (Untersuchung uÌber die Deutlichkeit der GrundsaÌtze der natuÌrlichen Theologie und der Moral) of 1764. Cf. Vahid 2011. We recall agreeable conversations with Hans Mooij and Simone Mooij-Valk about translating Kant and â vis-aÌ-vis the motto of this book â Thomas Mann. 15 But only in the case at hand, for A can also be another cognitive state than a belief, or even a non-cognitive state."
32,212,0.987,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","Therefore the robustness of the entire system is a function of mE, not the bare E. And because NE D 1 means that the average number of extinctions balances with the number of inclusions in the long time average, that corresponds to the transition point of the growth behavior. In other words, the following self-consistent condition should be satisfied for the critical number of interactions per species: m E.m / D 1. Let us now focus on the relevant parameter in the argument p above, mE. We find that the decrease of E is slower than 1=m (roughly  1= m). Therefore mE is a sub-linearly increasing function of m, and it crosses the critical value 1 around m D 13. This means that the mean-filed treatment can explain the transition in the growth behavior of the system. In addition, this theory give us the simple understanding of the transition mechanism. It originates from the balance of the two effects: although having more interactions makes each species robust against the disturbances (addition and extinction of the species relating to that species), it also increases the impact of the loss of a species. In consistent with this success in explaining the transition by the mean-field analysis, we can find essentially same phase diagram in slightly modified models, such as the model with giving a randomly distributed degrees for the newly added species, the one with different distribution functions for the link weights, and so on [11]. In the classical diversity-stability relation based on the linear stability of dynamical systems, an intrinsic stability is assumed for each element to ensure the stability of each element when that has no interactions. For the system to remain stable, each element may have essentially only one interaction that is not weak comparing to the given intrinsic stability [2]. In the present mechanism, we do not assume any kind of intrinsic stability to the elements: an element with no interaction immediately goes extinct. Even so, the system with 10 or more interactions per element can grow. In this sense, the condition we have identified is very realistic. Indeed in the real systems, it is quite often to find moderately sparse networks: the average degree is in the order of 10, not order of 1, and that seems not dependent on the system size. This novel relation between the connection in the system and its robustness might be a origin of this."
8,163,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","7.7 Concluding Remarks Rolf Hagedornâs work, introducing concepts from statistical mechanics and from the mathematics of self-similarity into the analysis of high energy multiparticle production, started a new field of research, alive and active still today. On the theory side, the limiting temperature of hadronic matter and the behavior of the Hagedorn resonance gas approaching that limit were subsequently verified by first principle calculations in finite temperature QCD. On the experimental side, particle yields as well as, more recently, fluctuations of conserved quantities, were also found to follow the pattern predicted by the Hagedorn resonance gas. Rarely has an idea in physics risen from such humble and little appreciated beginnings to such a striking vindication. So perhaps it is appropriate to close with a poetic summary one of us (HS) formulated some 20 years ago for a Hagedorn-Fest, with a slight update. HOT HADRONIC MATTER (A Poetic Summary) In days of old a tale was told of hadrons ever fatter. Behold, my friends, said Hagedorn, the ultimate of matter. Then Muster Mark called in the quarks, to hadrons they were mated. Of colors three, and never free, all to confinement fated. But in dense matter, their bonds can shatter and they freely move around. Above TH , their colors shine as the QGP is found. Said Hagedorn, when quarks were born they had different advances. Today they form, as we can see, a gas of all their chances."
214,92,0.987,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"between independent variables (inputs) and dependent variables (outputs). For example, if you measure the speed of an object at different points in time, you can develop a relationship based on those observations. If you are dropping the object with no air resistance, so the acceleration is gravity, you can develop a relationship between the velocity and the time. For the surface of the earth, you would get (in metric units) v = v0 + 9.8 t, where t is measured in seconds, and v is in meters per second. This is an approximate form of the equation of motion, which might work very well for similar cases, but would not work for a different situation. Physical laws contain more information than statistical or empirical methods and, therefore, are more suitable for dynamical systems where the environment for statistically based parameters might be different. For example, the gravitational acceleration is dependent on the mass of the object that is doing the attraction and the distance from the center of that mass (Earth, in this case). So the dynamical system approach works on the moon: You can calculate different acceleration (a) based on the lunar mass. But the empirical result (using 9.8) would not work on the moon. The danger with statistical or empirical models is being âout of sampleâ: There is some condition where the model does not work. This may be obvious in our example, but it is not always obvious. So are dynamical models always better? Only when a good description of the system can be made. For many processes, we turn to empirical or statistical relationships. Even many fundamental properties of the world around us are made up of many different conditions at the molecular or atomic level, so we have to describe the process empirically. As an example, the chemical properties of a substance, like the freezing temperature and pressure of water, are related to small-scale motions of molecules (all governed by our velocity equation), but we cannot measure each molecule. So we measure the collected behavior of all the molecules in a sample and build an empirical model of the freezing point of water as a function of temperature and pressure. Thus climate models do contain empirical models of processes, coupled together in a dynamical system. They contain a representation of the freezing point of water, for example. These processes are tied together using physical laws, which help us to make sense of the interconnection between the processes. Some processes are simple or well described (like water freezing), and some are very complex. But these statistical models are sometimes necessary. Tying them together with physical laws (like conservation of energy and mass) is an important constraint on climate models. These conservation constraints help to reduce uncertainty."
307,21,0.987,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"under pressure from their employer, they would probably agree that the mechanism would have to be extremely simple. Fortunately for us all, the pump has already been developed by evolution, but it is very far from being simple; it is an extremely complex piece of machinery, so complex that how it works is still not completely understood. For an intriguing illustration of this, the reader is encouraged to consult the fascinating joint paper by Lakatta and DiFrancesco [47] in which they debate the following fundamental question: How is the heartbeat initiated? It is remarkable that such a basic question is still open. Two plausible and completely different mechanisms are discussed, with supporting experimental data and mathematical models for both. The interested reader can also consult Li et al. [50] for an introduction to this discussion. Even if the exact mechanism for initiating the heartbeat is still under debate, it is completely clear that every normal heartbeat is initiated in the sinoatrial node. From that node, an electrochemical wave spreads throughout the cardiac muscle. With every beat, billions of cardiac cells undergo an action potential that is a characteristic temporal change of the transmembrane potential of the cell V, defined by V D Vi"
108,145,0.987,Bordieuan Field Theory as an Instrument for Military Operational Analysis,"was conducted using field theory, with the theoretical terms sometimes clearly shown and sometimes indicated through the use of synonyms. Even though the actors did not always think in field theory terms, it is still possible to apply that perspective. To that should be added that much indicates that MiloÅ¡eviÄ and Tudjman, without any actual theoretical knowledge, could think and act in a manner reminiscent of the field theory approach. That is something you can and also probably actually do if you are an actor on the field. The opposite can be said about many of the other actors who really did not try to understand the social field and thus really did not take the duel-situation between them and their counterparts seriously. One can also see that it was possible to extrapolate the coming events by studying the actors and that their rationality also could be perceived. This is exactly the information one needs if one wants to affect actors by changing the social structure on a field. PSYOPS could be applied right through the whole event chain, not only during the phase when open war was played out. Even a bombing or an assassination of other actors than those you want to affect can be part of a PSYOPS operation. Exactly what could have been done is speculative and beyond the point of this text. The important thing is that it was possible to operate in this setting with a social field perspective as guidance. The aim has been to show that field theory can fulfil the criteria necessary for a theory to qualify as a valuable instrument for military planning. The discussion has demonstrated that social theory can be employed as an operational tool. The theory can serve to encourage reflection and therefore refine oneâs thoughts, provided that it is first understood and assimilated as a way of thinking. This provision was fulfilled when the theory was shown to be able to be linked, for example, to the questions and issues that a military intervention might have to face. The theory was also shown to be applicable to the empirical example given. One counter to this claim is of course that it is a theoretical examination of empirical data that has been conducted. The next step is to apply the theory in an actual empirical study. But for this to be possible the theory must first be further developed and then practical measures taken to enable theoretical ambitions to be realisedâin other words a task outside this study. One question that is inherent and needs to be answered following this type of study is: what resources are required to enable the creation of a field theory image of military area of operations?"
10,109,0.987,Governance For Drought Resilience : Land and Water Drought Management in Europe,"The structure of the Governance Assessment Tool and the guiding questions it poses in each cell, enables any individual stakeholder (e.g. a project leader, or policy advisor, or policy makers) who understands the dimensions and criteria to assess the governance context s/he is working in. All it requires is a few hours to assess the situation for each cell, on the basis of knowledge held by heart. Obviously such assessment is limited by the degree of correctness of such estimates. But that is no reason to be negative about it. Such an individual thought experiment at least turns implicit knowledge and perceptions into explicit ones that can later be shared with others in a systematic way. It also serves the purpose that the individual stakeholder becomes more aware of the issues on which there is uncertainty or even lack of knowledge. Lastly, assuming that the perceptions of such âinsiderâ make some sense indeed (as often will be the case), it provides an assessment of oneâs own working circumstances that can be practical in ï¬nding ways to improve them or otherwise deal with them. A next step in elaborated use of the tool is when a group of practitioners interactively uses it for a systematic brainstorm on the governance context of their common policy or project. This could take for instance the form of a half day workshop. Compared with the previous approach there are more people that can contribute knowledge and that can counter one-sided bias in perceptions, creating a degree of âinter-subjectivityâ. The joint effort is also an important aspect in itself, as it provides a basis for sharing information and sharing perceptions, that can later be of utmost value for productive collaboration (HarmoniCOP 2005). The session can be concluded by brainstorming on how to deal with the governance context about which by then a joint understanding has evolved. In as far as differences of opinion occur and persist, the session has probably pinpointed more precisely than before where the disagreement is all about. A variant of the above is the situation in which an experienced analyst, for instance a scientist that worked with the tool more often, leads the session, turning it into a guided workshop. An obvious advantage is that the governance expert has a good understanding of the precise meaning of the concepts and the reasons why they are included in a model explaining the degree to which the context is"
124,263,0.987,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"1 Branching Time and Ontic Frugality Our best current theory of the physical world implies that certain events occur in an irreducibly indeterministic way. For instance, if a radioactive atom decays, then its doing so is not the result of a prior sufficient physical condition. Instead, its decay is an irreducibly probabilistic process about which the most that can be said is that the atomâs decay was something very likely to occur within a certain interval of time. At no time, however, was its decay physically determined to occur. So too, on certain views about freedom of will, in some cases agents act or choose freely, and according I am grateful to participants at the, âWhat is Really Possible?â Workshop, University of Utrecht, June, 2012, for their comments on an earlier draft of this chapter. Research for this chapter was supported in part by Grant #0925975 from the National Science Foundation. Any opinions expressed herein are solely those of the author and do not necessarily reflect those of the National Science Foundation.. M. Green (B) Department of Philosophy, University of Connecticut, 101 Manchester Hall, 344 Mansfield Road, Storrs, CT 06269-1054, USA e-mail: mitchell.green@uconn.edu T. MÃ¼ller (ed.), Nuel Belnap on Indeterminism and Free Action, Outstanding Contributions to Logic 2, DOI: 10.1007/978-3-319-01754-9_7, Â© The Author(s) 2014"
249,497,0.987,Advances in Proof-Theoretic Semantics (Volume 43.0),"intuitive idea of this step is that we can introduce an assumption in the course of a derivation. If a proof of C from A is given, we can, by introducing the assumption Aâ§ B and discharging the given assumption A, pass over to C. That Aâ§ B occurs only as an assumption and cannot be a conclusion of any other rule, demonstrates that we have a different model of reasoning, in which assumptions are not just placeholders for other proofs, but stand for themselves. The fact that, given a proof A â§ B and a proof of the form"
235,285,0.987,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","22.5 Perception and Forward Tactics Toward Unknowns Whatever oneâs personal inclinations toward (in)determinism may be â one might characterize our situation either as an ocean of unknowns with a few islands of preliminary predictables; or, conversely, as a sea of determinism with the occasional islands or gaps of an otherwise lawful behaviour â every such inclination remains strictly means relative, metaphysical and subjective. Maybe such preferences says more about the person than the situation; because a personâs stance is often determined by the subconscious desires, hopes and fears driving that individual. Choose one, and choose wisely for your needs; or even better, âif you can possibly avoid it [211, p. 129],â choose none, and remain conscious about the impossibility to know. Let me finally quote the late Planck [410] concluding that [409, p. 539] (see also Earman [186, p. 1372]) â. . . the law of causality is neither right nor wrong, it can be neither generally proved nor generally disproved. It is rather a heuristic principle, a sign-post (and to my mind the most valuable sign-post we possess) to guide us in the motley confusion of events and to show us the direction in which scientific research must advance in order to attain fruitful results. As the law of causality immediately seizes the awakening soul of the child and causes him indefatigably to ask âWhy?â so it accompanies the investigator through his whole life and incessantly sets him"
78,340,0.987,The Onlife Manifesto : Being Human in a Hyperconnected Era,"In scientific terms, humans are treated as mere scientific objects, i.e., they are elucidated with a view to predict and/or to manipulate them7. As pointed out by Arendt, the scientific discourse is indexed on necessity: âwhat science and the quest of knowledge are after is irrefutable truth, that is, propositions human beings are not free to rejectâthey are compellingâ (Arendt 1978, p. 59). In scientific terms, contingency is just another name for âepistemic failureâ, a not-yet-known. By denoting contingency with the term uncertainty, i.e., as a negative, certainty is made the norm or the ideal. And scientific knowledge is paired with certainty of facts, even after several decades of quantum mechanics, which rather teaches us that uncertainty and indeterminacy are intrinsic to scientific knowledge as well. This scientific register positions humans as an object of enquiry, a âmaterialâ, inherently heteronomous i.e., as fully determined by external materials, forces and processes. When considered in ethical terms, as Arendt put it ironically, âattemps to define human nature almost invariably end with some construction of a deityâ¦â (Arendt 1959, p. 12). Furthermore, she reckons that freedom has wrongly been identified with sovereignty in political and philosophical thought: âIf it were true that sovereignty and freedom are the same, then indeed no man could be free, because sovereignty, the ideal of uncompromising self-sufficiency and mastership, is contradictory to the very condition of plurality. No man can be sovereign because not one man, but men, inhabit the earthâand not, as the tradition since Plato holds, because of manâs limited strength, which makes him depend upon the help of othersâ (Arendt 1959, p. 210). Understanding freedom as sovereignty has a huge price, the price of reality: âsovereignty is possible only in imagination, paid for by the price of realityâ (Arendt 1959, p. 211). Ethical/philosophical narratives of what it is to be human contend with the need to escape from, or at least to balance with,"
85,1,0.987,Bayesian Methods in the Search for MH370,"Uncertainty is all pervasiveâwhether it relates to everyday personal choices and actions, or as background to business and policy decisions, or economic and climate predictions. In recent times, few things have attracted as much attention as the uncertainty surrounding the ï¬nal whereabouts of MH370. How to deal scientiï¬cally with uncertainty? Put simply, on the one hand there are events or outcomes of interest that we donât know; on the other hand, pieces of information that we judge relevant in some sense that we do know. We need to assess what we believe about the unknowns, given the knowns. Formalising our measure of uncertainty in terms of probabilities, the scientiï¬c approach is encapsulated in the so-called Bayesian statistical paradigm, in which beliefs about the unknowns are quantiï¬ed by a probability measure conditional on what we know. But typically, our state of knowledge itself gets modiï¬ed over time and a method is therefore needed to reï¬ne and update beliefs as new information is acquired and assembled. The logical, mathematical rule for carrying out this updating is Bayes theorem, hence the term Bayesian Methods to describe the analytic and computational toolkit that has been developed for updating beliefs as evidence changes or is added to. It is this toolkit that has been employed in the search for MH370 and this fascinating book provides a blow-by-blow case account of how the various strands of evidence have been brought together to give an overall probabilistic assessment of the ï¬nal whereabouts of the plane. This has been an extremely complex task and the authors are to be congratulated on setting out systematically and coherently the science and mathematics driving the evidential equations. But, in addition to the complex modelling there remains the task of pulling out the Bayesian probability messages from the tangle of data that has been assembled. The computational methods for achieving this are of relatively recent origin and, on a personal note, I am delighted to have played a small part, with Dr. Neil Gordon, in the signal processing revolution that is now the particle ï¬lter method of analysis."
8,573,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","while the corresponding equation in terms of the average number of pions (contained in all these fireballs together) would look horribly complicated. This result (23.43), which in the framework of this model is exact, shows once more how simple things become once the interaction is hidden in the mass spectrum."
8,659,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Together with its low mass (empirical) part, this then constituted the so far unknown mass spectrum .m/. Inserting this into (25.10) shows that the integral does not converge if T > T0 : the partition function has a singularity at T0 whose nature depends on the value of a. Thus there were three predictions: â¢ for sufficiently high collision energy, the p? distribution should be (very approximately)  exp.p? =T/ with T . T0 [often called the exp.6p? / law], â¢ the mass spectrum should grow exponentially, â¢ T0 in the p? distribution and in the mass spectrum should be the same. I presented these predictions and the whole model in a theory seminar in the fall of 1964. The result was disastrous. Nobody would believe it and I was shouted at: âbut the mass spectrum does not grow exponentiallyâ."
8,620,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Ë.Ë; ; / has a singularity at 0 .Ë; /. Its nature (pole, branch point) depends on g.Ë; V; /. In principle, it is possible to continue the analytic function Ë.Ë; ; /, defined by the integral representation in Eq. (24.2) for Re  > 0 , into the whole complex plane beyond the convergence domain of the integral. Therefore it might well be possible that quantities derived from Ë.Ë; ; / have a physical meaning for  values where the integral representation of Ë.Ë; ; / does not exist. That this is indeed the case and that the singularity at 0 is absent in meaningful physical quantities will now be shown. It implies that the singularity at 0 has nothing to do with a phase transition, in contradistinction to singularities of Z.Ë; V; /. It is convenient to define a new function whose limit is Ë.Ë; ; /: ËW .Ë; ; / WD"
275,605,0.987,Foundations of Trusted Autonomy,"dilemma was recently resolved [54] using game theory to show that the course of action in the game depends on the observerâs beliefs - which are necessary to making a choice but not rationally justifiable - about the ability of the oracle machine to predict their actions, in much the same manner as the observation of quantum states depends on the observer. The connection from autonomy to economics rests on the proposition that an autonomous agent is exactly an economic agent, by another name. An autonomous agentâbe it living, a social construct, or a machineâis an entity that exercises choice, by making decisions to act one way or another. Yet any decision to act in a particular way is just an allocation of resources under that agentâs control to one of the course of action options as understood by the agent, and this matches the definition of an economic agent. To be slightly more formal: an economic agent is any actor (e.g. an individual, company, government body or other organisation) that makes decisions in aiming to solve some choice problem within some economic system, and an economic system is any system involving production, consumption and interchange of resources by such agents [11, 12, 21, 29, 42]. The difference between an autonomous agent and an economic one is a matter or emphasis that directs subsequent research problem choices. When we talk of artificial intelligence, it usually means we are mostly interested in developing algorithms and their technical implementations in machines; when we talk of economic agents, it means we are mainly concerned with individual and system-wide outcomes given different mixes of different kinds of interacting resource-allocating agents under various environmental conditions. In particular, I am concerned with autonomy as particularly featuring decisions about the allocation of self under uncertainty, and hypothesise that self-allocation corresponds with the same boundary that delineates a notion of uncertainty that has come to be known as ontological uncertainty [13, 25, 30, 44, 50]. Interacting selfallocating agents entails logical paradox in general, which underpins formal limits on knowledge and thus the intrinsic uncertainty of such systems. This uncertainty arises without any recourse to exogenic âshocksâ. Hence familiar notions of agent utilitymaximising rationality that operate within worlds of linear certainty and stochastic risk necessarily break down under conditions of fundamental uncertainty. This kind of effect has been studied extensively in the economic literature. For instance, suppose that agents have a high ability to predict a policy-makerâs actions, that at least one endogenous variable is completely controlled by the policy-maker, and that the reward to the policy-maker of their actions is dependent on whether the policy is anticipated. Under these simple conditions [16], there is no unique rational course of action for the policy-maker, in general. Rather, different incommensurate yet equally viable theories may indicate distinct optimal policies, and consequently the agents cannot form rational expectations because any rational expectation must contain a theory of the policy-makerâs behaviour. In a setting such as this, economically rational behaviour is basically incompatible with the formation of economically rational expectations. This is an instance of an incompleteness phenomenon occurring in a purely economic setting, and one that bears directly on autonomous machines."
58,122,0.987,Enabling Things to Talk,"What elements need to be protected depends on the considered scenario. However, the IoT ARM was derived from the synthesis of a wide range of use-case areas, and identifying elements to be protected becomes rapidly very broad and multi-faceted. Instead, we decided to focus on the least common denominator of all use-case scenarios on which the IoT ARM is built. In other words, this analysis only looks at general elements to be protected, and this study is thus a good but non-exhaustive starting point for the study of a particular scenario to which the IoT ARM is going to be applied. The scenarios encompassed by the IoT ARM include: â¢ Transportation and logistics; â¢ Smart home; â¢ Smart city;"
232,22,0.987,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"3 Entry into Resilience: A Way to Cope with the Extreme Situation The concept of resilience is only relevant following damage, loss, an accident, trauma, etc. Pre-event is the domain of prevention, prudence, or even precaution. To be able to discuss resilience, you ï¬rst have to survive. Even this is not enough, as, while it can be a sustainable situation, survival is fragile. The concept has positive connotations. Resilience is an asset, it represents progress; being or becoming resilient does not mean returning to the nominal, pre-shock state. This is anyway impossible because the system, whatever form it takes, remembers the event (albeit for a limited period of time). Although the concept is in fashion, there is no universal deï¬nition that can be applied to all domains. That said, the English term âresilienceâ, itself derived from the Latin verb resilire (to bounce), is made up of re (again) and salire (rise), which implies a retroactive effect [27]. While in the 1970s the term was associated with the ability to absorb and overcome the effects of signiï¬cant, unexpected and brutal disruption to ecological systems [28], hybrid deï¬nitions have since emerged in many disciplines including geography [29], psychology [25], sociology [30], organizational sciences [31], ergo-psychology [32], etc. Within this smorgasbord of deï¬nitions, two fundamental ideas prevail: community,2 and the process.3 In the absence of a consensus, resilience can be deï¬ned as the capacity of a system to absorb disturbances and reorganize itself during ongoing changes [33]. It is probably more relevant, especially in the case of an accident as serious as that at Fukushima Daiichi, to place less emphasis on states of equilibrium as âfrontiers as a function of the domain of attractionâ because paradoxically, highly fluctuating instability can also foster entry into resilience [28]. In practice, a system can be very resilient, yet fluctuate signiï¬cantly and therefore be fairly unstable. This approach seems more relevant in the case of Fukushima Daiichi where, given the enormity of the shock, it was more important to preserve relationships in the socio-technical system than to return to the previous equilibrium as quickly as possible, which in fact proved to be an unrealistic expectation [34]. Contemporary views of nuclear safety see the concept of resilience as a post-crisis process, part of a community dynamic that stresses organizational adaptability [35]. It has a predictive dimension that helps the organization to overcome adversity and get back on track [27]. However, this predictive dimension, and a fortiori entry into resilience, must not neglect the role of probability, uncertainty [13] or even a âsurpriseâ dimension in the success (or failure) of its implementation [36]. Here again, there is a reference to a conscious capacity to ânavigateâ and ânegotiateâ [37] in order to cope with an extreme situation."
275,608,0.987,Foundations of Trusted Autonomy,"21.3 The Inadequacy of Bayesianism The connection between autonomy and economics runs very deep: questions about the formation and application of beliefs in decision-making, and hence criteria delineating rational from irrational beliefs, are as prominent in economic theory as they are in artificial intelligence. For instance, the central place of equibria in economics and game theory is really a statement of a convention about the rationality of beliefs, wherein rational beliefs are held to be exactly those that coincide with equibrium solutions [18]. The Bayesian interpretation of belief is predominant in the economic literature, as it also is in artificial intelligence; the criticisms of Bayesianism in economic theory [18] apply equally in autonomous systems research and may be seen, in essence, as another recognition of the inability for a currently dominant paradigm to adequately handle fundamental uncertainty. Bayesianism is not really a single position, but any mix of three main postulates. Firstly, an agent should have probabilistic beliefs about unknown facts, typically given as a probability measure over all possible relevant states. Note that economics often uses a stronger version than computer science and artificial intelligence by dropping the relevance qualification and thus presuming that agents must have beliefs as probability measures about absolutely everything. Secondly, Bayesian priors should be updated to yield a posterior measure in accordance with Bayesâ Law. Lastly, each agent should choose the decisions that maximise expected utility (or sometimes equivalently minimise expected cost) with respect to that agentâs Bayesian beliefs. While the third postulate, in conjunction with the first and second, is hardly unknown in computer science, it is particular common in economic theory. No combination of these postulates has been immune from serious criticism in the economic literature. The Bayesian approach presumes a prior, and thereby does not deal with the manner in which the prior is obtained. This situation is sometimes known as state space ignorance or sample space ignorance. Though the second postulate seems technically safe, it has been shown to be descriptively inadequate [51]. Moreover, the economic literature arguably under-estimates the importance of complexity limitations on Bayesian updating; indeed, the computer science literature has been long focussed on this as the main difficulty. The third postulate has been attacked repeatedly in the economics literature since the Allais Paradox [2], which revealed strong inconsistencies between observed human choices and what maximising util-"
140,90,0.987,R.J. Rummel : An Assessment of His Many Contributions,"what decisions are made which creatively regulate that output. The âpolitical systemâ concept in the work of Easton (1953) ï¬ts this model, yet Easton himself, to my knowledge, did not take the next step to a âï¬eldâ framework. Early development of ï¬eld theory in the social sciences begins with psychology, chiefly in the work of Lewin (1939). In his framework, individuals (and groups) are conceived of as goal-seeking life forms in environments which, over time, induce learning. Environments are perceived as opportunities and obstructions to goal attainment, motivating people to adapt their behavior. The goals themselves vary as basic needs at the biological level are adequately met.3 All these ideas constituted my own âlittle ï¬eldâ as I tried to understand the purpose of the DON Project and what my mentors were about. As I recall, Rudy began his thinking about ï¬elds with Wrightâs idea of states propelled by interests and power in an n-dimensional space deï¬ned by their attributes, and Lewinâs idea of people interacting in ï¬elds constituted by their personalities. Over the next four or so decades, Rudy made their insights for understanding international conflict quantitatively and systematically researchable, at least in principle, and applicable to Guetzkowâs and his concern with preventing war. Lewin sketched his theory as an equation, B = F(P,E). Behavior B is a function of the life space of an individual, the personality P and environment E (Fig. 5.1). Here, the irregular sections represent an âintellectual geographyâ or life space, that is, events or situations through which a person travels over time and which influence the person P moving from some origin O towards a desired goal G. Both the person and the environment are in flux, thus different people at different times experience different situations. Each person (or group) may well constitute one of those irregular shapes in the graphâeach shape representing their âï¬eldâ of beliefs and predispositions, as each pursues their own goals in interaction with P. Although Rudy takes exception to this graphical representation, at the end of his critical review of the literature, he notes that he is in substantial agreement with Lewinâs equation.4 Further, consider one of Rummelâs concluding graphs (Fig. 5.2). As I see it, this is clearly a more detailed rendering of one of Lewinâs irregular shapes, some details applying Maslowâs concept of basic needs.5 Thus, Rudy is integrating a variety of theories and, as such, makes an original contribution to at least one type of ï¬eld theory. Rudy represented his ï¬eld theory of international politics by this simple equation, Xij Â¼ aj1 f 1 Ã°Ã Ã¾ aj2 f 2 Ã°Ã Ã¾    Ã¾ ajp f p Ã°Ã Ã¾ Uij"
116,37,0.987,Moral Reasoning At Work : Rethinking Ethics in organizations,"success story of business ethics and corporate social responsibility. With a negative outcome, the responsible executives would most likely have received criticism for wasting the investorsâ and the ownersâ money to no avail. The research literature acknowledges three categories of moral luck. The most prominent one is resultant moral luck, where the entrepreneur story from the north of Norway and Merck and the river blindness medicine are examples. Then there is constitutive moral luck, which has to do with the elements affecting a personâs character. Nature and nurture, genetic heritage and culture, can affect the extent to which a person is respectful, honest, kind, and benevolent in his or her interactions with other people. Good or bad luck plays a significant part in the formative processes, yet we tend not to take it into account when praising or blaming people for the character traits they have. The third category goes under the name of situational moral luck, and concerns the moral tests a person faces or avoids, and the extent to which character traits become publicly exposed. A person can be morally fortunate to never face situations where her moral weaknesses are exposed, or morally unfortunate to never get a chance to demonstrate personal courage and honesty, since the situations she faces do not call for the application these moral qualities. There is room for a fourth category of moral luck, not yet identified or discussed in the studies of this phenomenon. We can call it relational moral luck, and it concerns the social environment a person finds himor herself in at the time of decision-making. At crucial points in the process of judging and reasoning about what to do, the decision-maker depends on feedback from others, in the form of support or opposition to the ideas that are on the table. In an organization, he or she needs colleagues who intervene and question the assumptions that are present in the reasoning. I return to the concept of relational moral luck in the final chapter of the book, where I dwell more explicitly on the nature of the thought processes that lead from contemplation of options and alternatives, to action, and the extent to which their quality depends on the social side of decision-making. To what extent does moral luck pose a challenge to the coherence of our moral reasoning? Nagel and Williams thought that they identified a deep tension in the way we think about right and wrong when they introduced the concept. Moral luck is no doubt a thought-provoking concept and can serve as a reminder that success and failure often DOI: 10.1057/9781137532619.0006"
307,431,0.987,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"kN oi D koi =: In Fig. 13.13, we compare the open probability density functions of the three models for three different sets of parameters. In the left panel of Fig. 13.13, we show the open probability of the wild type (solid line), the mutant ( D 10), and the mutant in the presence of the theoretical open blocker. We see that the effect of the mutation is completely repaired by the drug. Other cases are shown in the center and right panels. The effect of the drug is still good but the effect of the mutation is not completely repaired. These observations are confirmed in Table 13.1. Furthermore, we have tested a large variety of parameters and the results we show here (center and right panels) represent the most difficult cases we could find in experiments. Therefore, we conclude that the theoretical open state blocker illustrated in Fig. 13.12 works very well."
311,3023,0.987,The Physics of the B Factories,"25.2 Benchmark new physics models Editors: Emi Kou, Jure Zupan (theory) In this section we review the impact of the B Factories on our understanding of the flavor structure in the Standard Model (SM) and on constraining new physics (NP) models. The most important overall result of the B Factories physics program is the fact that the CP violation observed in flavor changing processes with quarks is due to the Kobayashi-Maskawa (KM) mechanism (Kobayashi and Maskawa, 1973). For instance, prior to the B Factories, the kaon sector was the only system where CP violation was observed (see Section 16.1). The observed strength of the CP violation in mixing, Ç«K â 2.3 Ã 10â3 , was consistent with the KM mechanism with an O(1) CP phase in the CKM matrix. While encouraging, this by no means constituted a proof that the KM mechanism was really the origin of the observed CP violation. The first test of the KM mechanism was then done by the measurement of sin 2Ï1 by the B Factories. By now the KM mechanism has been tested at the level of â¼ O(10%), while deviations from its predictions at levels smaller than this are still allowed. In the KM mechanism there is only one weak phase, providing a single source of CP violation. The consistency of Ç«K with the observed CP violation in Bd0 âB 0d mixing and the measurements of the sides and angles of the standard CKM Unitarity Triangle all point to this common origin of CP violation. In addition, the size of Îms and the recent LHCb bound on the size of the weak phase in Bs0 â B 0s mixing both agree with the KM mechanism within errors. This agreement between b â d, b â s and s â d transitions is nicely summarized in the CKM fit plot (see Fig. 25.1.1). Another important indicator that the CP violation we are observing in flavor changing processes of quarks is due to the KM mechanism, comes from a comparison of a fit with only CP conserving observables with a fit with only CP violating observables. Both fits point to the same region in the ÏÌ and Î·Ì plane, which is a strong test of KM nature of CP violation (see Fig. 25.1.2). The measurements at the B Factories also had a direct impact on new physics models. For instance, a measurement that sin 2Ï1 is O(1) immediately excluded approximate CP models. In these models all the couplings which govern the low energy phenomena are real or almost real, with imaginary components always much smaller than the real ones. In the SM, the observed Ç«K and Ç«â² , representing the CP violation in the kaon sector, are small, which can be explained by the smallness of the CKM matrix elements entering into the description of these observables. In the approximate CP models it would be small because CP symmetry is only slightly broken and thus all CP violating phases are small. A set of well motivated realizations in the SUSY framework was put forward (Abel and Frere, 1997; Babu and Barr, 1994; Babu, Dutta, and Mohapatra, 2000; Eyal, Masiero, Nir, and Silvestrini, 1999; Eyal and Nir, 1998). For instance approximate CP conservation could naturally solve the âSUSY CP problemâ explaining"
315,212,0.987,Situating Children of Migrants Across Borders and origins : a Methodological Overview,"My methodological approach is characterized by the combined use of both quantitative and qualitative methods. This approach is relevant to our understanding of a social process such as this transition in the life course â the transition to adulthood â for a twofold reason. Firstly, it enables a reconstitution of any decisive events, the context in which the transition takes place, the people with whom these young adults are in contact, and the action they take. The approach considers both the diachronic and the synchronic dimensions of events, focusing particularly on socialization processes, and on the way in which the passage of time determines the actions of individuals.5 Secondly, through combining the two methods it was possible to (1) In the sense used by Bertaux and Bertaux-Wiame (1988: 23), âThe idea, although evident, that a life course can be much more easily determined by the transmission of a resource than by the imposition of a constraint gives to the concept of determination a whole new meaningâ."
223,346,0.987,Knowledge and Action (Volume 9.0),"two combined into one category) as indicators of understanding belief and knowledge (see Fig. 12.5).10 Figure 12.5 suggests that the sequence of the establishment periods conforms to the one I proposed in GÃ¤rdenfors (2008). An analysis of the uses of these words in different contexts is required in order to establish the connection with intersubjectivity more clearly than I have in this chapter. Note that know, think, and believe do not quite follow the usual S shape. Their trajectories may partly be explained by the many idiomatic uses of these words, which make their frequencies increase at a rate more constant than that of other words. Although I can present only a limited number of examples in these pages, it should be clear that my hypothesis on establishment periods is rich in empirically testable predictions. I invite corpus linguists and child development researchers to continue testing it. Further evidence of the domain called organization of semantic knowledge is the way that metaphors do not come alone. Lakoff and Johnson (1980) convincingly argued that metaphors are organized around schemas such as âargument is war,â âtime is a resource,â and âmore is up.â I have proposed that a metaphor expresses an It is difficult to identify any expression that corresponds to understanding emotions (empathy), for this capacity develops well before words are learned."
360,167,0.987,Compositionality and Concepts in Linguistics and Psychology,"tion, except perhaps in the case of finding semantic treatments for attitude reports.52 And the task of a formal semantics is to explain how complex meanings are constructed from simple meanings. For that purpose one needs to start with the simple meanings as a given. Although both of those claims are true, at least as the task is conceived by the Objectivists, they seem also to have forgotten their rationales for finding semantic compositionality attractive in the first place: the arguments from understandability, from productivity, from learnability, and from creativity. Certainly all of these arguments make the claim that the justification of semantic compositionality lies in the way it facilitates mental processing. And thus, even if the picture that âlanguage is about the worldâ is correct, these arguments nevertheless presuppose or require that language also embody features of oneâs mental life. A natural place for the Objectivists to find this missing connection is to claim that the terminal nodes of their meaning structures are given by mental items, such as concepts. And then they can go ahead andâfrom this starting pointâbe entirely compositional. In fact, Iâm sure that most of the philosophers-semanticists who give any thought to the issue probably believe something along those lines. But letâs look at what this would entail. If the psychological results are correctâand they have been replicated over and over, at least in regards to simple binary combinationâthat mental concepts allow both for differences in degree of typicality of the concept and also for differences in degree of membership (Hampton and JÃ¶nsson 2012, pp. 386â87), then this would have to be a feature of the meanings at the terminal nodes of the semantic structure. The lexical item âfurnitureâ, for instance, would have to admit in the world of different degrees of truth (and not merely different degrees of typicality) for âThis chair is furnitureâ and âThis clock is furnitureâ. I think philosophers-semanticists would not countenance this.53 In response to all these competing pressures, I surveyed the prospects for a twotiered semantic theory: one aspect being the Objectivistâs âconnection to the worldâ and the other aspect being the Subjectivistâs account of mental activity. As it turns out, some of the works that claim to provide a two-tier theory actually are within just one of the Subjectivist or Objectivist camp, in that both of their tiers have their foundational starting point, or common nexus, entirely within one camp. I also mentioned a few theories that appear to be genuinely two-tiered, but noted that they require further input, especially from the Subjectivist side of the theory. To agree to anything like the two-tiered theory I outlined, one needs to accept two claims: First, that linguistic utterances are to be judged as to how well they âfitâ the (external) world. One way to put this, and I did, is to say that what is meanto by an utterance concerns the actual world, and not any conceptual representation. The two come apart when 52 Such as John wants to marry a princess, the ambiguity of which (a specific princess versus any"
275,534,0.987,Foundations of Trusted Autonomy,"for present purposes their collective moral (to wit, a real-life kernel of PACU in human society), and since the form of creativity involved is not the one we place at the center of TACU. We do encourage readers to read about the stunning experiments in question. By the way, this may be as good a place as any to point out that these experiments only establish that many, or at least most, subjects exercise their freedom and creativity to routinely lie. The reader, like the two authors, may well not be in this category. 4 While widely known for Wealth of Nations, in which the unforgettable âinvisible handâ and phrase and concept appears, Smith was an advocate only of markets suitably tempered by morality."
249,117,0.987,Advances in Proof-Theoretic Semantics (Volume 43.0),"ânaiveâ variant T + turns on assumptions which are independent of the suitability of its language for expressing the second clause. Once these points are acknowledged, a number of other questions naturally arise: (1) having eliminated the second clause as the direct source of the Kreisel-Goodman paradox, what other principles might be to blame? (2) was Goodman correct to conclude the most appropriate response to the paradox was to conceive of the universe of constructive proofs as stratified in the manner described by his theory T Ï ? (3) what is the status of his [16] proofs of consistency, soundness, faithfulness, and the interpretatibility of Heyting arithmetic for T Ï ? (4) are such results available for unstratified variants of T Ï ? and (5) might such systems be of independent conceptual or technical interest? A truly systematic exploration of these issues is beyond the scope of the current paper. What we hope to achieve here is the more modest goal of laying out the various principles on which the paradox appears to depend and assessing them relative to the goal of providing an âinformally rigorousâ account of the BHK interpretation of the sort envisioned by Kreisel and Goodman."
8,309,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","the pressure, and infinitely high temperature. Under such extreme conditions, traditional black body radiation no longer remains, but rather the conditions are found akin to the high-energy collisions of nucleons. And then when strongly interacting matter is present the temperature cannot be infinite, but only about 1012 K, and the pressure is not anymore proportional to the energy density but only proportional to its logarithm. This is a different scenario of the beginning of the Universe than was previously thought. A beginning is seen in experiments at CERN, where a proton melts with another for 10 23 s into boiling primordial matter. Moreover, it cannot be excluded that even entire stars consist of boiling primordial matter. We can wonder if this Big Bang, the origin of everything, including the beginning of time is an equally unsatisfactory assumption as is the existence of the very final building blocks of matter. Just as you can ask: and how did that building block come about?, so you can ask: and what was before Big Bang? How did it happen? We do not know. Maybe we will find one day that this question in a similar way is irrelevant asâpossiblyâthe one about the final building blocks. I close with an anecdote: on the bulletin board of a German university the following could once be read among lecture announcements: Tuesdays 9â11 AM, free for all discussion session about the structure of the Universeâonly for the advanced. signed X. We will, alas, always be beginners (see Fig. 16.1). In 1992 a Summer School took place that united experts and students working on hadron production and quark-gluon plasma in laboratory and cosmology. The meeting was organized by G. Belletini, H.H. Gutbrod and J. Rafelski with the principal sponsor being the NATO Scientific Affairs Division. Next page presents in abridged format the meeting poster."
360,440,0.987,Compositionality and Concepts in Linguistics and Psychology,"theories of conceptual combination. This is particularly true for the false properties, which would have no reason to arise in the combined conceptâs representation under any of the theories of conceptual combination. The modiï¬cation effect also suggests that property veriï¬cation is not a matter of looking into the concept to see if the property is there. Now, as Spalding and GagnÃ© point out, if one does not do property veriï¬cation by inspecting the contents of the combined concept, perhaps one also does not make property decisions about any concept by inspecting the contents of the concept. How, then, does one make such decisions? We believe that the current results, and the previous results of research on the modiï¬cation effect, point to the need for an alternative framework, based on the incorporation of reasoning processes and meta-knowledge about the role of modiï¬cation in human communication, that could be used in forming and testing psychological theories of concepts and conceptual combination, and in particular theories of the relationship between properties and concepts. Surprisingly, there is relatively little modern research speciï¬cally into the relationship between properties and concepts (see, e.g., Laurence and Margolis 1999). Historically, however, the relation between properties and concepts has been investigated in great detail in philosophy, particularly in the work of Aristotle and Thomas Aquinas. (Spalding and GagnÃ© 2013) have argued that the Aristotelian-Thomistic (A-T) view of concepts might have much to tell us about concepts (see also Spalding and GagnÃ© 2015). They showed that the A-T view is not the same as the modern âclassicalâ view, which was (correctly) ruled out by concepts research in the 1970s and 1980s, and discussed several aspects of modern concepts research that ï¬t surprisingly well with the A-T understanding of concepts. Therefore, we end this chapter with a description of properties and concepts in the A-T view, and the suggestion that taking this philosophical view of concepts seriously could have real beneï¬ts for research in the psychology of concepts, and particularly for our understanding of the relationship between properties and concepts. To begin, it is important to realize that thinkers in the A-T tradition have thought deeply about properties and concepts and about how one reasons about things in the world, and have made many distinctions at levels of granularity that are far ï¬ner than those in modern concepts research. Also, it should be noted that making judgments about properties is not limited, in the A-T view, to philosophy of concepts, but also explicitly involves the philosophical domains of logic and reasoning. The discussion here is necessarily brief and introductory, but there is much serious discussion of A-T thinking on properties (see, e.g., Reynolds 2001; Wippel 2000, Part 2), concepts (for an overview, see, e.g., Mercier 1950a) and in the related judgments people make (see, e.g., Mercier 1950b). Indeed, some of the terminology in common modern use is taken from this tradition, though having lost much of its technical meaning in the transition. Property is perhaps the most obvious case. Property in modern usage is a part of the concept, and as such is a representation of any characteristic believed to be associated with members of the category denoted by the concept. Property in the A-T tradition is quite different. First, property is a shorthand for proper accident, and a proper accident always inheres in things in the world, not in concepts. Accidents contrast with substances. Substances, to a ï¬rst"
249,107,0.987,Advances in Proof-Theoretic Semantics (Volume 43.0),"We will come back to discuss the second concern described by Beesonâi.e. that it assumes that p is a proof of A expresses a mathematical proposition âon the same levelâ as expressed by A itselfâin the course of comparing the Theory of Constructions to systems like ITT (wherein p is a proof of A is regarded as a judgement as opposed to a proposition). But with regard to the first issue he raises, note that while Kreisel appears to have introduced the second clause precisely so as to avoid the form of impredicativity discussed in Sect. 2, Beeson suggests that it is this step itself which introduces impredicativity into the interpretation of intuitionistic implication. Although Beeson also fails to expand upon the precise form this impredicativity takes, it again seems likely that what he also has in mind has something to do with the self-applicability of the proof relation. For note that not only does the formulation of the second clause require that we countenance the existence of proofs p which stand in the proof relation to statements A which may themselves refer to other particular proofs q (e.g. for A of the form R(B, q)), but also the case where A may contain a quantifier over all proofs (e.g. for A of the form âx R(B(x), x)), presumably inclusive of p itself. A potentially related point about the existence of proofs with this property is made by Weinstein in the following remark about the second clause: If [â¦] we suppose that universal quantifications over the universe of constructions applied to decidable properties have decidable proof conditions then we may view [(P2â )] as providing an assignment of decidable proof conditions to each formula of the language of arithmetic [â¦Â¶â¦] This means of securing the decidability of the proof conditions for formulas of arithmetic is not without cost. The alternative statement of the proof conditions for conditionals is self reflexive in a way that the original explanation was not. Both Kreisel and Goodman noticed that this self reflexivity leads to paradox in a theory of constructions which includes a"
173,76,0.987,"intelligent Human Computer interaction : 9Th international Conference, Ihci 2017, Evry, France, December 11-13, 2017, Proceedings","achieved before the gesture is completed. This might make it possible to have an âeducated guessâ about the gesture being performed very early on, leading to more natural interaction, in the same way that humans can anticipate the reactions or statements of conversation partners. In this classification problem, it is easy to see why âahead of timeâ recognition might be possible as the gestures differ sufficiently from each other from a certain point in time onwards. A weak point of our investigation is the small size of the gesture database which is currently being constructed. While this makes the achieved accuracies a little less convincing, it is nevertheless clear that the proposed approach is basically feasible, since multiple cross-validation steps using different train/test subdivisions always gave similar results. Future work will include performance tests on several mobile devices and corresponding optimization of the used algorithms (i.e., tune deep LSTM for speed rather than for accuracy), so that 3D hand gesture recognition will become a mode of interaction accessible to the greatest possible number of mobile devices."
49,419,0.987,Artificial Intelligence and Cognitive Science IV,"5.3 Epistemic logic One of the most prominent current approaches to modeling information and cognition is epistemic logic. Epistemic logic dates back to the seminal works of von Wright [24] and Hintikka [14]. This subsection offers a sketch of its basics. The language of epistemic logic extends the language of classical propositional logic by a family of knowledge operators Ki, where i ranges over some set of agents G: Ki p is read âagent i knows that pâ. Epistemic models for a set of agents G are structures M = (W, {Ri}i â G, V), where W is a non-empty set, every Ri is a binary relation on W and V is a valuation, i.e. a function from the set of propositional atoms to subsets of W. Informally, W is thought of as the set of epistemic alternatives or possible worlds. However, it is usual to refer to them in a more neutral manner as âpointsâ. Next, Ri is an epistemic indistinguishability relation for the agent i: Rixy iff i cannot distinguish between points x, y. To be more specific, i cannot distinguish between x and y if she does not have access to information that would render one of the points as obviously incorrect. For example, if I do not know whether it is sunny in London, then I cannot distinguish between any x containing the fact that it is sunny in London and any y containing the fact that it is not sunny there. In most applications, the indistinguishability relations are assumed to be equivalence relations, i.e. reflexive, symmetric and transitive. Truth of formulas is relative to points: most importantly, KiA is true at a point x iff A is true at every y such that Rixy. Hence, i knows that A iff A holds at every epistemic alternative. This is in line with our intuitions â if I know that A, then points with non-A are obviously not sound epistemic alternatives. Epistemic logic clarifies several somewhat involved scenarios such as the muddy children puzzle and is often used in computer science (see Fagin et"
342,22,0.987,Semiotics in Mathematics Education,"â¢ the âIntensional Interpretant, which is a determination of the mind of the uttererâ; â¢ the âEffectual Interpretant, which is a determination of the mind of the interpreterâ; and â¢ the âCommunicational Interpretant, or say the Cominterpretant, which is a determination of that mind into which the minds of utterer and interpreter have to be fused in order that any communication should take place.â (Peirce 1998, p. 478, his emphasis) It is the latter fused mind that Peirce designated the commens. The commens proved to be an illuminating lens in examining the history of geometry (Presmeg 2003). The complexity and subtlety of Peirceâs notions result in opportunities for their use in a wide variety of research studies in mathematics education. Applications in mathematics education are as follows. As an example, let us examine the quadratic formula in terms of the triad of iconic, indexical, and symbolic sign vehicles. The roots of the equation ax2 Ã¾ bx Ã¾ c Â¼ 0 are given by the well-known formula x1;2 Â¼"
82,102,0.987,Fading Foundations : Probability and The Regress Problem,"It might be true that more people are inclined to interpret (2.1) along the lines of internalism and evidentialism.43 Under that interpretation, Ai and A j are both beliefs or propositions, and P is construed as subjective or epistemic or logical probability. The point we want to make here is that (2.1) can just as easily be understood in accordance with externalism and relabilism. Under this interpretation, Ai and A j can be beliefs, perceptual appearances, memories, and so on. For example, if Ai is my belief in the proposition that a cow is grazing in front of me, and A j is my seeing a cow grazing in front of me, then (2.1) states that it is more likely that my belief in a grazing cow is true, given that I have this perception, than when I do not have this perception. Here P is an objective probability, depending on the frequency of events, where the events are âseeing a grazing cowâ and âbelieving that there is a grazing cowâ. Whether or not (2.1) holds here is determined by empirical research or, more generally, by past performance. Is it the case that my seeing a cow grazing is more often followed by a belief in a cow grazing than that my perception of a horse jumping is followed by a belief that a cow is grazing? The answer is presumably in the affirmative, so (2.1) is satisfied. The thing justified need not be a belief of which the probability is determined on the basis of perceptual appearances. It can also be the other way around. In cases of wishful thinking or of harbouring strong suspicions, my beliefs or my desires can cause in me certain perceptual appearances. Here the causal course runs in the opposite direction. Again, we determine empirically whether or not a causal process is in fact taking place, and thus whether or not (2.1) is satisfied: some people are more prone to wishful thinking or to being suspicious than others. It might happen that a causal process gives rise to a false belief. Optical illusions are a classic example. When I am walking in the desert, the refraction of light from the sky by heated air can cause me to believe that there is a sheet of water in front of me. This causal process is probabilistic (at some times I am more vulnerable to this optical illusion than at others), but it is assumed that (2.1) is fulfilled (I mostly fall prey to the illusion when walking in the desert). Goldman will presumably say that this is not a reliable beliefforming process, and that my belief that there is a sheet of water in front of me is not justified. As we will explain in the next section, however, we are not proposing to define justification as the condition of probabilistic support 43 Thus ReneÌ van Woudenberg and Ronald Meester have argued that âthe tradi-"
264,216,0.987,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"Doing Our Research: Describing Teaching and Interpreting Shifts in Practice A major task for any lesson analysis is how to chunk or divide the lesson into analytic units. In our analysis, we ï¬rst need to infer the lesson goal, or object of learning, as this is key to our analysis, and we then proceed to chunk the lesson into what we call mathematical episodes. We begin by watching the video-recording and simultaneously (re)reading the transcript to identify the intended object of learning that we know is not synonymous with what is enacted (Marton & Tsui, 2004). We look for what is announced in some way, typically by the teacher at the start of the lesson, often stated as a topic or written on the board. For example, in one of our lessons, Mutliplying Algebraic Expressions was written on the board, at the same time as the teacher said, âToday we are going to learn to multiply expressionsâ. The intended object of learning was carrying out a procedure for different products, which as it transpired in the lesson included two or more monomials; a monomial and binomial; and then two binomials. We then chunk the lesson transcript into mathematical episodes that are identiï¬ed by a shift in focus of attention with respect to content, typically marked by a task that encompasses selected example(s), and bears some relation to the stated object of learning. We identify a next episode by the introduction of a new task and focus on a new example. This chunking produces a number of episodes, and again by way of example in this particular lesson, Episode 1 focused on multiplying single terms, paying attention to laws of exponents. Episode 2 was marked by the shift to the product of a monomial and binomial, and so on. We then examine each episode for its exempliï¬cation, explanatory communication and learner participation. Table 1 provides a summary of how we have categorised key elements of MDI. It is beyond the scope here to discuss each of these in detail. I will focus only on examples and legitimating criteria so as to communicate how our analysis works to describe what is made available to learn in a lesson or set of lessons through the set of examples offered, and the criteria for what is to count as mathematics are communicated.There is more detail on each of these and the remaining elements in Adler and Ronda (2015, 2017b)."
253,763,0.987,"Autonomous Driving : Technical, Legal and Social Aspects","Based on this argument, an evolution across all dimensions would seem to be a possible approach for overcoming the testing challenge. Dimensions here refers, for example, to the speed, the area of use but also the degree of automation. A distinction can be made between two perspectives in selecting the evolution steps: From the perspective of a function developer, due to the reduced speed and the limited access to the scene, the interstate during a trafï¬c jam is a suitable starting scenario. From the perspective of the previously presented statistical considerations, a meaningful starting scenario would be one in which the human as a comparison group would perform as badly as possible, i.e. making as many errors as possible. As many errors as possible means a short distance, making the veriï¬cation of the performance easier. The revolutionary stepâan autonomous vehicle without evolutionary intermediate stepsâcontradicts this approach and seems unlikely."
118,344,0.987,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"Developing a framework for a Cultural Risk Analysis, i.e. to carry out a cultural analysis, requires a paradigmatic shift in human consciousness similar to the one that took place in the Enlightenment. And this will be extremely difficult because it is a shift requiring both the rational (cognition) and the emotional (feeling). It will require both risk-as-analysis and risk-as-feelings; it will require both moral reasoning and emotional morals. As any good engineer knows (at least those who have taken my class), a redundant and diverse system has order of magnitude higher reliability if the system is built of âANDâ gates rather than âORâ gates.21 Perhaps Thomas Kuhn [34] said it best, ââ¦that is why a law that cannot even be demonstrated to one group of scientists may occasionally seem intuitive to others. Equally, it is why, before they can hope to communicate fully, one group or the other must experience the conversion we have been calling a paradigm shift.â And, âJust because it (a paradigm shift) is a transition between incommensurables, the transition between competing paradigms cannot be made a step at a time, forced by logic and neutral experience. Like the gestalt switch, it must occur all at once (though not necessarily in an instant) or not at all.â"
108,16,0.987,Bordieuan Field Theory as an Instrument for Military Operational Analysis,"definitions, simply because the term EBO was formulated without any normative content. One often has an idea of what one would like to achieve on a particular operation, but there has been no theoretical framework to enable a link to the problem at issue. One approach is to create specific theoretical terms to facilitate analysis of military operations. However, it might also be of value to do what is often done in academic circles, to apply a fresh, untested line of thinking to the material under study. This is the approach that will be adopted in this text, albeit only purely tentatively. This shows that it is far from impossible for terms to be given value without even the definitions of the term itself being defined, it is like a casino where the stakes are being thought over and the winner defines the content of what was at stake. A more reasonable approach is, of course, to define what is being discussed before proceeding with a power struggle over the same subject of discussion. A theoretical perspective will now be presented that gives an alternative perspective to military planning, or it will at least provide an understanding of how an operations area can be analysed using theory. It is worth stressing that this does not necessarily mean presenting a theory that will lead to new practices on the field. Rather it so happens that much of what is advocated by the theory actually already occurs on the field. The problem is that the practices being examined here have hitherto lacked any form of explanatory foundation, other than that proven experience has shown that they work well. If the practices are given a theoretical explanation this may illuminate how current practice can be further developed. Therein lies the benefit of a theory that can be applied to the practices under discussion here. I emphasise this point here, but believe it to be so important that you will find it repeated throughout the text: theories are used mainly to generate issues of interest that will be played out empirically. Theories are not primarily used to provide answers to questions. If the latter were the case empirical research would not be necessary, theory alone would suffice to explain reality. This is an unempirical process that should be avoided. Only in exceptional cases where there is a lack of empirical foundation can one generalise using an empirical approach, and assume that a situation will play out in a certain way. Nonetheless, theory is important in empirical research, as it helps us structure the reality which empirical data consists of. Military intervention in an area where armed conflict is taking place is, to say the least, a risky undertaking. There is a mass of information that"
192,424,0.987,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"fore represent the discourse of the Master. Indeed, Schuyt (2014) explicitly berates their lack of (notably legal) expertise. A third option is the discourse of the hysteric, criticising the perversity of the system as such. From a psychoanalytical perspective, even the (apparently negative) figure of cynics/hysterics may play a positive role, revealing gaps in established discourse, highlighting blind spots or deliberative routines which rightfully invoke objections, because something of importance has been forgotten or eclipsed, something of value which now has become impossible to articulate (Zwart 2016b). Yet, although the discourse of the hysteric may be effective in the sense that others are pressed into action, it often represents a temporary and unsustainable option, resulting in a deadlock, in self-marginalisation. Ultimately, a Lacanian analysis endorses the discourse of the analyst, taking the floor when others (S1, S2, $) have already spoken, revealing the extent to which these others are spoken and driven by desire, by a truth unknown to themselves (Verhaeghe 2001). The analyst is basically a rhetorician, an expert in the dynamics and modes of discourse (Lundberg 2012; Lacan 1977â1978, p. 4). In the case of university discourse, the analyst focusses on symptoms of professional uncertainty, ambivalence and unease, camouflaged by the expertâs apparent fluency and subtlety. And whereas the use of vignettes (as part of the tool-box of university discourse) often entails the suggestion that it is possible to bridge the gap between problem and solution, the discourse of the analyst will focus precisely on these gaps, because it is precisely here that the real challenges are likely emerge (âmind the gapâ: Verhaeghe 2001). Moreover, the discourse of the analyst is closely connected with education, with the âformation of the scientific mindâ, as Bachelard once phrased it, although now the focus has shifted from epistemology as such (the methodologies and technologies of knowledge production) to academic authorship as a practice of the Self, fostering individuation. The oblique perspective challenges the science â humanities divide (M2 â M3). Instead of vignettes (short, formulaic stories), the discourse of the analyst prefers to work with extended case histories, in the form of science biographies and autobiographies for instance, or in the form of science theatre, science cinema and science novels:"
360,141,0.987,Compositionality and Concepts in Linguistics and Psychology,"And he mentions certain entailments as also falling under the sort of âcoercionâ he is concerned to capture, such as (5) John started the car defeasibly implies John started the engine of the car In Asherâs account, another aspect of the TYPE portion of lexical entries concerns how they interact with the TYPE-entries of other lexical items in a longer stretch of discourse. This interaction is governed by the underlying logic of the mental side: his adaption of classical type-theoretic logic to become a proof-theoretic semantics for the TYPE side of his theory, as I suggested in Sect. 6.2. Iâve argued that TYPEs are concepts, mind-dependent entities with fine-grained content. TYPE s have an internal semantics that is given at least in part in terms of the rules by which they combine with other TYPEs; they are proof-theoretic objects. TYPEs can be associated with other TYPEs â what we might think of as traits; each trait we associate with a TYPE is a constraint on the introduction rule for the TYPE. . . . TYPEs as proof objects provide an internal semantics for natural language sentences and discourses that complements the external semantics given by ordinary intensions. We can even distinguish between different conceptualizations of the same physical object or of the same property, so in some respects the structure of types has the capacity to make finer distinctions in meaning than intensional semantics can. But the purpose of the internal semantics is not the same as that of the external semantics. Intensions are the soul of a theory of meaning â they are needed to determine truth, reference, and other external, semantic properties that link language to the world we talk about. TYPEs and their adjustments are the heart of a theory of predication and"
217,821,0.987,Finite Difference Computing With Pdes : a Modern Software Approach,"for the PDE in question, we have in fact a scheme that reproduces the analytical solution, and many of the schemes to be presented possess this nice property! Finally, we add that a discussion of appropriate boundary conditions for the advection PDE in multiple dimensions is a challenging topic beyond the scope of this text."
8,307,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","technical defect, so far not functioned quite right: one has usually introduced only the few lowest mass particles in self-consistent bootstrap circles; the more stable particles one takes, the better the particle bootstrap should function, so all stable particles need to be included, after this is done there can no longer be an objection. On the lighter side, we recall that only when MÃ¼nchhausen has yanked very strongly at his hair, was he able to move, and then not only himself, but taking with him the swamp, and the Earthâthe whole world. (c) It is noteworthy that in the realm of todayâs particle physics (or High Energy Physicsâwe have seen that these two terms mean the same) no evidence is found that the existing principles of relativity and of quantum theory need to be corrected or extended in any way; even though we are in a new situation. (d) After my report, it might seem as if the end of elementary particle physics has come. However, what I have presented arises from speculative hypothesis. And even if everything were correct, we would not come to an end, but find ourselves at a new beginning: in all the above considerations only strong interactions were considered, and not in terms of particular form of forces, but only in terms of the ever-changing composition of the âelementary particles,â and we have never spoken about their individual characteristicsâtherefore our conclusions were completely independent of all these additional known particle properties. Thus we have described the average behavior, the statistical behavior. But the main focus of high energy physics is precisely on all these more detailed individual properties of the new particles and the forces acting between them. And there is the question, why these forces? In this regard we stand at a new beginning. (e) Many physicists still believe in the possibility of exploring deeper and further to ever more elementary building blocks. One must follow this line experimentally and cannot be misled by intellectually satisfying speculation into believing that the scientific question is settled. (f) I have tried to describe everything in everyday language, in words, that we physicists use, when we talk about such things at tea. To you, the reader, everything must look very mysterious, especially the claim that each âelementary particleâ in different ways has been created from all the others. Take it to be âasif-speechâ, as a blurry image of what can be formulated much more precisely with the help of mathematics or technical jargon. With this report I also, as an aside, hope I have made you understand why we high energy physicists yearn so much for the next European 300-GeV accelerator, which will now probably be built."
84,76,0.987,Eye Tracking Methodology,"1.1.7 Posnerâs âSpotlightâ Contrary to the serial âwhatâ of visual attention, the orienting, or the âwhereâ, is performed in parallel (Posner et al. 1980). Posner et al. suggested an attentional mechanism able to move about the scene in a manner similar to a âspotlight.â The spotlight, being limited in its spatial extent, seems to fit well with Noton and Stark; Noton and Starkâs empirical identification of foveal regions of interest. Posner et al., however, dissociate the spotlight from foveal vision and consider the spotlight an attentional mechanism independent of eye movements. Posner et al. identified two aspects of visual attention: the orienting and the detecting of attention. Orienting may be an entirely central (covert or mental) aspect of attention, whereas detecting is context-sensitive, requiring contact between the attentional beam and the input signal. The orienting of attention is not always dependent on the movement of the eyes; that is, it is possible to attend to an object while maintaining gaze elsewhere. According to Posner et al., orientation of attention must be done in parallel and must precede detection. The dissociation of attention from foveal vision is an important point. In terms of the âwhatâ and the âwhereâ, it seems likely that the âwhatâ relates to serial foveal vision. The âwhereâ, on the other hand, is a parallel process performed parafoveally, or peripherally, which dictates the next focus of attention."
249,511,0.987,Advances in Proof-Theoretic Semantics (Volume 43.0),"content to what can be expressed by formulas of IPC2, it was called a âreductiveâ rather than âfoundationalâ approach. As described in Schroeder-Heister [63] this can be carried over to a framework that employs higher-level rules, making the reference to IPC2 redundant. However, as the handling of quantified rules in this framework corresponds to what can be carried out in IPC2 for implications, this is not a presupposition-free approach either. The viability of both approaches hinges on the notion of equivalence, that is, the idea that meanings expressed by equivalent propositions (or rules in the foundational approach), one representing the content of introduction-premisses and the other one representing the content of eliminationconclusions, is sufficient to describe harmony."
306,4,0.987,"Early Geometrical Thinking in the Environment of Patterns, Mosaics and Isometries","abstraction. Assuming that the ï¬rst geometrical knowledge is passive, comprehensive, global, and, therefore, static, conferring dynamism to geometric reasoning starts to be an important need. This new trend should be interpreted broadly. For a long time, the role of manipulation in early geometrical tasks was not associated with reasoning but rather referred to childrenâs working style, age, and ways of gathering information about the world. The speciï¬cs of childrenâs work indicated the use of different items for manipulation. It seemed obvious that the childrenâs work in active environments would more suitable for them, and functioning at the symbolic level would not be available. This style of work rather was implied by the Piagetian approach, which deals with the necessity of interiorisation of actions in the process of building mathematical concepts. However, it was not connected with the approach suggested by Gray and Tall (1994), which pointed to the necessity of joining processual with conceptual understanding. The pro-ceptual approach was hardly accepted by either the theory or practice of teaching geometry. Currently, designing a manipulative educational environment is focused on building a scheme for deep understanding of geometric concepts. The emphasis here is set not so much on observing objects in motion nor on the ï¬nal results of manipulation, but on the ability to predict the result of the transformation. In this survey, the research on the understanding of geometrical concepts has been grouped around two main issues: the understanding of geometric ï¬gures and the functioning of these ï¬gures in space. The problem of understanding these ï¬gures that was indicated by Van Hieleâs theory is still worth considering and research has brought much information on how it takes place in childrenâs minds. The research reveals many limitations that give children trouble with the transition to higher levels of understanding. This research direction is dominant. Studentsâ understanding of ï¬gures has been repeatedly carried out by analysing the classiï¬cation of the ï¬gures they have made and their ability to describe objects and exclude counterexamples. On the other hand, research shows that children who are functioning in a static situation based on the recognition of a geometric object are doing it much less successfully than those that have the possibility of analysing the object given to them for manipulation. At this stage of research, it is worth dealing with the level that has often been referred to as the zero level (earlier than the level that was described as the ï¬rst level in Van Hieleâs theory). Four to six-year-old children can and do successfully discover the world of geometry in many areas, but they do it in their own speciï¬c way. To use those early experiences for creating further stages of understanding, knowing, describing, and understanding them is required. Such research is performed too rarely. Additionally, this type of research requires speciï¬c methodology and being skilful in making proper observation and in analysing childrenâs behaviour."
249,233,0.987,Advances in Proof-Theoretic Semantics (Volume 43.0),"The fundamental opposition between realism and anti-realism concerns therefore, according to Dummett, the key notion of the theory of meaning, i.e. the notion in terms of which the meaning of the statements of the given class is to be explained: truth according to the realist, evidence according to the anti-realist. However, since Dummett holds that meaning is to be explained in any case in terms of truth-conditions, and that for the anti-realist truth can consist only in the existence of evidence, the realism/anti-realism opposition, in the final version he offers, concerns the notion of truth to be used in explaining meaning: the bivalent notion according to the realist, some non-bivalent notion according to the anti-realist.6 The criterion of realism is therefore, in Dummettâs opinion, the acceptance/refusal of the bivalence principle concerning truth. Notice that this raises immediately a question: if we are ready to oppose the realistic, bivalent, notion of truth with other, non-bivalent, notions, we must ask at which conditions a notion can be considered as a notion of truth. I shall return to this question later. For the time being let me observe that the story of the semantic characterization of the realism/anti-realism debate is not finished. After Dummett it has been observed that all the anti-realistic notions of truth on the market (truth as assertibility, truth as existence of a verification, and so on) share a general characteristic: that truth is an epistemic notion, and therefore is essentially knowable. Knowability has therefore been identified as the essential property of truth, and anti-realism has been characterized as the view that every truth is knowable. It is precisely this feature of anti-realistic truth that the paradox is intended to hit. In this paper I shall first argue that an intuitionistic solution to the paradox is available; then I shall examine the position of neo-verificationism concerning the paradox; finally I shall briefly consider the general question of how a rational discussion of alternative logics is possible at all."
123,93,0.987,Fallibility At Work : Rethinking Excellence and Error in organizations,"a misunderstanding of the causes of his conduct, a simplistic and technical response to a complex set of challenges connected to fallibility and the interaction between human beings and technology. Second, the words most emphasized by Gimmestadâs main boss in the conversation after the event were that he trusted that there would be no repetition of that particular kind of mistake. âI am sure that you will never again land with the brakes on in your pilot career.â He has turned out to be right about that, but on hindsight, Gimmestad believes that his bossâ words made him exaggerate his attention to the brakes, at the expense of other and equally important aspects of the situation before, during, and after a flight (Gimmestad, 2016). When a person is encouraged to focus on one particular aspect of a complex situation, it can lead a blindness to other significant aspects, as documented in studies in perception psychology (Mack, 2003; Chabris & Simons, 1999). When you tell a pilot or a professional in other settings that they are not likely to that particular mistake again, it can create a strong motivation to make your words come true. That in itself can trigger aspect blindness since it draws the professionalâs attention to one particular aspect of the situation, much as in the gorilla experiment (Chabris & Simons, 1999), mentioned in Chap. 2. Gimmestad says that the period after the dramatic landing was one where he was particularly attentive to the brakes, and made himself vulnerable to overlook other important matters in the cockpit. That might have been the time in his career when the safety of flying with him was at its lowest. Inattentional blindness is a phenomenon that poses a threat to safety, and to the success of other collaborative processes. One by one, individuals have a limited ability to perceive what goes on around them, and depend upon colleagues to intervene when they are blind to significant aspects of what goes on in their work environment. As noted earlier, the experience of being blind to something that is right in front of their eyes comes as a considerable surprise to participants in experimental studies. It can generate a realization that we are dependent to a high degree of input from other peopleâs perspectives in order to get a rich and adequate understanding of what goes on in our work environment. The next section focuses on a model central to systematic efforts in aviation to counter the pervasive threat of inattentional blindness. It is a model"
360,187,0.987,Compositionality and Concepts in Linguistics and Psychology,"I have argued that the way in which people interpret simple semantic rules such as relative clause modiï¬cation can be accurately modelled by a deeper analysis of the intensional meaning of the words. What is the evidence then that such intensions have the prototype structure that leads to the patterns of overextension seen above? If we consider most common content words in natural languageânouns, verbs, adjectives and adverbsâthen it is often the case that neither the extension nor the intension are easy to pin down. The meaning of function words like prepositions is even harder. Consider for example the following common uses of âonâ in English: (3) The cup is on the table. (4) I got paint on my shirt. (5) Harry is on holiday. (6) The train is on platform 2. Any attempt to deï¬ne the extension of situations to which âonâ applies is likely to end up simply as a disjunctive list of different cases. The fact that prepositions do not easily translate between languages supports this claim (Bowerman and Choi 2003a, b). When I told a French friend that I had travelled to Paris âsur le trainâ, the puzzled response was to ask if it wasnât very windy sitting on the roof. Similar problems arise with adjectives such as âfreshâ or âopenâ (Murphy and Andrew 1993), with multiple inter-related senses determined by context. For further discussion see Rice (1992). Returning to the (perhaps) simpler case of nouns, consider a simple everyday term such as âï¬shâ. First there is a potential ambiguity arising from the domain of discourse. Fish features in cookery and food and as such its extension may include creatures such as squid, oysters and lobsters. Fish is also part of a commercial"
124,535,0.987,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"to do that. But then, differences between STIT and PDL tools may just amount to different legitimate decisions on how one individuates actions.11 The problem of individuating actions has been discussed extensively in philosophy (see footnote 3 on pg. 588 of Horty and Belnap (1995) for a concise explanation), and it also shows in modeling practice in computer science. This brings us to an important philosophical issue which we have thus far swept under the rug. In PDL models, actions are labels of transitions and this basic sorting of transitions by their labels seems to suggest a particular ontology of actions and events. In STIT models, there is no such sorting and, indeed, the only way to characterize an action is by reference to the outcomes. This raises an important question for the philosophical logician: Does adopting PDL as a logic of actions force one to take sides in philosophical debates about the ontology of events and actions? Our response is to bracket this question since we feel that both STIT and PDL models are open to a wide range of philosophical interpretations, regardless of the original intended interpretation of these logical frameworks. However, we certainly admit that this rather mathematical âformal modelingâ view is itself controversial and we welcome (and enjoy) debates on this issue. Nonetheless, we hope that the comparative points we are making in this chapter still make sense."
311,672,0.987,The Physics of the B Factories,"15.3.5 CP violation in background components A subtlety raised in Chapter 10 is the issue of correctly accounting for any CP asymmetry (time-dependent or timeintegrated) in background modes when performing a timedependent analysis. This issue is not signiï¬cant for the case of charmonium decays such as B 0 â J/ÏKS0 , where there is very little background, however it should be considered when analyzing modes with signiï¬cant levels of background such as B 0 â Ï+ Ïâ . There are two types of CP violating background that may occur (i.e. direct and mixing-induced CP violation, see Chapter 16) from neutral B mesons, and charged B mesons may only violate CP via direct decay. In addition one may need to consider the BB background, where the B signal candidates are formed by combining the daughter particles of the true Btag and Brec . In general the reconstructed |Ît| values of these background events are smaller than the true ones as the reconstructed Btag and BCP vertices tend to be closer to each other. Such an eï¬ect can be taken into account by replacing the B lifetime in the exponential decay of Eq. (10.2.2) with an eï¬ective lifetime. This is particularly relevant for ï¬nal states with charm mesons in them as discussed in Chapter 10, but is also manifest at a lower level for B backgrounds without charm decays. Generally one assumes that any bias for the latter class of B decays is negligible. Having corrected for the above reconstruction eï¬ects one is faced with having to address the issue of a physical asymmetry in the background decay channel. In the case of a neutral B decay the asymmetry will be of the form of Eq. (10.2.8). One has to account for tagging and resolution eï¬ects, and typically it is assumed that it is valid to use the same tagging and resolution parameters for the background channels as for the correctly reconstructed signal. Ideally one should generate samples of Monte Carlo simulated data for each CP violating background mode with the values of S and C as measured in data. This way any dilution from mis-reconstructing a given channel is taken into account when setting the values of the eï¬ective S and C required to model the CP asymmetry of a given background mode. In cases where there is no measurement of the asymmetry parameters, but it is reasonable to expect a non-zero asymmetry, one varies the eï¬ective values of S and C between +1 and â1 to estimate the maximal eï¬ect a given background would have on the signal. CP violation"
8,793,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","undergone some development more recently making it clearer, more consistent, and perhaps more convincing. The details may be found in [9] and the references therein. Here a simplified naive presentation is given. We note, however, that our present interpretation is non-trivially different from that in [9]. The basic postulate of statistical bootstrap is that the mass spectrum .m2 ; b/ containing all the âparticlesâ, i.e., elementary, bound states, and resonances (clusters), is generated by the same interactions which we see at work if we consider our thermodynamical system. Therefore, if we were to compress this system until it reaches its natural volume Vc .m; b/, then it would itself be almost a cluster appearing in the mass spectrum .m2 ; b/. Since .p; ; b/ and .p2 ; b/ are both densities of states (with respect to the different parameters d4 p and dm2 ), we postulate that .p; ; b/Ë"
28,271,0.987,A History of Self-Harm in Britain,"the book. This happens in the Conclusion rather than the Introduction because it is easier to reflect in a comprehensible way upon this process when the argument has been laid out. Third, I sketch (very briefly) some of the ways in which self-harm-as-affective-regulation is now within the orbit of neurological explanations. Fourth, I expand upon the political significance of the internal, emotional understandings of self-damaging behaviour. Finally, I reflect upon the implications of this historical account of self-cutting and self-poisoning for human behaviour in general."
311,567,0.987,The Physics of the B Factories,"12.3 Analysis details 12.3.1 Generators In order to perform an angular analysis it is important to have simulated data with the correct angular distributions. This allows one to calculate, for example, the correct eï¬ciencies on the signal (see the next subsection) and to study how well (with how much bias) the angular ï¬ts described in Section 12.4 can extract the ï¬tted parameters. Here is a brief explanation of how this is achieved in the EvtGen event generator (Lange, 2001) introduced in Chapter 3. The crucial point is that decay amplitudes, and not probabilities, are used for each step in the generation of a decay chain. This allows one to include all angular correlations in the entire decay chain. Each particle is described according to the value of its spin and mass by an object with the corresponding number of degrees of freedom. Each decay in the decay chain is handled by a speciï¬c model taking into account the spin of the initial and ï¬nal state particles. Relevant parameters can be given as arguments to the decay model. For example in the case of the model describing the decay of a scalar to two vector mesons, the six arguments are the magnitude and the phase of the three helicity amplitudes."
82,296,0.987,Fading Foundations : Probability and The Regress Problem,"The no starting point objection is also at the heart of Richard Fumertonâs âconceptual regress argumentâ against justificatory chains. On several occasions Fumerton has distinguished between two âregress argumentsâ in support of foundationalism: the epistemic and the conceptual regress argument.8 The first boils down to the finite mind objection against infinite chains. It states that âhaving a justified belief would entail having an infinite number of different justified beliefsâ while in fact âfinite minds cannot complete an infinite chain of reasoningâ.9 In the previous chapter we have explained why we think that this objection does not succeed. The conceptual regress argument, on the other hand, appears to be a rewording of the no starting point objection. Fumerton calls it âquite differentâ from the epistemic regress argument, and âmore fundamentalâ.10 It states that an infinite justificatory chain is vicious because we can only understand the concept of inferential justification if we accept that of noninferential justification: [I]f we are building the principle of inferential justification into an analysis of the very concept of justification, we have a more fundamental vicious conceptual regress to end. We need the concept of a noninferentially justified belief not only to end the epistemic regress but to provide a conceptual building block upon which we can understand all other sorts of justification. I would argue that the concept of noninferential justification is needed . . . in order to understand other sorts of justification . . . .11"
297,1147,0.987,The R Book,"It is good practice to save the results of ï¬tting the model in a named object. Naming models is very much a matter of personal taste: some people like the name of the model to describe its structure, other people like the name of the model to be simple and to rely on the formula (which is part of the structure of the model) to describe what the model does. I like the second approach, so I might write model <- lm(growth~tannin) The object called model can now be used for all sorts of things. For instance, we can use the predict function to work out values for the response at values of the explanatory variable that we did not measure. Thus, we can ask for the predicted growth if tannin concentration was 5.5%. The value or values of the explanatory variable to be used for prediction are speciï¬ed in a list like this: predict(model,list(tannin=5.5))"
264,258,0.987,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"(Brousseau, 1981) was an instance of a more global didactic phenomenon. This work of mathematization of the didactic ï¬eld itself was not developed further, but it strongly influenced my conception of resource development, a crucial point as far as action is considered. For instance, in the resources associated with the research on differential equations mentioned above, I tried to overcome the trap of linear descriptions and to open the dynamics of situations, envisaging, for instance, possible bifurcations. I also tried to approach more explicitly the key issue of the sharing of mathematical responsibility between teacher and students than was usual in classical engineering design at that time and whose underestimation appeared as a major source of denaturation. The spontaneous conception of reproducibility was thus proven to be an obstacle to the dissemination of didactical designs coming from research and their productive use for action. I would not say that this didactic obstacle has been overcome. Many current educational resources still implicitly convey the same notion of reproducibility by giving the impression to the reader that classroom and individual trajectories can be ï¬xed by a succession of tasks and questions, without damage. However, this misunderstanding about what can and cannot be reproduced with what consequences is only one of the many difï¬culties met in the transition from research to action."
363,83,0.987,History and Cultural Memory in Neo-Victorian Fiction,"Atkinsonsâ ingenuity reclaims for habitation, water transport and farming via a complex system of dykes, sluices and drainage, but which is ever susceptible to flooding. For Tom, the empty, flat, formless Fens symbolise his version of vacant reality, âand no one needs telling that the land in that part of the world is flat. Flat, with an unrelieved and monotonous flatnessâ (2). The dykes, sluices and channels that give the Fens their provisional shape represent history, the stories and âthings made to happenâ with which that reality is filled. Just as the land which is reclaimed appears firm and solid, so do the âfragile islandsâ (341) built out of the formlessness of reality give the appearance of solidity, truth and actuality. Initially the novel distinguishes between two means by which chaotic, formless reality is filled, or reclaimed. The first is embodied in the languorous, âphlegmaticâ Cricks who, âborn in the middle of that flatness, fixed in it, glued to it even by the mud in which it aboundsâ, outwit reality by telling stories: âand thereâs no saying what meanings, myths, manias we wonât imbibe in order to convince ourselves that reality is not an empty vesselâ (41). Stories, told by both his mother and father as well as read in books, infuse Tomâs childhood: âmade-up stories, true stories; soothing stories, warning stories; stories with a moral or with no point at all; believable stories and unbelievable stories; stories which were neither one thing nor the otherâ (2). Since Waterland is an assemblage comprising history and fiction, and makes no uncomplicated distinction between these categories, historical narratives are included as story-telling possibilities here. Indeed history-teacher Tom becomes the most prominent embodiment of this method for outwitting reality. The second means of filling an empty reality is embodied in the âsanguineâ Atkinsons, Crickâs maternal ancestors, who, being from the hills of Norfolk, look down and âsee in these level Fens â this nothinglandscape â an Idea, a drawing-board for plansâ (17), the opportunity to make things happen: and thereâs no saying what consequences we wonât risk, what reactions to our actions, what repercussions, what brick towers built to be knocked down, what chasings of our own tails, what chaos we wonât assent to in order to assure ourselves that, none the less, things are happening. (41) The Atkinsons are associated with the (apparently) solid images of âcivilisationâ, maps, bricks, buildings and projects. Indeed their New Brewery is a monument to the narrative of Progress, and the description"
223,244,0.987,Knowledge and Action (Volume 9.0),"A Short Overview of Reflective and Impulsive Styles of Thinking Theories of Reflection The idea that human behavior is based on active, reflective thought guided by the principle of attaining beneficial things is old and makes intuitive sense. It is difficult to argue why people would actively decide to act in a fashion that they know is bad for them without some belief that the action would ultimately be positive. In this conception of human thought, negative outcomes can be explained by a lack of information. The Greek philosopher Socrates, for example, proposed that people would otherwise act in ways that were good for them. From a social psychological perspective, this kind of thinking is exemplified in expectancy-value theories and the concept of homo oeconomicus (e.g., Fishbein & Ajzen, 1975). The theory of planned behavior (Ajzen, 1985) is an established example of an expectancy-value model (Conner & Armitage, 1998). It depicts behavior as a function of several specific mental factors. In this conceptualization the three determinants of behavior are the attitude toward the behavior, the subjective norm relevant to the behavior, and the perceived behavioral control over the behavior. An attitude toward a specific behavior is generated by multiplying the evaluation of a possible perceived outcome of the behavior (a value) by the perceived likelihood of that outcome (an expectancy) and then summing the results of this multiplication for all possible outcomes. Similarly, the subjective norm is calculated by multiplying the actorâs motivation to comply with another personâs expectation by the perceived likelihood that that person holds that expectation over all persons. By contrast, perceived behavioral control is a function of the perceived power of behavior-inhibiting or behavior-facilitating factors multiplied by the likelihood that the actor has access to these factors. The assumption in the theory of planned behavior is, therefore, that a human actorâs calculation of these three determinants of behavior is optimally based on all available information. Once the determinants are established, the actor will integrate"
153,351,0.987,Solving Pdes in Python : The Fenics Tutorial I,"As above, we have used abs in the expression for E above to ensure a positive value for the sqrt function. It is important to understand how FEniCS computes the error from the above code, since we may otherwise run into subtle issues when using the value for computing convergence rates. The first subtle issue is that if u_e is not already a finite element function (an object created using Function(V)), which is the case if u_e is defined as an Expression, FEniCS must interpolate u_e into some local finite element space on each element of the mesh. The degree used for the interpolation is determined by the mandatory keyword argument to the Expression class, for example: u_e = Expression(âsin(x[0])â, degree=1)"
187,79,0.987,Managing The Complexity of Critical infrastructures : a Modelling and Simulation Approach (Volume 90.0),"Abstract The ultimate target of Modelling and Simulation (M&S) activities in the ï¬eld of CIP is to provide Models, Methodologies and tools to help in the analysis of different crisisâ scenarios and, subsequently, in crisis management decision making. A CIsâ disruptions scenario is simply a sequence of random events following a well-deï¬ned chronological order. Generally, each identiï¬ed scenario produces a set of consequences which is a function of: the initiating event, the concerned CIs and the geo-organizational context of the disrupted CIs. Formal sciences represent the reality of our surrounding world. But formal sciences are imperfect and what we call ârealityâ is the projection of the inaccessible âRealityâ on our world. This projection is the only reality we are talking about in formal sciences. Subsequently, formal sciences construct objects in which small parts of the sensible reality are grasped and formalized. These objects can be called âmodelsâ. We are limiting our interest here to formal sciences and engineering activities that cover both conceptual and phenomenological modelling processes. Models are ï¬rst validated before being admitted in the construction of a global model of the sensible reality. Regarding our focus on crisis scenarios modelling, simulation and analysis (MS&A), engineersâ ambition is to simulate not only independent isolated phenomenon but also interacting multi-physic multi-scale phenomenon."
213,280,0.987,Collider Physics Within The Standard Model : a Primer,"and the theoretical ambiguities (especially in the estimate of 1=mb corrections), is not compelling, but a substantial activity is under way on both the experimental and the theoretical side (see, for example, [248]). Watch this space! In conclusion, the CKM theory of quark mixing and CP violation has been precisely tested in the last decade and turns out to be very successful. The expected deviations from new physics at the EW scale have not yet appeared. The constraints on new physics from flavour phenomenology are extremely demanding: when adding higher dimensional effective operators to the SM, the flavour constraints generically lead to powers of very large suppression scales  in the denominators of the corresponding coefficients. In fact, in the SM, as we have discussed in this section, there are very powerful protections against flavour-changing neutral currents and CP violation effects, in particular through the smallness of quark mixing angles. In this respect the SM is very special and, as a consequence, if there is new physics, it must be highly non-generic in order to satisfy the present flavour constraints. Only by requiring new physics to share the SM set of protections can one reduce the scale  down to O.1/ TeV. For example, the class of models with minimal flavour violation (MFV) [152], where the SM Yukawa couplings are the only flavour symmetry breaking terms also beyond the SM, have been much studied and represent a sort of extreme baseline. Alternative, less minimal models that are currently under study are based on a suitably broken U.3/3 or U.2/3 flavour symmetry (the cube refers to the QL D uL ; dL doublet and the two uR and dR singlets, while U.3/ or U.2/ mix the three or the first two generations) [81]."
269,152,0.987,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"Against Reciprocity: Dynamics of Power in Interdisciplinary Spaces Abstract: All too often, âreciprocityâ emerges as the (imagined) organizing logic of interdisciplinary collaboration â with collaborators invited to forms of mutuality, fair exchange, and so on. In this chapter, we show what is missing from this analysis, which is any account of power. The chapter thus takes up an analysis of how power works in an interdisciplinary space â setting out, in particular, some of the different financial, epistemic, and cultural resources that belong to different disciplines. But if the chapter sets itself against one fantasy (fair exchange), it also wants to dispel another â and this is the fantasy of power confronted by the frankly-spoken truth. The chapter argues instead for what it might mean to think interdisciplinary collaboration as a practice of subjugation, which different collaborators may just have to learn to live with."
8,495,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","When "" varies between m` and 1, Teff .""/ varies between T0 and 2T0 , the latter value is approached only for "" values m . In cosmic ray jets, transverse momenta of pions up to the order p D "" D 1 to 1.2 have been reliably measured [11]. Assume then p D "" D 1 and m D 1. Then Teff D 4T0 =3 and with m D 2, we find Teff D 6T0 =5. This is at the upper end of the measured spectrum. At the lower end, Teff ! T0 . The present analysis is very rough and incomplete. It illustrates only the mechanism. It is not inconsistent with the assumption that nothing serious would happen in reality (the two-dimensional case). If that turned out to be so and if the general case gave a similar result, then it would allow one to conclude from the transverse momentum distribution [namely, the deviations from a pure exp.""=T0 /] something about the average or most frequent mass m of fireballs. The heavier the fireballs, the less the actual distribution will deviate from an exponential. On the other hand, the chain of decays will then contain more members and this may increase the deviations again. In any case it will at least cause a larger effective temperature. Actually, the T which is needed to fit the spectra seems to increase somewhat with the primary energy, although not more than by a factor of two, when the primary energy varies by a factor of one million. In spite of this very crude analysis, we believe that it is sufficient to make it very likely that the apparent increase in the temperature is entirely due to kinematics and that our T0 is indeed independent of the primary energy. A more careful and more realistic (two-dimensional) discussion of this problem is highly desirable. Open Access This book is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source and credited."
249,382,0.987,Advances in Proof-Theoretic Semantics (Volume 43.0),"(1)â(3) show that the relation  is, in fact, a familiar one from the semantics of intuitionistic logic, since they are nothing other than rules for the treatment of the connectives in the usual Kripke model semantics, when we take the sets Î± of atomic sentences as the nodes (worlds) of the model, and the relation Î± â Î² as the relation of extension. Thus the proof-theoretic trappings of Dummettâs presentation conceal a notion whose structure is the same as the standard model-theoretic or semantic one. One connective remains to be considered, namely, negation. As Dummett notes, the only way to treat negation that is consonant with his general procedure is to take Â¬F as an abbreviation for (F ââ¥), where â¥ is a sentential constant governed by the following introduction rule: from premises that are all the atomic sentences, it may be inferred. Dummett allows there to be infinitely many atomic sentences; in fact, this treatment of negation fares poorly if there are not. For if A1 , . . . , An exhaust the atomic sentences, then the introduction rule just mentioned yields the validity of inferring Â¬(A1 â§ . . . â§ An ) with no premises. Thus on logical grounds alone we would be able to infer that not every atomic statement is true, and this is surely an unacceptable result. If there are infinitely many atomic sentences, then this treatment of negation can most easily be incorporated into our forcing relation by requiring that the domain of sets of atomic sentences Î± that we consider is always finite. Then the stipulation above becomes: Î±  â¥ for no Î±."
315,256,0.987,Situating Children of Migrants Across Borders and origins : a Methodological Overview,"In the following, we introduce some advantages and limitations of our method that emerged during the implementation of this procedure. At the end of the section, we discuss possible future applications of the method. Several scholars have argued that the use of a history calendar, as compared to a standard biographical questionnaire, improves the validity and quantity of the data collected (Barbeiro and Spini 2015; Belli 2007; Belli and Callegaro 2009; Glasner and Van der Vaart 2009; Martyn and Martin 2003). The calendar is a flexible tool that allows the development of sequential (in the same column or domain of life) and parallel (by comparing the data in different dimensions of life) retrieval strategies (Belli and Callegaro 2009: 35). In addition to the advantages related to the use of a history calendar, there are specific advantages in combining it with narrative biographical interviews. First, during the interview, both the interviewer and the interviewee can compare the data on the calendar to improve the quality of the data. If the interviewee remarks on a difference, he or she can correct what he or she said or correct the date of an event or period on the calendar. If the interviewer observes some differences, then he or she can point it out and ask for more precise information. Sometimes, during the in-depth biographical interview, the interviewee remembered and added some life events on the calendar (particularly for the last column about subjective life events). Second, the graphical representation of the respondentâs life trajectory proves to be very useful at the moment of the narrative interview. The interviewer can ask questions about a specific dimension and life period and concretely point to it on the calendar with a finger. The interviewee can situate the question on his or her own life course and easily understand the question. Alternatively, it is the interviewee who tells his or her own story, also while answering to a question, while pointing at the relevant event or year on the calendar with a finger.. Third, the calendar may make possible the narration of a difficult life event (parental death, illness or negative critical events) that would otherwise be left aside in the narrative interview. In fact, it is sometimes easier for the interviewee to write"
8,206,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn",Critical Curve from the Lattice Calculations In order to compare the SBM with lattice results one should take into account that the latter are not obtained from calculations performed with the physically realized quark mass spectrum. One finds [20â22] that the quark mass dependence is well parameterized through the relation .mH a/2 D .mH a/2phys C b.m a/2 ;
58,94,0.987,Enabling Things to Talk,"Figure 6.2 could give rise to the impression that we prescribe a sequential approach for generating architectures: (1) Define the scope, i.e. the business goals; (2) Create the Physical Entity View and the IoT Context View; (3) Define requirements; and (4) Generate the remaining views. This type of sequential approach to architecting lies, for instance, at the heart of the waterfall approach (Royce 1970). This interpretation of Fig. 6.2 is indeed true if all arrows in Fig. 6.2 are understood as arrows in time. However, they can also be understood as logical dependencies. For instance, in order to conduct the requirements process, we need a set of formulated business goals, an IoT Context View and a Physical Entity View. If we interpret the process described in this Section in the latter way, it can be mapped onto a plethora of popular architecting methodologies, such as Model-Driven Engineering (MDE) (Miller and Mukerji 2003), Pattern-Based Design (Gamma et al. 1994), and the Spiral Model (Boehm 1988). The only limitation we see is in the choice of views. Some architectural methodologies prescribe different sets of views. Some of them, for instance the 4 +1 approach, lack some of the views we prescribe (mainly the information and context views) (Kruchten 1995). In this case we could choose to embed the 4+1 framework into the process described in this Section. On the other hand, other methodologies comprise views that are not part of the IoT ARM set. In this case, the option is to integrate the IoT ARM views (and the manner in which they are derived) into this other methodology."
8,755,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","As the critical curve is reached at finite energy density, nothing prevents it being reached in actual particle collisions and nothing prevents it even being passed over, provided the collision was energetic enough. Considering hadrons as quark-gluon bags, the hadronic gas becomes then on the critical curve a giant quark-gluon bag, and it should be described as an interacting quark-gluon gas on the other side. This, at least, is our (J. Rafelski and R. Hagedorn) present interpretation [35] (Fig. 26.8). Fig. 26.8 Physical interpretation of different regions of the .T; / plane as proposed in [35]"
223,268,0.987,Knowledge and Action (Volume 9.0),"Conclusion The RIM offers a multitude of predictions that can help improve the understanding of the link between knowledge and action, whether it be explaining the reasoning processes behind complex plans such as the Trojan Horse, the seemingly selfdestructive flight of Icarus, or even the conflict between rationality and impulse as epitomized in Odysseusâ suffering of the Sirenâs song. Although effortful, reflective processing may occur in fluent synergy with impulsive processing, there are often conflicts between the two systems. Their resolution is a question of available reflective resources and motivation to use them. But whether the systems work in concert or struggle against one another, the pathway to behavior is ultimately the sameâbehavioral schemata are activated depending on the results of both"
305,7,0.987,Quantum Computing for Everyone,"solve but also for the new ideas they introduce. These underlying ideas have been and are being incorporated into a new generation of algorithms. After looking at algorithms, we switch gears and briefly look at how quantum computation can be used to simulate quantum processes. Chemistry, at its most basic level, is quantum mechanical. Classical computational chemistry works by taking quantum mechanical equations and simulating them using classical computers. These simulations are approximations and ignore the fine details. This works well in many cases, but in some cases it doesnât. In these cases you need the fine details, and quantum computers should be able to give them. This chapter also briefly looks at building actual machines. This is a very fast-growing area. The first machines are being offered for sale. There is even one machine available on the cloud that everyone can use for free. It looks likely that we will soon enter the age of quantum supremacy. (We explain what this means.) The book concludes with the realization that quantum computation is not a new type of computation but is the discovery of the true nature of computation."
208,137,0.987,Actors and the Art of Performance,"In the end, it is âlike a dreamâ to the student.43 Canât we ourselves take a new turn here and âdream,â even assert that within the event of acting, thinking and emotionality are intertwined in a fruitful intimate dynamic? And that this event is not about liberation from affect, but the cleansing of affect to reveal its thoroughly noble quality, its ennoblement. But how exactly is this expressed in emotions? By regarding, by training regard for, others and their alterity. This slowly drains ressentiment of its poison. Face to face there can be no more objectification and no judgment. A gaze into the face of the Other and the response made has to do with respons-ibility. By sensitizing and training the senses in this way, the stage becomes the site of an ethics of responsivity, a site of experiencing and re-membering (anamneses) the importance of alterity. Preemptively.44 Would this not in the end be âlike a dreamâ for us?"
223,426,0.987,Knowledge and Action (Volume 9.0),"natural objects are treated as persons, although such anthropomorphization occurs as well. In many Australian examples, Aborigines do not just talk about the land and its features but may address it directly, as when expressing their respect or even their pity when the land has not been cared for properly. In Aboriginal Australia, a typical indication of a country1 that has not been cared for is that no one has set fire to it and that it should be visited (see Rose, 1995). Cases differ as to what is subject to personalization. It could be animals, various supernatural beings, sacred places orâmost commonlyâa combination thereof (as in the Australian case of totemic Dreaming beings that involve animals, superhuman creative beings, and places). The main and more general point is not that a certain set of beings (animate or inanimate) can feature as personalized subjects, as partners with whom one may reason. Rather, it seems that anything can become personalized if it is treated as a person, by which I mean that this some-thing is taken not as a thing, an instance of a category, but rather as a unique subject with which one interacts. By contrast, many phases of decision making in present day economics, for instance, entail processes of depersonalization and isolation. The procedures of reasoning are regarded not as a dialogue between persons but either as the interaction between users and computational systems or as abstract systemic processes devoid of personal relations, aspirations, and apprehensions. Therefore, both the style of the dialogue and the partners in the dialogue may be much more variable than is apparent. Beyond this case of foragers on the move, it may be wise to consider procedural rationality broadly enough to allow inclusion of variations in how procedures unfold as particular forms of dialogue and how partners in this dialogue are personalized or depersonalized. Rationality would thereby cease to be a purely mental phenomenon. Instead, it would reside partially in forms of social communication and interaction as well as in features of the environment that western philosophy and science tend to discount as irrelevant but that can be important triggers or partners in the procedure of reasoning. Why does abductive reasoning describe my ethnographic cases so aptly? I do not think its capacity to do so is coincidental. Rather, it is because this mode of inference is not a stand-alone mode but one that is tied closely to the interacting, corporeal, and relational social beings that we humans are."
124,306,0.987,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"and in the discussions regarding determinism and indeterminism. One of his main interests had to do with the Master Argument of Diodorus Cronus and the search for the so-called Diodorean modality (Prior 1955). It was well-known that Diodorus had formulated his argument about 300 BC in order to demonstrate that the world is deterministic, and to argue for a reductive account of modal notions to temporal notions; specifically that possibility should be conceived as âwhat is or what is going to beâ (ÃhrstrÃ¸m and Hasle 1995, p. 15 ff; ÃhrstrÃ¸m and Hasle 2006). To Prior this gave rise to three interesting questions: 1. What is the formal structure of the modal logic in which possibility is defined in the Diodorean way, on the assumption that time is a linear and discrete sequence of instants? 2. How can a formal and valid version the Master Argument of Diodorus be formulated? 3. How can indeterminism be defended (in terms of tense-logical systems consistent with the assumption of free choice) against the valid versions of the Diodorean argument and similar arguments? Prior worked intensively with these and similar questions from 1953 to his death in 1969. In doing so he found it most useful to study the theories of temporal logic. According to Prior the use of temporal logic would make it possible to obtain a better understanding of the consequences of accepting the idea of free choice. In particular, he also realized that the notion of branching time could be most helpful in this respect. Question 1 above was fully answered during Priorâs lifetime. In fact, Prior dedicated a complete chapter of his Past, Present and Future to this problem and its solution (see Prior 1967, p. 20 ff.). As we shall see, the study of this question actually led to the construction of the first branching time models. Priorâs work with question 2 led him to the formulation of a reconstruction of the Master Argument (see Prior 1967, p. 32 ff.). Working with question 3, Prior developed some very important systems of temporal logic consistent with the assumption of free choice. In this chapter we shall mainly comment on his Ockhamistic system. When Prior died in 1969 many additional problems regarding temporal logic and indeterminism had been discovered. Since then several logicians and philosophers have continued Priorâs line of thinking. Clearly, Nuel Belnap is one of the most important writers who have contributed to the further exploration of tense-logical approach to the study of indeterminism and free action. Much of Nuel Belnapâs work has been carried out within a Priorean tradition. As we shall see Belnap has elaborated the Priorean view that, although we may formulate a so-called prima facie kind truth of contingent futures, such statements cannot be what Belnap has called âsettled trueâ. Belnap has described this inspiration from Arthur Prior in the following way: Although I suppose it is unscholarly, I have always thought that what I formulate using âsettledâ is indeed what he âmeantâ, and what he âwould have saidâ had he been aware of the mischief that could, alas, be caused by not making âsettledâ explicit. [Personal communication, 31 Oct., 2009]."
227,30,0.987,Problem Solving in Mathematics Education,"through reasoning. Unreasonable problems require a breakthrough in order to solve them. The problem, however, is itself inert. It is neither reasonable nor unreasonable. That quality is brought to the problem by the solver. That is, if a student cannot solve a problem by direct effort then that problem is deemed to be unreasonable for that student. Perkins (2000) also acknowledges that what is an unreasonable problem for one person is a perfectly reasonable problem for another person; reasonableness is dependent on the person. This is not to say that, once found, the solution cannot be seen as accessible through reason. During the actual process of solving, however, direct and deductive reasoning does not work. Perkins (2000) uses several classic examples to demonstrate this, the most famous being the problem of connecting nine dots in a 3  3 array with four straight lines without removing pencil from paper, the solution to which is presented in Fig. 1. To solve this problem, Perkins (2000) claims that the solver must recognize that the constraint of staying within the square created by the 3  3 array is a self-imposed constraint. He further claims that until this is recognized no amount of reasoning is going to solve the problem. That is, at this point in the problem solving process the problem is unreasonable. However, once this self-imposed constraint is recognized the problem, and the solution, are perfectly reasonable. Thus, the solution of an, initially, unreasonable problem is reasonable. The problem solving heuristic that Perkins (2000) has constructed to deal with solvable, but unreasonable, problems revolves around the idea of breakthrough thinking and what he calls breakthrough problems. A breakthrough problem is a solvable problem in which the solver has gotten stuck and will require an AHA! to get unstuck and solve the problem. Perkins (2000) poses that there are only four types of solvable unreasonable problems, which he has named wilderness of possibilities, the clueless plateau, narrow canyon of exploration, and oasis of false promise. The names for the ï¬rst three of these types of problems are related to the Klondike gold rush in Alaska, a time and place in which gold was found more by luck than by direct and systematic searching. The wilderness of possibilities is a term given to a problem that has many tempting directions but few actual solutions. This is akin to a prospector searching"
249,537,0.987,Advances in Proof-Theoretic Semantics (Volume 43.0),"and definitional reflection adds a strong component to such computation (see HallnÃ¤s and Schroeder-Heister [31], Eriksson [17]). This computational aspect is important to proof-theoretic semantics. We should not only be able to give a definition of a semantically correct proof, where âsemanticallyâ is understood in the sense of prooftheoretic semantics, we should also be interested in ways to construct such proofs that proceed according to such principles. Programming languages, theorem provers and proof editors based on inversion principles make important contributions to this task. Devising principles of proof construction that can be used for proof search is itself an issue of proof-theoretic semantics that is a desideratum in the philosophically dominated community. Theories that go beyond logic are of particular interest here, as theorems outside pure logic are what we normally strive for in reasoning. If we want to deal with more advanced mathematical theories, stronger closure and reflection principles are needed. At an elementary level, clauses a â B in a definition can be used to describe function computation in the form f (x1 , . . . , xk ) â (x1 , . . . , xk ) which is supposed to express that from the arguments x1 , . . . , xk the value f (x1 , . . . , xk ) is obtained, so that by means of definitional reflection f (x1 , . . . , xk ) can be computed. More generally, one might describe functionals F by means of (infinitary) clauses the bodies of which describe the evaluation of functions f which are arguments of F (for some hints see HallnÃ¤s [29, 30]). An instructive example is the analysis of abstract syntax (see McDowell and Miller [41]). There are several other approaches that deal with the atomic level proof-theoretically, that is, with issues beyond logic in the narrower sense. These approaches include Negri and von Platoâs [42] proof analysis, Brotherston and Simpsonâs [2] infinite derivations, or even derivations concerning subatomic expressions (see WieËckowski [73]), and corresponding linguistic applications, as discussed by Francez and Dyckhoff [19] and Francez et al. [20]. Proof-theoretic semantics beyond logic is a broad field with great potential, the surface of which, thus far, has barely been scratched. Acknowledgments I am grateful to Kosta DoÅ¡en, Thomas Piecha, Dag Prawitz, Luca Tranchini and John William Devine for helpful comments and suggestions. The completion of this paper was supported by the French-German ANR-DFG project âBeyond Logic: Hypothetical Reasoning in Philosophy of Science, Informatics, and Lawâ (DFG Schr 275/17-1). Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
307,1,0.987,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"The summer of 2013 was very good; we found a series of papers published by Gregory D. Smith and his coauthors. We spent several weeks trying to understand the paper [35], which introduces and carefully studies a stochastic model of calcium release from internal stores in cells. Then we found a whole series of papers [36, 57, 102, 103], and the results more or less kept us busy for months. The beauty of the theory presented in these papers is that they introduce a systematic way of analyzing models that are of great importance for understanding essential physiological processes. So what is this theory about? It has been fairly well known for a while that stochastic models are useful in studying the release of calcium ions from internal storage in living cells. Some authors even argue that this process is stochastic. That is debatable, but it is quite clear that stochastic models are well suited to study such processes. Stochastic models are also very well suited to study the change of the transmembrane potential resulting from the flow of ions through channels in the cell membrane. Both these processes are of fundamental importance in understanding the function of excitable cells. In both applications, ions flow from one domain to another according to electrochemical gradients, depending on whether the channel is in a conducting or nonconducting mode. The state of the channel is described by a Markov model, which is a wonderful tool used to systematically represent how an ion channel or a receptor opens or closes based on the surrounding conditions. In this context, the contribution of the papers listed above is to present a systematic way of analyzing the stochastic models in terms of formulating deterministic differential equations describing the probability density distributions of the states of the Markov models. As pointed out in the papers by Smith et al., this approach is not really new; the authors cite a number of earlier papers and we have been quite influenced by the paper of Nykamp and Tranchina [63] because of its elegant way of developing the deterministic differential equation describing the probability density functions of the states involved in the stochastic process. The key observation is that we can study stochastic release in two fundamentally different ways: (1) We can run a number of simulations using a stochastic model. Because of the stochastic state"
223,186,0.987,Knowledge and Action (Volume 9.0),"For Freud what followed from these observations was the conclusion that one ought to abandon this method of dream interpretation as lacking any substance. But Freud did not. After all, the knowledge does not really hide from the observer. One has only to search for it persistently. âIt is very probable, then, that the dreamer knows about his dream; the only question is how to make it possible for him to discover his knowledge and communicate it to usâ (p. 104). Hayek, confronted with a similar dilemma, decided, just like Freud, to ignore it. In his essay entitled âThe Creative Powers of a Free Civilizationâ (1960/1978), in which the lack of knowledge is a question of the distribution of knowledge in markets, Hayek first noted that any progress in civilization is the result of an increase of knowledge. In the real world, according to Hayek (1960/1978), it simultaneously holds true that âthe individual benefits from more knowledge than he is aware ofâ"
278,13,0.987,Taking Stock of Industrial Ecology,"There is a body of literature exploring this interpretation and the related question of how happiness and quality of life can best be promoted. Although this question is only briefly touched upon in the chapters in this book, the point clearly articulated by the Brandt Commission underlies many of the chapters. Jackson (2010) has offered one of the most succinct definitions of sustainability: Sustainability is the art of living well, within the ecological limits of a finite planet."
249,405,0.987,Advances in Proof-Theoretic Semantics (Volume 43.0),"meaning forced on the propositions A and B by the inference of D is this knowledge about A and B given by the knowledge about what kind of step the inference of D is. Hence the meaning of the meaning forced on the proposition, by the steps of the argument depends on what is considered to be known, when knowing only what kind of steps the steps of the argument are. In the previous section, âa self-contradictory argumentâ was explained to be an argument in which there is a proposition which is used in two or more ways such that not all of the ways of using the proposition are compatible. In this section âthe meaning forced on a proposition, by the steps of the argumentâ expresses precisely the way in which the proposition is used in the argument. Hence, we can explain what âa self-contradictory argumentâ is by saying that it is an argument such that the steps of the argument force several meanings on one of the propositions of the argument and that not all of these meanings are compatible. Yet another way to put this is to say that an argument is self-contradictory if and only if the steps of the argument force an ambiguous meaning on one of the propositions of the argument. Note that, as is clear from the example above, the meaning forced on a proposition by an argument is not an interpretation of the proposition but a constraint on how it may be interpreted. Now we change to how to formally express âa self-contradictory argument.â Let us by the meaning of a proposition mean an interpretation of the proposition. For instance, the wind is blowing is the meaning of the proposition B in the example above. Let A be a formula occurrence in a deduction in some formal system. To denote that A has a certain meaning, m say, we decorate A with m. More precisely, we shall write m : A to denote that A has the meaning m. We use these decorations to define meaning conditions. Meaning conditions are formal representations of the constraints given by the meaning forced on a proposition by an argument. For every formal system considered in this article we shall do the following. We shall define what the set of formal meanings is for decorating the formulas in deductions in the formal system and we shall give the meaning conditions associated with the formal system. Thus, through the meaning conditions we formally define what is informally described by âthe way in which a proposition is used in an argument.â By an assignment of meanings to the formulas in a deduction we mean a decoration of all of the formulas in the deduction. That a meaning is assigned to a formula means that the formula has been decorated with the meaning. The meaning conditions are given as constraints on the decorations, by formal meanings, of the formulas in the deductions. As an example let us consider, in the formal system N, a deduction consisting of an âE inference, Î± say. Let X , Y and Z be the major premise, the minor premise and the conclusion, respectively, of Î±. Let m x , m y and m z denote some meanings assigned to X , Y and Z , respectively. We decorate the formulas in the deduction as follows. my : Y mx : X mz : Z"
232,155,0.987,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"allowances for a psychological trait, which has been conï¬rmed by a large body of experimental study: people overestimate low probabilities and underestimate high probabilities. Another anomaly well known to economists is ambiguity aversion. This characteristic was suggested by Keynes and latter demonstrated by Ellsberg [8] in the form of a paradox. In his treatise on probabilities Keynes [9] posited that greater weight is given to a probability that is certain to one that is imprecise. Ellsberg has shown that just as there is a premium for taking risks, some compensation must be awarded to individuals for them to be indifferent to gain (or lose) with a one-in-two probability or an unknown probability with an expected value of one-in-two. Recent developments in economic theory offer several solutions to tackle this problem, in particular by specifying new types of utility functions, yet again [10]. What is important to keep in mind here is that individuals usually prefer the exposure to a hazard associated with a clearly deï¬ned probabilityâbecause experts are in agreementârather than the exposure to a hazard characterized by uncertain or fuzzy probabilitiesâbecause experts may disagree. Putting it another way, in the second instance people side with the expert predicting the worst-case scenario. More recently, Kahnemanâs work followed on that of Bernouilli, Allais and Ellsberg. He and his fellow author, Tversky, introduced loss-aversion: individual are more affected by loss than gain [11]. Kahneman also diverged from his predecessors in adopting a more positive approach. Observing the distortion of probabilities is a way to understand how our brain works rather than to build a theory where the decision-maker optimizes or maximizes the outcome. Kahnemanâs line of research is comparable to subjecting participants to optical illusions to gain a better understanding how our brain functions. For example, a 0.0001 probability of loss will be perceived as lower than a 1/10,000 probability. Our brain seems to be misled by the presentation of ï¬gures, much as our eyes are confused by an optical effect which distorts an objectâs size or perspective. This bias seems to suggest than our brain takes a short cut and disregards the denominator, focusing only on the numerator."
359,236,0.987,"Micro-, Meso- and Macro-Dynamics of the Brain","The answers to these questions came by positing that a foundational computation the brain performs is classification.12 Hayek described types of classification of increasing complexity (Fig. 8). Simple classification is the sorting of externally different objects into one of a set of different classes by virtue of their differing effects. One example of this is a machine that sorts balls of even diameter into a bin marked A and balls of an odd diameter into a bin marked B. The machine is said to have classified each ball into either group A or B. Simple classification of this sort can describe simple reflexes, which act to group external stimuli by the behaviors that are produced, often by a chain of very few neurons. Hierarchical classification13 occurs when successive acts of classification occur in successive stages. In this way, the groupings that occur in a previous stage become the objects to be grouped in the next stage. Multiple classification allows for stimuli to be in multiple groups at once and also for multiple stimuli to be classified differently than when they occur individually.14 It is this classification, carried out by the activity of postsynaptic neurons (as a function of presynaptic activity and the structure of anatomical connections), that builds up a system of relations. Here, we already see a conceptual overlap with some modern ideas. For instance, BuzsaÌkiâs (2010) reader concept is a framework for defining cell assemblies by virtue of postsynaptic effects (e.g., by collective effects on reader neurons). Similarly, an important aspect of integrated information theory, which will be discussed more below, is the defining of causal groups as differences that make a difference, in other words, defined by their causal postsynaptic effects (Oizumi et al. 2014). There are even mathematical theories of computation in dynamical systems, which have not been created or even used in thinking about neural systems, that use the same conceptual idea, such as epsilon machine reconstruction (Crutchfield 1994), and could potentially be used to analyze network function."
223,243,0.987,Knowledge and Action (Volume 9.0),"processes work, it makes sense to look at the two styles of thinking separately to gain an understanding of their interaction and of their actual effect on behavior. In this chapter we seek to illuminate the characteristics of these two processes, show their interactions with each another, and point out their common effect on behavior. To do so, it is first necessary to evaluate each system independently, examining historical and current perspectives on reflective and impulsive styles of thought. Thereafter, we present an integrative model of thinking and action in an endeavor to identify when which system of thought will be active and under what circumstances it will influence behavior."
82,49,0.987,Fading Foundations : Probability and The Regress Problem,"Do we have a finite mind? This is not so clear. We have finite brains, and minds supervene on brains, but does that mean that our mind is finite? What exactly does it mean to have a finite mind? That we cannot have an infinite number of beliefs? But how to count? Moreover, even if we have a finite mind in the sense that our beliefs are finite and therefore countable, this does not prevent us from saying many cogent things about infinities â how is that possible? The routine manner in which epistemologists have rejected infinite justificatory chains is reminiscent of the customary ways in which infinite causal chains have been cast aside. Again, Aristotle appears to have played a major roÌle here. His familiar arguments against infinite causal chains in his Physics and Metaphysics became a well-entrenched part of the philosophical canon. Yet Aquinas and other mediaeval scholars had already pointed out that Aristotleâs arguments may be more restricted than they appear: not every causal regress seems to be vicious, it all depends on what is meant by âcausal connectionâ. So let us take a closer look at Aristotleâs objection to causal regresses and the criticism thereof by the mediaeval schoolmen. This might help us to see why exactly it is that justificatory regresses have been rejected without much ado, and to assess whether such a hasty rejection is appropriate. In Chapter 8, in the final section, we will discuss causal chains in a more modern setting, namely that of causal graphs. Aristotleâs main argument against a causal regress is that it purports to explain a phenomenon, but in fact fails to do so. Suppose an event, an object, or a process A is explained by saying that it is caused by B, and B is causally explained by pointing to C, and so on. If this series were to go on indefinitely, it would remain unclear why A occurred in the first place. The only way to explain the occurrence of A is to refer to a principal or first cause, i.e. something that causes all the other elements in the series, but is itself uncaused. Aristotle stresses that his argument is not confined to a particular kind of causation, but applies to any of the four different causes that he distinguishes, i.e. material, efficient, final or formal: Evidently there is a first principle, and the causes of things are neither an infinite series nor infinitely various in kind. For, on the one hand, one thing cannot proceed from another, as from matter, ad infinitum . . . nor on the other hand 32 Fumerton 2001, 7. We will say more about the conceptual objections to infinitism"
124,410,0.987,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"3 Example: Vase (One Agent) We begin with a very simple example containing just a single agent a. Agent a can move a certain (precious) vase between indoors and outdoors. An element of indeterminism is introduced by allowing that it might be raining or not raining in any state, which is something that is outside the control of the agent a. Further, for the sake of an example, suppose it is forbidden, illegal, wrong for the vase to be outside in the rain. Let state atoms in represent that the vase is indoors, rain that it is raining, and red that the state is forbidden/illegal. out is shorthand for Â¬in; green is shorthand for Â¬red. Figure 1 shows a fragment of a transition system modelling this example, depicting the transitions from state s0 (inâ§Â¬rain). The labels green(a) and red(a) on transitions will be explained presently. Figure 2 shows the transitions from state s1 (out â§Â¬rain). They are shown in a separate diagram simply to reduce clutter. Not shown in the diagrams are the transitions from the other two states in the model, where it is raining. I have deliberately not included any transition atoms to name the actions by a. A perceived advantage of the stit treatment of action is that we are not forced to say exactly what action is performed by a when the vase is moved or left where it is. We need only say (in the example as I am thinking of it) that, whatever these actions are, the actions by a are the same in the two transitions Ï0 and Ï0â² (Ï0 â¤a"
360,460,0.987,Compositionality and Concepts in Linguistics and Psychology,"We call cases like (3-a) conceptually afforded. In these cases, some component(s) of the concepts contributed by two expressions in a phrase match in a way that indicates how they should be composed, and interlocutors avail themselves of such a suggestion. This matching invites the hearer to identify red as the color of the box in (3-a).3 In contrast, in referentially afforded cases like (3-b), specific, independently available information about the referent described by the phrase is used to guide the way in which the concepts in question are composed. This paper has three goals. First we develop this distinction, which has a precedent in Asher (2011), in an explicit manner and support it with empirical evidence we gathered in previous work. Second, we suggest modeling conceptually-afforded concept composition via (compositional) distributional semantics, which represents meaning as a function of the contexts in which words and phrases appear in naturally occurring language data, usually a large text corpus (Landauer and Dumais 1997; Turney and Pantel 2010). We consider this way of modeling concepts to be similar in some of its basic properties to the view of concepts espoused, for example, in Barsalou (2017). A fundamental hypothesis of some work in distributional semantics (e.g. Lenci 2008) is that the resulting semantic representations can be used to model the concepts associated with words. For this reason, we will present a brief introduction to distributional methods in Sect. 4. Finally, we propose a way to formally distinguish the two kinds of concept composition and integrate them into a more general framework for semantic analysis."
306,5,0.987,"Early Geometrical Thinking in the Environment of Patterns, Mosaics and Isometries","Another area of research has been the understanding of three-dimensional ï¬gures by children. It has been associated with criticism of the fact that dealing with solids takes place on higher levels of learning geometry. This goes against the natural way of discovering the world by children, which is made up of three-dimensional objects. Much of the space in this survey has been dedicated to research related to regularities in geometry. This seems reasonable, as so far patterning in geometry has been treated superï¬cially, mainly to determine whether a child can note suggested regularity. Researchers are of the opinion that the functioning of children in a world of regularity is important for not only their general mathematical development but also their geometrical. In the research on childrenâs understanding of regularity, one can distinguish a number of issuesâfrom the very fact of their perception of regularity by creating arrangements of surfaces to geometrical relations hidden in mosaics. Following Steen (1990), mathematics is actually seen as a âsearch for patternsâ: âIt is natural to try to ï¬nd the most effective ways to visualize these patterns and to learn to use visualization creatively as a tool for understandingâ (p. 3)."
232,258,0.987,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"The Chernobyl accident can be compared to the Holocaust or Hiroshima in the sense that it marked the start of a new world, âThe world of Chernobyl, the product of technoscience, appears to have made all cultural resources mobilized in such situations obsolete: it becomes impossible to use such a system of representation, analogy or experience to understand this world that has become so alien to mankind, denatured and unrecognizable, while at the same time, familiarâ [26]. The use of color quickly proved decisive in circumventing the problem of the âimpossible representationâ. In Mount Fuji in Red, the explosion of plants produces red and purple clouds of radioactive particles. A survivor explains this strange phenomenon, âHuman stupidity is boundless. Radioactivity is invisible, while technology has been developed that colors it when it spreads in the airâ [31]. In this case, the discourse links the nuclear industry to an industry that is as dangerous as it is counterproductive. The use of color is seen again in the ï¬lm The Land of Hope [33]. At the same time as an accident occurs in the Nagashima plant (a contraction"
175,683,0.987,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","just model the N-year output series that results from simulation of the historical series? The answer seems to depend upon how well behaved the input and output series are. If the simulation model is linear, it does not make much difference. If the simulation model were highly nonlinear, then modeling the input series would appear to be advisable. Or if one is developing reservoir operating policies, there is a tendency to make a policy sufï¬ciently complex that it deals very well with the few droughts in the historical record but at the same time giving a false sense of security and likely misrepresenting the probability of system performance failures. Another situation where stochastic data-generating models are useful is when one wants to understand the impact on system performance estimates of the parameter uncertainty stemming from short historical records. In that case, parameter uncertainty can be incorporated into streamflow generating models so that the generated sequences reflect both the variability that one would expect in flows over time as well as the uncertainty of the parameter values of the models that describe that variability (Valdes et al. 1977; Stedinger and Taylor 1982a, b; Stedinger Pei and Cohn 1985; Vogel and Stedinger 1988). If one decides to use a stochastic data generator, the challenge is to use a model that appropriately describes the important relationships, but does not attempt to reproduce more relationships than are justiï¬ed or that can be estimated with available data sets. Two basic techniques are used for streamflow generation. If the streamflow population can be described by a stationary stochastic process, a process whose parameters do not change over time, and if a long historical streamflow record exists, then a stationary stochastic streamflow model may be ï¬t to the historical flows. This statistical model can then generate synthetic sequences that describe selected characteristics of the historical flows. Several such models are discussed below. The assumption of stationarity is not always plausible, particularly in river basins that have experienced marked changes in runoff"
124,290,0.987,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"be a kind of action even though it also involves not performing an action. Another area where the framework of FF is helpful is in spelling out the different possible meanings of the much discussed expression âcould have done otherwiseâ in the free will literature (pp. 255â270). Belnap at al. show how certain puzzles in the literature concerning the relation of moral responsibility to the ability to do otherwise can be illuminated by distinguishing these different meanings of the ability to do otherwise. Their framework also helps to clarify and formalize the important distinction between so-called âsoft factsâ and âhard factsâ about the past, a distinction that plays a role in many debates about free will and determinism, but is not always carefully defined (pp. 145â174). In these ways and in others, philosophers who deal with the theory of action and free will in more informal ways have much to learn from the formal framework developed by Belnap at al. in this book."
72,273,0.987,New Frontiers in Social Innovation Research,"âjudgmentalâ, task (see Laughlin and Adamopoulos, 1982). Typically, the former condition holds in fields like mathematics where it is possible to distinguish clearly between a correct and a wrong answer, and there is a common praxis shared by those working in the field for arriving at problem solutions. However, this may not be the case when dealing with social problems where the difference between right and wrong may be based on value judgements not shared by all involved, and where there is a lot of uncertainty surrounding what is known about a problem (Funtowicz, 1993). Collective intelligence becomes increasingly difficult to employ when incorporating knowledge from different academic disciplines or non-scientific knowledge based in traditional cultures (Berkes, 2008) or unarticulated lay practices. It is almost impossible when the knowledge that one party professes to possess is dismissed as worthless by other parties, such as is common in highly politicised or value-laden debates (Head, 2008). Social innovation meets neither of these conditions. It is complex, with high coordination requirements, and requires judgmental evaluations. As such, it is tempting to say that social innovation is simply not a good arena to use collective intelligence. However, a deeper look at how social innovation happens makes this conclusion appear less certain."
217,5,0.987,Finite Difference Computing With Pdes : a Modern Software Approach,"array operations, where each operation is trivial to parallelize efficiently, rather than trying to develop a âsmartâ overall parallelization strategy. Analysis via exact solutions of discrete equations Traditional asymptotic analysis of errors is important for verification of code using convergence rates, but gives a limited understanding of how and why a correctly implemented numerical method may give non-physical results. By developing exact solutions, usually based on Fourier methods, of the discrete equations, one can obtain a physical understanding of the behavior of a numerical method. This approach is favored for analysis of methods in this book. Code-inspired mathematical notation Our primary aim is to have a clean and easy-to-read computer code, and we want a close one-to-one relationship between the computer code and mathematical description of the algorithm. This principle calls for a mathematical notation that is governed by the natural notation in the computer code. The unknown is mostly called u, but the meaning of the symbol u in the mathematical description changes as we go from the exact solution fulfilling the differential equation to the symbol u that is naturally used for the associated data structure in the code. Limited scope The aim of this book is not to give an overview of a lot of methods for a wide range of mathematical models. Such information can be found in numerous existing, more advanced books. The aim is rather to introduce basic concepts and a thorough understanding of how to think about computing with finite difference methods. We therefore go in depth with only the most fundamental methods and equations. However, we have a multi-disciplinary scope and address the interplay of mathematics, numerics, computer science, and physics. Focus on wave phenomena Most books on finite difference methods, or books on theory with computer examples, have their emphasis on diffusion phenomena. Half of this book (Chap. 1, 2, and Appendix C) is devoted to wave phenomena. Extended material on this topic is not so easy find in the literature, so the book should be a valuable contribution in this respect. Wave phenomena is also a good topic in general for choosing the finite difference method over other discretization methods since one quickly needs fine resolution over the entire mesh and uniform meshes are most natural. Instead of introducing the finite difference method for diffusion problems, where one soon ends up with matrix systems, we do the introduction in a wave phenomena setting where explicit schemes are most relevant. This slows down the learning curve since we can introduce a lot of theory for differences and for software aspects in a context with simple, explicit stencils for updating the solution. Independent chapters Most book authors are careful with avoiding repetitions of material. The chapters in this book, however, contain some overlap, because we want the chapters to appear meaningful on their own. Modern publishing technology makes it easy to take selected chapters from different books to make a new book tailored to a specific course. The more a chapter builds on details in other chapters, the more difficult it is to reuse chapters in new contexts. Also, most readers find it"
289,1354,0.987,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","where the computational order is T1 â T2 = T1+ â T2+ â§ T1Ï â T2Ï . Figure 2 illustrates the first fixpoint iterates. The fixpoint iteration starts from the set of all infinite sequences of program states. At each iteration, the final program states in Î© are added to the set, and sequences already in the set are extended by prepending transitions to them. In this way, we add increasingly longer finite traces, and we remove infinite sequences of states with increasingly longer prefixes not forming traces. In particular, the i-th iterate builds all finite traces of length less than or equal to i, and selects all infinite sequences whose prefixes of length i form traces. At the limit we obtain all infinite traces and all finite traces that terminate in a final state in Î©. Note that Î is suffix-closed. The trace semantics Î fully describes the behavior of a program. However, to reason about a particular property of a program, it is not necessary to consider all aspects of its behavior. In fact, reasoning is facilitated by the design of a semantics that abstracts away from irrelevant details about program executions. In the next sections, we define our property of interest and use abstract interpretation [14] to systematically derive, by successive abstractions of the trace semantics, a semantics that precisely captures such a property."
217,206,0.987,Finite Difference Computing With Pdes : a Modern Software Approach,"Dynamic physical sketches, coupled to the numerical solution of differential equations, requires a program to produce a sketch for the situation at each time level. Pysketcher18 is such a tool. In fact (and not surprising!) Fig. 1.20 and 1.21 were drawn using Pysketcher. The details of the drawings are explained in the Pysketcher tutorial19 . Here, we outline how this type of sketch can be used to create an animated free body diagram during the motion of a pendulum. Pysketcher is actually a layer of useful abstractions on top of standard plotting packages. This means that we in fact apply Matplotlib to make the animated free body diagram, but instead of dealing with a wealth of detailed Matplotlib commands, we can express the drawing in terms of more high-level objects, e.g., objects for the wire, angle , body with mass m, arrows for forces, etc. When the position of these objects are given through variables, we can just couple those variables to the dynamic solution of our ODE and thereby make a unique drawing for each  value in a simulation. Writing the solver Let us start with the most familiar part of the current problem: writing the solver function. We use Odespy for this purpose. We also work with dimensionless equations. Since  can be viewed as dimensionless, we only need to introduce a dimensionless time, here taken as tN D t= L=g. The resulting dimensionless mathematical model for , the dimensionless angular velocity !, the N and the dimensionless drag force DN is then dimensionless wire force S, D Ëj!j!  sin ; d tN D !; d tN SN D ! 2 C cos ; DN D Ëj!j!;"
235,298,0.987,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","In view of the aforementioned incompleteness and independence result one may ask [132, p. 148]: âHow common is incompleteness and unprovability? Is it a very bizarre pathological case, or is it pervasive and quite common?â Indeed, just as the set of computable reals is âmeagreâ in the set of reals, so is the set of provable (by constructive, algorithmic methods) theorems âmeagreâ with respect to all true theorems of mathematics. This has been proven in a topological sense of âmeagreâ in the context of GÃ¶delâTuring type incompleteness [105] (and not in the sense of independence of, say, the continuum hypothesis). Riceâs theorem [432] asserts that all non-trivial, (semantic) functional properties of programs are undecidable. A functional property is one (i) describing how some functions performs in terms of its input/output behavior, and (ii) which is non-trivial in the sense that some (of all perceivable) programs which have this input/output behavior, and other programs which donât. Riceâs theorem can be algorithmically proven by reduction to the halting problem: Suppose there is an algorithm A deciding whether or not any given function or algorithm B has any functional property. Then we can define another program C which first solves the halting problem for some other arbitrary function D, clears the memory, and consecutively executes a program E with has the respective functional property decided by A. As long as D halts, all may go well. But if D does not halt, the program C never clears the memory, and can never execute a program E with the respective functional property. Now, if one inserts C into A, in order to be able to decide (positively) about the functional property of E â and thus of C â A would have to be able to solve the halting problem for D first; a task which is provable impossible by algorithmic means. As Yanofsky observes this bears some similarity to the downward LÃ¶wenheimSkolem [580, footnote 20, p. 375], âstating that if there is a consistent way of using a language [[statements in mathematics . . . written with a finite set of symbols]] to talk about such a system [[with an uncountably infinite number of elements]], then"
134,33,0.987,Curriculum Reform in The European Schools : Towards a 21St Century Vision,"Changing the System We also need to make sense of the notion of change or alteration. Objects and relations between objects, educational systems and people change their form over time. An example of this change process at the epistemological level is the invention (insofar as the set of concepts and relations between them is new) of the notion of probability (cf. Hacking 2005) in the nineteenth century, and this changed the way social objects could be conceived and ultimately arranged. Change can occur in four ways: contingent ontological, planned ontological, epistemically-driven ontological, and in the transitive realm of knowledge, epistemological (cf. Scott 2011). With regards to the example above, the invention of probability, two phases of change can be identified. The first is where knowledge is created and thus operates at the epistemological level, the new arrangement of knowledge. The second is where this knowledge has real effects at the ontological level, so that new arrangements, new formations, new assemblages come into being. The dilemma is that the social world, in"
303,117,0.987,Multiculturalism and Conflict Reconciliation in the Asia-Pacific,"How, then, do socio-political relations constitute the subject? Here, probably, it is important to distinguish self-image from self-identity. Shih (2012) contends that self-identity is about drawing boundaries between the self and others in order to distinguish between them (p. 25). Identity construction is therefore intended to discover something different from the character of everyone else. This becomes a cause of violence, whether physical or discursive. In sum, identity making is a practice of violent âothering.â Imamura (2008) goes even further, arguing that violence is caused by what he calls the âoriginal division,â and this original division resides in the use of the âIâ that draws a boundary between âIâ and âYou.â This original division is inherently violent in the sense that it engenders a distance between entities, and this distance is stabilized and institutionalized through the universalization of specific subjects through a standardized vocabulary (Imamura, 2008, p. 73). Image, by contrast, is about the âevaluationâ of others. In this context, the subject âperforms in accordance with a certain consensually agreed upon role, explicitly as well as implicitly, between one and other who presumably evaluateâ (Shih, 2012, pp. 25â26). Because others are the mirror that reflects the image of the subject, the latter is inevitably involved in relationships with others. While identity is rigid in the sense that it is presumably an a priori construction existing before the formation of relationships, image is, by definition, context-sensitive and, therefore, flexible with respect to the relationship (Shih, 2012, p. 26). In this system of relationality, the subject is always changing and so is the system. Thus, âorderâ means the continuous transformation of subjectivity and relationality. There is no pre-given order or norms, but instead an interminable flow of relations. A reification of this system in IR is Chinaâs tributary trading system. Hamashita (1990) defines the tributary system as an always-changing system based on trade relations, which is inclusive of different elements. This inclusivity emanated from the core of the system, which actually had a relatively weak centripetal force (Hamashita, 1990, pp. 32â33). All relationships among member countries were bilateral rather than multilateral, so that no member faced exclusion as a result of the violation of universal norms and regulations, simply because there was no such thing. Rather, all bilateral relationships were dealt with on a case-by-case basis (Shih, 2013) and this resulted in constant systemic transformations. Since this system was not constructed on a foundation of strong centripetal power, unlike the hegemony generally familiar to the contemporary IR audience, all members in the tribute system were allowed to have their own âcentresâ (Hamashita, 2003, p. 20)."
21,174,0.987,intertwingled : The Work and influence of Ted Nelson,"Naturally having more to choose from is better than having less. But such a capability is meaningless if the inconvenience of finding and collecting candidates, for repeated side-by-side comparison, is too high. As the saying goes, âIn theory, theory is sufficient; in practice it isnât.â Thus, the efficiency of every step of the transclusion ecosystem is critical to the degree transcludable material will be used. Factors such as how well collections of material are organized will likely dramatically affect what actually happens, no matter the richness of the possibilities. Indeed, building on the well-known expression: If it isnât written, it doesnât exist. We might add: If itâs not in the library, it doesnât exist. (and) If itâs in the library, but is too hard to find, it doesnât exist. But in addition to accessibility is the issue of just what can be done with transcludable material. Since the purpose of most knowledge artifacts is to contribute something original, simply reusing existing material, as is, may not provide value sufficient for the task at hand. Thus expanding the set of transformations that can be performed on the transcluded material beyond exact copyingâis a way of expanding the design space for transclusion. This in turn admits for higher âvalueâ peaks that greatly increase the ROI of effort invested."
271,505,0.987,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"deeper insights into the nature of the communicative and mnemonic practices. In an ideal scenario, participants will also record conversations at family gatherings or at the dinner table, similar to the data analyzed by Keppler (1994). Data gathered in this way will be analyzed by drawing on a conversation analysis (Keppler 1994; Bergmann and Luckmann 1995). What becomes apparent with this methodology is that by focusing on actors and practices, we are required to take a cross-media approach, as these are the circumstances in which the communicative construction of memories takes place. Moreover, focusing on actors and thereby taking a people-centred perspective assists us in understanding the figuration and its limits. This type of methodology serves to understand in greater depth the various aspects of research participantsâ lifeworld experiences when it comes to family memory."
333,21,0.987,Uses of Technology in Upper Secondary Mathematics Education,"classroom group. As individuals, they have âfelt their way aroundâ the Cartesian space, searching for points that meet the equidistant criterion. On ï¬nding one, they recognize an isosceles triangle and experience a particular sensation of symmetry. However, based on their own point-based explorations, they can see each of the points in the shared space as a solution to a local problem. This supports a deep and flexible way of thinking about the locus of points and the perpendicular bisector, which has value beyond that which would be gained from the individual experience of a dynamic geometry environment alone."
249,492,0.987,Advances in Proof-Theoretic Semantics (Volume 43.0),"(Footnote 1 continued) in different ways. This distinction is analogous to that between assumptions and assertion, so that, when we prove an implication, we can at the same time regard this proof as a proof of the upper member of this implication from its lower members taken as assumptions. In the Grundgesetze Frege [21] even specifies rules of proofs in terms of this second-layer distinction. This means that he himself goes beyond his own idea of a single-layer system. See Schroeder-Heister [64]."
253,1096,0.987,"Autonomous Driving : Technical, Legal and Social Aspects","29.2.1.2 Dimensions of Acceptance Attitudes dimension Attitudes regarding acceptance that can be surveyed include mindsets, values, and judgments. These can be polled and interpreted on both individual and societal levels. Attitudes are signiï¬cant for acceptance research, as it is assumed that they can be read as willingness and intent for concrete actions ([13]: 82 f). Nevertheless, questions on the genesis of technology, its speciï¬c usage, the associated challenges, and frameworksâall in their speciï¬c contextsâcannot be captured with such measuring of attitudes ([11]: 46). A typical measuring instrument of the attitudes dimension of acceptance is the opinion pollâeven though such surveys quickly lead âto a simpliï¬ed picture of an opinionforming process based on the perceived properties of technologyâ ([14]: 35, translation by the authors). This is because they imply that technology sends out signals that spark off set reactions in the population or individuals. One-dimensional surveying of attitudes has been replaced in recent years, however, with greater insights into technology acceptance, and expanded into an analysis that incorporates attitudesâ contexts in particular. In this way, the focus of acceptance research shifted from the âdescriptive inventory of attitudes and actionsâ ([14]: 36, translation by the authors) to a more analytically aligned perspective. This takes greater account of the complexity in individualsâ perceptions and evaluations, expertsâ subjectivity, and the signiï¬cance of contextuality ([14]). Actions dimension The actions dimension of acceptance describes observable behavior, although acting in this sense may relate either to doing something or to refraining from it. Actions can manifest themselves in many ways, for instance in purchasing, using, and spreading (or the opposite, e.g. initiating protests), or in supporting other (decision-making and planning) activities. The dimension of actions is often equated with that of acceptance, as found in Lucke, for example (see [13]: 82). On the other hand, other authors do not view action, or a concrete intention to act, as imperative for acceptance ([15]: 19, [16]: 11). Schweizer-Ries et al. ([16]: 11) have depicted this reciprocity between the dimensions of actions and attitudes in a two-dimensional model (Fig. 29.2). Values dimension In many approaches, the values dimension is not viewed as a separate level of acceptance, but combined with the attitudes dimension. Values and norms, according to this argument, are also the basis of attitudes and therefore can only be separated from them with difï¬culty. The dimension of values comes into its own, however, when acceptance is visible on the level of actions, for instance in the use of a speciï¬c product. These actions may only accord with subjective individual values slightly or not at allâa person can own and use a car while being strongly ecologically-minded. This, in turn, may show itself more"
223,106,0.987,Knowledge and Action (Volume 9.0),"To the budding geographer the time-bound message could not be misunderstood: Capture the power of social relations in a net of scientific laws and then, like your friends in physics, chemistry, and medicine, you too will have acquired the means not merely for understanding the world but for changing it as well. If the natural scientists know how to construct rockets that take them to the moon, if they know how to generate energy by enriching uranium, if they know how to save lives by transplanting hearts, then your duty as a social scientist is to discover similar techniques for eradicating poverty! Before you accept that challenge, though, be sure to ask yourself first why no one now reads Plato and Aristotle for what they had to say about physics or medicine, then why so many continue to return to the plays of Sophocles and Shakespeare for their insights into the human condition of hopes and fears, love and hate. How does the circumstance that we have accumulated knowledge in some areas and not in others relate to Aristotleâs remark that we should look for precision in each class of things just as far as the nature of the subject admits and that it would be equally foolish to accept probable reasoning from a mathematician as to demand scientific proofs from a rhetorician? No small order given to a young man inexperienced in the actions that occur in life and therefore prone to pursue each object as passion directs. And yet, how could I possibly have ignored the challenge? GO ON, GO ON. The list of required readings included the classics of location theory, cognitive science, decision theory, systems analysis, matrix algebra, probability theory, spatial statistics, and a sprinkle of historical geography, all of it somehow yoked together in Walter Isardâs conception of Regional Science and its extension into Peace Science, the latter firmly anchored in the Quaker-thin interface of scientific knowledge and political action, John Deweyâs pragmatism and the collections of the Barnes Foundation never far away. A formative experience it was, the handsome fellowship that in 1963â1964 took me to North Armorica and the intellectual hubs of the Wharton School, Berkeley, and Northwestern. Great. Yet, in hindsight, the seeds might well have been planted 10 years earlier by the odd gymnasium teacher who did whatever he could to introduce his rowdy pupils to the concentric rings of von ThÃ¼nenâs isolated state and the cost curves of Alfred Weberâs isodapanes. Perhaps I was the only one to pay attention, but the truth is that I can still feel in my body the boyâs excitement when he literally saw why there were so many gauchos on the Pampas and so many steel mills along the Ruhr. The rhetorical power of geometric construction on the high wire, von ThÃ¼nenâs agricultural landscape depicted as an archerâs target with the bullâs eye as the central city on a homogeneous plain (more correctly the Junkerâs own estate), Weberâs factory finding its place of least cost, the Archimedean point that is located at the center of a Euclidean triangle whose corners are the concepts of transportation, labor, and agglomeration. Deep roots it has, the subsequent definition of geography as a geometry with names, essentially an exercise in the drawing and baptizing of points, lines, and planes. Picture and story merged into one."
249,143,0.987,Advances in Proof-Theoretic Semantics (Volume 43.0),"6 Conclusions and Further Work In this paper we have argued for two central claims: (1) that the apparent consensus that the Kreisel-Goodman paradox is engendered by the adoption of Kreiselâs second clause interpretations of â, Â¬ and â is mistaken; and (2) that the ability of a formal system to internalize reasoning about its own proofs plays a larger role in the paradox than is customarily acknowledged. Taken in conjunction, these observations point towards the possibility of responding to the paradox by developing a system which retains as many of the features of the unstratified theory T + as possible while seeking a conceptually motivated means of limiting the scope of the internalization principle The evident question is what form such a delimitation might take. Taken together with the observations we have recorded about the role of free variables and reflection principles in the paradox, one obvious proposal would be to consider subsystems of formalisms similar to QLP in which the scope of Lift is limited by the exclusion of quantifier or substitution rules akin to Eug. Although such a proposal may be justifiable in terms of Kreisel and Goodmanâs original foundational goals, a variety of questions remain open: (i) is a consistency proof similar to that described by Goodman [16] available for an appropriate subsystem of T + ? (ii) is it possible to prove the soundness and completeness of HPC in the sense of Val for such a system? (iii) are the second clause interpretations of the intuitionistic connectives required for such a result? (iv) is it possible to formulate a version of Goodmanâs interpretation of Heyting arithmetic relative to the relevant system? Needless to say, these questions will have to wait for another occasion."
124,206,0.987,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"Each agent will correspond to a unique set of partitions of histories at choice points, and along any history, there will be a particular set of choices the agent makes and without which the agentâs life would not have followed that history. So from the point of view of that history, it will seem that the agentâs choices will have accumulated to make the agent the individual she has become. From this point of view, a simple version of existentialism seems vindicated. But it is more complex than that, because the choices of others also influence the history one takes, and therefore the subsequent choices with which one is faced. In the end, then, the individual one becomes is profoundly influenced by the choices of other agents, as well as by her own, and this is where the simple existentialist picture fails. And there is another layer of complexity added when we look beyond a single history. The totality of the agentâs choices throughout the Belnapian world is also uniquely associated with that agent, and reflective not merely of what the agent does become (along this history or that) but also of the agentâs potential, which we may consider is equally essential to their identity. At this scale we begin to see the agent as a unique collection of possibilities for action. When we survey this larger picture, balancing the agentâs tree of possibilities against the developments of those possibilities along a given history, we find a new perspective on the old nature/nurture debate: we need not choose between nature and nurture as the sources of oneâs character, and we must indeed add a third factorâwill. Nature has its role in providing our potential as seen in the treewide totality of the choices available to us; will has its role through our choices; nurture has its role in the choices of others which influence and limit our choices. Together they work, along any given history, to create a uniquely matured version of the agent, different from what they could have become had they not had that potential, different from what they would have become had they made different choices, and different from what they would have become had others chosen differently, as well. Another aspect of the focus on agency rather than on agents, as we have begun to see, is that there is nothing in the models for such systems that rules out the possibility that a given agent has been making choices throughout time, and will continue to make choices throughout time along each history. This is, of course out of keeping with the fact that the logic is intended to reflect the situation of mortal agents and agents who are not perpetually active. If we undertake the task of constructing a quantified logic of action we can expect to be able to remedy this situation, perhaps by introducing a distinguished predicate is alive into the language and making appropriate provisions for its interpretation in our models. The many quandaries associated with quantified alethic modal logic tend to make us shy away from a quantified logic of action (which could hardly present fewer such challenges) at least for the moment. But let us contemplate a slightly enhanced propositional logic of action and use this to get at least a preliminary look at some of the challenges involved in taking account of the finitude of agents. Suppose that in addition to the other more or less standard components of models for our logic of action we include a function Q (for quick, in the sense of alive)"
82,404,0.987,Fading Foundations : Probability and The Regress Problem,"Indeed this does not depend on m at all, so the number of links may be finite, or infinite, with no change in the value of P(q). It will be recognized that this value is precisely the same as that for the infinite, uniform chain (see Section 3.7). So much for the usual class. What of the exceptional class, in which the infinite product of the Î³ âs is not zero? As we have seen, here the chain fails, in the infinite limit, to produce a definite answer for the target probability. The infinite loop on the other hand yields a unique value. To illustrate this, consider again the example (3.25):"
8,468,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","It is not clear whether Eq. (19.63), which puts a condition on the mass spectrum, is compatible with the fact that the mass spectrum should follow from the theory itself. It could be that this leads to an interesting self-consistency problem with further consequences (a kind of âbootstrapâ at high temperature). In any case, we see that the introduction of a mass spectrum .m ; T/ may resolve the apparent difficulty in reconciling the numerical value of T? , as found experimentally, with the requirement m Â¤ 0 (and not too small). We may presently at least hope that the value of the integral in Eq. (19.63) is near to one and consequently neither T? nor V0 have to have unreasonable values."
214,394,0.987,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"can illustrate the different possible climate outcomes (i.e., the different simulated changes in global average surface temperature) with a single scenario. Ensembles can be built the same way with other scenarios and compared, so that the different contributions to initial condition uncertainty (variations within a single scenario and model but with different initial conditions), model uncertainty (variations within a single scenario using different models), and scenario uncertainty (either of the ï¬rst two single-scenario ensembles with single or multiple models, but now for multiple scenarios) can all be assessed, and to some extent quantiï¬ed. There is much discussion about how to evaluate and weight models in a group: If a model resembles observations of the present, is it better and should it be given more weight14? If a model does not meet certain tests or standards, or has known problems, should it not be analyzed? In general, one problem with weighting models is that present performance on one metric in the past is not necessarily a good indicator of skill for the future. It is the same as the statement at the bottom of reports on the history of a ï¬nancial investment: âpast performance may not be an indicator of future returns.â Models with fundamental flaws are sometimes removed from analyses or given less weight. However, many times these âflawsâ are designed to constrain the models in some way, and are often justiï¬able. There is still much debate in the scientiï¬c community over how exactly to construct weighted averages across ensembles with multiple models."
360,102,0.987,Compositionality and Concepts in Linguistics and Psychology,"are identifying some mental representation (a percept?) of some object taken to be in the corner, and are classifying it as falling under the concept CHAIR? Given all this, it certainly seems obvious that there is some relationship or other between concepts and meaning. Objectivism denies this, and is deficient for this reason, the Subjectivist says. The intuitive and natural manner to characterize meaning in a Subjectivist framework is to say that the basic way for mental concepts to operate is by representing the worldâat least the world as seen from the point of view of the holder or owner of those concepts. A linguistic item then will designate or mean or be brought forth by (or will bring forth) the relevant mental concept of the language user, and will thereby represent the relevant aspect of the world. This is a type of âtwo-levelâ or âdual aspectâ or âtwo-tierâ semantics, and we will canvass that sort of response in the section below (Sect. 7). For now we just remark that the sort of two-tier theory being alluded to also has analogies with theories mentioned in McNally and Boleda (2017) and Winter (2017), and perhaps shows a remarkable convergence of opinion among theorists with widely different starting points. One formal approach to subjectivist theories of meaning is exemplified by socalled proof-theoretic semantics. Such theories have started by thinking of the meanings of the logical connectives as being given by the ways they can be introduced into a discourse and what sorts of âlanguage movesâ they can justify. For some technical reasons this led to adopting intuitionistic logic as the underlying framework for a theory of the use of such connectives (see especially Dummett 1991). Fuller versions of this idea try to generalize the range of such introduction and exploitation moves that can be given a formal exposition. Most of these theories have their formal roots in the intuitionistic type-theory of Martin-LÃ¶f (1980) and Prawitz (2006); see for example its development in natural language structures by Francez and Dyckhoff (2010); Francez et al. (2010); Francez and Ben-Avi (2015); Francez (2014, 2015). In this approach, meaning is taken to be a certain type of proof, namely a set of canonical derivability conditionsâstated in natural deduction format from which the derivability conditions are based. The idea is that the proof system reflects the âuseâ of the sentences in the linguistic fragment under consideration and thus allows recovery of their entailment and assertability conditions. This formal conception of semantics is opposed to the more usual formal version in linguistics and philosophy of language of describing the meanings as truth-conditions in arbitrary models, which are taken to be ways the world actually is, or might be (the latter for intensional language). In proof theoretic theories there is no notion of âtruthâ or of âmodelling the worldâ. There are only rules for the appropriate use of language. In more informal versions of proof theoretic semantics (Brandom 1994, 2000) this general strategy is called inferentialism and is seen as a âuse-based theory of meaningâ, which employs notions such as âlanguage entry and exit rulesâ, and âlanguage evaluation rulesâ.23 (A nice description of both the formal and informal aspects of the topic is in 23 âUse theories of meaningâ trace their ancestry to ordinary language philosophy, especially the"
107,285,0.987,"Symbiotic interaction : 5Th international Workshop, Symbiotic 2016, Padua, Italy, September 29â30, 2016, Revised Selected Papers","In this experiment we used a quantiï¬ed-self device to provide the robot with the presupâ position of a speciï¬c epistemic authority vis-Ã -vis the participant, and we tested this authority through reï¬exive sequences. Largely, we found (1) that reï¬exivity taken care of by the robot, has an eï¬ect on the participantsâ behavior, as a step into their personal epistemic territory, and (2) that the persons display practices that show the analogous commitment as in human-human interactions regarding the preferential organization of turns-at-talk in terms of adjacency, agreement, and epistemic balance. Even if they are aware of the robotâs limitations, participants display an attention to organize a particiâ pation framework (with rights and obligations), in which the robot is treated like a participant in its own right. Organization is what binds elements, events or individuals in a symbiotic relationship, that is, a potential synergy in which diï¬erent sign systems work together to build relevant action and accomplish consequential meaning. Thereâ fore, the scope of turn-design must not be limited to stream of speech phenomena"
84,447,0.987,Eye Tracking Methodology,"of values into categories, hence they are also known as categorical data (e.g., number of blue-eyed people in the sample population). Ordinal data are measured but only denote the order or position of a data point (e.g., the top five scorers on a midterm exam). Generally speaking, eye movement data are considered parametric because related metrics can be represented by a uniform (equal distance) interval/ratio scale. An interval scale is one composed of equal units, where, for example, the distance from 160 to 165 cm is the same (means the same) as that between 170 and 175 cm. Related to the interval scale is the ratio scale, where the latter is an interval scale with a necessary and absolute zero. Examples of ratio scales include reaction times or distance. Timing starts from zero, and in this scale it makes sense to say that if something completes in half the time of something else, then it is twice as fast. Note that in the above eye movement example, the number of fixations can only be considered on this scale if it makes sense to say that twice as many fixations counted on one ad is in some sense twice as meaningful or valuable as on the other (and zero fixations is also meaningful). Fixations can be interpreted in this way if we consider them as indicators of cognitive load; e.g., devoting twice as many fixations to a region may mean that twice as much cognitive effort is being exerted (an operational assumption). This might not always be a valid interpretation, however. For example, twice as many fixations on some conspicuous portion of the screen (e.g., an ad) may mean the viewer is bored or distracted by something entirely different from the ad content, and hence is not indicative of cognitive load at all (just the opposite!). Thus one must be very careful in considering the type of measurement being recorded, and how the variable is being operationalized."
8,457,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","and since zk ! 1 for Tk ! Tk0 , it follows that here also all N values become equally probable in the limit (in such a way, however, that N ! 5). Although the average number of fireballs is  5, the actual number can therefore hardly be predicted. The introduction of masses will, of course, suppress very large N values. Nevertheless, even then we have to expect enormous fluctuations in the multiplicities of final particles produced in collisions at (fixed!) high energy.10 Although the present treatment of the problem of the angular distribution is certainly not yet fully correct, it may indicate the direction in which one has to go. Nevertheless, we find quite satisfying results: â¢ The transverse momentum distribution is independent of:"
235,249,0.987,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","18.1 Sensitivity to Changes of Initial Value What is presently known as deterministic chaos [387, 458] â a term which is a contradictio in adjecto, an oxymoron of sorts â has a long and intriguing history, not without twists, raptures and surprises [170, 171]. As has been mentioned earlier (see Sect. 17.4 on p. 137) already Maxwell hinted on physical situations in which very tiny variations or disturbances of the state could get attenuated tremendously, resulting in huge variations in the evolution of the system. In an epistemic sense, this might make prediction and forecasting an extremely difficult, if not impossible task. The idea is rather simple: the term âdeterministicâ refers to the state evolution â often a first-order, nonlinear difference equation [360] â which is âdeterministicâ in the sense that the past state determines the future state uniquely. This state evolution is capable of âunfoldingâ the information contained in the initial state. The second term âchaosâ or âchaoticâ refers to a situation in which the algorithmic information of the initial value is ârevealedâ throughout evolution. Thereby, âtrueâ irreducible chaos rests on the assumption of the continuum, and the possibility to âgrabâ or take (supposedly random with probability 1; cf. Sect. A.2 on p. 171) one element from the continuum, and recover the (in the limit algorithmically Â© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_18"
124,189,0.987,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"(pastwards connection) Constraints 1â3 ensure that the moments in a model are organized into trees. Adding constraint 6 would ensure that there is only one such tree per model. We focus first on models which are pastwards connected. One interesting observation is that if, in such models, we take the histories as correlates of possible worlds, we find that to determine whether two individuals appearing in different histories within a single model are or are not identical, we need only trace them back in time to see whether they have a common origin at some moment included in both the histories. So provided we are able to trace identity back through time, we get a natural solution to what would have been the problem of trans-world identity but which is now recast as the non-problem of trans-history identity. But from another point of view, because the various histories in a branching time model are all connected it is reasonable to consider, as Belnap does, that the entirety of the structure in one pastwards-connected branching time model represents âour worldâ. Such a model depicts a world rich with internal possibilities, past and present, and rich with alternative histories, each of them a possible history of the actual world, rather than an actual7 history of a different possible world. 7 Actual according to an indexical understanding of that notion, that is."
8,727,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","is the partition function for the strongly interacting âN gas, i.e., for the simplest strongly interacting hadron gas. What now is .m/? We have the , N, all nuclei with their excited states, resonances, A resonances, N states and their resonances, etc. Only a finite number of them is known, but there are many more still unknown. The finite number of known states is, in general, sufficient to calculate some interesting quantities. This has been done for a long timeârecently and in the context of nucleosynthesis in the early universe as well as for the relativistic heavy ion collision in some pioneering papers [31, 32]. In particular, the two papers by A.Z. Mekjan [32] are an excellent introduction to many fundamental concepts and open questionsâmost recommended reading! What a finite number of states, included in the integral of Eq. (26.26) for ln Z, cannot do, is to generate a singularity of the partition function, in other words, generate a phase transition. As one sees from Eq. (26.26), ln Z.T; V/ is analytic in the entire right half of the complex T plane if OÅ.m/Â D mË , Ë < 1. If .m/ grows exponentially, .m/  CmË exp.m=T0 /, then the integral of Eq. (26.26) does not exist for Re.T/ > T0 and ln Z.T; V/ has a singularity at T0 , as first observed by Yu.B. Rumer [33], years before the SBM was proposed."
84,519,0.987,Eye Tracking Methodology,"This situation creates an artificial foveal scotoma and eye movement behavior if the situation is quite similar to the eye movement behavior of patients with real scotomas (Rayner 1998). In the boundary technique, developed by Rayner (1975), the stimulus changes as fixation crosses a predefined boundary. Rayner used eye movements to investigate reading because he found tachistoscopic methods inadequate: tachistoscopic (strobelike) presentation of letters and words relies on the presentation of material for very brief exposures to exclude the possibility of an eye movement during the presentation. Prior to eye tracked reading studies, this method was often thought of as being analogous to a single fixation during reading. Based on his and othersâ research, Rayner argued that what subjects report from a tachistoscopic presentation cannot be taken as a complete specification of what they saw. The argument for eye movement recording over tachistoscopic displays carries over to scene perception and is discussed further in the next section. In an example of the boundary technique below, a single critical target word is initially replaced by another word or by a nonword. The boundary paradigm allows the experimenter to be more diagnostic about what kind of information is acquired at different distances from fixation. In this technique, a word or nonword letter string is initially presented in a target location. However, when the readerâs eye movement crosses an invisible boundary location, the initially presented stimulus is replaced by the target word. The amount of time that the subject looks at the target word is influenced by the relationship between the initially presented stimulus and the target word and the distance from the launch site to the target location. The fixation time on the target word thus allows the experimenter to make inferences about the type of information acquired from the target location when it was in parafoveal vision. For example, the word fox is changed to cat when the eyes cross the boundary. the quick brown fox jumped over the lazy dog the quick brown cat jumped over the lazy dog"
285,726,0.987,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","The main goal of this chapter is to introduce a new way of thinking about pitch coding, grounded in CNS physiology. If there is a robust representation of pitch in the dominant ISI distribution in the AN (Cariani and Delgutte 1996a, 1996b), and if some neurons convert ISI directly into a corresponding firing rate, then it seems possible that the dominant ISI interval is coded as predominant firing rate. Evidently, the scheme proposed here is incomplete. Questions arise how and where a butte profile would be read out; how such a representation would mesh with the spectral representation needed for F0 above ~ 500 Hz; how phase-invariant this representation would be; etc. We conclude with some interrelated issues. An important issue is the effective bandwidth of central neurons. Several CN neuron types integrate over wide frequency regions (Godfrey et al. 1975; Winter and Palmer 1995): partials that are resolved at the level of the AN may be unresolved in these CN populations. The autocorrelogram-like display in the dominant ISI hypothesis sums across frequency channels (Cariani and Delgutte 1996a, 1996b): for such an operation a wide bandwidth would be beneficial. Another issue is whether there is a specific physiological subset of neurons or even a separate brainstem nucleus which codes periodicity via entracking. Entracking is observed in a diversity of structures and neuron types, suggesting a distributed mechanism, but this does not exclude the existence of a brainstem âpitch centerâ specialized in this form of encoding. For CN neurons showing entracking, convergence of multiple inputs from the AN is obviously required; the degree of entracking in responses beyond the CN suggests that there are multiple stages of such convergence. A strong form of the butte hypothesis is based on perfect entracking; a weaker form only requires a monotonic relationship between firing rate and pitch-related period without attaining equality. One of the most critical issues is phase invariance, which we see"
299,130,0.987,Happiness Is The Wrong Metric : a Liberal Communitarian Response To Populism,"consideration a host of social and psychological variables that influence preference formation. It is additionally beyond the scope of this chapterâand perhaps beyond the scope of human scienceâto present such a comprehensive, consolidated theory. The task of presenting a formal, universal theory is further complicated by the sheer variety of human experience. However, the lesson to be taken from these observations is simple: human preference formation cannot be reduced to a defined set of economic factors. Instead, it must be acknowledged that an indefinite number of environmental, biological, neurological, andâabove all, because how we are socialized determines to a large degree how we respond to the environmental, biological, and neurological conditions of our existenceâsociological variables, through processes that are not yet precisely described, account for vastly more variance in preferences than do economic factors. By taking a step back and considering the big picture rather than remaining wedded to a single disciplineâs assumptions, it becomes possible to sketch out a rough framework for how preferences might be formed and change, and hence the non-economic factors that ultimately affect choice behavior. Developing such a big picture theory would have three main benefits. First, it would go a long way toward helping to organize the sprawling literature on human preference and choice. Once a general framework is established, it will be much easier to understand how each study relates to the others, which, in turn, may help scholars to understand how the thousands of variables they disparately describe interact with each other. Second, it will facilitate a âcrossing of the Rubiconâ between economic and non-economic disciplines that will ultimately yield a stillincomplete but wiser, interdisciplinary approach to preference formation divorced from the âtheory-induced blindnessâ and jockeying for influence that is characteristic of todayâs theories. Third, such a framework would be much better for policymakers, who need a way of thinking about preferences that is universally applicableânot because it is a comprehensive theory of everything with models that are theoretically elegant but clumsy and unwieldy in the real world, but because of its flexibility and the ability to âplug inâ variables in the appropriate location when needed."
380,283,0.987,Grassroots Politics and Oil Culture in Venezuela : The Revolutionary Petro-State,"Theorizing The STaTe Adrian and Micheâs story tells a tale of a short-term political trajectory within the early years of the Bolivarian revolution. It also conveys a deeper script about the quest to transform not only state praxis, that is, how the state functions, but also to change the meaning of the relationships between society and the state. Scholarly inquiries into the state throw up a host of theoretical implications. Of particular relevance to our context is, how can we understand the relationship between the state and society, and how do we understand power and resistance both inside and outside the state? Anthropological theory-production about the state has problematized and criticized Foucauldian and Weberian approaches to the state that pose state and society in a dichotomist relationship. For Foucauldian-inspired scholars, the state derives its power and its appearance as a bounded totality through the aggregated diffusion of impersonalized rule, institutions and proceduresâthat is, governmentality (e.g., see Foucault 1991; Mitchell 2006). To Weberian scholars, âthe state appears monolithic to society because its work follows certain impersonal, rational and standardized routines which give it a sui generis qualityâ (Neuman 2005:195). In both cases, the state appears as a bounded locus of power, set apart from society. Anthropological scholars on the other hand, have argued that how the state appears, and how people view the state and position themselves in relation to it, is essentially a case of empirical inquiry. Gupta (1995), for example, asks whether the Western legacy of universalizing a dual cultural construction of stateâsociety relations stands up to scrutiny in the face of âincommensurable cultural and historical contextsâ (Gupta 1995:214). Drawing on ethnography from India, he argues that âwe should leave open the analytical question as to the conditions under which the state does operate as a cohesive and unitary wholeâ (Gupta 1995:229). Nuijten (2003) makes a similar argument when she teases out a duality in the way Mexican peasants both engage with and imagine the state. Power is not an abstract formalized rule, but rather a myriad of changing strategies that are personalized and continuously reinvented (Nuijten 2003:120). At the same time, she argues that governmental techniques such as stamps, maps"
82,248,0.987,Fading Foundations : Probability and The Regress Problem,"4.5 Tour dâhorizon Let us take stock. The epistemological regress problem, as we have introduced it in Chapter 1, led to a discussion of epistemic justification in Chapter 2. The idea that epistemic justification has something to do with âprobabilificationâ is widespread among contemporary epistemologists: practically all agree that âA j justifies Ai â at least implies that Ai is made probable by A j . Yet, as we have been arguing in Chapters 3 and 4, the far-reaching consequences of this unanimity about the regress problem in epistemology have been insufficiently understood. A few exotic cases excluded, talk about probability is Kolmogorovian talk. One of the theorems of Kolmogorovâs calculus is the rule of total probability, which enables us to determine the unconditional probability of q, namely P(q). If P(q) is made probable by an epistemic chain rather than a single proposition, then the value of P(q) is obtained from an iterated rule of total probability. It has often been thought that such an iteration does not make sense if it continues indefinitely, but, as we have seen in Chapter 3, this is simply a mistake. In all but the exceptional cases P(q) can be given a unique and well-defined value, even if the chain that supports it is infinitely long. The iteration in question is a complex formula that consists of two parts. The first part is a series involving all the conditional probabilities, the second part is what we have called the remainder term, which contains information 23 The phenomenon of fading foundations is not restricted to probabilistic chains"
259,127,0.987,The Little Book of Semaphores,"There is nothing wrong with the previous solutions, but just for completeness, letâs look at some alternatives. One of the best known is the one that appears in Tanenbaumâs popular operating systems textbook [12]. For each philosopher there is a state variable that indicates whether the philosopher is thinking, eating, or waiting to eat (âhungryâ) and a semaphore that indicates whether the philosopher can start eating. Here are the variables: Variables for Tanenbaumâs solution"
8,608,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","subtle point which touches on the limits of validity of our present interpretation of the mass spectrum. In this respect, we recall that the volume of fireballs now grows with the fireball massâthus the average density should be finite for T ! T0 . In a consistent model, we expect a finite energy density at T0 , so that the presently forbidden region beyond T0 will now become accessible. 4. Below 60 MeV, we find that the energy per baryon obeys roughly the simple relation  3T=2; however, below 20 MeV, our model includes too little nuclear structure to have enough predictive power. Above 60 MeV, we find that pion degrees of freedom absorb an increasing amount of the total energy, so that the âenergy per baryonâ (the total energy/number of baryons) exceeds more and more the energy which the baryons themselves carry. Looking ahead, we hope to enlarge our model by making the input more elaborate, by maintaining the particleâantiparticle symmetry, and by considering the particular importance of alpha clusters. It seems that a profound study of the âliquidâ phase will be rewarding since much of the structure of the liquid (maybe even the existence of a new âsolidâ phase) depends on the amount of nucleon structure we include in the input terms. An obvious first step in this direction is the possible introduction of effective masses (< free masses) of the bound nucleons, a feature that is very likely relevant to the understanding of the saturation of nuclear matter in the bootstrap description. We must also incorporate Fermi and Bose statistics and investigate models leading to a finite energy density at T0 . Open Access This book is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and sources are credited."
117,79,0.987,Care in Healthcare : Reflections On Theory and Practice,"approach here, deriving action from abstract principles (and justifying it by way of these principles), care ethics takes a fundamentally different path. Rather than basing its actions on an abstract rule and moving from here to practice, it takes practice itself as the foundation for selecting the action required. It thus does not follow a deductive model, but instead sees the immediacy and singularity of a particular situation as an instruction to decide on the action that seems most appropriate in that situation. Thus, while principlism applies rules, care ethics is concerned with a fitting response that must be developed based on the situation, since the specific nature of a situation cannot be confronted adequately simply by applying rules. This shows that, in terms of method alone, care ethics is not concerned with the criterion of generalisability or with a Kantian idea of universalism; rather, it focuses on understanding the particular and incomparable nature of the patient and their situation. Generalisability is replaced by singularity and particularity. This is reminiscent of hermeneutic ethics insofar as the particular point of view of care ethics lies specifically in inquiring into the particular and thus the unique nature of the other. It is therefore no coincidence that the hermeneutist RicÅur of all people advocates an ethics of care, nor thatâ drawing on the Aristotelian concept of phronesisâhe identifies âpractical wisdomâ as the methodical basis for ethical judgements. RicÅur wanted care to be understood as a guarantee that the unique nature of the other is protected against being taken over by generalising postulates. He sees the fundamental role of care in saving the otherness of the other. In summary, this aspect of situational specificity can be divided into three elements: (a) emphasis on immediacy and acknowledgement of immediate perception (b) recognition of the singularity of the situation (c) need for a creative resolution to conflict rather than one that is simply rule-based. Care ethics thus represents a progressive alternative to simple instrumental rationality."
235,97,0.987,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","12.1 The Quantum Canon At the moment, there exists a loosely bundled canon of quantum rules subsumed under the term quantum mechanics or quantum theory. It includes reversible as well as irreversible processes, and is prima facie inconsistent. As already von Neumann [552, 554] and later Everett [30, 206, 545] noted, there cannot be any irreversible measurement process nested in a ubiquitous uniformly reversible evolution of the quantum state. Both von Neumann and Everett called the former, irreversible, discontinuous change the âprocess 1â; and the latter, reversible, continuous, deterministic change the âprocess 2,â respectively. Stated differently, there cannot exist any irreversible many-to-one measurement scenario (other than pragmatic fappness) in a reversible one-to-one environment. Hence, if one wants to maintain irreversible measurements, then (at least within the quantum formalism) one is faced with the following dilemma: either quantum mechanics must be augmented with some irreversible, many-to-one state evolution, thereby spoiling the ubiquitous, universal reversible one-to-one state evolution; or the assumption of the co-existence of a ubiquitous, uniform reversible one-to-one state evolution on the one hand with some irreversible many-to-one âwave function collapse,â (by another wording, âreduction of the state vectorâ) throughout measurement on the other hand, yields a complete contradiction. How is such a situation handled in other areas? Every system of logic which is self-contradictory (inconsistent) â such that a proposition as well as its negation is postulated; or can be derived from the postulates â in particular, in a formal axiomatic system, is detrimental and disastrous. Because by the principle of explosion (Latin: ex falso quodlibet) any invocation of a statement as well as of its negation yields every proposition true. This can be motivated by supposing that both âPâ as well as ânot Pâ are true. Then the proposition âP or anythingâ is true (because at least âPâ is true). Now suppose that also ânot Pâ holds. But then, in order for âP or anythingâ to be true, âanythingâ needs to be true. However, if anything is derivable, then such a system lacks any descriptive or predictive capacity. In this Â© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_12"
360,142,0.987,Compositionality and Concepts in Linguistics and Psychology,"The one aspect I find missing from this otherwise compelling story is that there is no room here for individual variation of meanings. In particular, the mental realmâ as captured by the TYPE dimension of the dual-aspect lexical items and carried along via the proof-theoretic semantics to full sentence-thoughts is the same for all people, and it furthermore (appears to) contain exactly the objectively true features of the encyclopedic facts in the word-TYPEs when these are manipulated so as to give the meanings for the sentence-thought. It thus cannot deal with issues of false beliefs about items in the world; it will not give any account of why different subjects give individually different results on the many tasks about meaning that have been administered over the decades in cognitive psychology. Although Asherâs theory is not described this way, perhaps the alteration I would advise is to make his theory be a theory for an individual agent, allowing this agent to have his/her own set of encyclopedic âfactsâ for every lexical sense.43 Presumably there would also be individual differences in the underlying inference mechanisms of the proof-theoretic semantics also, another topic not envisaged in Asherâs picture. But this now would leave us without any way to accommodate âreal communicationâ, where the two conversationalists are âreallyâ talking about the same (mental) things. While there can always be some check against âthe objective factsâ due to the objective portion of meaning in Asherâs theory, this will not help conversation that relies on having some mutual understanding of the TYPE-portion of meaningâthe meanings . I leave this problem hanging, as a very important topic still to be solved by any two-tiered theory. And I turn instead to other aspects that need to be dealt with in the subjective, TYPE portion of the theory."
164,258,0.987,"Marginality : Addressing the Nexus of Poverty, Exclusion and Ecology","I quantitatively analyzed Child Attachment Interview (CAI) narratives of 12 selected girl participants from the earthquake group. I selected the participants on the basis that their narratives provided a broad sweep of themes that represented the entire CAI sample. For this chapter a simple treatment of narratives using thematic analysis (using actual sentences as themes), in keeping with principles of grounded theory (Glaser and Strauss 1967) and a phenomenological approach extended by psychological anthropology (Rosaldo 1984; Shweder and LeVine 1984; Shweder 1991; Berry et al. 1997) were favored over other qualitative techniques. Phenomenology seeks the psychological meanings that constitute a phenomenon through investigating and analyzing past examples of the phenomenon within the context of the participantâs lives (assuming that the capacity to live through events or respond to different situations greatly exceeds the capacity to know exactly âwhat we doâ or âwhy we do what we doâ). Narratives are also seen as the performance of the self or as a story of identity (Parker 2004). The analysis of the CAI transcripts helped unfold processes where identity and self were threatened and/or a certain kind of identification was either denied or affirmed."
28,180,0.987,A History of Self-Harm in Britain,"intervention ... it certainly is not right that mildness of method indicates lack of severity of psychological illnessâ.17 A year later Kessel and various collaborators talk of the dangerous fallacy of âusing this yardstick of physical damage to judge whether the patient needs psychological treatmentâ.18 The clinical object exists between therapeutic regimes, but (somatic) lethality is downplayed. The communicative overdose remains a tactical intervention between therapeutic regimes where the significance of the act is determined not by its physical consequences but its psychosocial context. The first major difference is the explicit archetypal method. In 1965 Kessel entitles his Milroy (public health) Lectures at the Royal College of Physicians âSelf-poisoningâ. These two articles are key in further publicising the terminological debate around attempted suicide. Rather than accept Stengelâs and Cookâs increasingly established modification of the term, Kessel finds attempted suicide, âboth clinically inappropriate and misleadingâ,19 advancing self-poisoning because it allegedlyâ describes the phenomenon without interpreting it along a single pathwayâ.20 However, Kessel is opening up and closing down various possibilities. His terminological offering is intended to sidestep issues of intention (âinterpretingâ here indicates assessments of intent), but collapses all possible behaviours into one archetype. These lectures describe a rather unorthodox practice in the promotion of self-poisoning stereotypes. Kessel sends an actor into six chemist shops in Edinburgh, instructed to simulate being in floods of tears, and to request two hundred aspirin. Kessel reports that she was served in every shop and only once was concern expressed: âTwo hundred? Are you all right? You ought to go and have a cup of teaâ. This state of affairs is described in strong terms as irresponsible.21 This expresses the twin facts that a âsobbing girlâ is typical of self-poisoners and that purchasing a large quantity of aspirin in this state is obviously a suicide risk â which also narrows the method of attempted suicide. Kessel concedes that definitions of self-poisoning are difficult and fraught with complexity, but he uses phrases like âmimicking suicideâ, âsimulation of deathâ, and âdrama was enacted for their own circleâ.22 Such phrases expose a simplification of intent: this is not Batchelorâs and Napierâs childhood emotional trauma surfacing, nor Stengelâs and Cookâs unconscious, ambiguous ordeal. This is performance, deception and drama. The object is still unarguably social, but now very much self-consciously so. This is clearest in one of his last publications on the subject: âThe respectability of self-poisoning and the fashion for survivalâ (1966). He claims that âit is common knowledge that you can take a lot"
249,192,0.987,Advances in Proof-Theoretic Semantics (Volume 43.0),"5 Conclusion The main conclusion is this: although the idea that the âgrounds for asserting a propositionâ are easily collected together as a unit is attractive, the different ways in which it can be done (disjunctive, conjunctive, with assumption discharge, with variable abstraction or parameterisation, â¦, recursion) generate (if the GE rules pattern is followed) many problems for the programme of mechanically generating one (or more) elimination rules for a logical constant, other than in simple cases. There are difficulties with the mechanical approach in [8]; there are similar difficulties in [13]. Without success of such a programme, it is hard to see what âGE harmonyâ can amount to, except as carried out in (e.g.) Coq [1] where strictly positive inductive type definitions lead automatically to rules for reasoning by induction and case analysis over objects of the types thus defined, and with strong normalisation results. A similar conclusion is to be found in [33]. Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
95,164,0.987,Elements of Robotics,"Activity 5.7: Odometry in two dimensions â¢ Write a program that causes the robot to make a gentle left turn for a specific period of time. â¢ Compute the pose (âdc sin Î¸, dc cos Î¸, Î¸ ) and compare the result with the values measured using a ruler and a protractor. Run the program several times and see if the measurements are consistent. â¢ Run the program for different periods of time. How does this affect the accuracy and precision of the odometry computation?"
294,365,0.987,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"6.1 Brute Force Methods The representation of a mathematical function f .x/ on a computer takes two forms. One is a Python function returning the function value given the argument, while the other is a collection of points .x; f .x// along the function curve. The latter is the representation we use for plotting, together with an assumption of linear variation between the points. This representation is also very suited for equation solving and optimization: we simply go through all points and see if the function crosses the x axis, or for optimization, test for a local maximum or minimum point. Because there is a lot of work to examine a huge number of points, and also because the idea is extremely simple, such approaches are often referred to as brute force methods. However, we are not embarrassed of explaining the methods in detail and implementing them."
214,437,0.987,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"slide.17 Enhanced flow can make glaciers at the edge of ice sheets flow faster (more icebergs). Models are attempting to simulate this. But the current versions of ice sheet models, even when trying to simulate these bottom (basal) lubrication processes, have not been able to get much mass loss at a rapid rate, and not as fast as observations over the past 20 years or so. This is a serious deï¬ciency in model evaluation, and one reason why projections of sea-level change are so uncertain. But as new processes are discovered, this may change. Or maybe estimates will be revised downward as we better understand the simulations and pieces of them. As long as we know what is missing and what the uncertainty is, we can gauge whether a prediction is wrong, and also in what direction. Is a projection an overestimate or underestimate? Or an upper or lower limit? Thus, what really makes a projection âbadâ is overconï¬dence, or underrepresentation of uncertainty. Often uncertainty is stated somewhere, but not presented well or ignored. The lesson is always to try to understand a projectionâs stated uncertainty. This is true in general, not just for climate models. The best practice for using models is to go back to the model documentation or description to make sure a proper representation of uncertainty is available, and an analysis of the model ï¬t for the purpose is assessed. For example, projections of changes to a phenomena based on models with a bad representation of the present phenomena (like the South Asian summer monsoon, for example) may fall into this category."
341,278,0.987,Freshwater Governance for the 21st Century,"it grapples with implementation of a new catchment-based approach because it is administratively not able to deal with water in the landscape (the province of another government body). Most significantly though, past framing has failed to account for the social in relation to the biophysical and, where present, treating it as an add-on rather than integral to the question of what has to be governed (Ison et al. 2007). Framing failure is often a precursor to maladaptive responses (practices, policies, investments â see Barnett and OâNeill 2010) because âframesâ are used by humans to negotiate the complexity of the world they experience by determining what requires attention and what can be ignored. Any framing choice brings with it systemic consequences; they shape practice and create pathway dependencies. As Lakoff (2010: 71â72) notes: All thinking and talking involves âframing.â And since frames come in systems, a single word typically activates not only its defining frame, but also much of the system its defining frame is in. Moreover, many frame-circuits have direct connections to the emotional regions of the brain. Emotions are an inescapable part of normal thought. Indeed, you cannot be rational without emotions."
82,107,0.987,Fading Foundations : Probability and The Regress Problem,"Fumerton gives two reasons why the âmaking probableâ relation may after all not be neutral. The first is that a ânormative feature of epistemic justification . . . may call into question the conceptual primacy of probability as a key to distinguishing epistemic reasons from other sorts of reasonsâ.49 The second is that âif one understands the relation of making probable in terms of a frequency conception of probability, one will inevitably beg the question with respect to certain internalist/externalist debates over the nature of justificationâ.50 Let us briefly look at each of these two reasons. The idea behind the first is that epistemic reasons differ from moral or prudential or legal ones, since an epistemic goal is not the same as a goal 47 The condition of probabilistic support is also neutral with respect to several"
269,194,0.987,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"âmade itâ within a system not designed for them. (See, in particular, The Notorious B.I.G.âs âPlaya Haterâ, in which he sang about the two kinds of people in the world, today: the âplayersâ and the âplaya hatersâ.) What is going on here? First, in the move from 1990s rap to this researcherâs diagnosing of the fractious â though, letâs face it, rarefied â space of an interdisciplinary workshop on neuroscience? Second, in our deciding to include this culturally and historically dense moment, one that invokes a specific linguistic terrain, often associated with a particular kind of black masculinity, and that then uses that terrain to complexify what regular commentary might simply have characterized as predictable cross-disciplinary critique of neuroscience on the part of the humanities and social sciences? First, let us consider the sentence, âThis room is filled with neuroscience playa haters.â FC believes this to be a moment of diagnosis, in which there was the identification (correct, FC believes) of some sense of grievance and envy on the part of at least some of those in the social sciences and humanities; as well as of a disparity in power between the neurosciences (strong) and those enviously attacking it (the weaker humanities and social sciences); as well as of the strangeness of running a conference about neuroscientific evidence that was filled with so many people âoutsideâ of the field and that contained so few practising neuroscientists. The sentence, on the one hand, can be interpreted as a powerful means both to gesture to a recent transformation in disciplinary hierarchies (the neurosciences were thrust into the limelight at the same time as the humanities were feeling increasingly anxious about their status) and a means to install (not wholly ironically) anyone issuing the sentence close to the heart of interdisciplinary power. But there is more going on here, too. The term âhaterâ, as it has emerged in the last couple of decades, conveys a complex mesh of affects: the âhaterâ not only is unable to be happy about the success of the person she or he is âhatingâ, but rather wants to undermine that person by attempting to expose her flaws or inadequacies. Here, then, is an acute analysis of the shape that critique of âthe neurosciencesâ, on the part of humanities and social science scholars, can sometimes take. FC remembers the atmosphere in the workshop as, indeed, at times, hostile against âneuroscienceâ â and hostile in a way that refused any real engagement with the heterogeneous methods or epistemologies of the neurosciences, and that was forcefully committed to trying to identify as many flaws in âneuroscienceâ as possible. She surmised that the use of the term âhaterâ, to describe some"
192,102,0.987,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"can only be brought to the fore and analysed with the help of specialised equipment. And this may explain Hamletâs failure, his impotence; his failure to act. In Hamletâs case, astronomy is still conducted with the naked eye, although one could see the platform as a kind of contraption, a Renaissance Stonehenge so to speak. A âphallicâ, telescopic instrument (Ï) would perhaps have allowed him to emancipate himself from traditional Gerede and palace intrigue, focussed on the intricacies of match-making, but this instrument is missing (âÏ). Hamlet remains a scholar, who reads, talks and writes, but practices with a sword rather than a telescope. Indeed, optical instruments are decidedly absent in his scholarly practice. Science becomes real science to the extent that technicity dominates the subject-object relationship, so that the object a is not only observed and analysed, but also (to a considerable extent) produced by research contraptions. But I will resume my analysis in Chap. 11 to indicate that Hamlet (as an experimental drama) continues to be relevant for understanding research practices up to today."
249,88,0.987,Advances in Proof-Theoretic Semantics (Volume 43.0),"One might think that this entails that the related notion of âconstructive validityâ which we might hope to characterize using a system in which the BHK clauses can be interpreted must be distinguished from âvalid with respect to a particular form of formal semanticsâ.12 Nonetheless, Kreisel and Goodman both appear to have viewed the Theory of Constructions as providing an âinformally rigorousâ analysis of constructive validity. In particular, both present versions of the following result for the systems described in [25, 26], and [17] (wherein T â is the relevant formulation of the Theory of Constructions): (Val) For all formulas A in the language of HPC, â¢HPC A if and only if there exists a term s such that â¢T â Î  (A, s) â¡ â¤. The left-to-right direction of Val can be taken to express a form of soundness for Kreiselâs interpretation of HPC into T â âi.e. if A is derivable from what are normally regarded as intuitionistically valid principles of reasoning, then A is indeed âconstructively validâ in the sense that there is some construction which witnesses its derivability. Conversely, the right-to-left direction of Val can be taken to express a form of completeness (also known as faithfulness) of the interpretationâi.e. if A is âconstructively validâ in the sense that Î  (A, s) holds for some construction s, then A is in fact derivable from intuitionistically valid principles.13 Although both Kreisel and Goodman announced versions of these results, the situation surrounding their claims is complicated by several factors which we will not consider in detail here.14 For what is more germane to our immediate concerns is not whether any particular variant of the Theory of Constructions satisfies Val, but rather whether such systems satisfy what can be understood as a generalized form of soundness which we will refer to as internalization. Note that if we are able to demonstrate the left-to-right direction of Val (say by induction on the length of proofs in HPC), then it also seems reasonable to suppose that we ought to be able to do this for all derivations carried out in T itself.15 This would suggest that the Theory of Constructions ought to satisfy a principle of the following form: (Int) If â¢T + s â¡ â¤, then there exists a term c such that â¢T + Ï sc â¡ â¤. Here c might either be taken as a new constant or as a complex term which is built up according to the structure of the derivation of s â¡ â¤. (Although we will return to discuss this issue in Sect. 5.5, for the moment we will assume the former interpretation 12 For discussion of the intuitive notion of constructive validity and its relationship to various formal semantics for intuitionistic logic, see (e.g.) Scott [37], Dummett [7, chap. 5], and McCarty [32]. 13 Compare Scott [37, p. 256]: âThe reason that A is intuitionistically (constructively, if you prefer) valid is that there is a specific term Ï [â¦] such that the assertion â¢ Ï â A is provable in the theory of constructions.â. 14 For instance, although Kreisel states versions of the completeness and faithfulness results ([25, p. 205] and [26, Sect. 2.311]), in neither case are proofs given. And although Goodman [16] contains complete proofs of both directions, the interpreting theory in his case is not T , but rather the stratified theory T Ï . 15 In fact, this is exactly how the soundness proof for HPC given by Goodman [16, Sect. 11â15] for T Ï proceeds."
13,63,0.987,Feeling Gender : a Generational and Psychosocial Approach,"See also Layton (2002) on this experience of how âoutdatedâ theories may fit some patients in the clinical setting well. Stable identities are not the same as coherent identities, which no psychoanalytic theory would assume. It was Freud who with his concept of the unconscious was the first to argue that the human psyche is internally contradictory. Thus, the self will always be in discord with itself in more or less painful ways."
342,4,0.987,Semiotics in Mathematics Education,"objects of mathematics are ideal, general in nature, and to represent themâto others and to oneselfâand to work with them, it is necessary to employ sign vehicles,1 which are not the mathematical objects themselves but stand for them in some way. An elementary example is a drawing of a triangleâwhich is always a particular caseâbut which may be used to stand for triangles in general (Radford 2006a). As a text on the origin of (Euclidean) geometry suggests, the mathematical concepts are the result of the continuing reï¬nement of physical objects Greek craftsmen were able to produce (Husserl 1939). For example, craftsmen were producing rolling things called in Greek kulindros (roller), which led to the mathematical notion of the cylinder, a limit object that does not bear any of the imperfections that a material object will have. Childrenâs real problems are in moving from the material things they use in their mathematic classes to the mathematical things (Roth 2011). This principle of âseeing an A as a Bâ (Otte 2006; Wartofsky 1968) is by no means straightforward and directly affects the learning processes of mathematics at all levels (Presmeg 1992, 2006a; Radford 2002a). Thus semiotics, in several traditional frameworks, has the potential to serve as a powerful theoretical lens in investigating diverse topics in mathematics education research."
249,307,0.987,Advances in Proof-Theoretic Semantics (Volume 43.0),"2 Equality Versus Identity To set the stage for the further discussion we would like to assume that we always use a first-order language for which we may have some non-logical axioms, and a fixed structure with universe A in which this language is interpreted3 by an interpretation function (Â·)M. Let us use Latin characters for terms of the language, and German (Gothic) ones for elements of the structure. Equality is understood as the relation t = s on the syntactical level between terms of the first-order language4 ; identity stands for the (trivial) relation a â¡ a on the semantic level, which holds only between an object in the structure and itself.5 The fact that identity is not entirely trivial comes from its use for terms in combination with the interpretation in the form (t)M â¡ (s)M.6 In this setting we can recast Fregeâs first observation that âif we were to regard equality as a relation between that which the names âaâ and âbâ designate, it would seem that a = b could not differ from a = a (i.e. provided a = b is true). A relation would thereby be expressed of a thing to itself, and indeed one in which each thing stands to itself but to no other thing.â [5, p. 157]. In our terminology we may say that we are not concerned with the semantical identity relation a â¡ a, but with the syntactical equality relation t = s. It is standard to axiomatize equality in first-order logic as a universal congruence relation, i.e., an equivalence relation compatible with all operations (functions and relations). As such, it mimics on the syntactic level just the properties which identity exhibits on the semantic level. But the domain of identity is simply A Ã A, and the elements of A are unique in the sense that a â¡ a, but a â¡ b for two elements a, b â A. On the syntactic side, however, the equality relation is defined for terms, and, clearly, two terms, though being interpreted by the same object a, may well be different. 2 In this paper, we restrict ourselves to an epistemic perspective, and we will not go into metaphysical"
82,377,0.987,Fading Foundations : Probability and The Regress Problem,"7.5 Making Coins We have formally proved that an infinite series of higher-order probability statements is strictly equivalent to an infinite justificatory chain of the probabilistic kind. However, we might still have qualms: how can we understand the matter in an intuitive way? Being able to check all the steps in an algebraical proof is one thing, it is quite another thing to âsee throughâ the series, as it were, and to appreciate what is actually going on. In this section we will try to allay these worries by offering a model that is intended to make the above abstract considerations concrete. The model is completely implementable; it comprises a procedure in which every step is specified. The model gives us a probability distribution over all the propositions as well as over their conjunctions. It satisfies the Markov condition in a very natural way, and we do not have to assume this condition as an external condition.26 Imagine two machines which produce trick coins. Machine V0 produces coins each of which has bias Î±0 , by which we mean that each has probability Î±0 of falling heads when tossed; machine W0 , on the other hand, makes coins each of which has bias Î²0 . We define the propositions q and A1 as follows: q is the proposition âthis coin will fall headsâ A1 is the proposition âthis coin comes from machine V0 â . 26 In Herzberg 2014 the Markov condition is imposed as an extra constraint. See"
84,187,0.987,Eye Tracking Methodology,"7.4 Virtual Gaze Intersection Point Coordinates In 3D eye tracking studies, we are often interested in knowing the location of oneâs gaze, or more importantly oneâs fixation, relative to some feature in the scene. In VR applications, weâd like to calculate the fixation location in the virtual world and thus identify the object of interest. The identification of the object of interest can be accomplished following traditional ray/polygon intersection calculations, as employed in ray-tracing (Glassner 1989). The fixated object of interest is the one closest to the viewer that intersects the gaze ray. This object is found by testing all polygons in the scene for intersection with the gaze ray. The polygon closest to the viewer is then assumed to be the one fixated by the viewer (assuming all polygons in the scene are opaque)."
297,997,0.987,The R Book,"Errors may be non-normal for several reasons. They may be skew, with long tails to the left or right. Or they may be kurtotic, with a ï¬atter or more pointy top to their distribution. In any case, the theory is based on the assumption of normal errors, and if the errors are not normally distributed, then we shall not know how this affects our interpretation of the data or the inferences we make from it. It takes considerable experience to interpret normal error plots. Here we generate a series of data sets where we introduce different but known kinds of non-normal errors. Then we plot them using a simple home-made function called mcheck (ï¬rst developed by John Nelder in the original GLIM language; the name stands for âmodel checkingâ). The idea is to see what patterns are generated in normal plots by the different kinds of non-normality. In real applications we would use the generic plot(model) rather than mcheck (see below). First, we write the function mcheck. The idea is to produce two plots, side by side: a plot of the residuals against the ï¬tted values on the left, and a plot of the ordered residuals against the quantiles of the normal distribution on the right. mcheck <- function (obj, ...){ rs <- obj$resid fv <- obj$fitted windows(7,4) par(mfrow=c(1,2)) plot(fv,rs,xlab=""Fitted values"",ylab=""Residuals"",pch=16,col=""red"") abline(h=0, lty=2) qqnorm(rs,xlab=""Normal scores"",ylab=""Ordered residuals"",main="""",pch=16) qqline(rs,lty=2,col=""green"") par(mfrow=c(1,1)) invisible(NULL) }"
8,480,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","A Logical Difficulty of the Model We have employed statistical thermodynamics of distinguishable particles. This is strictly speaking inconsistent, since nature certainly does not work this way. Indeed, even if we are right in saying that most contributions come from states in which all particles (resonances, fireballs) are different, there are certainly states in which, for instance, five C are already present in the first generation. To be really consistent, we should have worked out a statistics of, say, M different species of particles. Particles of the same species must then be considered to be indistinguishable (and a statistics, Bose or Fermi, to be prescribed) P and the number Ni of particles of each kind, as well as the total number N D Ni of particles, has to be left open as before. Finally, one lets M ! 1. One sees immediately that the number of particles of each single kind would then tend to zero and we should come back to our model. In the case of zero masses M one finds, however, that this does not work. One simply obtains Z.V; T/ , where Z.V; T/ is the usual partition function of indistinguishable particles, and in fact that of a massless Fermi or Bose gas, as the case may be. With M ! 1, everything diverges at any temperature. That is easily understood if one realizes that, for M ! 1, there is an infinity of states of our gas even if the total number N of particles in it is kept fixed: each single particle may be removed from the gas and be replaced by one of another species. From that operation, a new state with the same energy results, whence the sum over states is infinite. The situation becomes different if the particles have a mass. Then replacing a particle by one of another species means changing the energy of the system, and one cannot generate an infinity of different states of the same energy. Again, with the number M of kinds of particles going to infinity, one would find that the number of particles of a given kind tends to zero. It is hoped that such a statistics will become equivalent to our present model. This, however, has not yet been worked out. It"
103,369,0.987,Solar Particle Radiation Storms Forecasting and Analysis : The Hesperia Horizon 2020 Project and Beyond (Volume 444.0),"which allowed us to assess the early evolution of the shock in a system that does not possess the symmetries assumed in the MHD model. Our results show that the efficiency of particle acceleration crucially depends on the modelled properties of the shock in the corona. Conditions on different field lines vary very much and while the shock on some field lines is able to produce a relativistic particle event, it fails to do so on others. The most important factor governing the acceleration efficiency in our study was the AlfvÃ©nic Mach number of the shock: the higher the AlfvÃ©nic Mach number, the more likely the shock to accelerate protons to relativistic energies (Afanasiev et al. 2017). In our simulations, we focused on two events that differ in one important aspect: one of them (2012 May 17) is a GLE and the other one is not. Both of the events are associated with long-duration -ray events, which might seem contradictory, but actually is not. In the light of our simulations, there are two possible explanations for this. Firstly, as the particle acceleration efficiency at the shock varies a lot from one flux tube to another, the 1-AU proton event does not necessarily correspond to the best acceleration conditions on the shock surface. Thus, we may well observe a long-duration (pion-decay) -ray event due to shock-accelerated protons without a clear increase observed at 1 AU at energies required for pion production at the Sun. The Earth-based observer sees but a small fraction of the complete picture. The other, more subtle explanation deals with the strength of the turbulence in the foreshock region of the coronal shock. The CSA simulations show that the foreshock is extremely turbulent near the Sun, traps a large fraction of particles (almost all) in its vicinity and allows only a minor fraction to escape. While the SaP model contains the possibility to use enhanced foreshock turbulence, it is tuned to reproduce the observations when the shock is detected in situ. Therefore, the source function deduced from the 1-AU observation represents more the fraction of particles that can escape upstream than the fraction that can be transported downstream from the shock. The large discrepancy between the CSA and SaP modelled spectra of precipitated particles is, thus, partly explained by this effect, as well. Furthermore, these explanations shed light on the tendency to get lower numbers of precipitated >300 MeV protons in the SaP+DSP simulations (especially for the 2012 January 23 event), as compared to the observations. The transport model we employ for the downstream region has several important simplifications in it. Firstly, it employs a shock completely opaque to protons and, thus, allows downstream-advected particles to reside in the region between the shock and the Sun for as long as they get precipitated. The only loss process we employ is adiabatic cooling of the distribution, when the region between the shock and the Sun expands. On the other hand, we do not include any downstream reacceleration processes, which could also be important and would act in the opposite direction, helping particles to overcome adiabatic energy losses. Such processes include downstream stochastic acceleration (see, e.g., Afanasiev et al. 2014) and compressive acceleration in the highly-compressed regions close to the CME core (Kozarev et al. 2013). Therefore, we do not regard our model to be overly optimistic about the prospect of letting shock-accelerated protons precipitate over large time scales."
69,98,0.987,"Fundamental Approaches to Software Engineering : 21st International Conference, FASE 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","conflict, consistency restoration needs conflict resolution, and hence an essential development of the framework. There are also several open issues for the non-concurrent case considered in the paper (and its future concurrent generalisation). First, our pool of lens composition constructs is far incomplete (because of both space limitations and the necessity of further research). We need to enrich it with (i) sequential composition of (reflective) a-lenses so that a category of a-lenses could be built, and (ii) a relational composition of symmetric lenses sharing several of their feet (similar to relational join). It is also important to investigate composition with weaker junction conditions than we considered. Another important issue is invertibility, which nicely fits in some but not all of our results, which shows the necessity of further investigation. It is a sign that we do not well understand the nature of invertibility. We conjecture that while invertibility is essential for bx, its role for mx may be less important. The (in)famous PutPut law is also awaiting its exploration in the case of multiary reflective propagation. And the last but not the least is the (in)famous PutPut law: how well our update propagation operations are compatible with update composition is a very important issue to explore. Finally, paper [5] shows how binary delta lenses can be implemented with TGG, and we expect that MGG could play a similar role for multiary delta lenses."
249,522,0.987,Advances in Proof-Theoretic Semantics (Volume 43.0),"4 Proof-Theoretic Semantics Beyond Logic Proof-theoretic semantics has been occupied almost exclusively with logical reasoning, and, in particular, with the meaning of logical constants. Even though the way we can acquire knowledge logically is extremely interesting, this is not and should not form the central pre-occupation of proof-theoretic semantics. The methods used in proof-theoretic semantics extend beyond logic, often so that their application in logic is nothing but a special case of these more general methods. What is most interesting is the handling of reasoning with information that is incorporated into sentences, which, from the viewpoint of logic, are called âatomicâ. A special way of providing such information, as long as we are not yet talking about empirical knowledge, is by definitions. By defining terms, we introduce claims into our reasoning system that hold in virtue of the definition. In mathematics the most prominent example is inductive definitions. Now definitional reasoning itself obeys certain principles that we find otherwise in proof-theoretic semantics. As an inductive definition can be viewed as a set of rules the heads of which contain the definiendum (for example, an atomic formula containing a predicate to be defined), it is only natural to consider inductive clauses as kinds of introduction rules, suggesting a straightforward extension of principles of proof-theoretic semantics to the atomic case. A particular challenge here comes from logic programming, where we consider inductive definitions of a certain kind, called âdefinite-clause programsâ, and use them not only for descriptive, but also for computational purposes. In the context of dealing with negation, we even have the idea of inverting clauses in a certain sense. Principles such as the âcompletionâ of logic programs or the âclosed-world assumptionâ (which logic programming borrowed from Artificial Intelligence research), are"
78,176,0.987,The Onlife Manifesto : Being Human in a Hyperconnected Era,"Drawing on classical feminist sources as well as the work of Lefebvre (1971), Beck on âsubpoliticsâ (1997), and Giddensâ notion of âlife politicsâ (1991) as foci more appropriate to a second stage or late modernity, Bakardjieva describes subactivism first in terms of its locus in â¦the private sphere or the small social world. It blends ethics and politics, or oscillates around that fuzzy boundary where one merges into the other. It is rooted in the subject but necessarily involves collective identities often in an imagined formârecall Andersonâs (1983) imagined communities. It is constituted by numerous acts of positioningâoften in the imaginary vis-Ã -vis large-scale political, moral, and cultural confrontations, but also with respect to ongoing micro interactions and conversations. It is not about political power in the strict sense, but about personal empowerment seen as the power of the subject to be the person that they want to be in accordance with his or her reflexively chosen moral and political standards. (Bakardjieva 2009, p. 96; emphasis added, CE)"
285,745,0.987,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","Using these signals to tease apart temporal envelope cues from temporal fine structure cues is then a conceptual error. This impaired the conclusion that the pitch of unresolved complex tones is based only on fine structure information. Second, when thinking about pitch perception of unresolved complex tones in terms of interaction between envelope and fine structure, it appears that the limitation of phase locking is probably much less critical than when thinking in terms of fine structure only. In fact, it seems clear that there is no need to encode every phases of the signal to encode the most intense phases located nearby an envelope maximum. This explains that the simulation can extract a periodicity related to pitch when the carrier frequency is over 10 kHz (Fig. 3)."
289,1045,0.987,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","clear of this danger, but implies that our proofs cannot be quite as simple and perhaps cannot have quite the same structure as their paper counterparts. A key issue that we run against is the handling of existential quantifiers. According to what was said earlier, the specification of a sorting algorithm, say mergesort, should be, roughly: âthere exists a cost function f â O(Î»n.n log n) such that mergesort is content with $f (n), where n is the length of the input list.â Therefore, the very first step in a naÄ±Ìve proof of mergesort must be to exhibit a witness for f , that is, a concrete cost function. An appropriate witness might be Î»n.2n log n, or Î»n.n log n + 3, who knows? This information is not available up front, at the very beginning of the proof; it becomes available only during the proof, as we examine the code of mergesort, step by step. It is not reasonable to expect the human user to guess such a witness. Instead, it seems desirable to delay the production of the witness and to gradually construct a cost expression as the proof progresses. In the case of a nonrecursive function, such as insertionsort, the cost expression, once fully synthesized, yields the desired witness. In the case of a recursive function, such as mergesort, the cost expression yields the body of a recurrence equation, whose solution is the desired witness. We make the following contributions: 1. We formalize O as a binary domination relation between functions of type A â Z, where the type A is chosen by the user. Functions of several variables are covered by instantiating A with a product type. We contend that, in order to define what it means for a â A to âgrow largeâ, or âtend towards infinityâ, the type A must be equipped with a filter [6], that is, a quantifier Ua.P . (Eberl [13] does so as well.) We propose a library of lemmas and tactics that can prove nonnegativeness, monotonicity, and domination assertions (Sect. 3). 2. We propose a standard style of writing specifications, in the setting of the CFML program verification framework, so that they integrate asymptotic time complexity claims (Sect. 4). We define a predicate, specO, which imposes this style and incorporates a few important technical decisions, such as the fact that every cost function must be nonnegative and nondecreasing. 3. We propose a methodology, supported by a collection of Coq tactics, to prove such specifications (Sect. 5). Our tactics, which heavily rely on Coq metavariables, help gradually synthesize cost expressions for straight-line code and conditionals, and help construct the recurrence equations involved in the analysis of recursive functions, while delaying their resolution. 4. We present several classic examples of complexity analyses (Sect. 6), including: a simple loop in O(n.2n ), nested loops in O(n3 ) and O(nm), binary search in O(log n), and Union-Find in O(Î±(n)). Our code can be found online in the form of two standalone Coq libraries and a self-contained archive [16]."
166,307,0.987,Surveying Human Vulnerabilities across the Life Course (Volume 3.0),to the phenomenon under investigation. From a sociological perspective memory is not a simple restoration or reproduction of the past but a reconstruction of the life trajectory; this partly depends on the situation respondents are in when asked to recollect information on their experiences (Halbwachs 1994[1925]). For Halbwachs the recollection of events is the result of two interrelated thinking strategies. The first consists of âlocatingâ an event in its social context in terms of time and space. Like some sort of time travel people progressively place themselves in a frame that facilitates the recollection of personal events. In the second mnemonic process people transfer their focus from one single event to another focusing on its nature and its meaning (Halbwachs 1994 p. 201). The meaning of what is recollected has a social frame that depends at the same time on the different social groups the participant was a member of when a particular event occurred and on the social groups the participant belongs to at present. Coenen-Huther (1994) mentioned for instance that reports of past events differ according to the respondentsâ gender age or social class. In the case of gender she argued that women tend to structure their memory according to family events while men structure their memory in relation to the professional domain. In Halbwachsâ perspective some events and objects fade from memory because they are no longer meaningful if related to a social frame that no longer exists. For example when a specific social group to which the respondent belonged no longer exists at the moment of the interview the respondent is more likely to omit events experienced in relation to that group or situation. Such omissions are therefore not due to the fact that the events were not important in the past but because they are no longer relevant at the time of the interview. A comparison of two life events calendars completed by the same respondents the first completed 5 years after they left school and the second one completed 9 years later showed some time differences in the job history (CouppiÃ© and DemaziÃ¨re 1995). For example respondents were more likely to omit precarious jobs and short spells of unemployment in the second wave if the respondents had been employed in a permanent position between the two waves. Memory is therefore a dynamic process which generates information reconstructs life events and attributes meaning according to both current and past conditions. Answering retrospective biographical questionnaires as with any autobiographical process is thus not only a means to provide information on social phenomena but also represents processes of social construction that attach meanings to questions and answers. When respondents use biographical memory to communicate life events and experiences to others (e.g. an interviewer) they organize their memory in an ordered and coherent way in which casual and unexpected events may take on a coherent (and sometimes causal) structure (Bertaux 1981; Ricoeur 1985). The respondent becomes the author of a narrative in which he or she is the main character attributing a sense of coherence to his or her life. In other words in the autobiographical narrative respondents order their life events into a matrix that is culturally and socially constructed and gives sense and meaning to what happened (Bruner and Weisser 1991).
8,785,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","particles (quark bags) only. Thus one may view [7â9] the hadronic gas phase as being an assembly of many different hadronic resonances, their number in the interval .m2 ; m2 C dm2 / being given by the mass spectrum .m2 ; b/dm2 . Here the baryon number b is the only discrete quantum number to be considered at present. All bagâbag interaction is contained in the mutual transmutations from one state to another. Thus the gas phase has the characteristic of an infinite component ideal gas phase of extended objects. The quark bags having a finite size force us to formulate the theory of an extended, though otherwise ideal multicomponent gas. It is a straightforward exercise, carried through in the beginning of the next section, to reduce the grand partition function Z to an expression in terms of the mass spectrum .m2 ; b/. In principle, an experimental form of .m2 ; b/ could then be used as an input. However, the more natural way is to introduce the statistical bootstrap model [7], which will provide us with a theoretical that is consistent with assumptions and approximations made in determining Z. In the statistical bootstrap, the essential step consists in the realization that a composite state of many quark bags is in itself an âelementaryâ bag [1, 10]. This leads directly to a nonlinear integral equation for . The ideas of the statistical bootstrap have found a very successful application in the description of hadronic reactions [11] over the past decade. The present work is an extension [1, 9, 12] and application [1, 13] of this method to the case of a system containing any number of finite size hadronic clusters with their baryon numbers adding up to some fixed number. Among the most successful predictions of the statistical bootstrap, we record here the derivation of the limiting hadronic temperature and the exponential growth of the mass spectrum. We see that the theoretical description of the two hadronic phasesâthe individual hadron gas and the quark-gluon plasmaâis consistent with observations and with the present knowledge of elementary particles. What remains is the study of the possible phase transition between those phases as well as its observation. Unfortunately, we can argue that in the study of temperatures and mean transverse momenta of pions and nucleons produced in nuclear collisions, practically all information about the hot and dense phase of the collision is lost, as most of the emitted particles originate in the cooler and more dilute hadronic gas phase of matter. In order to obtain reliable information on quark matter, we must presumably perform more specific experiments. We will briefly point out that the presence of numerous s quarks in the quark plasma suggest, as a characteristic experiment, the observation Î hyperons. We close this report by showing that, in nuclear collisions, unlike pp reactions, we can use equilibrium thermodynamics in a large volume to compute the yield of strange and anti-strange particles. The latter, e.g., Î, might be significantly different from what one expects in pp collisions and give a hint about the properties of the quark-gluon phase."
124,275,0.987,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"5 The Assertion Problem In âIndeterminism and the Thin Red Line,â Belnap and I described what we termed the Assertion Problem as an issue that needs to be faced by anyone who theorizes about thought and action directed toward an open future. The problem was as follows. It would seem that one can make assertions about what one knows to be an open future, and in particular about aspects of that open future that are not yet settled by what has transpired thus far. One can assert that the coin will land heads, knowing full well that, as we may now suppose, the coin-tossing process is a fundamentally indeterministic one. But in the absence of a TRL, it is difficult to see how we can provide truth values to such assertoric contents as âThe coin will land headsâ when one history branching out of the moment of utterance contains a moment on which the coin lands head, and another history branching out of the moment of utterance contains a moment on which the coin lands tails. Were we to posit a TRL, then we could think of it as privileging one of these histories as the one that will happen, and thereby give us a state of affairs that settles the truth of the future-directed assertion. Barring that, it is not clear how we might characterize the context of utterance, or the circumstances of evaluation in such a way as to tell us whether the assertion is true. The context of utterance might provide values for indexical expressions, but it is less clear how the context of utterance selects one history from among all those that might be how things go. The intuitive datum seems, then, to be twofold: one can (i) reasonably, and, (ii) felicitously, make an assertion about an aspect of the future that is ontically open and thus not settled by what has gone thus far."
75,20,0.987,"Opening Science : The Evolving Guide on How the Internet is Changing Research, Collaboration and Scholarly Publishing","All of these umbrella terms struggle to find a clear definition and people often use them interchangeably when talking about current changes in scientific pursuits. We sought after defining each and every one of these terms in order to establish a coherent picture of how the change in knowledge creation is seen from different angles. Yet, what each of the terms means and how exactly it differs from the others is often unclear. If you ask five people how Mode 2 and Science 2.0 are associated you can be certain to get five different and possibly contradictory answers. All terms are somewhat born of the necessity that a term for the present changes was needed. Knowledge creation is a wide field and thus several terms emerged, whereof we would like to define only twoâmainly in order to use them in the discussions contained within this book. â¢ Science 2.0 refers to all scientific culture, incl. scientific communication, which employs features enabled by Web 2.0 and the Internet (in contrast to Science 1.0 which represents a scientific culture that does not take advantage of the Internet). â¢ Open Science refers to a scientific culture that is characterized by its openness. Scientists share results almost immediately and with a very wide audience."
192,90,0.987,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"course (a strategy known in phenomenology as á¼ÏÎ¿ÏÎ®); the floor is opened to other voices. A basic characteristic of the discourse of the analyst is that the ultimate target of desire, referred to by Lacan as the inexorable object a, comes into view, now in the position of the agent: triggering, commanding and frustrating the scientistsâ interminable work. This object challenges the prowess of scientists, arouses their desire, but continues to escape them, so that scientists emerge as tormented subjects ($ in the upper-right position). Thus, the novel becomes a philosophical laboratory where the (questionable) philosophemes of contemporary discourse are articulated and examined."
297,1004,0.987,The R Book,"Now, there is a signiï¬cant regression of y on x. The outlier is said to be highly inï¬uential. This makes our write-up much more complicated. We need to own up and show that the entire ediï¬ce depends upon the single point at (7, 6). This requires an explanation of two models rather than one. We cannot pretend that the point (7, 6) does not exist (that would be a scientiï¬c scandal), but we must describe just how inï¬uential it is. Testing for the presence of inï¬uential points is an important part of statistical modelling. You cannot rely on analysis of the residuals, because by their very inï¬uence, these points force the regression line close to them: reg <- lm(y1~x1) summary(reg) Call: lm(formula = y1 ~ x1) Residuals: 0.78261 0.91304 -0.08696 -1.08696 -0.95652"
124,500,0.987,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"ancestry, it just makes no sense to allow dense patches in the structure of possibilia, to say nothing of continuous ones. The motivation of (Closure) and, as it will turn out, also of (Anti-Symmetry), is more substantial. We want our theory of possible ancestry to respect the Kripkean claim of the necessity of (ancestral) origin: Any possible being has each one of its possible ancestors of necessityâa being with distinct ancestors just would be a distinct being. Therefore any possibility must be downward closed, i.e., for any being that it contains it must also contain each one of the possible ancestors of that being. In other words, possibly being an ancestor entails being an ancestor, so that in many situations we may abridge talk about possible ancestry to talk about ancestry.7 It turns out that the Kripkean claim also motivates (Anti-Symmetry). If ancestral origin were contingent, we might well have two distinct possible living beings A and B such that in one possibility being A is an ancestor of being B and in another possibility being B is an ancestor of being A. But because of the explication of the Kripkean claim in terms of (Closure), and in view of (Transitivity), A and B would be their own ancestors in each one of the two possibilities of this scenario, which obviously conflicts with (Irreflexivity). So, reflecting on the inadmissibility of such circular relations of ancestry as those between A and B lets us note that incorporating the relation of possible ancestry on the level of possibilia already does quite much to commit us to a Kripkean doctrine of the necessity of ancestral origin. Or, what probably amounts to the same thing, an incorporation of the relation of possible ancestry on the level of possibilia and a restriction of ancestral relations within each possibility like (Closure) make sense only when they are implemented together. Note that, according to the above definition, the empty set is admissible. This will be technically convenient later on,8 and it can also be motivated intuitively. For is it not possible that there is no (and there never has been any) living being at all? A similar claim that involved a truly unrestricted quantifier (i.e., that it is possible that there is nothing at all) might well be contentious. But in the case of the present framework, the intuitive background story has other entitiesâe.g., atoms, chemical compounds, water, air, and the planet Earthâbesides the possible living beings modeled by the elements of the domain D. Clearly an entirely uninhabited Earth is possible relative to some moments in time (especially in the far past), and we can even imagine entire possible histories in which live never evolves.9"
238,13,0.987,Nanoinformatics,"covariance. The inclusion of the covariance enables the interaction between the element type and crystal structure to be considered. A universal or complete set of representations is ideal because it can derive good machine-learning prediction models for all physical properties. However, finding a universal set of representations is nearly impossible. On the other hand, many elemental and structural representations have been proposed for a long time, not only in the literature on the machine-learning prediction but also in the literature on the"
305,57,0.987,Quantum Computing for Everyone,"denote these by E1 , E2 ,â¦ , En. The underlying assumption is that the result of the experiment, or measurement, will be one and only one of these n outcomes. Associated with outcome Ei is a probability pi . Probabilities must be numbers between 0 and 1 that sum to 1. In the case of tossing a coin, the two outcomes are getting a head and getting a tail. If the coin is fair, the probability of each event is 1/2. We return to the experiments involving the spin of a particle from the first chapter using a slightly more formal notation to describe them. Suppose that we are going to measure the spin in direction 0Â°. There are two possible outcomes that we will denote as N and S. Both of these outcomes will have an associated probability. We will denote by pN the probability of obtaining N, and pS the probability of obtaining S. If we already know that our electron has spin N in direction 0Â°, then we know that when we measure again in this direction we will get the same result, so, in this case, pN = 1 and pS = 0. On the other hand, if we know our electron has spin N in direction 90Â° and we now measure in direction 0Â°, then we are equally likely to obtain N and S as the outcome, so, in this case pN = pS = 0.5. Mathematics of Quantum Spin We will now present the mathematical model that describes quantum spin. It uses both probabilities and vectors. The basic model is given by a vector space. When we make a measurement there will be a number of possible outcomes. The number of outcomes determines the dimension of this underlying vector space. For spin, there are just two possible outcomes from any measurement, so the underlying vector space is two-dimensional. We will take the space to be ï2 âthis is the standard two-dimensional plane with which we are all familiar. This is fine for our purposes because we are only rotating our measuring apparatus in the plane. If we also wanted to consider all possible three-dimensional rotations of the apparatus, the underlying space would still be two-dimensionalâtwo is still the number of possible outcomes for each measurementâbut instead of using vectors with real coefficients, we would have to use vectors that involve complex numbers. The underlying vector space would then be the two-dimensional complex space denoted ï2. For the reasons listed in the previous chapter, ï2 is fine for our needs."
192,259,0.987,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"In a self-reflective mood, during a conversation with Paula Curry, Cantor confesses that scientific research is not as straightforward as is sometimes suggested. Most scientists suffer from what he refers to as a âdissociative personalityâ (p. 113). On the one side, they are rigorous believers in the experimental method with its set of rules, bent on advancing knowledge (in other words: S2). On the other hand, they remain fallible human beings with all the accompanying emotional foibles (in other words: $). One of the gravest occupational hazards in science, moreover, is simultaneous discovery. Sooner or later, somebody else will have the same idea. Scientists are driven by one desire: recognition by their peers (the Krausses of this world), but in order to obtain recognition, priority is essential. To score a Nobel Prize, one has to be the first to reach the summit. Thus, the push for priority is enormous. And the only way to establish priority is to be the first to publish. In other words, due to the confrontation with arginine (the object a), the self-contained expert (S2) falls victim to disruptive desire, and this results in a split (Spaltung) between adherence to methodological safeguards (S2) and the desire to maintain his advantage, his momentum, so that Cantorâs eagerness to score ($) suddenly seems to overrule his impeccable technique. Krauss is Cantorâs scientific conscience or superego. If the experiment proves impossible to replicate, Krauss may accuse him of sloppiness, or even fabrication: âSurely he is not calling you a â. Paula stopped shortâ, p. 109). And once someoneâs credibility in science is damaged, it can never be repaired. The only option left to Cantor is to do the experiment himself, to become his own Servant as it were, and to design a second experimental test, climbing Everest by a different route (p. 116). Because he cannot trust Stafford anymore (p. 113), he has to take the experiment literally in his own hands, doffing his costume for a lab coat. It is only via working through that the methodological requirements and desire for recognition can be reconciled again. Various instances of self-reflection can be discerned on the epistemic level. In his Nobel speech, looking back on his experiences, Stafford suggests that the failure of the Harvard team to replicate the results was due to a procedural discrepancy that was âreally quite trivialâ, adding that âif there is one lesson to be learned from this experience, itâs that even the smallest details should be put in oneâs notebook â¦ You never know which details may turn out to be crucialâ (p. 198). This self-reflection not only concurs with the principles of experimental methodology, but also with the psychoanalytic rule that one should report any observations; that one should take care not to exclude any of them, for in principle nothing is irrelevantâ. Even seemingly trivial details (the bagatelle) may prove to be highly significant (Freud 1917/1940, p. 297). But in the novel, the role of the analyst, listening to the dialogues (the flow of university discourse) with evenly-poised attention, and from an oblique perspective, falls to Leah, the expert in Bachtinian analysis (p. 82). She is not at all interested in proteins, membranes or arginine, but rather in the grammar of biomolecular discourse. When challenged to share her observations (by Jean Ardley, Leahâs supervisor, who happens to visit them), she points to the remarkably role of the term âweâ in experimental discourse. Why do scientists always use the pluralis majestatis (âWe, scientistsâ, âWe, the authorsâ) when speaking about science? What is wrong"
82,487,0.986,Fading Foundations : Probability and The Regress Problem,"Here A0 is the target proposition, which we sometimes wrote as q. In 3.4 we explained how to calculate P(q) by summing a geometric series. Here is the same story in terms of fixed points. The question is: is there a special value of P(An+1 ), say pâ , such that if we plug it into the right-hand side of (D.1), the very same value, pâ , results for P(An )? Indeed there is, for a unique solution of the equation Â© The Author(s) 2017 D. Atkinson, J. Peijnenburg, Fading Foundations, Synthese Library 383, DOI 10.1007/978-3-319-58295-5"
311,624,0.986,The Physics of the B Factories,"10â5 , then the expected number of observed events is 2.5 (The S.E.S. of an experiment is the branching fraction that would produce, given the experimentâs data set and eï¬ciency, an average over a statistical ensemble of one detected event). From Poisson statistics for Î¼ = 2.5, we calculate that the ensemble obtains the following results: â about 82 experiments observe no events; â about 205 experiments observe one event; â about 257 experiments observe two events; â the remainder, about 456 experiments, observe â¥ 3 events. For simplicity we assume that the experiments observe no background (this is typically the case for rare K and Ï decay searches). This assumption does not change our ï¬nal conclusions. The experiments that observe no events will set a 90% C.L. upper limit of 2.30 times the S.E.S. [see Section 36.3.2.5 and Table 36.3 of Beringer et al. (2012)] or 2.30 Ã 10â5 , which is below the true value. The experiments observing one, two, three, etc., events will set upper limits of 3.89, 5.32, 6.68, etc., times the S.E.S., which are above the true value. In this manner 8.2% of experiments obtain âincorrectâ upper limits, which is less than 10% of the ensemble and thus consistent with the deï¬nition of a 90% C.L. limit. Now suppose that each experiment that observed events looks at their candidate(s) and that some ï¬nd a kinematic or particle identiï¬cation variable (for at least one of the candidates) that is more than 2Ï away from the value expected for a signal event. These experiments then impose a 2Ï cut on that variable to eliminate the event(s) and adjust the S.E.S. upwards to account for the 4.6% loss in sensitivity. However, if up to 20 variables are potentially considered to be cut on, then the chance of an event surviving this procedure is only (0.9545)20 = 0.394. Therefore, after experiments observing events adjust a single cut value, approximately 82 + (1 â 0.394)(205) + [1 â (0.394)2 ](1 â 0.954)(257) = 216 experiments observe no events and set an upper limit of either 2.30 Ã 10â5 (no events originally observed) or 2.30Ã(S.E.S.)/0.954 = 2.41Ã10â5 . Both limits are below the true value. The fraction of experiments is 22%, which is larger than 10% and thus inconsistent with the deï¬nition of a 90% C.L. limit. The bias of the procedure has resulted in undercoverage. To avoid such bias, the decision whether to cut on a variable or not must be made before looking at signal candidate events. While a blind analysis does yield unbiased upper limits, it has a serious drawback in that it is possible to miss an obvious background, observe a large number of events in the signal region, and end up setting a poor upper limit. This situation does a disservice to the experiment, as the full âdiscriminating powerâ of the detector has not been utilized. Thus in practice, experiments carefully study signal candidates after all cuts have been ï¬nalized to check whether there are any due to a trivial background or instrumental problem such as the high voltage having been tripped oï¬. If such events are found, it usually is preferable to eliminate them and set a biased but useful upper limit rather than leave them and set an unbiased but not useful limit."
113,178,0.986,Pentecostalism and Witchcraft : Spiritual Warfare in Africa and Melanesia,"act that transmits occult powersâappear in the story. This understanding of kindoki, which we can take as the official Branhamist Christian explanation of kindoki through television, is somewhat different from Awakening Churchesâ explanations of how media is positioned in spiritual warfare. For the latter, viewing experiences (emotional unrest, social discord, etc.) are said to be indicative of either divine or demonic powers, which audiovisual footage transmits. Born-Again Christians refer to the Spirit of Hatred (molimo ya likunya), Jealousy (molimu ya zuwa), Sexual Deviation (molimo ya kindumba), etc. which are transmitted through images of Harry Potter, wrestling shows, science fiction, and music video clips of worldly and folkloric music. Shouting out Jesusâs name and changing the channel to a Christian TV station are strategies that Pentecostals of the Awakening Churches deem powerful enough to counter the footageâs negative influence (Pype 2012). Many Branhamist Christians, such as Fabrice, his parents, his pastor, and his friend, eschew television. While Fabrice does have a TV set at home, his family does not watch any television stations. Rather, they use the set together with a DVD player to watch DVDs and videotapes of sermons delivered by Brother Branham. Fabrice is more lenient towards the telephoneâan object about which Brother Branham did not preach. Still, usage is monitored closely, and, according to Fabrice, the same logic that shapes ideas about the devilâs hand in the television set are at play with the smartphone. âGood usageâ is perceived as crucial. Yet, not everybody knows what this âgood usageâ entails, nor are there ready-made instructions about this. Confusion regarding the possibilities of new technologies thus renders these objects suspicious in a religious scheme."
275,630,0.986,Foundations of Trusted Autonomy,"17 As an example of this, I recently played around with paradoxical statements about Peano Arithmetic - axioms about the behaviour of the natural numbers under the usual operators - using Provability Logic. Provability Logic system consists of familiar propositional logic with a modal operator  meaning âit is provable thatâ, its dual â¦ meaning âit is not disprovable thatâ, and LÃ¶bâs Theorem, which states that in any system containing Peano Arithmetic, any time we can prove that something implies its truth we may conclude that it is provable. We can use Provability Logic to explore and even to write computer programs to generate arbitrarily many generalisations of GÃ¶delâs Second Incompleteness Theorem [19] for us, by feeding it with paradoxical statements. Provability Logicâs implicit function theorem guarantees that we have unique solutions to a large class of self-referential expressions. For instance, the solution (â¥) â (â¥) to a paradoxical statement p â p happens to be direct restatement in Provability Logic of The Second Incompleteness Theorem, asserting that the system cannot prove that it is consistent, or equivalently, that if it can prove that it is consistent then it must be inconsistent. Here, the symbol â stands for if and only if, â is implication, and â¥ stands for contradiction. The statement â¥ says that the system is consistent. 18 Cantorâs Diagonalisation Argument was first used to prove that there are infinite sets that cannot be put into one-one correspondence with the natural numbers, and later to prove that the real numbers are uncountable, Russellâs Paradox whereby attempted formulations of set theory prior to Zermelo set theory are inconsistent, and GÃ¶delâs First Incompleteness Theorem, as well as the unsolvability of Turingâs Halting Problem [39, 45]."
223,412,0.986,Knowledge and Action (Volume 9.0),"State of the Art: Rational Choice Models of Mobility The mobility of hunter-gatherers is not a new field, so this chapter begins with a brief review of some of the existing anthropological models so as to prepare the ground for my theoretical argument. Probably the best known anthropological model in this respect is optimal foraging theory (Martin, 1983). It is particularly interesting because its application has not been limited to living hunters and gatherers but broadened to cover human behavior more generally. For instance, this theory has served to model human behavior in western-style museum exhibitions (Rounds, 2004). The assumption is that visitors to an exhibition optimize their visit by matching elements of high-interest value with low search costs and that there are some doâs and donâts that result in rules for deciding how long and in what order one should view the items at an exhibition. These rules (search rules, attention rules, quitting rules) are aimed not at the best possible solution but at one that is satisfactory given the environment as it is (p. 404). The original version of optimal foraging theory consists of theorems intended to explain when and how foragers move from one resource to another (see Kelly, 1995,"
32,523,0.986,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","to some extent, quantitative explanation. We look forward to a further discussion on many-body effect and on cluster-cluster interaction. The conventional social force model [12] assumes that each pedestrian experiences the social force from other pedestrians in his/her eyesight, which is described as an exponentially decaying repulsion. We expect that any isotropic, shortranged repulsive potential, including exponential one, would not deviate the overall property of the system from the results we obtained with linear elastic repulsion. On the other hand, introduction of anisotropic potential that reflects the fact that pedestrians react stronger to the situation in front of them is not clear and yet to be discovered. It is known that high crowd density leads to a turbulent movement of pedestrians and increases the risk of crowd disaster [16]. In spite of social demands to prevent such accidents from occurring and from spreading, their mechanism is yet to be uncovered, since experiments cannot be carried out due to ethical reasons, and observational data are hardly available. Previous pedestrian models assume that every agent is aware of its own destination and keeps driving itself until it reaches to that point. However, there are circumstances when pedestrians are not so conscious of where they are heading to. In fact we scanned the footage from the crowd disaster happened in Germany in 2010 and found that people sometimes behave as if they have lost or abandoned their initial destination in extremely dense crowd. To this end, we verified that our model, which has no fixed destination, could display bidirectional lanes similar to what is observed in pedestrian flows in a straight pathway. Here, the damping parameter in the model can be regarded as the quickness of oneâs reaction to a contact with neighbor walkers. However, the phase diagram shows that in higher density, the order develops for a broader range of the parameter, which does not meet with the empirical facts. By improving the model we expect that our result can lead to a further understanding on the mechanism of crowd disasters. Open Access This book is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
82,305,0.986,Fading Foundations : Probability and The Regress Problem,"Note that the above reasoning is independent of whether we embrace an objective interpretation of probability (assuming, for example, that the firefighters have propensities for handing over the water only now and then) or a subjective interpretation (in which we specify our degree of belief in A0 ). Both the objective and the subjective interpretation are bound by the rule of total probability, and that is all that counts here. This suggests that our approach is not restricted to epistemological series, but might be applied more generally to the metaphysical structures that Carl Gillet has been talking about. In fact, it might even be used to query similar reasonings in ethics. Richard Fumerton argued that his conceptual regress argument for foundationalism has a counterpart in the ethical realm. Suppose we are interested in whether an action, X, is good, and suppose we are being offered a series of conditional claims: if Y is good then X is good, if Z is good then Y is good, and so on, ad infinitum. Have we answered the original question? Fumerton believes we have not. At best we possess an infinite number of conditional claims, but this does not tell us whether X is good. Just as inferential justification only makes sense if there exists noninferential justification, instrumental goodness only makes sense if we assume that some things are intrinsically good: . . . the view that there is only instrumental goodness is literally unintelligible. To think that something X is good if all goodness is instrumental is that X leads to a Y that is good by virtue of leading to a Z that is good, by virtue of . . . , and so on ad infinitum. But this is a vicious conceptual regress. The thought that X is good, on the view that all goodness is instrumental, is a thought that one could not in principle complete. The thought that a belief is justified, on the view that all justification is inferential, is similarly, the foundationalist might argue, a thought that one could never complete. Just as one terminates a conceptual regress involving goodness with the concept of something being intrinsically good, so one terminates a conceptual regress involving justification with the concept of a noninferentially justified belief.17 The concept of intrinsic goodness stands to the concept of instrumental goodness as the concept of noninferential justification stands to the concept of inferential justification. Just as there are no good things without there being something that is intrinsically good, so also there are no inferentialy justified beliefs unless there are noninferentially justified beliefs.18"
78,44,0.986,The Onlife Manifesto : Being Human in a Hyperconnected Era,"4. Though we cannot deny that this attempt has yielded unprecedented results, we must also acknowledge that at some point the processed information must be reintegrated in what Stiegler (pace Husserl) has called our own primary retention (individual memory), to acquire meaning and to be part of our lifeworld (Stiegler 2013). 5. It is important, then, to note that the computational era is rooted in the most extreme type of dichotomous thinking: that of constructing discrete, machine readable bits. To be human, here, means to remember that life is continuous and plural and experienced rather than calculated. 6. The second problem with a dichotomy is that it assumes jointly exhaustive alternatives, which entails that the pairs forming the dichotomy cover all there is to be said about whatever they aim to describe. In his pivotal âThe duality of risk assessmentâ, Ciborra (2004) has elucidated how the hidden presumption that e.g. a risk analysis exhaustively describes a developing reality endangers the resilience of whoever depends on that analysis to remain safe. 7. Smart Grids, policing, medical treatment or the food industry should never assume that the data derivatives that inform their risk analyses cover all that is relevant. To prevent the kind of havoc that plagues our financial system we must instead keep an open mind, assuming that the computational decision systems that feed such critical infrastructure are as biased and fallible as any smart system necessarily must be. To be human, here, means to admit such fallibility as core to the wondrous fragility of life. 8. An interesting example of a dichotomy that confuses instead of clarifies what it means to be human in the computational era, is the dualism that pervades the domain of the philosophy of mind. The cartesian idea of a separate res extensa and a separate res cogitans that together describe reality has given rise to a series of interrelated problems that still haunt much of our understanding of e.g. responsibility and accountability in a world of distributed causation. To overcome the confusion that results from this kind of dualism I believe that we should not merely turn to overlapping instead of mutually exclusive dual pairs, but take leave of the idea that reality should necessarily be described in pairs altogether. 9. Whether it makes sense to think in pairs or in other types of distinctions should depend on the context and the aim of our thinking, not on a propensity to keep things simple. I would, therefore, rearticulate the heading and speak of: Beyond dualities. Long live plurality. Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
134,184,0.986,Curriculum Reform in The European Schools : Towards a 21St Century Vision,"current learning; present content to the learner; implement appropriate scaffolding processes; stimulate a performance by the learner; provide feedback to the learner which is a comment on their performance and allows corrective action to take place; and evaluate the corrected performance (cf. GagnÃ© 1985). Cognitivist theorists of learning commonly advocate instructional models of learning, because of the emphasis they place on invariant knowledge objects and schematic adjustments to accommodate these objects. A concept-formation learning process focuses on the re-forming of the conceptual schema held by the learner and one version of it is underpinned by an inferentialist pragmatist philosophy (cf. Brandom 2000). This positions knowledge and knowledge-development within networks of meaning, which are social in character and historical in origin. Learning is complex and potentially rich and rewarding, where the learner is presented with a mass of information, ideas, and opinions from a number of different sources (i.e. books, articles, lectures, seminars, emails, eseminars, personal communications and so on). What the learner does is shape this mass of information, and this shaping can take a number of different forms: partial shaping, complete shaping, discarding with no replacement, confusion, on-going, going backwards and forwards and so on. Shaping takes place against a scholarly background; aspects of which may or may not be implicit and where some but not all of its aspects can be surfaced for deliberation. Conceptual learning is irredeemably social, embedded, and selective. So the learner has to absorb some of the ideas they are presented with and discard or partially discard others. Again, this notion of concept-formation has elements of socio-cultural theories of learning. Reflection is a seminal form of learning. It has been variously described as critical reflection, reflective practice, reflective thinking and reflexivity. Whereas some see these terms as interchangeable and as having similar meanings, others have sought to differentiate between different types and levels of reflective activity (cf. Black and Plowright 2010). Not all reflection is critical reflection. Bolton (2010: 13) defined reflection (single loop activity) as âan in-depth consideration of events or situations outside of oneself: solitary or with critical supportâ, and reflexivity as a double loop process which includes reflection and reflexivity and is focused on"
247,184,0.986,Humanities World Report 2015,"to call yourself interdisciplinary. I think weâre well beyond the era in which borrowing methods and discourse from other disciplines makes one interdisciplinary. The bar that I would set today is that you have to be working with other people in different disciplines on common projects. NA10: Interdisciplinary work is almost by definition collaborative, because itâs very hard for people to control knowledge within one discipline, much less many. As8: Even in the single field of linguistics and culture, you may see a single scholar, or indeed scholars from a single country, with a different source of language and materials/resources that are oftentimes different. They have to work together and conduct collaborating research. E13: Iâd divide [interdisciplinarity] into two types: soft and hard. ... Hard interdisciplinarity involves specialists from different disciplines gathering together over a common object or text. As a result, their own individual ways of operating have to change. E2: Of course the term interdisciplinary is vague. Do we mean that scholars of different fields collaborate and that each brings in a specific expertise? Or do we mean, as some people claim, that we need to dissolve the disciplines altogether? (C) Post-disciplinarity: The last extract suggests a different concept altogether, also alluded to by another respondent: E9: Iâve heard the phrase post-disciplinarity used. Iâm not sure I understand it, but I donât think interdisciplinarity will or should take us to a point where disciplinary boundaries actually vanish. We shall leave post-disciplinarity on one side, since it was only of marginal interest to our respondents.7 By contrast, collaborative interdisciplinarity is extremely important to the humanities, as we shall see in more detail below. For the purposes of this chapter we shall refer to it as âmultidisciplinarityâ. One point should be uncontroversial: a multidisciplinary project will require individual researchers to be interdisciplinary in the first sense mentioned above (A)1, and to an extent the second (i.e. using the sources of other disciplines). But it surely does not require the third, (A)3, a strong sense of mastering another discipline; indeed the"
232,581,0.986,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"The attention toward examining the multiple impacts of disasters, and the challenges they make visible for large metropolitan areas, is not recent. One of the most famous historical examples is the 1755 Lisbon earthquake, which had a considerable influence on the emergence of scientiï¬c knowledge. The Great Lisbon Earthquake, estimated at M.8.5 to 9.0 on the Richter scale, is credited with transforming the social, philosophical, and metaphysical paradigms of the time. Some have gone so far as to state that this moment marked the beginning of the era of European Enlightenment [3â9]. Discussing the importance of the Great Lisbon Earthquake for continental philosophy, Gilles Deleuze argues that the event had an intellectual and metaphysical impact equivalent to the one of Holocaust in the twentieth century: It is very curious that in the eighteenth century, it is the Lisbon earthquake which assumes something like that, when across Europe people said: how is it still possible to maintain a certain optimism founded on God? You see, after Auschwitz raised the question: how it is possible to maintain a fading optimism about human reason. After the Lisbon earthquake, how is it possible to maintain the fading belief of rationality in divine origin?3 (My translation, [10])."
213,5,0.986,Collider Physics Within The Standard Model : a Primer,"of the Standard Model and bringing the reader up to the very frontier of present knowledge. The most touching aspect of these lecture notes is that reading them is just like listening to Guido. His style was direct and essential, and his logical thinking was always clear, profound, and focused on concepts rather than technicalities. From these lecture notes, the reader will not only learn about the Standard Model but also a way to approach physics. They are a faithful portrait of Guido, not only because they cover the field of his vast scientific activity but also because they convey his pragmatic and concrete vision of the world of physics. Guidoâs intellectual brilliance and physics intuition are perfectly reflected. They will be used regularly by generations of physicists and will remain as a tribute to an original and creative mind who did so much to shape the field of particle physics. Geneva, Switzerland June 2016"
148,223,0.986,Anti-fragile ICT Systems (Volume 1.0),"11.4 Overview of HTM The HTM learning algorithm models how learning occurs in a single layer of the cortex. Input to the algorithm is a continuous stream of input patterns from some kind of system. HTM builds sparse, invariant representations of pattern sequences representing repeated structures in the input stream. The algorithm learns which patterns are likely to follow each other, thus learning to predict future patterns. When the HTM receives a novel pattern, it will try to match it to stored patterns. Because inputs never repeat in exactly the same way, invariance of the stored sequences is vital to the ability to recognize inputs. Time plays a crucial role in HTM. Predictions can only be made on the basis of a sequence of earlier received patterns. Sometimes it is enough to know the previous pattern most recently received while at other times it is also necessary to know patterns received earlier. The ability to predict using variable-length sequences of patterns is due to the variable order memory of HTM. Note that HTM does not understand the meaning of patterns; it only knows what patterns are likely to follow particular observed patterns."
95,145,0.986,Elements of Robotics,"Cars cannot move up and down unlike helicopters and submarines which have greater freedom of movement. This is expressed in the concept degrees of freedom (DOF) which is the subject Sect. 5.10. Section 5.11 discusses the relation between the DOF and number of actuators (motors) in a robotics systems. The number of DOF of a system does not mean that a system such as a vehicle can move freely in all those directions. A car can move to any point in the plane and orient itself in any direction, but it cannot move sideways, so a difficult maneuver is needed during parallel parking. This is due to the difference between the DOF and the degrees of mobility (DOM), a subject explored in Sect. 5.12, along with the concept of holonomic motion that relates DOF and DOM."
317,19,0.986,Transitions in Mathematics Education,"temperature at a rate proportional to the difference in temperatures between them. Six independent mechanisms (basic processes) were identiï¬ed in the case study. Several of these processes seemed âdiscrete,â such as developing a chain of causal links (A causes B; B causes C; â¦), the totality of which was precisely the causality behind Newtonâs thermal laws. The chain was primed by one previously identiï¬ed intuitive idea, and contained at its core another very important and previously identiï¬ed intuitive causal link. Other parts of the construction seemed less âlogical and scientiï¬c.â For example, the central intuitive causal link was expressed in overtly anthropomorphic ways (âthe objects in contact want to be in equilibrium, and are freaking out because they are so far apartâ). Remarkably, the anthropomorphism (âwantingâ and âfreaking outâ) gradually disappeared from the studentsâ talk, and, in the end, they had a very professional-sounding version of Newtonâs laws: The rate of temperature change is proportional to the difference in temperatures. The anthropomorphism seemed to be a scaffold and not a permanent part of the construction, just like scaffolds are critical, but they come and go in the construction of a house (and may, thus, never be seen in before-and-after studies!). 2.5.2"
108,47,0.986,Bordieuan Field Theory as an Instrument for Military Operational Analysis,"form of warfare. The answer is affirmative, with the finesse that the enemy does not need to be completely identified but just by his methods and goals. The aim is to attack a critical vulnerability of the enemy by altering the social, political and economic reference system. In a paraphrase of a famous saying by Mao, one might say that it is a question of changing the water in which the fish are swimming. The critical vulnerability of the enemy is his political legitimacy in the mission area. The attack is conducted by changing the conditions of the field so that even unidentified political actors may also find themselves under attack. If the vulnerable point is attacked, the enemyâs centre of gravity will have to be adjusted accordingly. For this approach to be effective, the actor must be a genuine political actor, an actor interested in political gain. If the actor has no political ambitionsâperhaps in the case of organised crime for financial gainâthen there is also no reason not to countermine the actorâs position as a player on the field. However, it is important that there is a general perception that the actor is not a political actor. It is the judgement of the field, not that of the actor, that determines whether an actor is political or not.2 It is worth stressing the importance of qualitative knowledge of the actors on the field, in the same way that knowledge of the fieldâs own structure is key information. A field theoretical approach to operations should strive to give the enemy information of a type that is not disinformative. As it is a long process, it is important to not undermine the trust in the process with disinformation. The information presented should fit with the stated aim of influencing the actor to change his strategy. It involves information in the form of action rather than words, to clearly show the disadvantages of a strategy of violence by confronting him with a well-armed and wellequipped military force. It is also important to clearly show the alternative strategies that are possible. The latter should be combined with making it obvious that the positions of non-violent actors will be greatly enhanced, whatever their political standpoint. In this way, it will become clear to all that those with a non-violent political agenda will experience a considerable enhancement of their power, while the increase in power for the actors using violent means will be obstructed or will suffer a reverse. It is thus important to inform the enemy in the correct manner. As this will change the distribution of power on the field, one might see different and new"
278,360,0.986,Taking Stock of Industrial Ecology,"One can no longer even imagine that there could be a single standard of value by which to measure things. The neoliberals (â¦) are singing the praises of a global market that is, in fact, the single greatest and most monolithic system of measurement ever created, a totalizing system that would subordinate everything â every object, every piece of land, every human capacity or relationship â on the planet to a single standard of value. â David Graeber (2001). Toward an Anthropological Theory of Value: The false coin of our own dreams, p. xi."
8,679,0.986,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","about 1975 on, so many papers appeared that I must refer to the articles collected in the Bielefeld proceedings of 1980 and 1982 [94, 95], where references are given. In the present context, the approach from the hadron side is of interest. The obvious argumentâwho did invent it?âwould be: if you compress hadron matter sufficiently, hadrons will overlap and cease to exist as such. You then have a quarkgluon soup. The ideas was used in many papers and got much support from the MIT bags [96], which are quark-gluon matter inside, hadrons outside. As many of the properties of bags strangely resemble the clusters (formerly called fireballs) of the SBM, one might expect that bootstrap thermodynamics would be equivalent to the thermodynamics of a bag gas. Indeed, an explicit model constructed by Baacke [97] made a phase transition very likely to happen near T0 . Two years earlier, Cabibbo and Parisi [98] had already proposed that the singularity found in SBM at T0 should be related to the transition to the quark-gluon phase. These two arguments supported each other beautifully: SBM would provide for the singularity necessary for a true phase transition, while the bag gas model produced the crossing of the pressure curves of the plasma and the hadron gas near T0 . Unfortunately, SBM was still in an underdeveloped stage in which two arguments spoke against Cabibbo and Parisiâs interpretation: â¢ The energy density E ! 1 when T ! T0 . â¢ Quarks and gluons did not appear anywhere in SBM. Therefore, how could a singularity, which could not be reached at finite energy density, indicate a transition to a phase whose constituents did not appear in the model? I felt strongly this way. Some people, however, feltârightlyâthat these objections would be overcome by technical development without touching the essential features of SBM. Thus a number of papers, originating mostly in the fertile and critical soil of Bielefeld, investigated the various aspects of a phase transition, in the presence of an exponential spectrum: critical exponents, influence of fugacities, types of transition, and others [81, 99â106], to mention only a few references. At this time, W. Nahm, during a workshop at Erice [107], severely criticized the shortcomings of the model and pointed out that they would forbid its application in astrophysics and to phase transitions, while particle physics applications might still be reasonable [108]. His criticism coincided, however, with a new formulation of SBM, just presented at the same workshop, in which most of the inconsistencies were removed [109], and to which we now turn."
280,451,0.986,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"About 150 years ago, following H.W. Batesâ report on mimicry in insects (Bates 1862), Charles Darwin wrote to Bates and said: âIn my opinion, it is one of the most remarkable & admirable papers I ever read in my life. The mimetic cases are truly marvelous. . .â (Darwin 1863). Today, the mimicry phenomenon remains as an interesting evolutionary theme as ever, attracting the interest of both scientists and the public. To better understand the molecular mechanisms behind insect mimicry, we need to understand how wing and body color patterns evolve."
311,983,0.986,The Physics of the B Factories,"A quasi-two-body approach to extracting CKM parameters is not ideal as these modes often interfere with other resonances as well as non-resonant decays to the same final state. As a result, quasi-two-body measurements have an unknown uncertainty in their reported results that requires careful consideration. In principle, these eï¬ects can be taken into account by a Dalitz Plot (also known as a Dalitz Plane) analysis. The major advantage to the Dalitz Plot is that it gives access to the phases as well as the magnitudes of the resonances. Since the weak phase changes sign under CP but the strong phase does not, the weak and strong phase components can be extracted by subtracting or adding together the B meson flavor-tagged Dalitz Plots. In some Dalitz Plots, the weak phase can often be directly interpreted as one of the Wolfenstein angles e.g. Dalseno (2009). The mathematical formalism for a Dalitz Plot analysis is given in Chapter 13. In this section we consider the experimental problems in its implementation. The extension of quasi-two-body charmless decays to three-body charmless decays brings with it greater complexity but provides a deeper understanding of the decays"
307,418,0.986,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"13.4.3.1 The Probability Density Function of the Blocked State Is Proportional to the Probability Density Function of the Wild Type Closed State In Fig. 13.9, we show the open probability density functions of the wild type (defined by system (13.22), the mutant (defined by system (13.23) with  D 3/; and the mutant including the optimal drug (defined by system (13.27)). As expected, the open probability is completely repaired by the theoretical drug. In the right panel of the figure, we show the graph of c for the wild type (solid line) and for the mutant case in the presence of the open blocker. We show both c and b . We note that these graphs seem to have the same shape and we will show that they indeed differ only by a constant. We start by making the ansatz that for the solution of system (13.27) we have b D ."
99,275,0.986,Social Innovations in the Urban Context,"When analysing the cases of innovation, our aim was to find out whether there are recurring features that give them a distinct profile. Altogether they represent forms of acting and thinking that can be defined first of all in negative termsâbreaking up with the traditions both of what we call âindustrial welfareâ and the more recent wave of managerial and neo-liberal reforms. However, as we will show, these innovations can also be defined in positive terms. Recurring features point to a certain style of doing things, a shared culture and perspective of thinking and acting across national borders that makes a difference to the past. It was a key task of our analysis to deal with the question what can be generalised from these innovations, their approaches and the tools and instruments developed by themânot only in the special local system within which an innovation was taking place but also at the level of an international debate on"
289,1014,0.986,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","of the relational logic, we can conclude that (Ï1â² , Ï2â² )  QÌ. Finally, we prove that spÌ2 , Ï ââ skip, Ï â²  by showing that non-termination of the product implies the non-termination of at least one of the two original program runs. If the condition of a loop in the product remains true forever, the loop condition of at least one encoded execution must be true after every iteration. We show that (1) this is not due to an interaction of multiple executions, since the condition for every execution will remain false if it becomes false once, and (2) since the encoded states of active executions progress as they do in the original program, the condition of a single execution in the product remains true forever only if it does in the original program. A similar argument shows that the product cannot diverge because of infinite recursive calls."
117,410,0.986,Care in Healthcare : Reflections On Theory and Practice,"Space as a Social Product Henri Lefebvre (1901â1991) was a French philosopher and sociologist engaged with existential ideas (Elden 2004). In his prolific career, Lefebvre wrote more than 60 books and 300 articles covering a wide range of topics. In his work, Lefebvre shows an interest in the dialectic and he tends to work with three terms rather than the dualism of the two. He conceives the three as affecting each other simultaneously, without prioritizing one term over another. Instead of searching for a transcendence, a synthesis or a negation, he studies the continual movement between them. Lefebvre has written about space in The Production of Space (1974/1991). In this book, he argues that space is a social product, or a complex social construct (based on values and the social production of meanings), that affects spatial practices and perceptions. â(Social) space is a (social) product [...]; the space thus produced also serves as a tool of thought and of action [...] In addition to being a means of production it is also a means of control, and hence of domination, of power.â Although his work is complex and not about care practices, some of his insights might be helpful to better understand the meaning of space for humanizing healthcare. Lefebvre criticizes the binary notion of objective and lived space for still starting from the subjectivity of the ego. Lefebvre aims to a materialist version of phenomenology in which the epistemological perspective shifts from the subject that thinks, acts and experiences to the process of social production of thought, action and experience. According to him, space is fundamentally bound up with social reality. Space does not exist âin itself â; it is produced. Lefebvre proceeds from a relational concept of space and views space as a social product. This calls for an analysis that would include the social constellations, power relations and conflicts relevant in each situation. This would also imply the shift of the research perspective from space to processes of its production; the embrace of the multiplicity of spaces that are socially"
223,200,0.986,Knowledge and Action (Volume 9.0),"In science such a cognitive division is not only perceived as a matter of course but is also generally understood to be a functional characteristic of science as an institution. Not every scientist can work on just any question. And the role of every scientist cannot be classified in relation to itself, but only in relation to that of other scientists. It is therefore natural to speak of a cognitive functional differentiation in all societal institutions. In other words, it can make sense only to speak of a range of knowledge in groups of actors in comparison to symmetrically limited knowledge in other groups of actors, and not of knowledge and non-knowledge."
285,391,0.986,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","This study shows that by using resting-state fMRI in combination with independent component analysis we are able to detect differences in neural networks between substantially-sized subject groups. Resting-state fMRI is a valuable addition to traditional task-related fMRI, where a difference between conditions is obtained. This might not necessarily be the best paradigm to study tinnitus. The consequence of a task-related design is that, by using a subtraction approach, the tinnitus-related activity, present in both conditions, will not be present in the subtraction image unless the task affects the perception of tinnitus (Lanting et al. 2009)."
13,54,0.986,Feeling Gender : a Generational and Psychosocial Approach,"object-relations and the interrelational world of real subjects (Winnicott 1971; Benjamin 1995). Thus, the relational world gradually becomes both internally object-relational and interpersonally intersubjective: we may become both âlove objectsâ and âlike subjectsâ to each other, as Benjamin (1995) coins it. This means that whenever two people interact, there are at least two selfâother psyches at play. If the inner objects do not overwhelm and invade us, we may experience a non-narcissistic interaction based on mutual recognition. The more we are able to see the other as a whole and separate subject, the more we can also integrate negative feelings we may have towards the inner object. Coming to terms with the internal and external parents is a major developmental project and a lifelong internal process for most people (Chodorow 2012: 47). Winnicott points out that for the child, it is the intrapsychic aggression towards the object that makes it possible to recognise the other as a like subject, as it gives the child the possibility to experience that the other continues to exist in spite of the childâs aggression. Through this, narcissism or omnipotence is broken: there are others out there with a separate centre of existence and their own agendas. Benjamin states that in this way destruction is âthe Other of recognitionâ (1995: 48). The capacity to deal with both kinds of relationships is developmentally intertwined and therefore the pains of loss and the pleasures of attachment are equally determinant in subject formation (Layton 1998: 18â19). It never becomes a harmony, but implies continuous disruption and repair. In the intrapsychic as well as in the interpersonal space, the sense of self and others is constructed through transference, which includes the universal psychic capabilities of the human mind like positive and negative identifications (I see myself and the other as alike in some respectâand I can like it or not like it), introjections (I see something of the other as part of myself ), projections (I place something of myself in the other), affective ambivalence (I love and hate the other at the same time), disidentifications (this is not me!), splitting (dividing things into only good and only bad and projecting the aspects onto others or myself ), disavowal (refusing to recognise the reality of a traumatic experience) and idealisation (aggrandising and exalting the other and thereby also enlarging myself ).3 These"
264,1032,0.986,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"What Does it Mean to Teach Mathematics in General or a Particular Mathematical Concept or Process? What Does it Mean to Teach it Well? According to Ricardo Cantoral, to teach a fundamental mathematical concept (e.g., derivative), we need to ï¬nd a âcultural basisâ for it and help students anchor their understanding of the concept in this cultural basis (e.g., the idea of tasteâsweetness, salinityâcan be used as a cultural basis for the concept of rate and hence of slope of a linear function). Higinio Dominguez looked at a more social-interactive aspect of teaching and proposed that to teach well it is necessary to engage in âreciprocal noticingâ with students: âIn reciprocal noticing, what is noticed is not individual reasoning but rather the emerging and continuous influence of peopleâs reasoning WITH (not for) one another â¦.â Cristina Fradeâs perspective was focused on the person of the teacher, but it was not psychological: the question was how a person constructs her or his identity as a teacher. This construction is an important element of becoming a teacherâand therefore, a condition of âteaching wellâ. It is social in nature, by means of language: the individual develops a vocabulary with which to justify their actions, compare their past and present behaviors, and generally narrate their live stories. A theory of teaching well was the main concern of Yasuhiro Sekiguchiâs presentation. The theory of âSchool as Learning Communityâ (SLC) was proposed in the 1980s by Manabu Sato and has become the foundation of a school reform in Japan (Saito et al., 2015). Preparation for teaching well is sought through a well-developed system of âlesson studyâ conferences and developing collaboration between children, teachers, parents, and people in the region."
208,114,0.986,Actors and the Art of Performance,"Innocence of becoming Simultaneous dependency on the effectiveness of and on the absence of the will is a paradoxical problem no intellect can solve. It throws the actor into a state of contradiction. The greatest contradiction is the fact that his regulatory reason, his ratio, cannot control his will and must make space for capricious fabulation. Without the power of imagination, without creative inventiveness â which, diametrically opposed to conceptual reason, never has an inkling of its âresultsâ â there can be no artistic work. But it is easier to write or read about this late modern collapse than to put it into action oneself. This is the apex, the raw nerve, of the art of acting. The highest demands are made of the professional actor â impossible for a lay actor â by the paradox present in every production. Every evening, in opposition to the mythology of modernity, he must surrender to the innocence of becoming.19 Again and again he must willfully step into the voltage field of opposed poles, the conflicting powers of this innocence of becoming. The ability to meet this challenge is the actorâs know-how (techne). It is a long way to the intentionless intention20 of acting on stage."
8,195,0.986,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","9.3 Open Questions Another direction is represented by my recent attempts to visualize the geometric picture of hadron collisions. By comparison of ISR and LHC data it appears that protons become more dark (absorbing), and larger in size with an increase of energy. Analysis of elastic pp scattering data with use of the irrefutable unitarity condition gives rise to a speculative conclusion about very dense (absolutely black at the center) state of matter created at LHC energies in pp collisions [15]. A new critical regime of full absorption and rather wide spatial extension has perhaps been reached at the LHC. This could imply that a limiting temperature regime advocated by Hagedorn has been reached, and spatial expansion with increasing number of degrees of freedom prevails. That could correspond to the constancy of this new Hagedorn temperature. Further implications for pA and AA collisions should be studied to learn what kind of possibly new matter is produced in these processes. Highest LHC energies can lead to new completely unexpected features also discussed in [15]."
8,833,0.986,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","at P D 0, since both have the same energy density "" D 4B and therefore V.P D 0/  1= D U="" D U=4B. However, what we cannot see by inspecting Fig. 27.3 is that there will be a discontinuity in the variables  and T at this point, except if parameters are chosen so that the critical curves of the two phases coincide. Indeed, near to P D 0, the results shown in Fig. 27.3a should be replaced by points obtained from the Maxwell construction. The pressure in a nuclear collision will never fall to zero. It will correspond to the momentary vapour pressure of the order of 0:2B as the phase change occurs. A further aspect of the equations of state for the hadronic gas is also illustrated in Fig. 27.3a. Had we ignored the finite size of hadrons (one of the van der Waals effects) in the hadron gas phase then, as shown by the dash-dotted lines, the phase change could never occur because the point particle pressure would diverge where the quark pressure vanishes. In our opinion, one cannot say it often enough: inclusion of the finite hadronic size and of the finite temperature when considering the phase transition to quark plasma lowers the relevant baryon density (from 8â 140 for cold point-nucleon matter) to 1â50 (depending on the choice of B) in 2â5 GeV/A nuclear collisions [21]. The physical picture underlying our discussion is an explosion of the fireball into vacuum with little energy being converted into collective motion, e.g., hydrodynamical flow, or being taken away by fast pre-hadronization particle emission. Thus the conserved internal excitation energy can only be shifted between thermal (kinetic) and chemical excitations of matter. âCoolingâ thus really means that, during the explosion, the thermal energy is mostly convered into chemical energy, e.g., pions are produced. While it is at present hard to judge the precise amount of expected deviation from the cooling curves shown in Fig. 25.3, it is possible to show that they are entirely inconsistent with the notion of reversible adiabatic, i.e., entropy conserving, expansion. As the expansion proceeds along U D const. lines, we can compute the entropy per participating baryon using Eqs. (27.38) and (27.39), and we find a significant growth of total entropy. As shown in Fig. 27.3b, the entropy rises initially in the dense phase of the matter by as much as 50â100% due to the pion production and resonance decay. Amusingly enough, as the newly produced entropy is carried mostly by pions, one will find that the entropy carried by protons remains constant. With this remarkable behaviour of the entropy, we are in a certain sense, victims of our elaborate theory. Had we used, e.g., an ideal gas of Fermi nucleons, then the expansion would seem to be entropy conserving, as pion production and other chemistry were forgotten. Our fireballs have no tendency to expand reversibly and adiabatically, as many reaction channels are open. A more complete discussion of the entropy puzzle can be found in [1]. Inspecting Fig. 27.2 again, it seems that a possible test of the equations of state for the hadronic gas consists in measuring the temperature in the hot fireball zone, and doing this as a function of the nuclear collision energy. The plausible assumption made is that the fireball follows the âcoolingâ lines shown in Fig. 27.2 until final dissociation into hadrons. This presupposes that the surface emission of hadrons during the expansion of the fireball does not significantly alter the available energy"
275,53,0.986,Foundations of Trusted Autonomy,"2.4.1 MC-AIXI-CTW MC-AIXI-CTW [85] is the most direct approximation of AIXI. It combines the Monte Carlo Tree Search algorithm for approximating expectimax planning, and the Context Tree Weighting algorithm for approximating Solomonoff induction. We describe these two methods next. Planning with sampling. The expectimax planning principle described in Sect. 2.3.4 requires exponential time to compute, as it simulates all future possibilities in the planning tree seen in Fig. 2.2. This is generally far too slow for all practical purposes. A more efficient approach is to randomly sample paths in the planning tree, as illustrated in Fig. 2.3. Simulating a single random path at et . . . am em only takes a small, constant amount of time. The average return from a number of such simulated paths gives an approximation VÌ (Ã¦<t at ) of the value. The accuracy of the approximation improves with the number of samples. A simple way to use the sampling idea is to keep generating samples for as long as time allows for. When an action must be chosen, the choice can be made based on the current approximation. The sampling idea thus gives rise to an anytime algorithm that can be run for as long as desired, and whose (expected) output quality increases with time. Monte Carlo Tree Search. The Monte Carlo Tree Search (MCTS) algorithm [2, 11, 36] adds a few tricks to the sampling idea to increase its efficiency. The sampling idea and the MCTS algorithm are illustrated in Fig. 2.3. One of the key ideas of MCTS is in optimising the informativeness of each sample. First, the sampling of a next percept ek given a (partially simulated) history Ã¦<k ak should always be done according to the current best idea about the environment distribution; that is, according to M(ek | Ã¦<k ak ) for Solomonoff-based agents. The sampling of actions is more subtle. The agent itself is responsible for selecting the actions, and actions that the agent knows it will not take, are pointless for the agent to simulate. As an analogy, when buying a car, I focus the bulk of my cognitive resources on evaluating the feasible options (say, the Ford and the Honda) and only"
214,395,0.986,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"But broadly, there is the interesting property that the wisdom of crowds applies. If the models have random mistakes and their biases are uncorrelated, then the average of the models (the multi-model mean) tends to be better than most of the models. This is a big âif,â and many models have dependencies, but in practice (and in synthetic statistical tests), because the climate system is complicated, and most models meet basic measures of representing the system (conservation of energy and mass), the multi-model mean is usually a pretty good statistic. This makes ensembles quite valuable. One thing to remember is that there is lots of chaotic internal variability in the climate system, and the observed record is only one possible realization of that internal variability. There may or may not be an El NiÃ±o event this year, but the possibility is that there could have been. So with the real climate system, we have only one ensemble. We expect models to be different from observations in any given year, or even decade, if they are fully coupled and internally consistent. This also confounds prediction and evaluation. It is one reason why the present and the past do not fully constrain the future (see box). Why the Present and Past DO NOT Constrain the Future It is often assumed that a model must be able to represent the present (or past climate) correctly to represent the future. This is true. However, while necessary, it is not a sufï¬cient condition to constrain the future. Suppose that a model represents the present for the wrong reasons. Perhaps there is a large compensation of errors: Maybe an error in the radiation code letting in too much energy is compensated for by an error in the sea-ice model that reflects too much energy. If the planet warms up, the sea ice will go away, but the error in the radiation code will remain. It is also assumed that the more accurate a model is at representing the present (or the past), the better it will be at representing the future. This is also true, but it requires that âbetterâ representations be for those areas that matter for future climate, and this is not necessarily the case. What does that mean in practice? Letâs say you have two measures of model performance. One is based on clouds, and one is based on temperature. If model A scores 80 % on both temperature and clouds relative to observations, and model B scores 100 % on clouds but 20 % on temperature, then a simple average score says that model A scores 80 % and model B scores 60 %. But if present clouds are more important for future climate than temperature, model B may be better. In the previous example, the present-day model may score well on sea ice and lower on the radiation code, but the radiation code is more important for the future. Also, knowing the mean state today does not imply that you can predict the change in that state accurately in the future. It is usually a good indicator, but not guaranteed. That covers why the present does not necessarily constrain the future. Because with present climate we do not know what the response to a forcing (i.e., feedbacks) will be. But what about using the past: If we know"
214,67,0.986,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"given amount of forcing from CO2 and other gases. So it becomes a useful metric (measure) of where the system might be heading. We cover feedbacks in more detail when discussing the speciï¬c components of the climate system and how we model them in Sect. 3.2. As described in the box, there are both positive feedbacks (like the ice-albedo feedback) and negative feedbacks (like the temperature feedback) in the climate system. Put a lot of complex feedbacks together, and the climate system starts to sound less like the single screech of an electronic microphone and more like the complex tones of a symphony, perhaps one without a conductor. Many of these tones or combinations of tones have feedback loops, like the example of the sea iceâalbedo feedback described here. We discuss this feedback more in Chap. 5, after we introduce additional concepts about how the atmosphere works. To understand the climate system and how it will evolve over time, it is critical to understand these complex interactions. Most climate science is dedicated to understanding the components of the system, their coupling, and how they work now and have worked in the past, so that we can understand how they might work in the future. The imperative to understand the climate system stems solely from how society is affected by variations in high-frequency extreme weather events and lower frequency climate extremes and how they may change over time in response to different forcings within the system. Or even with no forcing of the system. The importance of understanding weather and climate is independent of any human influence. Even if we did not have reason to think the climate was changing due to human activities, it would still be important and critical to understand and simulate the climate system to predict the natural variability and potential changes in the system so that we can adjust. This is also called adaptation to climate changes, as in adapting our society to a new climate."
8,819,0.986,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","a relation we have used occasionally before [see Eq. (27.10)]. From Eq. (27.57), it follows that, when the pressure vanishes, the energy density is 4B, independent of the values of  and T which fix the line P D 0. This behaviour is consistent with the hadronic gas phase. This may be used as a reason to choose the parameters of both phases in such a way that the two lines P D 0 coincide. We"
285,160,0.986,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","Since the Silence and Virtual-Finger conditions of experiment 2 produced significantly different summation effects, it seems that a portion of the loudness constancy effect observed in experiment 1 may have been mediated by the residual sound at the occluded ear. Both the Silence and the Virtual-Finger conditions produced a collapse in externalisation for the monaural case, so it seems unlikely that the difference in effect can be attributed to this cause. On the other hand, 2.5â3 dB of summation was still observed in the VirtualFinger condition, whereas less that 1 dB was observed in the Loudspeaker condition of experiment 1. It appears, therefore, that the listenersâ awareness of meatal occlusion, which would only be present in the first case, may still play a role. A major caveat to these conclusions is that KEMAR was not designed to realistically simulate the effects of bone-conducted sound, which likely plays a major role in listening in a sound field with an occluded ear. While probe microphones in the ear canal might more accurately record the stimulation at the tympanic membrane, they would still not capture the stimulation occurring at the cochlea. The small effect of stimulus type observed in experiment 1, was reversed in experiment 2. Taking these small and inconsistent effects together, we found no evidence that binaural loudness constancy is greater for more ecologically valid stimuli such as connected speech. Indeed, for both speech and noise, occlusion of the ear led to little reduction in loudness (a small summation effect), suggesting that loudness constancy occurs independently of stimulus type. It should be noted, however, that quite prominent reverberation was present in our experiment, which reliably cued the existence of an external sound source."
223,92,0.986,Knowledge and Action (Volume 9.0),"In Laclau and Mouffeâs thinking this idea of an agonistic democracy is also extended to their own normative claim for radical democratization, as the very nature of the process of radical democratization is itself part of an agonistic debate and depends on a contingent, but at least largely shared, symbolic space (Mouffe, 2005, p. 121). This extension, however, still does not explain how such a political debate or deliberation takes place and what the radical democratic politics look like in action, not just as a starting point or outcome. At this juncture the interactionist outlook seems to be more useful. In general it is clear how the different traditions of social theory, ranging from mentalist and interactionist to textualist points of view, each address different complementary aspects of the political praxis put forward by Laclau and Mouffe (1985). To develop this approach to praxis theory further, it is essential to rethink concepts of reason and rationality so as to create space for pluralistic forms of rationality and for transversal reflections (Welsch, 1999), even for rational interventions. This space seems to have been obscured thus far by the concepts used by Laclau and Mouffe (1985), which were inspired mainly by Marxist and poststructuralist"
28,236,0.986,A History of Self-Harm in Britain,"patients â supposedly for reasons of brevity â as âcuttersâ.40 In this study the behaviour of âcuttersâ is significantly differentiated from socially embedded hysterical, cry-for-help communications as well as earnest suicidal attempts. There is a sense in which this behaviour is new and unsettling (repeatedly labelled âbizarreâ) and that it repeatedly (if subtly) confounds existing categories. This clinical object is not self-evident, but is the result of human analysis and intervention. McEvedyâs separation of this action from the social setting is based upon a differentiation between âspontaneousâ and âsusceptibleâ cutters â between those who perform the action on their own initiative, and those who try it in response to another patientâs actions. There is a subtle relationship between these groups, but there is little doubt that the copying is â ultimately â secondary to the unity of the syndrome of cutting. McEvedy notes, with regret, that he would like to re-categorise the patients in order to separate out those who perform the act without any imitation of others. However, isolating these nonimitators requires too much of a reworking of the material. He has no doubt that some of his Bethlem group are only classed as repeated self-lacerators (those with five or more cutting incidents) because they happen to be present during the self-cutting epidemic.41 This makes it clear that the key to the syndrome is in the internal impulse, not the social imitation. McEvedy argues that the most notable aspect of behaviour is the apparently unprovoked mood swings â bringing emotional states to the fore. These are distanced from the social environment in the case of Kay R., who is said to have cut herself over and over, regardless of environment or levels of stress. McEvedy finally distances cutting from a socially motivated phenomenon because the impulse (presumed to underlie all the behaviours â from cutting forearms to swallowing dominoes) seems so unorthodox that it cannot be explained by mere social pressure or stress. He reasons that there must be something preventing the (supposedly suicidal) impulse from being conventionally expressed.42 That it might also be connected with hysterical, susceptible imitators does not change the fact that the behaviour begins in a pathological, emotional, internal impulse rather than a disordered social setting. McEvedy is not entirely sure what might replace the powerful aetiological force of the social environment or imitation, but he speculates thatthe so-called âspontaneousâ cutterâs personality has not only a high level of hysterical traits, but also something that he labels âhostile tensionâ.43 This is the first mention of âtensionâ as a key motive force for âcuttersâ in Britain â something that is well established in the"
166,37,0.986,Surveying Human Vulnerabilities across the Life Course (Volume 3.0),"sense of the word2 but are also âproducedâ by the system that was used for observation and the context in which it operates (see DesrosiÃ¨res 2001; HÃ©ran 1984). This is not a limitation in the validity of surveys themselves, just an important point to keep in mind and to document before using data, and particularly when they have been collected in a complex, non-standard context. In other words, to take into account the characteristics of the data used and the way they were produced is a basic condition for their proper scientific use. It also means that it would be dangerous to separate the roles of methodologists and social scientists, as they are profoundly interdependent. This book as a whole demonstrates that the union of methodological and substantive issues is a prerequisite for a good scientific work."
124,89,0.986,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"account of classifying states of affairs as right or wrong, while utilitarianism was concerned with classifying actions. Hortyâs dominance ought clearly goes a long way towards closing another gap as well: the one between deontic logic and decision theory. Because of the weakness of dominance reasoning, however, Hortyâs account seems of limited value as a theory of choice. This chapter suggests how, with modest extensions, Hortyâs framework can move beyond dominance into the three main branches of the theory of decision: decisions under ignorance, decisions under risk and game theory. This leads to a motivational question: what is the point of trying to bring deontic logic âup to speedâ if we already have a successful decision theory? I close by suggesting two main ways in which deontic logic provides return benefits for decision theory. The first, noted at the outset of this chapter, is by offering rigorous analysis of foundational notions: causation, choice, counterfactuals and background states. That such analyses matter should be clear to anyone who has followed the history of decision theory as formulated by Savage, modified by Jeffrey, and re-formulated by causal decision theorists. For example, we claimed here that the states of decision theory are causal background contexts and provided an analysis of causal independence and background contexts within stit models. By contrast, Joyce (1999, 61) writes that states include all âaspects of the world that lie outside the decision makerâs controlâ. He tells us that future choices and events, if relevant to our present decision problem, must be incorporated into the background states for that decision. Now it is harmless to incorporate future choices and events into the background states if they are causally independent of the agentâs present choice, but not so harmless if their future occurrence is contingent upon present choices. Stit frames take care of this automatically: histories belonging to distinct states at m must be divided at m. This rules out treating future choices or processes as constituents of states at m. Future chance processes must be incorporated into decisions via conditional chances for outcomes (as described in Sect. 6). To handle sequential choices in the stit framework requires something like Hortyâs strategic ought (2001, Chap. 7), which takes us beyond the present discussion. As a second benefit, deontic logic offers a model for thinking about problems where decision theory and game theory cannot offer clear, uncontroversial solutions. One source of such problems is infinite decision theory, comprising decision problems in which an agent has to deal with infinite utilities, an infinity of possible acts, or both.30 Some of these problems are genuinely paradoxical and have no clear solution. In other cases, however, there are clear prescriptions, yet decision theory is silent because there is no optimal act. Because deontic logic is concerned with the truth of obligation sentences O[Î± cstit: A] even where A does not describe an act, it has the resources to offer advice in such cases. One example of this kind, noted earlier, is the greatest integer game. Decision theory cannot recommend the choice of any particular integer, but our deontic logics tell us that â[Î± cstit: An ] and Om [Î± cstit: An ] where An is the proposition âÎ± 30 See (Sorensen 1994) for examples."
8,969,0.986,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","How and What Happens When QGP is Created in the Laboratory? The reaction path into QGP in some early work has been a line placed in the temperature-baryon density plane, such as the one shown in Fig. 32.2 on page 403, with an arrow pointing from a hot thermal hadron phase into the QGP domain. For RHI collisions capable of forming QGP, such a picture can only apply if a mechanism of entropy production exists at hadron collision level that creates the thermally equilibrated hadron phase. While new particles are formed, this state dissolves into QGP. This process requires conversion of directed motion energy into locally equilibrated matter. Moreover, the system proceeds via a non-equilibrium stage where neither the particle abundance nor their spectra are close to conditions that are associated with the phase diagram properties. Thus a locally equilibrated matter emerges first amongst quark and gluon degrees of freedom. Presentation in the phase diagram of a RHI collision entrance path into QGP domain is thus not appropriate. In fact, for very high RHIC and LHC energies all scattering processes occur at quark-gluon (parton) level. Thus there is no connection whatsoever with models of hadron-hadron scattering that sometimes decorate in an explanatory way the AA collision process. At much lower energies, near to the presumed threshold of QGP formation, the reaction path at least in part involves hadron based processes described within kinetic non-equilibrium approaches. The question one may wonder about in this case is how Hagedorn could interpret hadron production, introducing his limiting temperature. âWhy the Hadronic Gas Description of Hadronic Reactions Worksâ is the title of a work suggesting an explanation long ago [13]: it is the formation of nearly equilibrated QGP that is, partonic gas, and the evaporation of hadrons from QGP fireball that produces the near equilibrium hadron particle abundances observed. I believe this is practically the case for all strong interaction reaction processes,"
202,218,0.986,"Security of Networks and Services in an All-Connected World: 11th IFIP WG 6.6 International Conference on Autonomous Infrastructure, Management, and Security, AIMS 2017, Zurich, Switzerland, July 10-13, 2017, Proceedings","QoE is defined as a metric for the relationship of a person that interacts as user with an application [21,25]. While QoS focuses on the relationship between systems, the authors recognize that a change in QoS only affects QoE if a personâs expectation is affected. Analogous to this approach, also the concept of Quality of Business (QoBiz) is introduced, the value of which only is affected by changes in QoE if a companyâs revenue is impaired. The key finding of these publications is that values of different quality aspects can be seen independent, even though they build upon each other, so that only weak coupling between these metrics can be assumed."
103,274,0.986,Solar Particle Radiation Storms Forecasting and Analysis : The Hesperia Horizon 2020 Project and Beyond (Volume 444.0),"7.5 Predicting >500 MeV SEP Events by Using the UMASEP Scheme Solar energetic particles (SEPs) are sometimes energetic enough and the flux is high enough to cause air showers in the stratosphere and in the troposphere, which are an important ionization source in the atmosphere. >500 MeV solar protons are so energetic that they usually have effects on the ground, producing what is called a Ground Level Enhancement (GLE). One of the goals of the HESPERIA project was the development of a predictor of >500 SEP events at the near-earth (e.g. at geostationary orbit). The implemented predictor, called HESPERIA UMASEP-500, makes a lag-correlation between the SXR flux and high-energy differential proton fluxes of the GOES satellites. When the correlation estimation surpasses a threshold, and the associated flare is greater than a specific SXR peak flux, a >500 MeV SEP forecast is issued. The lag-correlation is carried out using the High-Energy UMASEP approach explained in NÃºÃ±ez (2015). In this project, this approach uses 1-min SXR and proton data. Firstly, it generates a bit-based time series from the SXR time-derivatives and three bit-based time series from the time-derivatives of each of the P9âP11 channels of the GOES6-GOES15 satellites. The â1sâ in each bit-based time series are set when its positive time derivative surpasses a percentage p of the maximum value of the time derivative in the present sequence of size L (beyond which no influence is assumed in the SEP event to be predicted); otherwise, the flux level is transformed into a â0â. To avoid false alarms due to relatively strong fluctuations during periods of low solar activity, a threshold d is necessary, which is the minimum value to consider it as positive fluctuation (i.e., a â1â). This forecasting approach creates a list of cause-consequence pairs as follows: it takes the first â1â of the SXR-based time series, and the first â1â of the proton-based time series, to create a pair; it then takes the second pair of â1sâ in each time series, and thus successively, until all the â1sâ of the SXR-based time series are inspected. After following this procedure, if a â1â does belong to any pair, it is classified as an âoddâ. For each pair, the pair separation between the SXR-based â1â and the proton-based â1â is calculated. An ideal magnetic connection is detected when a sequence of SXR-based â1sâ in a row is followed by a sequence of proton-based â1sâ in a row. In an ideal magnetically connected event, all pairs have the same temporal separation, and no odd â1â has been found; in other words, an ideal magnetic connection is detected when all recently-measured strongest rises in the SXR flux are followed, some minutes later (i.e. the lag), by all recently-measured strongest rises in a proton channel. We say that this ideal magnetic connection would have a Fluctuation Correlation of 1. In general, we need a formula, described in NÃºÃ±ez (2015), that calculates the Fluctuation Correlation between the bit-valued SXR-based time series and a proton-based time series. A >500 MeV SEP event is triggered when the lagcorrelation is greater than a threshold r, and the SXR intensity of the associated flare is greater than a threshold f."
147,30,0.986,Trailblazing in Entrepreneurship : Creating New Paths For Understanding The Field,"are only able to access the real world through their mental world (Peirce, 1955), thus meaning the two worlds are entwined (Gergen, 1994). Finally, the pragmatic perspective argues that while people search for truth, they can never truly find it. Thus, what the community of inquiry deems is truth is merely the current best opinion and is itself only temporary (Haskel, 1984; Seixas, 1993). This perspective has implications for entrepreneurship. Namely, research on opportunities has often taken the view that opportunities are discovered or created and that creation dominates discovery in certain contexts and vice versa in other contexts (Alvarez & Barney, 2007). An interaction perspective of opportunity, however, provides an alternative path for future studies (consistent with Deweyâs [1939] characterization of mindâworld dualism). Under this perspective, potential opportunities do not belong exclusively to the domain of the mind or of the world; rather, they involve the inter-relationship (i.e., mutual adjustment) of both. Indeed, as Gergen (1994, p. 129) pointed out, a vexing problem can arise when there is division and isolation between the mind and the world: âWhen a real world is to be reflected by a mental world and the only means of determining the match is via the mental world, the real world will always remain opaque and the relationship between the two inexplicable.â Scholars can make future contributions to our understanding of opportunity by viewing potential opportunities as a conceivable means to think about and discuss the world that proves useful (through action) while simultaneously recognizing that opportunities are only tentatively held and are subject to modification as they enter and re-enter the environment. Research Opportunities from a More Interactive Perspective of Entrepreneurial Opportunity Community contributions to potential opportunity refinement. Following this line of thinking (i.e., viewing opportunity detection and refinement in an interactive manner), the notion of a potential opportunity should not only be considered as part of the initial creatorâs mind but also grounded in a community. For example, a potential opportunity that is not fully formed is likely to change after being presented to a community of inquiry as a result of that communityâs social forces, feedback, and criticism. In this context, many questions surrounding the community arise. What comprises a community member for a specific potential opportunity,"
191,77,0.986,Collaborating Against Child Abuse : Exploring The Nordic Barnahus Model,"What is also evident here and in the concrete implementation of Barnahus in Norway is that child-friendliness in time came to mean a child-friendly layout and interiors, and that the idea of establishing Barnahus as a separate house in a residential area was abandoned. Child-friendliness became an issue to be solved within mostly office buildings. In an analysis of the relationship between architecture and pedagogy in Waldorf education, BjÃ¸rnholt (2014) discusses how childcenteredness can be achieved through the manipulation of the physical environment and asks whether âthe act of modifying and transcending the limitations of the given building structures may be as important as the architectureâ (p. 120). The same question could be asked in relation to Barnahus. In summary, aesthetic-spatial considerations were high on the agenda throughout the process of establishing the Barnahus model in Norway. The idea of the aesthetic-spatial dimension of child-friendliness that was launched in the early texts about Barnahus was linked to safety as a primary need for children who have been victimised. Safety was furthermore understood as linked to a feeling of being welcome as a child, and not a victim, hence the weight put on toys, dÃ©cor and equipment that invites children to play or relax. One of the key ideas was also that any association with a formal institutionâeither police or hospitalâwould compromise the atmosphere of safety. The concept of atmospheres is difficult to define, but can be understood as âa sensuous âsomethingâ that takes place between things and people. Atmospheres may be ontologically difficult to grasp or contain, yet they play an important role in ordering spaces and social lifeâ (Bille 2015, 257). The concept of making or âstaging atmospheresâ (Bille 2015; Bille et al. 2015; BÃ¶hme 2013) through locality and symbolic dÃ©cor captures the thinking about child-friendliness in the texts referred to above. Staging atmospheres relates to âhow people actively try to shape experiences and moods of selves and others through organizing objects, bodies and spacesâ (Bille et al. 2015, 33)."
305,21,0.986,Quantum Computing for Everyone,"not acting in the conventional ways that filters work. More light comes through three filters than comes through two! We will give a brief description of what is happening. Later we will see the mathematical model that describes both spin and polarization. Recall our quantum clock. We can ask if the hand is pointing at twelve, or we can ask if the hand is pointing at six. The information we gain from either question tells us which of the numbers 12 or 6 the hand is pointing to, but the Yes/No answers are reversed. For the polarized squares the analogous questions are asked by rotating the square by ninety degreesânot one hundred and eighty. The information we obtain is the same. The difference is that if the answer is yes, the photon passes through the filter and we can perform more measurements on it, but if the answer is no, the filter absorbs the photon, so we cannot ask it further questions. The first two experiments involved just two sheets and are telling us exactly the same thing: When we repeat a measurement, we get the same result. In both experiments we are measuring the polarization in the vertical and horizontal directions two times. In these experiments, the photons that pass through the first filter have vertical orientations. The first experiment, where the second filter also has vertical orientation, we are asking the question, âIs the photon vertically polarized?â twice and we receive the answer âYesâ twice. In the second experiment, the second question is changed to âIs the photon horizontally polarized?â and receives the answer âNo.â Both experiments give us the same information, but the negative answer for the second question in the second experiment means that the photon is absorbed and so, unlike the first experiment, it is not available for further questioning. In the third experiment, the filter that has been rotated through fortyfive degrees is now measuring the polarization at angles of 45Â° and 135Â°. We"
363,32,0.986,History and Cultural Memory in Neo-Victorian Fiction,"construct their meaning. Intensely self-reflexive, these novels are, for Hutcheon, more interested in exploring how the past is constructed by texts than in engaging in their own ârecuperationâ or ârevivalâ of history (ibid.: 93). This expectation, that contemporary historical fiction should privilege a problematisation of representation over the portrayal of history, is evident in other writers who utilise Hutcheonâs formulation. Thus, in her discussion of the postmodern historical novel, Wesseling, too, argues that âpostmodernist writers do not consider it their task to propagate historical knowledge, but to inquire into the very possibility, nature, and use of historical knowledge from an epistemological or a political perspectiveâ (Wesseling, 1991: 73). Hutcheonâs account has proven very useful for understanding those texts that do foreground the problematics of representation, deploying an ironic playfulness that undermines even their own attempt to depict the past. However it is limited for understanding texts that eschew this mode, or which combine it with a range of other attitudes towards the past, ranging from ironic distance to affective identification. As we shall see, Sarah Watersâ faux-Victorian novels, for example, do reflect upon the way history is constructed, but this is firmly embedded within the novels as a thematic concern; they are more earnest and affectionate than ironic and parodic in their representation of the Victorian past. As Brian McHale argues, in making the genre representative of her particular description of postmodernism as âcomplicity and critiqueâ (see Hutcheon, 1989: 11) Hutcheon fails to account for the unique resonances of individual texts: âwhat strikes one sooner or later is the sameness of many of [her] readings. Can all of these very diverse novels, one begins to wonder, really mean so nearly the same thing?â (McHale, 1992: 22). Similarly, Suzanne Keenâs fascinating study of âromances of the archiveâ suggest that these novels evince historiographical views, like presentism and antiquarianism, that âpredate postmodernismâ and pose a broader range of historiographical questions than Hutcheonâs category accounts for (Keen, 2001: 61). Del Ivan Janik, too, argues, âthe new type of historical novel â¦ is not merely a subspecies of the postmodernâ ( Janik, 1995: 161).4 And Amy J. Elias, writing in the wake of postcolonial theory, opens up Hutcheonâs category to describe a variety of positions characteristic of what she calls metahistorical romance, ranging from âironic, even nihilistic deconstructionâ to âa reconstructed secular-sacred beliefâ (Elias, 2001: 143). More recently, Jerome De Groot argues that the historical novel âarticulates within it a complex of ambiguous imperatives towards the past â an attempt at authenticity, at real(ist) representation, at memorialisation, at demonstrating the"
192,387,0.986,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"For the time being, however, instead of reverting to absurdism (as an academic apostate so to speak), he keeps trying to discern some intelligibility in recalcitrant reality, but he seems only able to do this by tweaking his results. And step by step, the absurdity of his âsolutionâ, the absurdity of his fabrications, increases. How is it possible that his professional colleagues fall into this trap? Although tweaking your results may be part of certain research cultures, Diederik A. Stapel somehow manages to take this strategy to the limit, by drastically exaggerating this questionable type of behaviour, so that in his case the fraud itself becomes absurd (and indeed, grotesque exaggeration is a well-known technique in absurdist theatre of course). Psychoanalytically speaking this confirms the view that Stapel is basically a craving, tormented subject facing an epistemic crisis, whose misconduct is symptomatic for frustration and despair, as âby-productsâ of university discourse ($). S2 becomes an increasingly divided (spaltet) subject in the force field between the resurging absurdist worldview on the one hand and the recalcitrant object a on the other (S1 â S2 â a). By taking his deceptions one step further, building on the congruence between experiments and plays, his research really becomes theatre, so that eventually social psychology (as a particular strand of university discourse) gives way to absurdism (S2 â $)."
385,181,0.986,Advanced R,"Functions can return only a single object. But this is not a limitation because you can return a list containing any number of objects. The functions that are the easiest to understand and reason about are pure functions: functions that always map the same input to the same output and have no other impact on the workspace. In other words, pure functions have no side eï¬ects: they donât aï¬ect the state of the world in any way apart from the value they return. R protects you from one type of side eï¬ect: most R objects have copy-onmodify semantics. So modifying a function argument does not change the original value:"
173,320,0.986,"intelligent Human Computer interaction : 9Th international Conference, Ihci 2017, Evry, France, December 11-13, 2017, Proceedings","We estimate similarities between words corresponding to a pair of emotions by training neural word embeddings on large datasets [9]. This similarity gives a measure of relatedness between two emotion-indicating words in a general sense. For example, if the word âsadâ has higher similarity with word âangerâ than with the word âworryâ, then we assume that the relatedness between emotions âsadâ and âangerâ is higher than the relatedness between âsadâ and âworryâ. This may not necessarily be true with respect to every user, but is true in an average sense since the calculation is based on very large datasets of emotional content across several people. Since this measure is data-dependent, we have to choose appropriate datasets containing significant emotional content to get reliable estimates. We then normalize the similarity scores to obtain the likelihood probability. The details are as follows: Specifically, we train a skip-gram model on a large corpus of news articles (over a million words), blogs and conversations that contain information pertaining to peopleâs emotions, behavior, reactions and opinions. As a result, the model can provide an estimate of relatedness remoi âemoj between two emotions (emoi and emoj ) leveraging information across a wide set of people and contexts. This quantity is just capturing the relatedness between any two emotions in a general sense, and is not specific to a particular user. We compute likelihood probability of observing emotions emoj given emoi , Pl (emoj |emoi ) based on normalizing the similarities remoi âemoj over the space of all possible emotions under consideration. Thus, remoi âemoj Pl (emoj |emoi ) =  allâemo remoi âemoj The emotion pairs considered in Eq. (1) do not necessarily represent expressed or experienced emotions; the likelihood probability is just a measure of relatedness between a pair of emotions computed from large datasets."
346,471,0.986,"History Education and Conflict Transformation : Social Psychological Theories, History Teaching and Reconciliation","silencing and censoring could reduce them in the short term but their further implications are always negative in the long term. 3. Deconstructing the own nation through reflecting on the conflict. As mentioned above, political conflicts based on historical representations are very often based on two dimensions. This is to say they have at the same time a national and a recent character. The first feature is not a surprise at all because history education has been traditionally considered an essential piece of most of educational systems since nineteenth century, and even before, to build and maintain national identities. Therefore, it is precisely its national character in interaction with its recency the center of the conundrum to be solved. This is to say any attempt to discuss new information contradicting prior representations, based, for example, on new data obtained by historians, will be facing an intense defensive reaction because citizens national identities will be challenged. But if history education does not go in this direction trying to change stereotypical historical representations there is no way to contribute to conflict reduction and reconciliation. what to do then? This is precisely what previous pages about the teaching of historical narratives have tried to answer. This is to say I have analyzed a six dimensions view of historical narratives emphasizing how each one of them could be approached in such a way that they could help the citizens to contextualize their representations about the past and particularly their view about a monological and essentialist view of past events. As these dimensions have been analyzed before in detail, it is not necessary to consider again their possible contribution to reduce social and political conflicts but I think it is convenient to insist on the dramatic importance of establishment of the historical subject. Usually citizens establish this subject basically through a historical view based on an endurable and almost eternal continuity between themselves and diverse subjects of the past. This is to say when citizens along the world use a âweâ as subject of something that happened in the past they represent themselves as the only and genuine descendants of an idealized historical subject which does not exist anymore. In other words, this âweâ consists of an incredible mixture of present and past but probably most of the citizens are not really conscious of such a mystification. As stated before, the establishment of this imaginary historical subject is probably the nucleus of the rest of the narratives dimensions"
308,97,0.986,Contemporary Bioethics : Islamic Perspective,"The participle âas it were (ka in Arabic)â is a reminder of the essentially symbolic nature of the Qurâanic discourse about God. Therefore, the Light is not God himself, but refers to the enlightenment which He bestows on a particular revelation (the Lamp) which shines in the heart of an individual (the Niche). The light itself cannot be identiï¬ed wholly with any one of its bearers, but is common to them all. As Muslim commentators (especially al-Ghazali) pointed out, light is a particularly good symbol for the divine Reality, which transcends time and space. The image of the olive tree in this verse was interpreted as allusion to the continuity of revelation, which springs from the ârootâ and branches into multifarious variety of religious experience that cannot be identiï¬ed with or confused by any one particular tradition or locality; it is neither of the East nor the West [34]. Some of the attributes describe the essence of God (al-dhat). For instance, Power, Knowledge, Will, Hearing, Sight and Speech were attributed to God in the Qurâan. However, they were distinct from Godâs unknowable essence, which always would elude simplistic understanding [35]. The Qurâan is the eternal word of God. The Muâtazilite theologians alleged that Qurâan is not eternal but was created by God, like any of His creatures. This is one of the items where Muâtazali differed greatly from other Sunni theologians. For example, al Karabasi [36] a later Muâtazili declared that the written and spoken Arabic of the Qurâan was uncreated in so far as it partook of Godâs eternal speech, but once it is written or recited by humans, their action and doing so is deï¬nitely created, as humans are but the creatures of God. This notion is agreed upon by a number of Sunni theologians like Abu al-Hasan al-Ashâari (260â330) and Abu Mansur Al Maturidi (d 337), and even Ahmed ibn Hanbal said it at one stage, but later refused to say anything except that the Qurâan is the actual word of God [37]. Some of his followers, later on, claimed that even the recitation or the written Qurâan is the eternal word of God. It seems more of a bickering, for all the Sunni, and most of other sects apart from Muâtazilis, agree that the Qurâan is the pristine eternal word of God. The actual writing or reciting by humans is an act done by mortals and creatures of God and hence, their act is perforce created. God said in the Qurâan:"
213,81,0.986,Collider Physics Within The Standard Model : a Primer,"2.2 Non-perturbative QCD The QCD Lagrangian in (1.28) has a simple structure, but a very rich dynamical content. It gives rise to a complex spectrum of hadrons, implies the striking properties of confinement and asymptotic freedom, is endowed with an approximate chiral symmetry which is spontaneously broken, has a highly nontrivial topological vacuum structure (instantons, U.1/A symmetry breaking, strong CP violation which is a problematic item in QCD possibly connected with new physics, like axions, and so on), and an intriguing phase transition diagram (colour deconfinement, quarkâ gluon plasma, chiral symmetry restoration, colour superconductivity, and so on). How do we get testable predictions from QCD? On the one hand there are nonperturbative methods. The most important at present is the technique of lattice simulations (for a recent review, see [272]): it is based on first principles, it has produced very valuable results on confinement, phase transitions, bound states, hadronic matrix elements, and so on, and it is by now an established basic tool. The main limitation is from computing power, so there is continuous progress and good prospects for the future. Another class of approaches is based on effective Lagrangians, which provide simpler approximations than the full theory, valid in some definite domain of physical conditions. Typically at energies below a given scale L, particles with mass greater than L cannot be produced, and thus only contribute short distance effects as virtual states in loops. Under suitable conditions one can write down a simplified effective Lagrangian, where the heavy fields have been eliminated (one says âintegrated outâ). Virtual heavy particle short distance effects are absorbed into the coefficients of the various operators in the effective Lagrangian. These coefficients are determined in a matching procedure, by requiring that the effective theory reproduce the matrix elements of the full theory up to power corrections."
51,340,0.986,How Generations Remember,"personal experiences as a point of departure when probing into questions of memory and generation in Mostar. As shown in the book, individuals are not only exposed to changing political contexts but are also confronted by their personal past and present experiences, which serve as the backdrop against which they rethink the past in the present. By now we have become so sensitive to the idea of the flexibility of the past that we tend to forget that it restsâat least to a certain degreeâon an experiential base (Irwin-Zarecka 1994: 17). Appadurai (1981) argues that the past does not offer an infinite source for interpretation but is always told within certain cultural norms and rules. In this book I have demonstrated that the past is not an unlimited resource by showing how it is genuinely influenced, not only by a predefined discursive space but also by peopleâs personal experiences. By looking at the intersection of memory and generations, both continuities and discontinuities"
271,621,0.986,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"concept of figuration. What is the boundary of a figuration? What separates one figuration from another? How do people switch from one figuration to another? Can they be part of several figurations at the same time, and how does this work? These questions were difficult enough to answer before deep mediatization. Today, the ramifications are almost impossible. The concept of a frame of relevance offers us a way to understand the increasingly complex linkages between people, in a way that no paradigm I am aware of can do. Let us return to the game of cards. The four players share a frame of relevance: the game. However, they may be playing their game in a placeâ say, a barâwith other people. Presumably, they also share a frame of relevance, though less intensely, with these people. Possibly, their relations with the people in the room vary. Maybe the husband of one of the players is there. Marriage is typically a two-person figuration. The other players may have other shared frames of relevance with this person: family, friend, neighbour. These nested and overlapping figurations can all be captured and analyzed with the concept of the âframes of relevanceâ, which can be expanded endlessly upward, downward and outward. Now imagine a game of cards that is played online. Maybe all four players are in different corners of the world. One may be home alone, one in a train, one in a bar, one surreptitiously playing a game at work. In a mediated situation, the permutations are endless. To make up a figuration, physical co-presence is not necessary at all. Especially in such complex mediated cases, thinking of figurations as delineated by shared frames of relevance is a fruitful innovation. The consequence, of course, is that everybody is always part of many figurations at the same time, spread across different locations. But this âcomplex and also contradictoryâ situation, as Hepp, Simon and Sowinska observe (Chap. 3), is the normal state of affairs for most people today. With this budding new paradigm, we at least have the words to describe it."
375,42,0.986,Musical Haptics,"potential for research concerned with exploring the utility of model-based control for musical instruments, especially from the perspective that the model internalized by the musician is one that describes the mechanical interactions between his or her own body and the musical instrument. This chapter is but a first step in this direction. Before leaving the questions we have raised here, however, we will briefly turn our attention to how the musician might learn to manage such coupled dynamics, proposing that the robustness, immediacy, and potential for virtuosity associated with acoustic instrument performance is derived in large part from engaging interactions that involve both the active and passive elements of the sensorimotor system."
208,93,0.986,Actors and the Art of Performance,"tried out or ignored different aesthetic forms, whether he has asked or refrained from asking ethical questions, and whether and how he has answered the question of art. The answer he has given will give his face an expression, will make his body matter or not,3 and with time, in the course of an actorâs life, it will bring out the difference between one actor and another. You only need to watch the stage closely. This is not a moral judgment. Luckily, it takes all kinds. The question remains of the concept of thinking that is inimical to actors. What do actors, and by no means only actors, mean by thinking when this prejudice takes hold of their heads and hearts? Theater and thinking have long had an ambivalent relationship involving a conflict, the roots of which go back to antiquity. On the one hand, ancient philosophers considered the act of thinking, in analogy to theater, as the practice of contemplation (theoria), in which the person philosophizing could, in a state of amazement (thaumÃ¡zeo) apprehend the actual truth (Ã³ntos on theÃ³s). On the other hand, Plato in particular believed theater was the adversary of thinking because, due to its intimacy with the realm of the emotions, it is closest to that part of being human which is furthest removed from the best in us â the noetic realm of reason.4"
346,221,0.986,"History Education and Conflict Transformation : Social Psychological Theories, History Teaching and Reconciliation","CONCLUSION Concluding these brief notes on situations when historical education turns into parrhesia, I think that it is important to pay attention to the differences between emotional reactions of participants described in the study that we used as an example of this field of research (Leone and Sarrica 2014). I propose that, all results taken together, clearer information provided in the parrhesia condition allowed participants to better regulate their emotional reactions (frijda 1986), especially their selfconscious or moral emotions (Lewis 2008). The exemplum given in the research described in this chapter shows how these emotions may be seen not only as a barrier (Bar-Tal and Halperin 2013) yet, if well regulated, as a motivational resource (frijda 1986) to get to know a formerly hidden aspect of oneâs own historical past. Of course, not all historical sensitive issues require a parrhesiastic narrative, but those breaking a long-lasting social denial of past in-group faults. Only in this last case, in fact, psychological processes linked to first emotional reactions become crucial, since there is not a consolidated and widespread historical culture framing this knowledge, silenced since teacherâs intervention. In such a situation, literal social denial (Cohen 2001) on past moral indignities of the group produces a lack of historical knowledge that makes parrhesiastic teaching a risky yet unavoidable communication move, since there is a need to break a social silence"
45,195,0.986,Measurement and Control of Charged Particle Beams,"reading for the corresponding quadrupole. The result is a âV plotâ, as shown in the right figure. The minimum in this plot determines the BPM reading at which the beam is centered in the quadrupole. 3.1.3 Sextupole Excitation In present-day storage rings, it is often assumed that the sextupoles are well enough aligned with respect to the quadrupoles that only the quadrupole alignment has to be verified. An orbit off center in a sextupole will result in vertical dispersion, betatron coupling, or beta beating. Although, in principle, also the sextupoles in a storage ring can be aligned by changing their strength and measuring the induced orbit shift (which is a quadratic function of the excitation) there is little experience with such a scheme. To reach the same"
249,157,0.986,Advances in Proof-Theoretic Semantics (Volume 43.0),"classical, material, implication (see [9], Chap. 14). We will not go further into categorial proof theory, which deals with the equations between deductions suggested by such categories with additional structure. Let it be said only that it is remarkable how equations important in mathematics in general, or in particular fields of mathematics, reemerge as perfectly sensible equations between deductions (see [7]). What was surmised above is that the structure that deductions make with such operations is an essential ingredient in the notion of deduction. Could one go as far as to take this as the main ingredient? As in category theory, the structure of arrows would be the main thing. And, as in category theory, the arrows would be more important than the objects. With deductions the objects are the premises and conclusions, and these premises and conclusions, whatever they are preciselyâ propositions, commands, questions, problems, or something elseâwould not precede the arrows. Deducing would not be preceded by asserting, or another function of language. When functions are reified as sets of ordered pairs, the active, dynamic, component in the notion of function is lost. This component, which comes from psychology, is also lost in the reification brought by the categorial notion of function, where a function is an arrow in a category. Categorially, functions in the category Set, where the objects are sets and the arrows are functions, are characterized through composition and identity functions. The same operations characterize deductions in general. Although Set has the structure that deductions make, it is not natural for its objects to be called premises and conclusions, and for its arrows to be called deductions. One reason may be the nature of these objects, which are not in the field of propositions (see the end of the preceding section). Another reason might be that we have too many of these deductions. Any two objects would be connected by a deduction, except when the premise is not empty while the conclusion is empty. Deductions, in the categories where arrows may be more naturally designated by that term, are usually more discriminatory. There are more objects not connected by arrows. The structure of deductions imitates the structure of the category Set even more when they involve the binary connectives of conjunction, disjunction and implication, together with the nullary connectives â¤ and â¥. This structure, appropriate for intuitionistic propositional logic, imitates Set with the biendofunctors of product, coproduct and exponentiation (for exponentiation we have covariance in the base and contravariance in the exponent), together with the terminal and initial objects, i.e. a singleton and the empty set. Still, the arrows of the category Set could not be taken as deductions, but only as their model. (The question of models of deductions was discussed in [6].) This is the model that stands behind the standard proof-theoretic semantics for intuitionistic logic, which through the CurryâHoward correspondence is tied to the typed lambda calculus. Matters become clearer when in conceiving this semantics we do not conform to the dogma mentioned at the beginning. When we look upon this semantics hypothetically, and not categorically, as in the typed lambda calculus, we will end up in the categorial setting of Set. With the typed lambda calculus we also end up in the sets of Set, but the categorial setting is hidden."
235,49,0.986,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","m = m 1 = m 2 , we would obtain h(m, m) = Î´(h(m, m)), thereby again clearly contradicting our definition of Î´. In summary, there is a limit to self-inspection, as long as one deals with systems of sufficiently rich phenomenology. One of the assumptions has been that there is no empirical self-exploration and self-examination without changing the sub-system to be measured. Because in order to measure a subsystem, one has to interact with it; thereby destroying at least partly its original state. This has been formalized by the introduction of a âdiagonal-switchâ function Î´ : P ââ P without a fixed point. In classical physics one could argue that, at least in principle, it would be possible to push this kind of disturbance to arbitrary low levels, thereby effectively and for all practical purposes (fapp) eliminating the constraints on, and limits from, self-observation. One way of modelling this would be a double pendulum; that is, two coupled oscillators, one of them (the subsystem associated with the âobserved objectâ) with a âvery largeâ mass, and the other one of them (the subsystem associated with the âobserverâ or the âmeasurement apparatusâ) with a âvery smallâ mass. In quantum mechanics, unless the measurement is a perfect replica of the preparation, or unless the measurement is not eventually erased, this possibility is blocked by the discreteness of the exchange of at least one single quantum of action. Thus there is an insurmountable quantum limit to the resolution of measurements, originating in self-inspection."
228,196,0.986,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"In practical applications (a fuzzy system, e.g.), a pair of values should be considered as a result of inference: the truth value of the premise part of a rule, and the OFN calculated in accordance with the Definition 5. Comparability with conventional fuzzy inference operators is important to preserve in general similar usefulness in practical situations for the new conceptions. Therefore, behavior of the output of the inference in boundary cases is compatible with classical fuzzy solutions (see [3, 18, 19]). If there is no compatibility in the premise part âX is Aâ, then the rule is not activated, and on other side if the activation is full, then the result is the exact value from a conclusion."
372,787,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"Here, we have used N D ËNN , so we are considering the same observing time as in the Nyquist-sampled case but sampling Ë times as rapidly. Note that !s is correspondingly reduced. (2 is one case of the general quantization efficiency factor, (Q (introduced in Sect. 6.2), where Q is the number of quantization levels. Equation (8.25) gives the relationship between the correlation coefficients for a pair of signals before and after two-level quantization. This result includes the case of autocorrelation in which the two signals differ only because of a delay. Thus, we"
192,411,0.986,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"and eventually derailing experience, namely that social psychology (as an established paradigm) is unable to reach the level of specificity needed to comprehend everyday existence the way novels, plays and movies do. The contradiction resurges between the specificity of fiction and the generality of academic research (M2). In order to reconcile the two, established social psychology must be sublated (in the Hegelian sense of the term Aufheben) by combining the experimental method with the âspecificity principleâ, so that social psychology becomes sensitive to the complexities, nuances and idiosyncrasies of everyday life. It would also entail a reconciliation of social psychology with the genres of the imagination practiced by Stapel during the pre-academic stage of his career. In other words the specificity principles aims to achieve a negation of the negation, a convergence of the quest for general cognitive mechanism (social psychology) with a radical openness to the transient theatre of everyday existence, bringing social psychology on a higher level of complexity (M3). But unfortunately, this third moment proves unreachable (M2 â | M3) so that his project of transforming (sublating) the paradigm falters. The reconciliation of social psychological theory and everyday phenomena proves unattainable. More precisely, Stapel is unable to overcome the tension between reality as a littered, messy, disorderly environment and the mapping techniques of social psychology. He keeps looking for the causal factor x which allows us to address a problem by pushing a button, but this presupposes that the world is as makeable and modifiable as the behaviour of test animals in research facilities. While aiming to develop a level of specificity comparable to novelists or playwrights, Stapel continues to turn research subjects into lab rats in a labyrinth. But the human world proves too recalcitrant and transient to be captured in Stapelâs mousetraps. This leads to discontent and stagnation ($). The publication factory falters. This crisis precedes his misconduct. Fabrication and falsification are desperate efforts of a craving subject to remain faithful to the logic of the established paradigm, so that the infringement could be seen as a desperate act of fidelity, committed in response to the fact that the social psychological fact-fabricating factory as such is faltering. But social psychology fails to realise a cleansing of reality, so that it is ironical that the normative trinity eventually calls for a âcleansing operationâ to clean up the Stapelgate mess. Could journal articles such as Stapelâs Science paper (a fingered, fabricated story) be regarded as a piece of fiction, a Zola-like, literary experiment? This interpretation is negated by the decidedly dichotomous or even stereotypical style of thinking of Stapelâs paper (clean versus littered environments, prejudiced versus non-prejudiced subjects, etc.), while novelists and playwrights tend to depict a greyer and more ambiguous ambiance. The stereotyping inherent in social psychology itself is a mental cleaning device for cleaning up the messy chaos of reality. Stapel becomes a novelist only in his subsequent, post-traumatic effort to overcome the crisis, namely via writing Derailment as a âtherapeuticâ practice of the Self, an auto-pathography which at least partly allows him to overcome his Î´Î¹ÎµÏÏÎ¯ÏÎ¸Î·Î¼ÎµÎ½ (Aristophanes), his Spaltung (as discussed in Chaps. 2 and 8), while being sufficiently âspecificâ to acknowledge realityâs messiness. It is an act of apostasy, mirroring or counteracting Kouwerâs classic, for whereas Kouwerâs publication"
82,266,0.986,Fading Foundations : Probability and The Regress Problem,"In the same vein, it could be admitted that we, with our finite minds, are capable of calculating the probability of a target proposition (in the previous chapters we have after all done so), but are incapable of giving an infinite number of reasons for this proposition, since the latter would require an infinity of consciously performed acts. Because epistemological justification is about giving reasons, and not about making calculations, the finite mind objection applies in full force. A similar reaction to our views has been voiced by Adam Poslaskowski and Joshua Smith.15 They argue that, although âvaluable lessonsâ can be drawn from our formal results, it is âentirely unclearâ that these results meet a basic requirement, namely âproviding an account of infinite chains of propositions qua reasons made available to agentsâ.16 Podlaskowski and Smith call this âthe availability problemâ: Given the distinctive emphasis that Peijnenburg, Atkinson, and Herzberg place on calculability, we have doubts about the extent to which (on their account) an infinite chain of propositions can serve as reasons that are available to an agent. (This is what shall be called the availability problem facing the distinctive brand of infinitism under consideration).17 . . . it is hard to see, more generally, how the emphasis on calculability yields a notion of available reason (or availability) that can serve the infinitistâs purposes.18"
395,137,0.986,Beyond Safety Training : Embedding Safety in Professional Skills,"The starting point for this chapter is that for lessons of past accidents to be learned, i.e. taken into account in future decision-making, experts must maintain a âsafety imaginationâ(Pidgeon and OâLeary 2000). A lack of safety imagination is linked to a psychological rigidity that restricts decision makers in their ability to link their work to the possible consequences. The question is therefore how a safety imagination can best be fostered. Researchers in the ï¬eld of naturalistic decision-making (e.g. Klein 1998) have identiï¬ed stories as an effective knowledge source for decision-making in critical contexts, because they are a powerful tool in pattern matching and mental simulation. Stories convert experiences into memorable, meaningful lessons (Klein 1998; Polkinghorne 1988). As Schank (1990) puts it, We need to tell someone else a story that describes our experience, because the process of creating the story also creates the memory structure that will contain the gist of the story for the rest of our lives."
3,53,0.986,Instructional Scaffolding in STEM Education : Strategies and Efficacy Evidence,"informed by activity theory, but there is a recognition that the function of the tools provided to learners can vary, even when the physical form of the tools stays the same (Belland, 2010; Belland & Drake, 2013; Belland, Gu, Armbrust, & Cook, 2015). An instructional approach grounded in activity theory takes a decidedly postmodern approach, in that it allows for multiple approaches and recognizes the importance of individual perspectives and those of members of the culture in which the student is operating (Friesen, 2012; Hlynka, 2012; Solomon, 2000). Furthermore, such an approach would welcome the type of critique and dialogue that one would expect to see in a scientific laboratory or conference/publishing venue. Thus, such approaches would likely involve addressing a central, ill-structured problem (Jonassen, 2011; Jonassen & Rohrer-Murphy, 1999). Furthermore, students would be provided considerable latitude to address the problem in the manner that best suited them."
360,344,0.986,Compositionality and Concepts in Linguistics and Psychology,"The three experiments compared acceptability proportions when participants were asked to categorize instances of two complex expressions. From the perspective developed here, which follows Black (1937), a central goal of semantic theory is to account for such results on acceptability. It is important to note that a priori, it is not self-evident that acceptability proportions reflect typicality preferences for the complex concept. Typicality effects are only studied here insofar as they help to predict acceptability, as measured in truth-value judgement tasks. The typicality data that were tested in Leeâs, Poortman et al.âs and Poortmanâs experiments concern the head concept, and not any complex concepts. As Fodor (1981) claimed, it becomesincreasingly harder to study typicality information when it comes to com-"
223,358,0.986,Knowledge and Action (Volume 9.0),"Connecting aesthetic approaches to the analysis of the construction of social space therefore enriches the understanding of the relational processes of generating shared meaning and agreeing on how to behave in the current situation. Furthermore, the aesthetic dimension of experience plays a role in defining the scope for future social space because it has the âcapacity to animate actorsâ imaginations and actionsâ (Woodward & Ellison, 2010, p. 46). In this chapter we use this integrated relational conceptualization of social and physical space to analyze data from a series of action experiments we organized in 2009 in Israel. We invited people in small mixed groups to explore together how to envisage a future social space in the same setting. We consciously intensified attention to the aesthetic dimension of the process from the outset by choosing a fine-arts studio as the setting and by providing art materials for the participants to use there, sharing the assumption that âcreative activity with portable, discrete objects allows an extension of potential spaceâ (Woodward & Ellison, 2010, p. 50). For this chapter we have decided to apply an aesthetic approach to the data analysis by focusing only on the visual documentation in order âto avoid committing the cognitive and rational error of ignoring the bodies of the people involved in the decision process and only considering their mindsâ (Strati, 2000, p. 20). Our objective is, therefore, to explore how much one can learn about processes of constructing current and future social space, in which physical relations are integrated, by including aesthetic dimensions of the experience in the analysis. The next section of the chapter describes the context in which we conducted the action experiments. It is followed by an explanation of the methodology that was used to collect and analyze the data. We then present an analysis of the sessions, in which we identify different configurations that evolved during the interactions of the participants with one another and with the physical aspects of a studio. In the final section of the chapter, we present our conclusions about how to conceptualize and analyze social and physical space in an integrated manner and suggest next steps."
117,164,0.986,Care in Healthcare : Reflections On Theory and Practice,"Work on Wanting: Sociomaterial Will-Work Before going into the ethnography of ADL encounters in which wanting is aligned, I want to highlight two methodological interventions that I make with this chapter. Firstly, I contend that the term âwillâ suggests a coherence that hides the relational nature of coming to want something. I therefore suggest that, rather than understanding the will as something we âhaveâ, we should understand it as something we âdoâ in unfolding sociomaterial interaction. Secondly, since my interest here is in understanding the alignment that is strived for in the process of wanting in dementia care settings on a daily basis, I differentiate between âwillingâ and âwantingâ. I separate a more cognitive intending, pertaining to the realm of the legal and long-term decision making (âwillingâ), from a more immediate, emotionally and physically informed activity (âwantingâ). I understand wanting10 to be a fundamental expression of subjectivity, including activities such as desiring, longing, wishing and, significantly, not wanting, which is done in unfolding sociomaterial interaction on an everyday basis."
228,121,0.986,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"can have a huge influence on results, including the opportunity to reverse fuzziness, that is, to have outputs that are crisper than inputs. In our previous works, we paid special attention to direction-related interpretation of OFNs, that is, the above-mentioned new aspect of information that enables us to distinguish between pairs ( f A , g A ) and (g A , f A ). One possibility is to refer here to a trend of fuzzy observation or measurement [9]. Indeed, decomposition of a fuzzy numberâs membership function onto two ordered components establishes an interesting background for representation of a trend by means of the up part, which is a natural beginning, and the down part, which is a natural end of observation. For example, by reversing the ordering of the OFNâs components one might specify whether a given observed imprecise value is generally likely to increase or decrease. Surely, one could claim that such information is expressible also in classical fuzzy logic by adding new trend-related linguistic variables. However, that would result in a more complex fuzzy-rule-based representation, leading towards a less intuitive framework for conducting arithmetic calculations on measurements. We refer to Chap. 4 for further details on possible interpretations of information represented by the OFN model. For now, let us add that by introducing an order of components ( f A , g A ) and, this way, letting Aâs up and down parts be potentially both increasing and decreasing, we enter a far richer space of outcomes of arithmetic calculations. In Chap. 4 we show that operations on such ordered pairs may lead towards results that are not interpretable as standard fuzzy numbers, as some elements of R correspond to multiple memberships. One could think of it as a special case of some extensions of fuzzy set theory [14]. One could also refer to original ideas of Lotfi A. Zadeh who, in his paper [22], stated that âThe concept in question is that of fuzzy set, that is a âclassâ with a continuum of grades of membership.â Thus, the OFN model could be interpreted as a new way of assigning real numbers with the continuum of grades of membership. Certainly, further theoretical studies in this respect are necessary as well."
306,25,0.986,"Early Geometrical Thinking in the Environment of Patterns, Mosaics and Isometries","instruments used (e.g., rulers, compasses) and the respective geometrical conï¬gurations, and a discursive process producing arguments and proofsâ (Kuzniak and Nechache 2015, p. 545). Recently, research in mathematics education has turned its attention to the problem of âlanguage and semiotic aspects in the construction of mathematical knowledge, both in an individual and in a social construction perspectiveâ (Boero and Consogno 2007). In particular, starting from the assumption that the language is fundamental since the mathematical objects are not directly accessible Duval (1993), elaborated a theory based on this main idea: the learning of mathematical objects is necessarily conceptual and an activity on them is possible only using âregisters of semiotic representationsâ (Duval 1993). He provides a very rich theory about it based on the assumption that there is no knowledge without representation. Moreover, two kinds of transformations are mathematically relevant: the âtreatmentâ (Duval 1993, p. 41), the transition from a representation to another in the same register, and the âconversionâ (Duval 1993, p. 421), the transition from a representation in a register to another in a different register. The transition from a semiotic representation to another and vice versa is essential for the conceptual learning of mathematical objects: âThinking in mathematics depends on the synergy of several registers and not on the activity of a single system. Unlike what occurs in other ï¬elds, mathematical concepts are only understandable within such a synergyâ (Duval 2006, p. 21). Duval (2005) afï¬rms that geometry requires a cognitive activity very complex but complete, since it stimulates the gesture, the language, and the seeing. It is a ï¬eld of knowledge that implies the cognitive joining of two very different representation registers: the visualization of the shapes and the language; a synergy between visualization and language is fundamental to understand geometrical arguments. Duval (2005) identiï¬es the origin of the difï¬culties in geometry in the intuition which relies on perception. According to psychological studies, perception plays a fundamental role in the visualisation process: âBy perception the visual thought organises itself as a starting point of insight and reflection, mental activities which contribute to the formation of conceptsâ (Marchini et al. 2009, p. 62). Perception is a process of selection and organization, cognitive activities connected with knowledge and understanding. Nevertheless, visual perception may hinder the way of seeing geometrical ï¬gures. Following Duval (2005), this way depends on the activity in which it is involved. There are two ways of seeing a ï¬gure: iconic and not iconic. The second is a sequence of operations of geometrical property identiï¬cation that implies that the âdeconstructionâ of the shapes has been visually recognized. This brief overview of theories that underlie geometrical knowledge has been focused mainly on those elements that affect research conducted at the lowest educational level. However, even such a selective approach shows that theoretical background can be varied, taking into account many different aspects of building the geometrical knowledge of students. It also shows the speciï¬cs of the geometric research that distinguish it from research in other areas of education."
187,287,0.986,Managing The Complexity of Critical infrastructures : a Modelling and Simulation Approach (Volume 90.0),"scenarioâs, parameters, environment variable settings and experimental control settings. Furthermore, environment disturbances coming from connections with live entities may impact the behaviour of the M&S system. During the execution of the model, human input can become part of the displayed behaviour. This can be from trainees, but also from operators such as opponent players to provide stimuli to the trainees or Subject Matter Experts (SMEs) that interfere with the execution of the simulation for some purpose dictated by the Simulation Frame (e.g., keeping the execution within a desired regime). So, all in all the M&S process consists of cutting away all elements of the real and imaginary world that are not needed for the purpose at hand, then apply various abstraction techniques to make the model suited for use, then the model is executed in order to obtain results (e.g. a trained operator or an optimized CI conï¬guration). These results are then applied in some form or another in the real world. And that last part is exactly where the risk exists. When the M&S-based solutions to problems are applied in the real world there is a risk that those results are not fully appropriate. There can be a number of causes: the purpose for the M&S endeavour was not what was ultimately needed, maybe the simuland did not contain all needed elements of the real and imaginary world, maybe some abstractions were too large and important details were abstracted away, maybe the implementation and execution of the model or the interpretation of itâs results introduced errors. If the results of M&S are never used in the real world, e.g. if it is used for entertainment purposes or as a hobby, then there is no problem. But for CI this is not the case. The possible sources of errors may for example lead to operators of actual CI taking wrong actions if M&S was used for their training. If it is used for determining the best possible conï¬guration of interconnecting CI, it may result in a system that performs less than desired. The conclusion is that we need to be sure that the M&S results are ï¬t for purpose before actually applying them to the real world. There are two processes that do exactly that: Veriï¬cation and Validation. Therefore the take away message of this part is âYou have to do Veriï¬cation and Validation because there is risk involvedâ."
249,308,0.986,Advances in Proof-Theoretic Semantics (Volume 43.0),"issues. This seems also to be Fregeâs position, as he speaks explicitly about âour knowledgeâ (see the citation in the previous footnote). 3 Here, the notion of structure includes the possibility that its universe is taken from the âreal worldâ. 4 We make here a slight abuse of the word ârelationâ; strictly speaking, a relation is a semantic concept and âsyntactic relationâ cannot be anything more than a formula formed by use of a relation symbol. 5 For a recent philosophical discussion of the concept of identity see also [14]. 6 This distinction of equality and identity may help to unravel Fregeâs initial footnote in Sense and Denotation explaining that he uses equality âin the sense of identity and understand âa = bâ in the sense of âa is the same as bâ or âa and b coincideâ.â [5, p. 157, slightly changed translation]; of course, Frege didnât have the modern distinction of syntax and semantics at hand."
363,118,0.986,History and Cultural Memory in Neo-Victorian Fiction,"the work of the two Victorian scholars, but also the two storylines, so that Byattâs twentieth century characters are (often unconsciously) influenced by the Victorians in a number of ways.7 For example, throughout the novel images of green, white and gold link Christabel (274), the Princess in the glass coffin (63), Melusine and Maud (38â9), making them recall each other physically and psychologically. Thus, while we can, in a sense, separate out the two storylines at the level of plot, symbolically they are increasingly entangled by this web of imagery and by complex repetitions and allusions. This is particularly important for understanding the relationship between present and past as it is represented in the novel. The novel does not conceptualise the past as distinct, and separate from the present, but rather as entangled and entwined. For the novel, the Victorians continue to have an embodied afterlife today, as part of our cultural memory, mediated in part by the imaginative texts they have left behind them, which continue to shape the present. Since it is the text that is the privileged medium of the past Roland only embodies the ideal reader when, toward the end of the novel, he realises that as he comes closer to possessing the truth of Ashâs life, he feels distanced from Ash himself. His attempts to possess the end of the story, and thus Ash himself, are self-defeating. This realisation leads him back to Ashâs text as medium. He reads, not searching for allusions to other texts, nor hunting for hints about Ashâs life, but enjoying a reading in which he can again hear the language sing. It is an epiphanic moment that restores to Roland âthe days of his innocenceâ, when he âhad been not a hunter but a readerâ (469â70). It is a reading that, we are told, âis violently but steadily aliveâ (470), engaging both âsensuous alertness and its opposite, the pleasure of the brain as opposed to the viscera â though each is implicated in the other, as we know very well, with both, when they are workingâ (471).8 His is a âsensationalâ reading â one that appeals to and evokes the appetites and emotions at the same moment that it engages the reason and intellect. In her discussion of the âneo-sensationâ novel Kelly Marsh argues that the appeal to sensation in the antecedent genre of Victorian sensation fiction undermined the notion that reason governed (particularly menâs) actions and suggested that there were visceral truths, as well as intellectual ones. The sensation novelist ventured the idea that sensation was a strong motivating force, exploring its effects and evoking it in their readers (Marsh, 1995: 109). She notes that the term âsensationâ is âvexedâ, but traces Thomas Boyleâs etymological pursuit of it which finds that the two meanings of the term given in the Oxford English Dictionary are paradoxical, evoking two"
101,330,0.986,Finite Difference Computing with Exponential Decay Models (Volume 110.0),"This program is easy to read, and as long as it is correct, many will claim that it has sufficient quality. Nevertheless, the program suffers from two serious flaws: 1. The notation in the program does not correspond exactly to the notation in the mathematical problem: the solution is called y and corresponds to u in the mathematical description, the variable A corresponds to the mathematical parameter I , N in the program is called N t in the mathematics. 2. There are no comments in the program. These kind of flaws quickly become crucial if present in code for complicated mathematical problems and code that is meant to be extended to other problems. We also note that the program is flat in the sense that it does not contain functions. Usually, this is a bad habit, but let us first correct the two mentioned flaws."
223,84,0.986,Knowledge and Action (Volume 9.0),"perspectives. In modernity it is also recognized that there is not just one single type of rationality but different types, which cannot be reduced to each other. Each type determines its own principles. Developing Kantâs ideas about theoretical, practical, and aesthetic rationality further, Habermas (1984) paradigmatically distinguished between cognitive, moral, and aesthetic rationality. Habermas built not only on Kantâs work but also on that of Max Weber, who first made rationality a key concept in modernistic thinking and used the term specifically in the sense of purposive rationality or economic rationality, the meaning it is also often has in colloquial language. It thereby denotes the strategic choice of the best means to reach a certain goal. In this way rational decision-making became of central interest and positioned rationality and action theory as core concepts in high modernity. Weber elaborated the role of rationality for individual everyday actions and called attention to the tendency toward disenchantment, that is, toward continuous differentiation and rationalization. Rationalization in this sense designates a historical drive toward a world in which âone can, in principle, master all things by calculationâ (Weber, 1919/1946, p. 136), by rational decision-making. This process of rationalization was not limited to the economic sphere but was extended with its own rational logics also to law and administration, the social and political spheres, and other domains. As a prerequisite, a peculiarly rational and intellectual type of personality or person of vocation was presupposed. Modern scientific and technological knowledge slowly pushed back the germinating grounds of human knowledge, such as religion and metaphysics, and created a culture of âobjectificationâ (Versachlichung). At the same time, there was a loss of substantive-value rationality, the emergence of a polytheism of value fragmentation, and the related tensions between these two developments, in other words, rationality without reason in practice. It is in this framework that one must also situate geographical action theory as put forward by Benno Werlen (Chap. 2, in this volume or 1987, 1995, 1997) in the phenomenological tradition of Alfred SchÃ¼tz (1932). According to this school of thought, the internal mental intentionality directed to outer objects is what ascribes meanings to these objects, as people do through their everyday place-making and everyday spatially differentiated actions. This geographic action theory can be interpreted as the subjectivist version of what Schatzki, Knorr-Cetina, and Savigny (2001) and Reckwitz (2002) designated as the mentalist paradigm in social theory. This approach contrasts with the objectivist version of mentalism, which stems from classical structuralism as exemplified by de Saussure (1916/1972) in linguistics, LÃ©vy-Strauss (1969) in anthropology, Althusser (1965/2005) and Emerson (1984) in Marxist economics, and Piaget (1970) in psychology. One could also add the more contemporary version of psychological structuralism (Lacan, 2002); behaviorist psychology (Skinner, 1938; Watson, 1913); and cognitive psychology (e.g., Broadbent, 1987), including cognitive linguistics (Fauconnier, 1999). The approach diverges from behavioral geography (Golledge & Stimson, 1996) as well, for which human behavior is an effect of structures in the unconscious mind in relation to structured situations and is thus part of the objectivist mentalist tradition."
65,158,0.986,Handbook of Ocean Wave Energy,"Combining the uncertainty in the grid design with the uncertainties in the model boundary data (e.g. the waves on the model boundary, the bathymetry, the variation in the seabed bottom friction coefï¬cient, the choice of source term models), it can be seen why setting up a spectral wave model has been likened to a âblack actâ. Unfortunately, there is no simple procedure that can be followed to guarantee the accuracy of the numerical wave model. A good match between the model prediction and measured data may provide some conï¬dence in the model. However, this is strictly limited to the particular sea-state and the model may be less accurate when applied to different wind and wave conditions. Thus, it is generally recommended that any numerical model should be validated using a wide range of different conditions that include what may be expected over the whole year. Unfortunately, this data is not always available, and the wave resource model may be validated against only a limited data-set or even no data-set at all. In general, and in these cases in particular, it is clearly necessary to be aware that the wave resource is not fully validated and the WEC performance based on this data should be treated as such."
223,347,0.986,Knowledge and Action (Volume 9.0),"âidentity in topological or geometrical structure between different domainsâ (GÃ¤rdenfors, 2000, p. 176). That is, a word that represents a particular structure in one domain can be used as a metaphor to express the same structure in another domain. Once a metaphor has established such a mapping, it can be exploited to provide other metaphors from the same domain. An example of such a mapping is the designation of certain computer programs as viruses. This metaphor drawing on the biological domain has created a new way of looking at this class of programs. It has suddenly opened up possibilities for expressions like invasive viruses, vaccination programs, and hard-disk disinfection."
359,21,0.986,"Micro-, Meso- and Macro-Dynamics of the Brain","showed that hippocampal activity during theta could reflect more than a representation of current state and may reflect a vicarious trial-and-error important for planning (Schmidt et al. 2013). A similar observation has been made by decoding position using CA3 firing rates at the choice points. This analysis reveals transient moments in which CA3 represented positions ahead of the rat, sweeping down the potential paths before the rat made its decision (Johnson and Redish 2007). These findings are closely related to the fact that the phase of spiking contains information about heading direction in two-dimensional environments (Huxter et al. 2008), as would be expected if theta sequences code for upcoming positions. Overall, studies to date have demonstrated that theta sequences always begin with place representations behind the subject and end with representations of the future. However, the exact span coded by theta sequences has not been addressed carefully. If the cells that are active at the trough of the CA1 theta cycle code for the current position in the context of past and future locations, how is the span of the past and future determined at the physiological level? One possibility is that theta sequences code for a fixed amount of time or distance around the current location. Alternatively, each geometric segment (e.g., individual corridors) and event along the journey could be represented separately as a âneural wordâ and such words would be concatenated, perhaps via sharp wave ripples (Foster and Wilson 2006; Davidson et al. 2009; Wu and Foster 2014), to represent the entire journey from the beginning to the end. Yet another possibility is that the start and end (reward) locations of a complex trajectory through a maze are coded in a given cycle. This final option raises the question of just how much space could be segmented within a theta cycle. Data collected in our lab demonstrate that theta periods segment the environment either according to goals or to environmental geometry. As a rat ran down the track, the probability that it occupied any given position given the observed CA1 spiking pattern was computed by comparing the instantaneous rates to a template of the session averages, the cellsâ place fields. When these posterior probability distributions were calculated at every theta phase (Zhang et al. 1998), we observed theta sequences that started at one end of the track and finished at the other (Fig. 2). Thus, in addition to the goal being represented at late theta phases (Wikenheiser and Redish 2015), our findings show that the start location is represented at early phases. Combining these observations, the phase code is defined by the current location in the context of a past bounded by a journeyâs beginning and a future bounded by the journeyâs end. Separation of the future and past boundaries is assured by the strongest inhibition at the peak of the theta cycle (BuzsaÌki 2002). Recall the studies in which place fields expanded when familiar environments were stretched. How do place fields expand with the environment? An answer begins to emerge when one considers that the theta sequences are anchored to the boundaries. The amount of space represented within the sequence, the compression, dictates the resolution of the spatial code. When boundaries are moved apart, either in the stretched environments or for journeys of different lengths, theta sequences that are bookended by those boundaries necessarily represent more space which, in"
49,50,0.986,Artificial Intelligence and Cognitive Science IV,"1 Introduction Intelligent robots as commonly depicted in science fiction are able to interact with their environment, with people and to perform various human like actions. Often it seems that we are only one step away from building such machines. However, despite of a long research in the field it is not the case. We may be able to build highly sophisticated robotic bodies or, theoretically, a complex intelligent system controlling the actions of a robot but so far we have failed to make the robot perform some tasks we people consider very basic. For example, our robots cannot see as we do. We did not build a robust, multipurpose system giving a robot the ability to visually recognize objects in its environment - yet. Humans (indeed many animals) do it easily thanks to their highly developed brains. Learning what the environment consists of is the first step in the development of an intelligent behavior. Jeff Hawkins has explained a memory-prediction theory of brain function in 2004 [1]. This theory was among the first that provided unified a unified basis for thinking about the adaptive control of complex behavior and it is in the"
124,29,0.986,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"the idea of individuals in branching histories, such as famously in Lewisâs argument against branching (Lewis 1986, 206ff): Using the resources of CIFOL, it is possible to model individuals and sortal properties successfully in a branching histories framework. Good news, surely, for those of us who believe that we are just that: individual agents facing an open future of possibilities. In line with the development of BT, BST and stit, CIFOL is developed from a semantical point of view. The interface with a formal logical language is, however, much more pronounced in the case of CIFOLâthe fact that we are considering a predicate logic necessitates close attention to the syntax as well. (For example, as the framework is required to remain first-order, while lambda-abstraction is unfettered, lambda-predicates may only occur in predicate position.) Naturally, it is to be expected that there can be fruitful discussions of CIFOLâs proof theory and metatheory. Nuel Belnap, in his contribution to this volume, gives a highly interesting overview of a truth theory that can be developed within CIFOL+, a minimal extension of CIFOL. Given the frameworkâs intensionality, it is possible to define terms representing the cases, and based on those, one can develop the theory of the mixed nector âthat Ï is true at case xâ. You will, we hope, not go wrong in expecting further striking results about CIFOL and its connection to indeterminism and free action in the near, albeit open future. Acknowledgments Research leading to these results has received funding from the European Research Council under the European Communityâs Seventh Framework Programme (FP7/20072013) / ERC Grant agreement nr 263227, and from the Dutch Organization for Scientific Research, grant nr NWO VIDI 276-20-013. I would like to thank Nuel Belnap for continuing inspiration, and both Nuel Belnap and Antje Rumberg for helpful comments on a previous draft. Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
8,667,0.986,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","While all these years much talk was about quarks, but experimental searches did not reveal any; confinement and QCD were still far in the future. It was speculated that maybe quarks are so heavy that they would be practically unobservable. Bootstrap thermodynamics was presumably the best way to calculate qq production rates as a function of their mass (applying perturbation theory to this problem was a then fashionable nonsense). The model was used to predict [17] KK, pp, dd, and qq pair production with the result that the production rates of the first three agreed with experiment and that for quarks a mass of 4â5 nucleon masses would make them practically unobservable (with the techniques known at that time), even if they could, in principle, exist as free particles. If anyone had suggested to me then a qq plasma, I would have declared it impossible (alas, again). One often-heard objection to the model was that resonances (inside a fireball) would not live long enough to justify treating them as âparticlesâ. Matthiae [18] proved, using the principle of detailed balance, that they might just live long enough. It was tempting to apply the model to astrophysics: the Big Bang and neutron stars [19]. This triggered a number of papers, notably the one by Huang and Weinberg [20] and by Wheeler [21]. The then proposed âlimited temperature (T0 ) Big Bangâ depended entirely on the (at that time reasonable) interpretation of the model as yielding an infinite energy density at T0 . For an easily readable summary of the situation up to 1972, see [14], where no prior knowledge about statistical bootstrap is required."
264,576,0.986,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"These deï¬nitions exemplify the historical struggle with two key problems in how we think about functions: conceptualizing the function itself as an object, and conceptualizing the domain and range of a function as objects."
257,198,0.986,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","To demonstrate the applicability of our characterization, we use it to analyze a tag-based monitor proposed by Dhawan et al. to enforce heap safety for low-level code [15]. In prior work [5], we and others showed that an idealized model of the monitor correctly implements a higher-level abstract machine with built-in memory safetyâa bit more formally, every behavior of the monitor is also a behavior of the abstract machine. Building upon this work, we prove that this abstract machine satisfies a noninterference property similar to Corollary 1. We were also able to prove that a similar result holds for a lower-level machine that runs a so-called âsymbolicâ representation of the monitorâalthough we had to slightly weaken the result to account for memory exhaustion (cf. Sect. 4.5), since the machine that runs the monitor has finite memory, while the abstract machine has infinite memory. If we had a verified machine-code implementation of this monitor, it would be possible to prove a similar result for it as well."
270,179,0.986,The Huawei and Snowden Questions : Can Electronic Equipment From Untrusted Vendors Be Verified? Can An Untrusted Vendor Build Trust into Electronic Equipment? (Volume 4.0),"10.9 Verification Through Formal Methods One of the methods referred to in the Common Criteria is code verification. This consists of writing an accurate specification of how the software should behave and performing a rigorous mathematical proof that the piece of software actually conforms to the specification. The correctness of this proof can in turn be checked automatically by a relatively simple program. In Chap. 9, we discussed formal methods in some detail and concluded that both hope and despair are associated with this approach. The hope is related to the fact that the approach itself carries the promise of being watertight. If there is a formal and verifiable proof that there are no malicious actions in the software code, then a mathematical certainty has been established. The despair is due to two facts. First, it can be argued that formal methods only move the complexity from understanding the software to understanding and proofreading the specifications. The second â and this is the more serious one â full formal specification and verification are considered prohibitively costly for larger systems [11]. This is further supported by the fact that very few pieces of software have been reported to have undergone full formal verification as specified in the highest assurance level, EAL7, of Common Criteria. One particular development that seems to address both the complexity of specification and the cost of formal verification is model checking [9]. This consists of building an abstract model of the software that is sufficiently small so that its state space can be made the subject of an exhaustive search. Although this is a promising approach, our problem pops up again in the question of how to build the model of the software. This task is intrinsically difficult and has to be carried out by human experts [5]. The likelihood of an intentional backdoor in the code showing up in the model is therefore quite similar to the likelihood of it being detected by reviewing the code itself. Model checking is therefore not a silver bullet to our problem, so we move on to seeing if code review can help."
346,270,0.986,"History Education and Conflict Transformation : Social Psychological Theories, History Teaching and Reconciliation","message about epistemology, i.e. about the ways how knowledge about the past comes about, but also constitutes the history narrative in terms of its content. In âtraditional history teachingâ, teachers will represent the past in a way that is determined by some kind of ideology, usually of nationalist origin, but it could also be MarxistâLeninist as in the case of Soviet history teaching. Maintaining the image of a valuable in-group by way of a celebratory past from the perspective of a certain group does not allow presenting alternative narratives on an equal footing to the self-serving version. This style of teaching is clearly propagandistic: favouring a selfserving version of history at the expense of alternative views with the aims of influencing the studentsâ future action. In contrast, if involved in critical history teaching, the teacher will employ contents that contest national myths and deconstruct celebratory narratives. A critical and multiperspective approach to history motivates students to consider alternatives to their own views, which may have been or are currently dominant with an adversary or even inimical groups or countries. This approach proceeds by offering complementary historical interpretations, weighing their evidence and accepting them as possible alternatives to the studentsâ âindigenousâ perspective. This involves critical self-reflection as well as learning to respect alternatives to oneâs own position. The goal of communication is raising an emancipatory and tolerant consciousness of othersâ life worlds, rights and values. Such communication style does not defend a specific historical interpretation, but offers several side by side. In some respects, this approach reminds of the term of âdiffusionâ.1 âPropagandaâ and âdisseminationâ can be seen as two opposite ends of a continuum. we conceive âpropagationâ as the intermediate space between the two poles (fig. 2). Spanning a range of possible teaching approaches, âpropagationâ can be seen both as a milder form of propaganda and as a more standpoint-based (or âbiasedâ) version of dissemination. It can appear as critical identity work such as when offering support to the student identity building together with critical reflection on narratives traditionally used by the studentsâ in-group. However, it can also appear in a form much closer to propaganda, if an in-group-serving selection is made from academically adequate knowledge. This kind of âpropagationâ would not be pure âpropagandaâ, as the chosen accounts themselves would not be knowingly distorted. But it would be closer to âpropagandaâ than to"
217,409,0.986,Finite Difference Computing With Pdes : a Modern Software Approach,"The former suggests gR D gL , and the former then leads to gR D gL D I =2. Consequently, u.x; t/ D I.x  ct/ C I.x C ct/ : (2.76) The interpretation of (2.76) is that the initial shape of u is split into two parts, each with the same shape as I but half of the initial amplitude. One part is traveling to the left and the other one to the right. The solution has two important physical features: constant amplitude of the left and right wave, and constant velocity of these two waves. It turns out that the numerical solution will also preserve the constant amplitude, but the velocity depends on the mesh parameters Ât and Âx. The solution (2.76) will be influenced by boundary conditions when the parts  ct/ and 12 I.x C ct/ hit the boundaries and get, e.g., reflected back into the domain. However, when I.x/ is nonzero only in a small part in the middle of the spatial domain Å0; LÂ, which means that the boundaries are placed far away from the initial disturbance of u, the solution (2.76) is very clearly observed in a simulation. A useful representation of solutions of wave equations is a linear combination of sine and/or cosine waves. Such a sum of waves is a solution if the governing PDE is linear and each sine or cosine wave fulfills the equation. To ease analytical calculations by hand we shall work with complex exponential functions instead of real-valued sine or cosine functions. The real part of complex expressions will typically be taken as the physical relevant quantity (whenever a physical relevant quantity is strictly needed). The idea now is to build I.x/ of complex wave components e i kx : bk e i kx : (2.77) I.x/ "
249,213,0.986,Advances in Proof-Theoretic Semantics (Volume 43.0),"3 Categorical Harmony in Comparison with Other Principles Here we have a look at relationships with Belnapâs harmony and the so-called reflection principle and definitional equations by Sambin and his collaborators. The categorical approach to harmony poses several questions to Belnapâs notion of harmony. As we saw above, implication â in intuitionistic logic is right adjoint to conjunction â§. Suppose that we have a logical system L with logical constants â§ and â¨ only, which are specified as the right and left adjoints of diagonal â as in the above. And suppose we want to add implication â to L. Of course, this can naturally be done by requiring the right adjoint of â§. Now, Freydâs adjoint functor theorem tells us that any right adjoint functor preserves limits (e.g., products), and any left adjoint functor preserves colimits (e.g., coproducts). This is a striking characteristic of adjoint functors. In the present case, the theorem tells us that â§ preserves â¨; in other words, â§ distributes over â¨. Thus, defining implication according to categorical harmony is not conservative over the original system L, since the bi-directional rules for â§ and â¨ only never imply the distributive law. Note that sequent calculus for â§ and â¨ allows us to derive the distributive law without any use of implication; yet the bi-directional rules alone do not imply it. Although proponents of Belnapâs harmony would regard this as a strange (and perhaps unacceptable) feature, nevertheless, this sort of non-conservativity is necessary and natural from a category-theoretical point of view. Furthermore, conservativity may be contested in some way or other. One way would be to advocate categorical harmony against Belnapâs on the ground of the Quinean holistic theory of meaning, which implies that the meaning of a single logical constant in a system, in principle, can only be determined by reference to the global relationships with all the other logical constants in the whole system. If the meaning of a logical constant depends on the whole system, then adding a new logical constant may well change the meaning of old ones. Non-conservativity on logical constants is arguably a consequence of a form of holism on meaning, even though it violates Belnapâs harmony condition. Anyway, we may at least say that the principle of categorical harmony, or Lawvereâs idea of logical constants as adjoints, is in sharp conflict with Belnapâs notion of harmony, in terms of the conservativity issue. Another distinctive characteristic of adjoint functors is that any of a right adjoint and a left adjoint of a functor is uniquely determined (up to isomorphism). By this very fact, we are justified to define a concept via an adjunction. This actually implies that Belnapâs uniqueness condition automatically holds if we define a logical constant according to the principle of categorical harmony. Thus, uniqueness is not something"
360,30,0.986,Compositionality and Concepts in Linguistics and Psychology,"Barsalou 1999, 2008a, 2010). From this perspective, a symbolic representational structure, such as a relation, predicate, frame, or schema, has arguments, variables, or slots, whose values vary across instances and situations (e.g., Barsalou 1992). Given these kinds of representational structures, concept composition becomes readily feasible (e.g., GagnÃ© and Spalding 2014; Wisniewski 1997). To implement the combination of an object with a property, for example, a variable in the frame for the object is set to a speciï¬c value. When representing the concept composition of green flamingo, for example, the color variable in the flamingo frame is set to the value green. Similarly, to implement the combination of two nouns, a frame assigns the two nouns as arguments of different variables within a relation, predicate, frame, or schema. When representing the concept composition of flamingo fare, a frame for eat could take flamingo and fare as values of the variables for agent and theme, respectively. Other accounts of concept composition assume that feature sets for individual concepts are combined in varying ways to produce the meanings of complex phrases (e.g., Hampton 1997; Hampton and JÃ¶nsson 2012). Again, the respective features are typically assumed to be amodal symbolic representations. When representing sports that are games, for example, people combine the feature sets for sports and games to produce the combined concept. Amodal representations such as frames and features may play central roles in the representation of concepts, and in turn, during the process of combining them. Nevertheless, other important representational processes appear to play important roles in concept composition as well, in particular, multimodal simulations and linguistic forms. If so, then theories of concept composition may beneï¬t from taking them into account. Multimodal simulations. Earlier, in the introduction, the construct of multimodal simulation was introduced when deï¬ning concept. To reiterate briefly, a distributed multimodal network for a category in the brain attempts to simulate the kind of neural and bodily states that occur while interacting with a categoryâs members. When simulating a bicycle, for example, the brain and body reenact the kinds of states that occur while experiencing bicycles, including how they look, where they are used, how to ride one, and associated internal states. Considerable evidence has accumulated supporting the proposal that simulation is a basic computational process in the brain, not only in conceptual processing, but in all cognitive processes, ranging from perception to social cognition (e.g., Barsalou 2008a, 2016b). Certainly, multimodal simulation is not the only representational process in the brain, but it appears to be a central one used widely and frequently. If so, then it may contribute signiï¬cantly to the process of combining concepts. To date, little research has addressed the role of simulation in concept composition directly. One set of experiments, however, provides some initial evidence (Wu and Barsalou 2009). Across three experiments, a preliminary prediction was that when participants produced the features of an object, they would simulate a typical experience of the object, reporting features salient in the simulation. As a consequence, features occluded in the simulation, and therefore not salient, wouldnât be produced frequently. When participants produce features of a typical lawn, for example, they should simulate a green lawn. Because green and grass"
217,346,0.986,Finite Difference Computing With Pdes : a Modern Software Approach,"because then the application of the standard scheme at a boundary point i D 0 or i D Nx will be correct and guarantee that the solution is compatible with the boundary condition ux D 0. Some readers may find it strange to just extend the domain with ghost cells as a general technique, because in some problems there is a completely different medium with different physics and equations right outside of a boundary. Nevertheless, one should view the ghost cell technique as a purely mathematical technique, which is valid in the limit Âx ! 0 and helps us to implement derivatives. Implementation The u array now needs extra elements corresponding to the ghost points. Two new point values are needed:"
118,578,0.986,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"17.2.3 Greater Public Good and Rationality, by Denia Djokic, University of California, Berkeley In a society comprising many stakeholders, there is no consensus on the definition of the âgreater public good.â For the case of each stakeholder, this utilitarian construct is based on a certain combination of: information, misinformation, different ways of interpreting the same information, lack of information, and most of all, different value systems, some of which do not always fit into the neat frame of a traditional cost-benefit analysis. In student discussions at this summer school, we tried to delve deeper into the meaning and function of the cost-benefit analysis. We asked questions such as whether seemingly âirrational decisionsâ were merely a different framing of the same cost-benefit analysis, where different stakeholders (e.g., nuclear engineers in contrast to the public) simply weigh the risks and the benefits differently. There is no simple answer, and furthermore I have no doubt that the solution depends on much more than just âcommunicationâ between the stakeholders. A good first step, however, is to encourage this kind of thought among the population that has traditionally been a major influence in top-down decisions: the nuclear engineering community. All these insightful and fruitful discussions at the summer school made me wonder: why is it that we nuclear engineering students are not usually challenged to think this way? We seem to be well trained in our field, and yet there seems to be a very large gap in our education. Undoubtedly, nuclear engineering students from UC Berkeley and the University of Tokyo are well educated in the breadth and depth of the discipline. However, in my nuclear engineering graduate school training to date, I have found that we are groomed to be inside-the-box thinkers without the necessary training to understand nuclear issues holistically. To solve technical problems, we are taught to draw clear boundaries around a limited problem, because without a clear definition, you cannot find a solution. Despite the fact that this method of solving problems often breaks down when scaling up to societal levels, the rhetoric among nuclear"
223,337,0.986,Knowledge and Action (Volume 9.0),"Action Domain Experiments on how one perceives the movement of persons and other objects (e.g., Giese & Lappe, 2002; Giese & Poggio, 2003; Johansson, 1973) have suggested that the kinematics of movement contain sufficient information to identify the underlying dynamic force patterns. Runeson (1994, pp. 386â387) has gone further, claiming that one can directly perceive the forces that control different kinds of motion. The process is automatic; one cannot help but see the forces. This capacity seems to develop early in infancy (White, 1995). Thus, the force domain can be understood as a shared domain for purposes of communication. In GÃ¤rdenfors (2007b) and GÃ¤rdenfors and Warglien (2012), that analysis was extended to actions and the forces involved in generating those actions. The basic premise is that an action can be represented as a pattern of force vectors. The force pattern for running is different from the force pattern for walking; the force pattern for saluting is different from that of throwing (Vaina & Bennour, 1985). Note that forces as represented by the brain are psychological constructs and not Newtonâs scientific concept. Similarities between actions should be studied in order to identify the structure of the action space. This investigation can be done with the same basic methods as those used for objects. Walking is more similar to running than it is to throwing. Little is known about the geometrical structure of action space. I make the rather weak assumption that the concept of betweenness remains meaningful. An action concept can then be characterized, like other concepts, as a convex region, in this"
363,202,0.986,History and Cultural Memory in Neo-Victorian Fiction,"28).2 Whiteâs discussion of the narrative properties of even traditional histories highlights the way in which the question of what will count as history is often contested over the question of generic appropriateness. For White, all narrative histories are subject to generic considerations; the same facts can be shaped into a variety of generic forms, and the same facts accrue different meanings, depending upon whether they are given a tragic, comic, or epic structure (ibid.: 29â30). He argues that while some genres are considered more appropriate to the telling of particular events â he points to the often unarticulated stipulation that âa serious theme â¦ demands a noble genre, such as epic or tragedy, for its proper representationâ â there are actually many genres that can produce meaningful versions of the same events. White takes Art Spiegelmanâs Maus: A Survivorâs Tale (1972, 1973â1991), which uses the form of a comic book to present the events of the Holocaust, as an example of the successful deployment of a âlowâ genre to produce âa particularly ironic and bewildered view of the Holocaust â¦ one of the most moving narrative accounts of it that I know of, not least because it makes the difficulty of discovering and telling the whole truth about even a small part of it as much a part of the story as the events whose meaning it is to discoverâ (ibid.: 31).3 Although Whiteâs persuasive argument about the value of a variety of genres in the telling of history focuses upon traditional histories, it points, too, to the ways in which historical fictions, and even counterfactual histories, can play a role in producing meaningful, competing narratives of past events despite, or even because, of their particular generic, and figural, properties. In the last few decades, the proliferation of fictional accounts of the Victorian period, when considered in relation to each other and to other constructions of the period by historians, politicians and cultural critics, form a textured, diverse, and at times contradictory picture of a period that continues to fascinate the contemporary imagination. These accounts revise and contest each other, so if the period is always written and rewritten, a palimpsest, its meaning is also never fixed. In Waterlandâs terms, these constructions and reconstructions of the Victorian period ensure that our histories of the era are continually dredged. They serve to remind us of the fictiveness, that is, the constructedness, of all the significations that we ascribe to it. Thus, the question of what will count as history continues to hover, contested. In fact, because of the problematisation of narrative as a means for representing past reality, which has highlighted, too, the shared conventions of historical and fictional narratives and elided their discursive differences, history and fiction appear again, in some"
305,120,0.986,Quantum Computing for Everyone,"interpretation is a way of trying to explain how the mathematical theory relates to reality. Perhaps, at some point there will be an insightful genius like Bell who can show that the different interpretations lead to different conclusions that can be experimentally differentiated, and that experiments will then give us some reason for choosing one interpretation over another. But at this point, most physicists subscribe to the Copenhagen interpretation. There is no convincing reason not to use this interpretation, so we shall use it without further comment from now on. The final topic of this chapter shows that Bellâs theorem is not just of academic interest. It can actually be used to give a secure way of sharing a key to be used in cryptography. The Ekert Protocol for Quantum Key Distribution In 1991, Artur Ekert proposed a method based on entangled qubits used in Bellâs test. There are many slight variations. We will present a version that uses our presentation of Bellâs result. Alice and Bob receive a stream of qubits. For each pair, Alice receives one and Bob receives the other. The spin states are entangled. They are always â â + â â . If Alice and Bob measure their respective qubit using the same orthonor-"
192,44,0.986,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"Before addressing Lacanâs views on scientific integrity and research misconduct, I will first outline his views on science as such. For Lacan, science basically entails a process of symbolisation which proceeds via instruments and gadgets (1972â 1973/1975, p. 104), producing discursive âemissionsâ on a massive scale. Modern science eliminates (âdecomposesâ) the world as we know it from naÃ¯ve lifeworld experience, replacing it with a completely different kind of universe, composed of symbols (signifiers) referring to concepts (molecules, electrons, quarks, etc.) that represent enigmatic entities whose ontological status (whose materiality or realness) poses a challenge to human imagination (1972â1973/1975, p. 49). The progress of science is the progress of the symbolic order, consuming, incorporating, transforming and obliterating nature as described by Aristotle (1980), namely as ÏÏÏÎ¹Ï: that which emerges, comes forward on its own accord, having its own inherent principles of change, that which is simply there, without our doing. Nature becomes obliterated and dissolved in the course of the ongoing symbolisation or hominisation of the planet (Lacan 1953â1954/1975, p. 291). Science notably entails a symbolisation of the phenomena of life (1954â 1955/1978, p. 43). Rather than understanding life as such, the aim of science is to understand specific bio-molecular processes with the help of instruments and contrivances, such as clocks, microscopes, X-ray diffraction (XRD), etc. (1954â 1955/1978, p. 96, p. 344) resulting in symbols (letters, figures, formula, graphics, etc.) of various kinds: a form of understanding which does not allow us to see living nature as it is, but rather aims to control and manipulate biological processes. Scientific explanation depends on the use of signifiers (discursive elements which are easily modifiable, notably when sitting in front of a computer screen) which structure scientific experience (1955â1956/1981, p. 216). Thus, symbolisation is the language of precision technology and relies on technologies of knowledge Â© The Author(s) 2017 H. Zwart, Tales of Research Misconduct, Library of Ethics and Applied Philosophy 36, DOI 10.1007/978-3-319-65554-3_2"
235,125,0.986,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","One of the ways thinking of this kind of âbreathing in and out of individuality and entanglementâ is in terms of sampling and scrambling information, as quoted from Chiao [251, p. 27] (reprinted in [350]): âNothing has really been erased here, only scrambled!â Indeed, mere re-coding or âscrambling,â and not erasure or creation of information, is tantamount to, and an expression and direct consequence of, the unitary evolution of the quantum state."
342,20,0.986,Semiotics in Mathematics Education,"Because we have reached awareness, the object now becomes an object of knowledge. But knowledge is not an array of isolated facts or events. Rather, it results from a linkage between facts, and this link, Peirce argues, requires us to enter into a level that goes beyond quality (ï¬rstness) and factuality (secondness). This new level (thirdness) requires the use of symbols. Commenting on the subtleties of the interrelationships amongst ï¬rstness, secondness, and thirdness as either ontological or as phenomenological categories SÃ¡ens-Ludlow and Kadunz (2016) mention the following: Peirceâs semiotics is founded on his three connected categories, which can be differentiated from each other, and which cannot be reduced to one another. Peirce argued that there are three and only three categories: âHe claims that he has look[ed] long and hard to disprove his doctrine of three categories but that he has never found anything to contradict it, and he extends to everyone the invitation to do the sameâ (de Waal 2013, p. 44). The existence of these three categories has been called Peirceâs theorem.â¦ He considers these categories to be both ontological and phenomenological; the former deals with the nature of being and the latter with the phenomenon of conscious experience. (SÃ¡enz-Ludlow and Kadunz 2016, p. 4)"
208,100,0.986,Actors and the Art of Performance,"am master over âmyâ actions. Our grammar constantly turns us into perpetrators.8 Thus we break with and ignore everything to do with passivity, with the pathic, with everything that can and does befall us without warning. The ego is seen not only as the precondition of being able to feel something but also and at the same time as the cause of and reason for those feelings. However, while the ego is the precondition for being able to feel something, feelings do not introduce themselves; rather, we are overcome by them. We are stricken by them. Disappointment hurts, fear lames us, hate distorts us, and we are thrown into chaos. Calamities befall us; happiness is not something we can calculate, not even the happiness of the high card, the greatest number. Theater is incendiary. It can inflame the enlightenment concept of the subject and the pragmatism of analytical abstract thought, which seeks to calm itself with quantifiable criteria. The stage attacks the dominance of such rational, causal thought. The ego is exposed; its sovereignty is assaulted, shaken at its core because of this vulnerable exposition. This is meant as a literal occurrence, not as a figurative image. A painful event that causes discomfort. Offends. The tower of our modern self-assurance begins to crack. Not theoretically, but dramatically, involving all senses. Physically, because on stage the actor has to deliver, with his own body, the tenacious problem of modernityâs collective interpretation of the self. He is forced into facing the experience that the ego is not the master of its own house, that whether or not he succeeds is not a matter of free will. He has no control over it, and no normative structure in the world can give it to him. No talent, no system, no method. Whether or not his acting is felicitous is out of his hands and thus uncertain. It is bestowed upon him."
124,511,0.986,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"is ripe, though not with either of them individuallyâin keeping with the childhood slogan that blackberries are red when theyâre green.â20 Secondly, incompatibility comes in two flavors, extrinsic and intrinsic. The decisive question is this: Is the incompatibility due to some external factor or is it founded in the possible objects themselves? A clear case of extrinsic incompatibility is provided by (Example 1), because there is nothing in the possible children themselves that explains their joint incompatibility, which rather rests on those external factors determining the maximum number of children a woman can bear. A clear case of intrinsic incompatibility can be found in (Example 3), because it is due to the very nature of some possible daughter cells that they are not possible siblings and hence are incompatible. (Example 2) might be less easy to classify, but the strict reading of the principle of the necessity of origin we adopt for its scope would tip the scales in favor of construing the time of conception as an internal factor, because of its individuating force. Thirdly, all three examples taken together provide ample evidence for the importance of the metaphysical phenomenon of ontological competition: Some possible individuals can come to be only by cutting off the chance of existence for others (of course, they do not literally struggle). We think that this phenomenon has not received the attention it deserves. So, let us get on with modeling it in our formal ontology! Extrinsic incompatibility is easily accommodated in the general framework laid out in Sect. 3, at least if we decide to invest species membership into a model. We need only add to Ï a species-relative cardinality constraint on the number of direct descendants of any pair of possible parents, and there will be only possibilities that conform to the intuitions behind (Example 1). With intrinsic incompatibility, however, we reach the limits of what our theory of possible ancestry in its current state can achieve. There just is no natural way to model those cases of incompatibility that are due to the nature of the incompatible possibilia themselves by any general statements that act as constraints on either the global or the local level.21 In the scenario of (Example 3), what leads to the incompatibility of the possible cells D1 or E1 is no property that they might share with some other possible cells, but their individual nature. How are we to enhance our theory of possible ancestry to give it the power to model intrinsic incompatibility? There is the somewhat brutal way of adding an arbitrary filter that admits only some of our admissible sets: To the structure of possibilia âD, âºâ§ and the admissible sets defined in terms of it we add P, which is a subset of the set of admissible sets. No element of P may contain any collection of possibilia that according to our intended interpretation are intrinsically incompatible. We obtain the theory TPAcarve+filter from 20 Brandom (2008), 123. 21 We could model intrinsic incompatibilities by adding a large number of singular statements as"
244,752,0.986,"Advancing Human Assessment : The Methodological, Psychological and Policy Contributions of Ets","In sum, Herzog et al. (1985) have shown that the Petrie (1967) induction procedure generates reliable kinesthetic-aftereffect scores that correlate in the theoretically expected direction with reliable measures of personality and behavior. It is testimony to the importance of reliability when attempting to demonstrate the construct validity of a conceptually derived variable. However, a major disadvantage of the Petrie procedure must be acknowledgedâan hour of individual administrationâthat is likely to limit the incentive of investigators to pursue further research with the procedure. It is hardly surprising then that research on the personality implications of the kinesthetic aftereffect essentially ended with the Herzog et al. investigation. Virtually all of the research on the kinesthetic aftereffect-personality relationship has been interindividual (trait based). Baker et al. (1979) can be credited with one of the very few studies to explore intraindividual (state-based) variation. Baker et al. sought to determine whether the menstrual cycle influences kinesthetic-aftereffect. On the basis of evidence that maximal pain occurs at the beginning and end of the cycle, Baker et al. predicted greater kinesthetic aftereffect reduction (a larger aftereffect), âdamping down of subjective intensity of incoming stimulationâ (p. 236) at those points in the cycle and hence a curvilinear relationship between the kinesthetic aftereffect and locus in the menstrual cycle. Employing three samples of college-age women, quadratic-trend analysis yielded a significant curvilinear effect. The effect remained statistically significant when controlling for possible confounding variablesâtiredness, oral contraceptive use, use of drugs or medication. Untested is the possibility of social-expectancy effects, participants at or near menses doing more poorly on the kinesthetic-aftereffect task simply because they believe women do poorly then. But, as Baker et al. observed, it is difficult to conceive of such effects in so unfamiliar a domain as the kinesthetic aftereffect. Did personality research related to the kinesthetic aftereffect disappear from the psychological scene following the definitive Herzog et al. (1985) study? Not entirely, for the personality questionnaire used to validate the kinesthetic aftereffect became the primary instrument for assessing the augmenter-reducer dimension initially made operational in the kinesthetic-aftereffect laboratory tasks. A prime example of this change is represented by the Larsen and Zarate (1991) study. The 45-item questionnaire developed by Herzog et al. shifted from dependent to independent variable and was used to compare reducersâ and augmentersâ reactions to taking part in a boring and monotonous task. Compared to augmenters, reducers described the task as more aversive and were less likely to repeat it. Further, reducers, relative to augmenters, exhibited more novelty seeking and sensation seeking in their day-to-day activities. Despite its promise, the augmenter-reducer construct seems to have vanished from the contemporary personality scene. Thus, it is absent from the index of the latest edition of the Handbook of Personality (John et al. 2008). The disappearance readily lends itself to speculation and a possible explanation. When there is a senior, prestigious psychologist advancing a construct whose predictions are highly similar to a construct advanced by younger psychologists of lesser reputation, the formerâs construct is likely to win out. Consider the theory of extraversion-introversion"
360,428,0.986,Compositionality and Concepts in Linguistics and Psychology,"of a modiï¬ed concept (e.g., clay machine). In addressing this question, we apply an approach in which a relation-based interpretation of the combined concept (GagnÃ© and Shoben 1997; Spalding et al. 2010), along with speciï¬c knowledge about the constituent concepts and meta-knowledge, is used in subsequent reasoning processes to infer the content of the combined concept (GagnÃ© and Spalding 2011, 2014b; Spalding and GagnÃ© 2015 ; see also Connolly et al. 2007). We should note that this ârelations and reasoningâ approach contrasts with many theories of conceptual combination, which tend to assume that the interpretation of a combined concept is the construction of a set of properties true of the combined concept. These properties are assumed mostly to be drawn from the constituent concepts, with some drawn from experience with the combined concept itself (see, e.g., the selective modiï¬cation model of Osherson and Smith 1981; Smith and Osherson 1984; Smith et al. 1988 or schema based models such as Murphy 1988, 1990; Wisniewski 1997 or prototype combination models such as Hampton 1987, 1991). In turn, these more feature-based notions of conceptual combination tend to assume a kind of implicit âcontainerâ metaphor of concepts (see, e.g., Laurence and Margolis 1999, for a discussion of the container metaphor). Such theories tend to assume that properties are contained in the concepts (or, alternatively, that the concept just is the set of properties, or that the concept is made of the properties), and that property veriï¬cation is, therefore, a matter of somehow inspecting the concept to see if the property is there. A schema theory of concepts, for example, presents the concept as the structured set of properties (i.e., the schema itself is the concept, and the schema is made of the properties in a certain structured set of relationships to each other). Property veriï¬cation then, is a matter of somehow seeing whether the property is a part of the schema. The relation and reasoning approach and the more feature-based approaches to conceptual combination therefore treat property veriï¬cation quite differently. We should note that in this discussion of the various models, we have grouped schema-based and property-based theories together because both approaches assume that properties are parts of concepts (that is, they both use a container metaphor of concepts). However, these two approaches differ in other aspects; for example, property-based theories were derived from propositional logic and schema-based theories were derived from predicate calculus (see Barsalou and Hale 1993). These differences, although important, are not relevant for the purposes of our current investigation. Recently, a series of research projects have aimed at understanding property veriï¬cation in combined concepts in order to evaluate competing theories of conceptual combination (Connolly et al. 2007; GagnÃ© and Spalding 2011, 2014b; JÃ¶nsson and Hampton 2008, 2012; Hampton et al. 2011; Spalding and GagnÃ© 2015). The most important result of these studies has been the modiï¬cation effect: modiï¬cation leads properties that are true of the head noun of the modiï¬er-noun phrase to be less true of the modiï¬ed concept (e.g., baby ducks have webbed feet is judged less true than ducks have webbed feet) while properties clearly false of the head are judged less false of the modiï¬ed concept (e.g., purple candles have teeth is less false than candles have teeth), despite the fact that the properties were explicitly chosen to be unrelated to the modiï¬er that was used. Indeed, the same effects arise even when"
124,213,0.986,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"1 Introduction This chapter weaves together two themes in the work of Nuel Belnap. The earlier theme, launched in âTonk, Plonk, and Plinkâ (1962), was to propose conditions under which logical rules determine the meanings of the connectives they regulate. The later theme was the employment of semantics for the open future in the foundations of logics of agency. The first theme leads to the second in the following way. Consider the natural deduction rules for standard propositional logic, which by the lights of âTonk, Plonk and Plinkâ successfully define meanings for the connectives &, â, â, and v. What are the meanings so assigned? Is there a way to give a semantical characterization of what the demand that a connective obey its rules says about the connectiveâs meaning? It will be shown here that on at least one reasonable criterion for fixing meaning of a connective by its rule governed deductive behavior, the natural deduction rules for (classical) propositional logic do not fix the interpretation embodied in the standard truth tables, but instead express an open future semantics related to Kripkeâs S4 possible worlds semantics for intuitionistic logic. Part of the basis for this connection has already been worked out (Garson 1990, 2001). This chapter reports new results on disjunction, and explores the relationships between this open future semantics and supervaluations. The upshot is that the semantics actually expressed by the standard natural deduction rules for propositional logic is a semantics for an open future. Since that semantics is the one the rules of propositional logic actually fix, it is reasonable to think that that is the interpretation of the connectives that we have (secretly?) employed all along. It is not just that the conception of an open future is built into our idea of agency, it is already found in the foundations of classical logic. It will be no surprise then, when this interpretation shows itself to be useful for locating what is wrong with fatalistic arguments that attempt to close the door on a open future."
139,409,0.986,Programming for Computations - MATLAB/Octave (Volume 14.0),"Through this formula we can quickly generate the solutions for a rod made of aluminum, wood, or rubber â it is just a matter of plugging in the right Ë value. Figure 5.3 shows four snapshots of the scaled (dimensionless) solution .Nx; N tN/. The power of scaling is to reduce the number of physical parameters in a problem, and in the present case, we found one single problem that is independent of the material (Ë) and the geometry (L)."
296,209,0.986,Becoming a World-Class University : The Case of King Abdulaziz University,"A third pitfall might be mentioned too: the lack of backward reasoning. Many planners desire to reach the future by updating the present. Yet, a strategic approach requires that an aspiring vision is developed, that the values of the institution are stated, and that a point of time in the future is specified when the aspiring vision is supposed to be realized, while the stated values are maintained and in place. Then, given the endpoint of planning, backward reasoning should be used in order to derive what actions are necessary and when. Forward reasoning used exclusively might lead the planning institution to a state of uncontrolled dynamics. Note that backward deduction is the tool used in the theory of dynamic optimization."
214,326,0.986,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"The preceding chapters gave an overview of the climate system and its components as well as a primer on how we simulate those components, but even so weâve just skimmed the surface. Why should we trust a climate model? Generally, we gain trust in a model through evaluation and validation of the model. We then use the model to make projections of the future. In this chapter, we describe the basics of how climate models are evaluated and how they are tested. The language and terms used in this discussion can be confusing. For example, the terms validation and evaluation are often used to mean different things, and a projection is not the same as a forecast. We will see why shortly. Testing models is a critical part of the development process."
139,429,0.986,Programming for Computations - MATLAB/Octave (Volume 14.0),"d) Consider the physical application from Sect. 5.1.4. Run this case with the  rule and  D 1=2 for the following values of t: 0.001, 0.01, 0.05. Report what you Filename: rod_ThetaRule.m. Remarks Despite the fact that the Crank-Nicolson method, or the  rule with  D 1=2, is theoretically more accurate than the Backward Euler and Forward Euler schemes, it may exhibit non-physical oscillations as in the present example if the solution is very steep. The oscillations are damped in time, and decreases with decreasing t. To avoid oscillations one must have t at maximum twice the stability limit of the Forward Euler method. This is one reason why the Backward Euler method (or a 2-step backward scheme, see Exercise 5.3) are popular for diffusion equations with abrupt initial conditions. Exercise 5.6: Compute the diffusion of a Gaussian peak Solve the following diffusion problem: @2 u D Ë 2; u.x; 0/ D p 2 2"
360,317,0.986,Compositionality and Concepts in Linguistics and Psychology,"Poortman et al. (2017). These examples help us observe common principles in the complex interplay between acceptability and typicality. Example 1 HAIR In one of Leeâs experiments (Lee, 2017: Fig. 5.4), participants were shown a picture of hair that was atypically dyed focal red. 87% of the participants accepted this picture as an instance of the concept RED HAIR. Therefore, we may reasonably conclude that the stimulus was quite acceptable as an instance of HAIR.7 However, in a color preference task for the concept HAIR, only 24% of the participants preferred focal red to a more natural hue for hair (RGB value: 201,113,13). In accordance with naive intuition, we may conclude that hair that is dyed focal red is an acceptable instance of HAIR, but is atypical for the category. Example 2 SHAKE In one of Kerem et al.âs experiments, participants were shown two pictures of a man shaking infant beds (Fig. 2a, b), as well as an incomplete transitive sentence âthe man is shaking (something)â.8 Although both pictures clearly show the man doing the activity reported in the sentence, 87% of the participants preferred Fig. 2a as a âbetter illustration for the sentenceâ. We conclude that a situation with two patients is an atypical instance of the concept SHAKE, despite its high acceptability. Example 3 WALK and WRITE In one of the experiments by Poortman (2017), participants were instructed to rate the oddness of a situation where a person walks and writes at the same time. 7 Lee did not directly check the degree of acceptability for HAIR . Overextension effects (Hampton"
269,62,0.986,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"research that gets to the heart of the historical, theoretical, and conceptual frameworks of the science are, we suggest, rarer. FC recalls that both authors expended significant energy, over the course of many drafts, adjusting the wording so that it carried enough punch for the cultural theorist and historian, and enough nuance for the neuroscientist: in particular, the many (many) annotated drafts of the article reveal, for her at least, ongoing, shared anxieties about the politics of word choice; about how, or whether, normative claims might be made; and about if and how to include sentences that pointed to the future of the field. And yet despite these risks and efforts, co-authoring brought, for FC, significant pleasures. For a cultural theorist engaged with the history of science, the opportunity to co-author and publish in a scientific journal shifted her position of annunciation from the usual disciplinary one of external commentary to one of audible authority within a field. She surmised that for a neuroscientist, the opportunity to participate in meta-conjecture, and to intervene in the overarching frameworks and potential ontologies at work in oneâs own field, might offer a somewhat unusual, and yet potentially productive, intellectual position vis-Ã -vis one of the scientific objects that is the focus of much of his research. What collaborators cannot control, of course, is how they get read, or how they will be subsequently positioned against one another. At one stage in this process, FC followed up a shared invitation, to an interdisciplinary event, to ask whether it would be of interest to hear a linked presentation (from both FC and her neuroscientific collaborator) that included some reflection on how this cross-disciplinary partnership had developed. She received a warm reply â but one that warned her of the preponderance of âcutting-edge neuroscienceâ at the event, and thus likely scepticism about a talk that would go âtoo deeply into the genealogy of a fieldâ. Some historical context was welcome, she was told â but perhaps not too much. The key was not to work too hard at âreaching across the divideâ (between the sciences and the humanities), but simply to present cutting-edge (neuroscientific) work. Her respondent lamented, additionally, that humanists were sometimes âquite lazyâ and not up to speed with the science (this writer, we note, came from the humanities). There are many important points embedded here â not a few of which we are inclined to agree with. But what is especially fascinating is the way in which a concern about an ostentatious reaching across the divide ended up not only reinscribing an unequal dyad of science (cuttingedge, worthy of admiration, and excitement) and the humanities (that"
315,291,0.986,Situating Children of Migrants Across Borders and origins : a Methodological Overview,"The cumulative, intercultural, intergenerational and dialogic analysis of these discourses, representations and narratives is undertaken through interpretation by the team as a whole. It draws on what Soulet (2012) calls the realistic imagination, a constrained, collective, interpretative activity. The constraints are those of thoroughness, authenticity, relevance and recognition of subjectivities. This analytical activity is implemented both through a conceptually designed interpretative framework and within a grey area, even a margin of discretion, occupied by the different members of the team. Searching for meaning leads to an intergenerational and intercultural reinterpretation of histories, practices and representations. The research team, fueled by the participants they encountered, reveals new configurations of meaning which provide a different take on the intergenerational relations, transmissions, circulations, exchanges, co-constructions and social relations in which they belong. These hypotheses of meaning can thus be applied to texts which give voice to the actors and to the collective imagination of the research team. Each personâs sociological intuition, colored by his or her language and culture of origin, generation, experience and knowledge, and gender, is called upon in a cumulative emic perspective (PaillÃ© 2012); new accounts are redrawn, written and narrated. The examples below illustrate several possible links which give meaning to multistranded discourse and dialogue. Grandmotherâmother dialogue from the same trio through individual interviews. This dialogue demonstrates the intergenerational circularity of the apprenticeships. From my daughter I learned that each one of us can be patient when we want to be. From my granddaughter I learned that the way we behave depends on the environment in which we grew up (YU4SGM). As I have already said, my mother taught me to be patient, and to respect others. My daughter taught me to be brave and to accept those things that I cannot change (YU4SM)."
372,1782,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"The denominator in Eq. (14.75) is the total intensity. Ë.y/ is the Faraday depth, which increases monotonically into the source as long as the sign of the longitudinal magnetic field direction does not change. In any case, we can superpose all the radiation from the same Faraday depth and write the integrals in Eq. (14.75) as a function of Ë instead of y, yielding"
382,20,0.986,"A History of Male Psychological Disorders in Britain, 1945â1980","fraction of the entire field of depressive phenomenaâ and that it was a âcultural category constructed by psychiatrists in the westâ. By definition, he argued, âit excludes most depressive phenomena, even in the westâ.64 Kleinman developed these ideas over a long career as a psychiatrist and anthropologist, expounding the notion that âcultural values and social relations shape how we perceive and monitor our bodies, label and categorise bodily symptomsâ, and that we therefore âexpress our distress through bodily idioms that are both peculiar to distinctive cultural worlds and constrained by our shared human conditionâ.65 Kleinmanâs ideas were soon well-established and later expanded by a group of other anthropologists and psychiatrists interested in crosscultural psychiatry. Laurence Kirmayer, whose interest in the subject was rooted in his own familyâs experience of immigration to Canada, became another key researcher in the field.66 Kirmayer pointed out the conceptual confusion in the use of the term somatisation, setting out three distinct meanings that could be found in contemporary literature. In western biomedicine, for example, patients were expected to recognise that the roots of their distress lay in psychological or social conflict and articulate them as such to a physician. However, if somatic symptoms presented without organic cause, patients were assumed to be somatising. A second interpretation, and the one promoted by Kleinman, was that somatic symptoms present in place of an emotional problem where the body is a metaphor for social and emotional experience. Finally, psychoanalytically inflected theories of somatisation inferred that emotions could give rise to somatic signs and symptoms.67 Kirmayer pointed out that, despite the differences in these interpretations, they nonetheless all shared a common core: that âsomatisation always involves a discrepancy between where an observer believes a problem, concern or event is located, or how he expects it to be expressed, and the subjectâs experience and expression of it in the bodyâ.68 There has been criticism of the broad notion of somatisation on a number of levels. Biological psychiatry claims that the concept is relativistic: if our perception and presentation of symptoms is entirely culturally determined, there can be no âtrueâ psychiatric disorders, proving problematic for clinical practice and treatment. Some also argue that the notion of somatisation somehow buttresses a dualistic concept of medicine, which presumes the physical body is isolated from the mind, proposing instead that emotion is âembodiedâ in bodily processes.69 These matters are still widely debated and are difficult to untangle. Two psychologists from the University of California, Berkeley,"
249,235,0.986,Advances in Proof-Theoretic Semantics (Volume 43.0),"2.1 (2â² ) is intuitionistically valid Intuitionists do not agree with Dummett and other neo-verificationists that meaning is to be explained in any case in terms of truth-conditions. According to them, Â«The notion of truth makes no sense [â¦] in intuitionistic mathematicsÂ»7 ; the key notion of the theory of meaning is the notion of proof, and understanding Î± (knowing its meaning) is to be explained as being capable to recognize the proofs of Î±. The content of a mathematical statement Î± (what Frege would have called the thought expressed by Î±) is characterized by Heyting as the expectation of a proof of Î±. What a proof of Î± is, is explained by recursion on the logical complexity of Î±, under the assumption that we have an intuitive understanding of what is a proof of an atomic statement. This is the BHK-explanation. I think a revision of this explanation is necessary concerning disjunction and existential quantification.8 According to Heyting, a proof of Î± â¨ Î² is either a proof of Î± or a proof of Î²; however, even the intuitionists consider, for instance, âPrime(n) â¨ Â¬ Prime(n)â, where n is some very large number, as assertible even if neither âPrime(n)â nor âÂ¬ Prime(n)â is; I propose therefore to revise the BHK-explanation in the following way: A proof of Î± â¨ Î² is a procedure such that its execution yields,9 after a finite time, either a proof of Î± or a proof of Î².10 An analogous modification of the clause for âxÎ± can be similarly motivated. Summing up, the revised version of the BHK-explanation I will make reference to is the following:"
289,52,0.986,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","If we apply Î»x : â. f x to 3, which is fine since the function can take any input, the first translation runs smoothly in Î»B, while the second one will raise a cast error (Int cannot be cast to Bool). Similarly, if we apply it to true, then the second succeeds while the first fails. The culprit lies in the highlighted parts where any instantiation of a would be put inside the explicit cast. More generally, any choice introduces an explicit cast to that type in the translation, which causes a runtime cast error if the function is applied to a value whose type does not match the guessed type. Note that this does not compromise the type safety of the translated expressions, since cast errors are part of the type safety guarantees. Coherence. The ambiguity of translation seems to imply that the declarative system is incoherent. A semantics is coherent if distinct typing derivations of the same typing judgment possess the same meaning [20]. We argue that the declarative system is âcoherent up to cast errorsâ in the sense that a well-typed program produces a unique value, or results in a cast error. In the above example, whatever the translation might be, applying Î»x : â. f x to 3 either results in a cast error, or produces 3, nothing else. This discrepancy is due to the guessing nature of the declarative system. As far as the declarative system is concerned, both Int â Int and Bool â Bool are equally acceptable. But this is not the case at runtime. The acute reader may have found that the only appropriate choice is to instantiate f to â â â. However, as specified by rule M-Forall in Fig. 8, we can only instantiate type variables to monotypes, but â is not a monotype! We will get back to this issue in Sect. 6.2 after we present the corresponding algorithmic system in Sect. 5."
26,96,0.986,Cognitive Supervision for Robot-Assisted Minimally Invasive Laser Surgery,"Given the research problem outlined in the previous chapter, here we focus on the development of a methodology to learn the temperature dynamics of tissues subject to thermal laser ablation. Chapter 2 introduced the equations that govern the generation of heat within the tissue during laser irradiation, and the associated increase of temperature. Such relations constitute the starting point of our investigation: based on these, we hypothesize that the superficial temperature of tissue can be modeled, at any given instant in time, as the superposition of Gaussian functions. Nonlinear fitting methods [1] are used to calculate the Gaussian parameters (amplitude, mean and variance) that describe the distribution of temperature at discrete moments in time. The learning problem is formulated as a supervised nonlinear regression, which aims to predict the temporal evolution of the Gaussian parameters. To understand the viability of the proposed approach we first consider a simple scenario, which does not involve motions of the laser beam. Later in the chapter we present an extension of this methodology to a more general case, to estimate the temperature dynamics produced by a scanning beam."
78,160,0.986,The Onlife Manifesto : Being Human in a Hyperconnected Era,"selfhood: Nissenbaum draws on the account of human beings developed by James Rachelsâone that begins (again) with an account of selfhood as inextricably interwoven with the specific roles and relationships we engage in (1975). None of this is accidental for our project. As I have documented earlier, Luciano Floridiâs information ontology, as he himself emphasizes, âdraws on the emphasis on the interconnection between all things familiar from recent environmental and feminist philosophiesâand, importantly, from such non-Western views as Buddhism and Confucian thoughtâ (Ess 2009, p. 161). This is to say: Floridiâs information ontology, among its many other virtues, brings forward precisely the ways in which computational technologies and computer networks facilitate and enable our sense of selfhood as relational beings first of all. But as it does so, it thereby reiterates at least parallel understandings of selfhood qua relational found in both modern (Western) feminism, ecology, and phenomenologyâif not in at least some version of Kantâas well as in both ancient Western and Eastern frameworks. Insofar as this is true, then our focus in the (analogue-) digital age on interactions and relationality rightly highlights these as brought forward in striking new ways. But it may be more accurate to say that this is a renewed focus, one that has been brewing for quite some time in modern Western philosophy (if not in Kant, then certainly in phenomenology)âand one that would not seem unfamiliar to ancients in either Western or Eastern worlds."
360,143,0.986,Compositionality and Concepts in Linguistics and Psychology,"8 (Some) Aspects of Current Theories of Mental Concepts that Require Attention Objectivist theories of meaning are just fineâso far as they go, which is not very far when it comes to mental life. (And one might reasonably think this is a very major shortfall!) On the other hand, Subjectivist theories need some further work, even within just the mental side of meaning with which theyâve been concerned. This section is about a group of topics that need to be addressed more fully than they have been, in order that one can take the Subjectivist theory of meaning seriously enough to be included in the meanings portion of a two-tier theory of meaning. I mention three areas that seem to be âobvious problemsâ that need to be dealt with before it is possible to develop any deep mental theories that invoke concepts., including the"
310,75,0.986,Online Hate Speech in The European Union : a Discourse-Analytic Perspective,"belief systems are conceived and communicatedâ (Charteris-Black 2005: 3, our italics). In particular, metaphors of THREAT, LEECHES or PARASITES are typically used to ostracise the non-natives (Musolff 2015; Baker et al. 2008), and have also been described as being coherent with the beliefs, actions, or imaginings of the person using them. In a way, metaphors can reveal the underlying conceptual frame of their producer and give access to a set of assumptions made by competent members of a discourse community about the âtypicalâ aspects of a member of a minority or any person belonging (or appearing as belonging) to that group. This then leads to the conceptualisation of metaphors as creating or conï¬rming stereotypes (Zinken 2003). In light of the above, the aim of the present section is to understand how in the context of the small Orthodox island of Cyprus, where almost no refugees had landed during the summer of 2015 and where religion plays an active part in politics and everyday life (cf. Baider 2017), xenophobic metaphors are used to construct the social Other in social media. Due to space limitations, this section focuses only on our analysis of comments retrieved on the basis of the keywords ârefugee(s)â, âmigrant(s)â and âforeigner(s)â in line with the common C.O.N.T.A.C. T. methodology.27 For these particular keywords, we collected 2446 comments. Our analysis of these comments in terms of polarity revealed that more comments were negative than positive but not overwhelmingly so, as Table 3.3 shows. Having collected and classiï¬ed our data in this way, we then proceeded to identify the most common linguistic means that are used to negatively categorise the social groups at hand. Here, the most frequent means include metaphors, insults, proverbs and irony/sarcasm, but for the purposes of this section we will focus on metaphors as a means for Othering migrants, foreigners and refugees. Most work dealing with the use of metaphors in discourse related to migration (see, for instance, Santa Ana 1999) has shown that it is often being conceptualised as a"
192,379,0.986,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"Thus, rather than as a symptom of pre-fraudulent cynicism, Stapelâs Jos Jasperâs lecture must be regarded as an (albeit anecdotal and non-normative) articulation of budding epistemological despair. What Stapel is pointing out (albeit in a jocular tone of voice, somewhat reminiscent of Erasmusâs Praise of Folly) is that formal (frontstage) academic accounts of experimental practice in social psychology papers tend to conceal and disavow the continuous improvisations that are actually conducted backstage. Rather than as a cynical swindler, Stapel poses as a disenchanted, divided subject, addressing issues, sharing and discussing his growing discontent in university discourse before an audience of peers. Let me start with Stapelâs concern that social psychological âfactsâ may actually be created by the research paradigm or theoretical perspective we use. This problem was already addressed by Friedrich Nietzsche in his famous exclamation Facta! Yes Facta Ficta! published in Daybreak (1881/1980, Â§ 307). Nietzscheâs exclamation plays on alliteration and highlights an intriguing etymological affinity between fact and fiction, terms which are usually seen as conceptual opposites. The alliteration reminds us of the fact that the word fact is actually derived from the Latin verb facere (to make, to fabricate, to create), while the word fiction actually comes from the Latin verb fingere (to form, to shape; literally: to bring forth with oneâs fingers). Thus, the etymology reveals that scientific facts are fabricated in laboratories. The alliteration emphasises that facts and fictions are related phenomena because âfactsâ (like fictions) are made, are produced, rather than given, and this notably applies to scientific facts. Indeed, all scientific facts are fabricated, in the literal, non-pejorative sense of the term. They are products of laboratory research practices; they are artefacts, produced with the help of experimental set-ups and technical contrivances. What Nietzscheâs alliteration reminds us of, is that all scientific facts are âfingeredâ: they are coloured, tainted if you will, by the knowledge production processes which brought them forth (and whose fingerprints they bear). The etymology reveals that there is something fundamentally artificial about scientific âfactsâ, so that the listing of âfabricationâ as one of the three major forms of scientific misconduct is at least"
269,87,0.986,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"In short, if there is now an invitation to âthe socialâ from (some) parts of the biological sciences, then there is simultaneously an openness to think biologically from (some) parts of contemporary social theory. And if such moves are often partial and contested, they nonetheless form a gap into which the researcher interested in interdisciplinary experimentation might insert herself."
297,1847,0.986,The R Book,"The fundamental tool of spectral analysis is the periodogram. This is based on the squared correlation between the time series and sine/cosine waves of frequency Ï, and conveys exactly the same information as the autocovariance function. It may (or may not) make the information easier to interpret. Using the function is straightforward; we employ the spectrum function like this: spectrum(Lynx,main="""",col=""red"")"
359,142,0.986,"Micro-, Meso- and Macro-Dynamics of the Brain","The decoding curves tracking distinct features of the current trial typically rise and fall at different times, thus providing precious indications about when, and in which order, the respective representations begin to be explicitly coded in brain activity. For example, Fig. 2 illustrates how we decoded the time course of perceptual, motor, intentional and meta-cognitive error-detection processes from the very same MEG/EEG signal (Charles et al. 2014; for another application to the stages of invariant visual recognition, see Isik et al. 2014). In addition to tackling the when question, machine learning may also tell us for how long a given neural code is activated and whether it recurs over time. To this aim, we asked how a pattern classifier trained at time t generalizes to data from another time point t0 . This approach results in a temporal generalization matrix that contains a vast amount of detail about the dynamics of neural codes (King and Dehaene 2014). If the same neural code is active at times t and t0 , then a classifier trained at time t should generalize to the other time, t0 . If, however, the information is passed on to a series of successive stages, each with its own coding scheme, then such generalization across time should fail, and classifiers trained at different time points will be distinct from each other. More generally, the shape of the temporal generalization matrix, which encodes the success in training at time t and testing at time t0 for all combinations of t and t0 , can provide a considerable amount of information about the time course of coding stages. For instance, it can reveal whether and when a given neural code recurs, how long it lasts, and whether its scalp topography reverses or oscillates. When comparing two experimental conditions C and C0 , it can also reveal whether and when the series of unfolding stages"
185,123,0.986,The Essence of Software Engineering,"â¢ Instead of âborrowing and rewritingâ other peopleâs words when it comes to the more voluminous detailed supporting guidance, it is better to simply reference the original sources of this guidance. Essentialized practices such as this one work on the principle that novice teams need support from expert coaches to be successful. The cards become a tool for expert coaches to use to help teams to adopt, adapt and assess their team practices, or for expert teams to use in the same way. Finally note that, when presented electronically as browsable HTML images, the association and reference links can all be navigated electronically, as can other link elements on other cards. Find User Stories (Activity Card) â gives guidance to a team on what they should actually do, in terms of (in this case): â¢ A description of the activity. â¢ An indication of the Competencies and Competency Levels that we need for the activity to be executed successfully. For instance the card requires Stakeholder Representative competency at level 2 and Analysis competency at level 1 (all of which is defined in the Essence kernel, and can be immediately drilled into from the electronic browsable HTML and cards) â¢ An indication of the space that the Activity operates in â i.e. what âkind of thing it helps us doâ (the generic kernel âActivity Spaceâ â in this case âUnderstand the Requirementsâ) â¢ An indication of the purpose of the activity expressed as the end-state that it achieves â in this case a User Story is Identified and a physical Story Card produced that expresses the value associated with the User Story. Note that activities are critical because without them nothing actually ever gets done - it is remarkable how many traditional methods inundate readers with posturing and theorizing, without actually giving them what they need, which is clear advice on what they should actually do! User Story (Alpha) â a key thing that we work with, that we need to progress, and the progression of which is a key trackable status indicator for the project â you can think of Alphas as the things that you expect to see flowing across Kanban boards, described here in terms of: â¢ A brief description that makes clear what this thing is and what it is used for. â¢ A sequence of States that the item is progressed through â in this case from being Identified through being Ready for Development through to being Done. (Think of these as candidate columns on a Kanban Board â although teams may want to represent other interim states as well depending on their local working practices). â¢ The âparentâ (kernel) Alpha that the multiple User Stories all relate to (the Requirements in this case). Story Card (Work Product Card) â gives guidance on the real physical things that we should produce to make the essential information visible â in this case a key defining (though often forgotten) feature of the User Story approach is that we use"
192,398,0.986,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"As indicated, the âfathersâ deplore the âgeneral culture of careless, selective and uncritical handling of research and dataâ. A methodologically defective research practice (âsloppy scienceâ) constitutes âan unintended and unexpected findingâ of the inquiry (p. 47). âNumerous discrepancies were found between the way the experiment was actually carried out as could be deduced from the available data and the research material, and what was stated in the articleâ, thus obscuring what actually had happened and thereby ârendering the experiment unverifiable for othersâ (p. 51). But it is clear that in such statements the fathers are articulating their imperatives in a top-down, distanced manner (M1). Researchers who are actively involved in a work-floor setting will be aware of the inevitable discrepancies between context of discovery (the laboratory world) and context of justification (the paper world of research papers). This discrepancy is part of their daily experience no doubt (M2). In real laboratory life, there will always be the parallax between context of discovery and context of justification; there will always be a gap between âmonitoring proceduresâ on the one hand and the actual research culture on the other (which enabled Stapelâs violations to go unnoticed for quite some time). For all the complaints about the âimpossibilitiesâ the committee members discover (such as: âimpossible p-valuesâ), or about the âpeculiaritiesâ and the âsloppinessâ of the work of Stapel and his environment, the public degradation by the committees fails to answer questions such as why, in 1999, a formal international visitation committee had given this same research environment an exceptionally high score, suggesting outstanding international excellence. But the key question, dialectically speaking, is how to sublate this crisis, how to bring this mess of contradictions to a higher level (M2 â M3)? As indicated, I find it remarkable that, while trawling through Stapelâs oeuvre in search of indications of fraud, the committees more or less ignored the content of this oeuvre: the actual papers in which many of the tensions between discovery and justification (M2) are explicitly addressed. As if the normative trinity did not read the 137 papers which they âscrutinisedâ. And this is remarkable because precisely this seems to be part of the malaise: the widespread malady of counting, citing, indexing and processing scientific papers instead of really reading them. This reluctance to really read the content may actually reflect a major aspect of the current academic crisis, which is confirmed rather than repaired by the report. The committees analyse the oeuvre18 in a rather functional manner, using the hashtag symbol (#) to indicate whether a particular paper is considered contaminated by fraud. But they ignore, for instance, the content of a paper by Stapel and others (not-retracted, without the # label, and therefore apparently non-fraudulent) which analyses a previous, widely-discussed Dutch scientific misconduct case, namely the Diekstra case (Stapel et al. 1999). The authors had asked Dutch social psychologists (as respondents) to reflect on âa widely published plagiarism scandal involving a Dutch psychologist which affected themselves and the image of their professionâ (p. 397) and their analysis resulted in the intriguing conclusion that âexpert investigators of social influenceâ (i.e. social psychologist) who feel âframed of âtaintedâ by certain manipulations may often"
214,334,0.986,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"in Fig. 9.1, but also due to systematic errors in measurement. It is often as important to know the uncertainty of a measurement as it is to know the numeric value of the measurement. Sometimes, knowing the uncertainty is even more valuable: If you do not understand the uncertainty in an observation, it is not possible to understand if a model is statistically the same (correct) or different (wrong) compared to an observation. If the mean temperature is 68 Â°F (20 Â°C), and a model predicts 72 Â°F (22 Â°C), is the model wrong? If the expected error or uncertainty in the observation is Â±4 Â°F (2 Â°C) or larger, the model is correct. If the observed uncertainty is smaller than Â±4 Â°F (2 Â°C), then the model is wrong. Figure 9.1 addressed the sampling uncertainty of not knowing the âtrueâ distribution. Figure 9.2 illustrates the difference in distributions. If there is a lot of variability or spread about the mean (which can be measured statistically by the standard deviation; see Chap. 10) in the model and observations, then the model is not statistically different from the observations at some level of probability. Separating the black and blue curves is hard; separating the red and blue curves is easier, even though the red and black curves have the same mean. Scientists often try to estimate a conï¬dence level, or conï¬dence interval, for a distribution as a way of understanding the expected error. If an observation has uncertainty, a 95 % conï¬dence interval indicates we are 95 % certain to be within a given range. In Fig. 9.1d, this range is about 95â105, so 5 % of the observations fall outside this range. When comparing models to observations, if the conï¬dence interval for the model overlaps the observation, then the model is not signiï¬cantly Distributions"
95,353,0.986,Elements of Robotics,"The control algorithms in Chap. 6 used exact mathematical computations to determine the signals used to control the behavior of a robot. An alternate approach is to use fuzzy logic control algorithms based upon rules. A cruise control system might have rules of the form: â¢ If the car in front is far away or the car in back is near, set the speed to fast. â¢ If the car in front is near, set the speed to slow. The logic is âfuzzyâ because the rules are expressed in terms of linguistic variables like speed whose values do not have precise mathematical definitions, but only imprecise linguistic specifications like fast and slow. A fuzzy logic controller consists of three phases that are run sequentially: Fuzzify The values of the sensors are converted into values of the linguistic variables, such as far, closing, near, called premises. Each premise specifies a certainty which is the probability of our belief that the variable is true. Apply rules A set of rules expresses the control algorithm. Given a set of premises, a consequent is inferred. Consequents are also linguistic variables such as very fast, fast, cruise, slow, stop. Defuzzify The consequents are combined in order to produce a crisp output, which is a numerical value that controls some aspect of the robot such as the power applied to the motors. The following sections present the three phases of fuzzy control for the task of a robot approaching an object and stopping when it is very close to the object."
32,377,0.986,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","with an exponent close to 1=2, which is quite similar to the distribution found in our model. Moreover, this skewed profile is found also in biological coevolution. The data estimated from the fossil record show a skewed profile [3]. Although a q-exponential fit is proposed in [3], the stretched exponential function fits the data as well. It indicates the universality shared by sociological and biological systems. The second example is the lifetime distribution of Japanese firms. The lifetime distribution of Japanese firms which went bankrupt in 1997 is well approximated by a simple exponential function [26], hence it seems consistent with the (nonmodified) Red-Queen picture. Thus, we can conclude that the age-dependent mortality picture is not valid at least for Japanese firms. On the other hand, these data are not sufficient to reject the modified Red-Queen hypothesis even though the distribution is not skewed. This is because the period of the measurement is only one year, which is much shorter than the typical lifetime. A measurement on longer time scales will deepen our understanding of the bankruptcy dynamics of enterprises. Another possible reason for the inconsistency with our model is that the model assumes that the migration frequency of new species is independent of N. If we assume an N-dependent migration rate, we will get a different lifetime distribution. Data on the occurrence frequency of new enterprises would clarify this point. Several lifetime distributions of media contents have also been investigated. Media contents can be interacting with each other as most of these are competing for the attention of potential consumers. The first data set of media contents is movie popularity [27]. The cumulative distribution of the persistence time of a movie fits a stretched exponential form with an exponent about 1:6, indicating that it decays faster than exponentially. This quick decay is explained by a few observed stylized facts and an assumptions that a movie is withdrawn when the gross income per theater gets below a threshold value. The observed statistics shows a 1=t decay in gross income of a movie per theater, indicating that the mortality of a movie increases with age due to aging. That is why we see a faster decay than a simple exponential function. Another example of media contents is comic series. Figure 16.6 shows the cumulative lifetime distributions of comic series that have run in three major Japanese weekly comic magazines. We defined the lifetime of each comic series as the duration between the first and the final issues, and collected data from a Wikipedia article which contains the list of the start and end dates for each series. Since the termination of a series should strongly depend on its popularity, the series are competing with each other for a limited number of concurrent series. As we see in the figure, all these distributions show approximate exponential decays. Thus, the Red-Queen picture looks the most reasonable hypothesis for comic series. This is clearly different from the movie duration. We conjecture the reason of the difference is that the comic series has a new story every week while a movie is not renewed. One of the possible reasons that the dynamical graph model is not applicable is that the number of concurrent series does not show fluctuations, which is a key for the modified Red-Queen hypothesis."
8,757,0.986,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","26.4 Is There Equilibrium in the Relativistic Heavy Ion Collision? Nobody expects global equilibrium (except perhaps in selected âcentralâ collisions), but there are good reasons to doubt even that there is local equilibrium, because the duration of a collision, the lifetime of resonances, and the time needed to create a particle are all of the same order of magnitude. In particle collisions one can agree that thermodynamic equilibrium does not require a number of collisions of existing particles, but that the quantum mechanical probability distributions governing the creation of particles are such that the new-born particles seem to come from an equilibrium state [12]. This might be different in the relativistic heavy ion collision where many particles are already present before the collision and have to undergo collisions individually and/or coherently. The argument for equilibrium seems to be valid in particle physics because the thermodynamical model which rests heavily on it was on the whole successful. With very few free parameters chosen once and for all, it covered collisions of different sorts of particles with lab energies between 10 and 1,000 GeV, describing rather well the features of particle production for all sorts of particles and with production rates ranging over 12 orders of magnitude. While many models are quantitatively superior in restricted areas, the thermodynamical model was (and still is) the only one covering the whole reasonably well. There are also some failures: 1. Two-particle correlations are not well described by the simple thermodynamical model [38]. 2. The large p? processes observed at ISR energies [39] are not predicted by the model. The second failure is particularly interesting because it concerns only about one per thousand of all produced particlesâthe rest behave as the model predicts. Figure 26.9 shows what happens qualitatively. The p? distribution takes off the straight line predicted by the model at p? & 1:5 GeV/c and stays higher up depending on the height of the collision energy. Why does this not kill the model?"
363,129,0.986,History and Cultural Memory in Neo-Victorian Fiction,"system (425). Rolandâs confession of love recognises this paradox: âI love you ... It isnât convenient. Not now Iâve acquired a future. But thatâs how it is. In the worst way. All the things we â we grew up not believing in. Total obsession, night and day. When I see you, you look alive and everything else â fades. All thatâ (506). His avowal of love acknowledges that all such declarations must always be citations but also that in his quotation he has, as Umberto Eco puts it, âsucceeded, once again, in speaking of loveâ (Eco, 1984: 32â3). In the dialogue of mutual possession so integral to the novelâs representation of reading and loving it is the imagination that holds in tension theoretical knowingness and the desire that subverts it. The ways in which love can be spoken becomes more interesting than the ways in which it cannot. Ventriloquising Ash and LaMotte enables Roland and Maud to relinquish some of their self-possession in order to take possession of the other, and be possessed in turn. This is represented by the scene in which they make love. It is figured as both possession and dispossession, and the language Byatt uses signifies death as well as life. Or, more specifically, a life that inheres in death: in the morning, the whole world had a strange new smell. It was the smell of the aftermath, a green smell, a smell of shredded leaves and oozing resin, of crushed wood and splashed sap, a tart smell, which bore some relation to the smell of bitten apples. It was the smell of death and destruction and it smelled fresh and lively and hopeful. (507) Byattâs use of sex as a vehicle for characterising Roland and Maudâs mutual possession captures the paradox of life coexisting with death, of self-possession cohabiting with dispossession. In this novel, love destroys autonomy and self-possession, and yet it is also somehow necessary to the individual, and much desired. Or, as Belsey writes, âdesire is inevitable (ânecessityâ) Christabel calls it, twice â¦ and at the same time dangerous â beyond the pleasure principle, destructive, angry, âa wrecker,â as Maud puts itâ (Belsey, 1994b: 696). Love, in this romance, embodies the paradox of âpossessionâ itself; it involves possessing the beloved, but at the same time, being possessed, possession and dispossession. Bronfen, too, observes that in the novel âto take possession in love need not always mean that the beloved is reduced to an object of possessionâ (Bronfen, 1996: 132). This is the lesson that Roland and Maud must learn. They must rethink, or, more precisely, re-feel the theoretical position they have"
17,13,0.986,The Philosophy of Mathematics Education,"and on being able to control or possess it (âgetting itâ). Thus these metaphors presuppose a static âbankingâ model, interpreting understanding as the acquisition, ownership or possession of knowledge (Sfard 1998). But secondly, there is an ideological assumption that understanding a concept or skill is better, deeper and more valuable than simply being able to use or perform it successfully. Skemp (1976) distinguished ârelational understandingâ from âinstrumental understandingâ, and posited the superiority of the former. However his co-originator of the distinction Mellin-Olsen (1981) used it to distinguish the modes of thinking of academic students from that of apprentices, thus bringing in a social context and even a social class dimension to the distinction, and imposing less of an implicit received valuation. If we want to assert the superiority of relational understanding over instrumental understanding it needs to be done on the basis of a reasoned argument, and not taken for granted as obvious. Skempâs own argument was based on the psychology of schemata, based on Piagetian theories, but this have been challenged by a number of alternate theories of learning including socio-cultural theory and social constructivism, drawing on Vygotskyâs theory of learning. According to Vygotsky knowledge is not something that the learner possesses but is a competence inferred from the learnerâs manifested ability to complete a task, either unaided, or, with the help of a more capable other, in what is termed the learnerâs zone of proximal development. Given current challenges to the underlying theories of learning, the assumption that relational understanding is superior stands in need of justiï¬cation. Some scholars have challenged the unquestioned pre-eminence of relational understanding. Hossain et al. (2013) question the accepted good of the related notion of âunderstanding mathematics in-depthâ because, as they show, its role in the identity work of some student-teachers is troubling to them.1 for example, one student teacher with the pseudonym Lola experiences a conflict between the imposed good of relational understanding, when studying England, and her own success within the norms of instrumental understanding that she internalized in her Nigerian upbringing (Ernest in press). Others have challenged the uncritical promotion of understanding within the mathematics education community because of its incoherence. Llewellyn (2010) questions âunderstandingâ partly because of slippage in the use of the term so that it encompasses both its relational and instrumental forms. However, her deeper critique is that in use it carries with a whole host of problematic assumptions about who can own âunderstandingâ in terms of ability, gender, race, class. Understanding is produced as hierarchical, particularly in relation to gender, social class and ability. It belongs to the privileged few, the ânaturallyâ able, which are often boys (another unhelpful and unnecessary classiï¬cation). To suggest that girls have a âquest for understandingâ is over simplistic and gendered and in the ï¬rst instance we should unpack how each version of understanding is constructed. â¦ Finally I suggest that student teachers In later work Skemp (1982) refers to instrumental understanding as âsurfaceâ and relational understanding as âdeepâ understanding, thus preï¬guring the depth metaphor in the more recently coined term âunderstanding mathematics in-depthâ."
213,13,0.986,Collider Physics Within The Standard Model : a Primer,"a field with suitable (depending on the particle spin) transformation properties under the Lorentz group (the relativistic spacetime coordinate transformations). It is remarkable that the description of all these particle interactions is based on a common principle: âgaugeâ invariance. A âgaugeâ symmetry is invariance under transformations that rotate the basic internal degrees of freedom, but with rotation angles that depend on the spacetime point. At the classical level, gauge invariance is a property of the Maxwell equations of electrodynamics, and it is in this context that the notion and the name of gauge invariance were introduced. The prototype of all quantum gauge field theories, with a single gauged charge, is quantum electrodynamics (QED), developed in the years from 1926 until about 1950, which is indeed the quantum version of Maxwellâs theory. Theories with gauge symmetry in four spacetime dimensions are renormalizable and are completely determined given the symmetry group and the representations of the interacting fields. The whole set of strong, electromagnetic, and weak interactions is described by a gauge theory with 12 gauged non-commuting charges. This is called the âStandard Modelâ of particle interactions (SM). Actually, only a subgroup of the SM symmetry is directly reflected in the spectrum of physical states. A part of the electroweak symmetry is hidden by the Higgs mechanism for spontaneous symmetry breaking of the gauge symmetry. The theory of general relativity is a classical description of gravity (in the sense that it is non-quantum mechanical). It goes beyond the static approximation described by Newtonâs law and includes dynamical phenomena like, for example, gravitational waves. The problem of formulating a quantum theory of gravitational interactions is one of the central challenges of contemporary theoretical physics. But quantum effects in gravity only become important for energy concentrations in spacetime which are not in practice accessible to experimentation in the laboratory. Thus the search for the correct theory can only be done by a purely speculative approach. All attempts at a description of quantum gravity in terms of a well defined and computable local field theory along similar lines to those used for the SM have so far failed to lead to a satisfactory framework. Rather, at present, the most complete and plausible description of quantum gravity is a theory formulated in terms of non-pointlike basic objects, the so-called âstringsâ, extended over much shorter distances than those experimentally accessible and which live in a spacetime with 10 or 11 dimensions. The additional dimensions beyond the familiar 4 are, typically, compactified, which means that they are curled up with a curvature radius of the order of the string dimensions. Present string theory is an all-comprehensive framework that suggests a unified description of all interactions including gravity, in which the SM would be only a low energy or large distance approximation. A fundamental principle of quantum mechanics, the Heisenberg uncertainty principle, implies that, when studying particles with spatial dimensions of order Âx or interactions taking place at distances of order Âx, one needs as a probe a beam of particles (typically produced by an accelerator) with impulse p & â=Âx, where â is the reduced Planck constant (â D h=2). Accelerators presently in operation, like the Large Hadron Collider (LHC) at CERN near Geneva, allow us to study collisions"
105,235,0.986,"Computer and information Sciences : 31St international Symposium, Iscis 2016, KrakÃ³w, Poland, October 27â28, 2016, Proceedings (Volume 659.0)","â As mentioned before, in the grounding process, only the availability of instances that can be passed as parameters to methods, and the predetermined mapping between concepts and parameters, are considered, with no pre-conditions for method calls. This is a major flaw, since it may be that two methods have exactly the same parameter set, but they perform very different functions, and the wrong one gets called. â The choreography specification is disparate from the capability specification (pre-conditions, post-conditions), whereas they are in fact intimately related and intertwined. The actions specified in the choreography should actually take the initial state of the ontologies to their final state, through the interaction of the requester and web service. This fact is completely overlooked in WSMO choreography. â Choreography engine execution stops in WSMO when no more rules apply. A natural time for it to stop would be when the conditions specified in the goal are satisfied by the current state of the ontology stores. Again this is a design flaw, which is due to the fact that the intimate relationship between the capability specification and choreography has been overlooked."
253,707,0.986,"Autonomous Driving : Technical, Legal and Social Aspects","20.3.3 Existence Uncertainty For the performance of automated driving, existence uncertainty is at least as relevant as state uncertainty. It expresses the probability that the object in the representation of the vehicleâs surroundings actually corresponds to a real object. For example, emergency braking of an automated vehicle should only be triggered in the case of a very high existence probability for a detected obstacle. While the estimation of state uncertainties using Bayes estimation methods is well founded in theory, the existence probability in todayâs systems is still mostly determined on the basis of a heuristic quality measure. An object is taken as conï¬rmed if the quality measure exceeds a sensor- and application-dependent threshold. For example, the quality measures are based on the number of measurements that have conï¬rmed the object, or simply the interval between the initialization of the object and the current point in time. Often the state uncertainty of the object (Sect. 20.3.2) is also used for the validation. An approach with a better theoretical foundation is the estimation of a probability-based existence probability. This ï¬rstly requires a deï¬nition of the speciï¬c object existence. While in some applications, all real objects are taken to be existent, the object existence can also be limited to the objects that are relevant in the current application. Additionally, a limitation to the objects that can also be detected with the current sensor setup is also possible. In contrast to a threshold procedure, this determination of the existence probability enables a probability-based interpretation option. For example, an existence probability of 90 % means that there is a 90 % probability that the measurement history and the motion pattern of the object were created by a real object. Consequently, the action planning of the automated vehicle can use these probabilities when evaluating alternative actions. A known algorithm for calculating an existence probability is the Joint Integrated Probabilistic Data Association (JIPDA) procedure, also based on the Bayes ï¬lter, which was ï¬rst introduced in 2004 by Musicki and Evans [15]. This procedure additionally uses the detection and false alarm probabilities of the sensors, which are presumed to be known. The calculation of the current object existence probability is performed similarly to state estimation in the Kalman ï¬lter in a prediction step and an innovation step. The existence prediction is performed using a Markov model of the ï¬rst order. The predicted existence of an object is given by the Markov chain pk Ã¾ 1jk Ã°9xÃ Â¼ pS pk Ã°9xÃ Ã¾ pB pk Ã°6 9xÃ"
147,317,0.986,Trailblazing in Entrepreneurship : Creating New Paths For Understanding The Field,"future. Indeed, in this book, we have hopefully established a number of trailheads (based on current state-of-the-art knowledge about important aspects of the entrepreneurial process) and then offered some insights into possible trails that can be blazed from these trailheads. The proposed trailblazing has largely focused on the notion of individualsâ thinking, feeling, and acting in relation to potential opportunities embedded in communities and often extreme contexts. This is not to say that there are not important trails to be blazed at the entrepreneurial team, firm, institutional, regional, and other levels of analysis. These levels of analysis are not within our area of expertise, so it is more difficult for us personally to highlight potential trails at these levels of analysis even though we believe that they exist. We look forward to seeing advancements at these levels of analysis as well. Regardless of the level of analysis (or across levels of analysis), maintaining an open mind to novelty is critical. We need to have an open mind about the philosophical approach. This does not mean that an author should use a different philosophical perspective for each paper (although he or she certainly could) but that we should be open enough to allow others to have a different philosophical perspective underlying their research. If we converge on a specific philosophical perspective, perhaps as the result of more closed-minded editors, reviewers, and authors, then we discourage trailblazing and âkill offâ an important source of potential new insights. We are not advocating an approach of âanything goes,â but we, as scholars, should (should is a strong word, but it emphasizes our strong belief) judge research based on the traditions and expectations in which it is embedded. In a similar way, it is important that entrepreneurship scholars remain open-minded to different theoretical lenses and ways of generating new theory. Indeed, to the extent that people can apply new theories to entrepreneurship research, there is an increased chance of uncovering new insights into entrepreneurial phenomena as well as making contributions back to the literatures from which these âborrowedâ theories come. Therefore, while we can borrow theories to understand entrepreneurial phenomena, it is important to go one step further and look toward âblendingâ to make a contribution back to the solutionsâ origins. That is, in applying a specific theory (from outside the entrepreneurship domain), what adaptions to that theory are required to apply it to an aspect of the entrepreneurial context? Exploring this question provides a basis for"
165,98,0.986,New Methods for Measuring and Analyzing Segregation,"A third factor that may help explain why the separation index has not been used more widely requires a longer discussion. It is that S is occasionally viewed as a measure of group isolation and exposure rather than a measure of uneven distribution. At one level I view the controversy as minor because most technical reviews correctly characterize the separation index as a measure of uneven distribution (e.g., Zoloth 1976; James and Taeuber 1985; White 1986; Reardon and Firebaugh 2002). But there are contrasting descriptions of S in the literature so the issue warrants a brief side discussion. Massey and Denton (1988) categorize S (which they refer to as V and eta squared) as an âexposureâ measure rather than a measure of uneven distribution. One reason they offer for doing so is that, unlike D and G, S does not have a definite relationship to the segregation curve. This concern should be set aside for two reasons. The first reason is that Massey and Denton themselves do not apply this criterion in a consistent way. For example, they classify the Theil entropy index (H) as an index of uneven distribution but, like S, H also does not have a definite relationship with the segregation curve. The second reason to set aside this concern is that many authoritative reviews of measures of uneven distribution disregard the segregation curve when evaluating indices (e.g., Zoloth 1976; Stearns and Logan 1986; Reardon and Firebaugh 2002). S is equal to the eta squared ( hÂ² ) statistic from an individual-level analysis of variance predicting the mean of the binary variable of race (0â1) by area of residence (e.g., the categorical variable of tract). Relatedly, S is equal to the square of the individual-level correlation between race (coded 0â1) and p (computed for area of residence)."
278,4,0.986,Taking Stock of Industrial Ecology,"to change. This is clearly one of the reasons for the success of its scientific language and the enormous relevance of its ongoing work. Ultimately, of course, there are limits to the extent that scientific language can shift the boundaries of its own meaning. The difficult course between precision and adaptability is navigated over and over again, as societyâs needs also change. Circumstances alter; culture reinvents itself. The challenge for science is to respond to those changes. History is almost as replete with defunct disciplines, which failed in that task, as ecology is of extinct species. In the long run, perhaps, the arbiter of success is not longevity, but usefulness. To have survived and thrived for over 25 years is of course a remarkable achievement. But what really counts is the light that industrial ecology has shed on some of the most pressing issues of our time. This volume is a fitting testament to that success. Guildford, Surrey, UK"
311,992,0.986,The Physics of the B Factories,"butions have turned out not to be very satisfactory and other more complex functions are called upon. This can partly be explained as the influence of poorly understood resonances (such as the Ï/Îº or the higher mass resonances mentioned above). As a result ânon-resonantâ has come to mean anything that is not modeled by a resonance. In practical terms, this means the distributions often have to come from MC simulations, oï¬-resonance data or sideband data, or a combination of all three. In the case of sideband data, MC samples must be used to remove events from B meson decays that are also present and to determine possible diï¬erences in the background shape between the sideband and signal regions. Linear interpolation between bins can be used where needed. The backgrounds are constructed separately for both the B 0 and B 0 events and a p.d.f. or histogram is formed taking into account any asymmetry that might be present in the background distributions (see, for example Eq. 20 in Aubert (2009h)). The observables that are used in the ML depend on the analysis under consideration. Typically, a combination of ÎE, mES , multivariate discriminant, position in the Dalitz Plot and charge (flavor) of the B meson candidate is used. Sometimes a cut is applied to the observable first (e.g. on the multivariate discriminant) and then this observable is excluded from the fit. This usually happens for observables that are correlated with position in the Dalitz Plot. As in two-body and quasi-two-body decays, certain D meson decays to the same or similar final state can be used as a calibration channel and allow for correction to fitted parameters derived just from MC simulation. Although many of the resonances in the Dalitz Plot can be predicted from previous quasi-two-body measurements, there is still a large uncertainty in the number and type of resonances that should be included in any particular model. Examples include the exact parameterization of the non-resonant three-body decay component, the Ï/Îº with masses in the region 400 â 600 MeV/c2 and widths that are large and uncertain, the Ï(782), the Ïc0 and Ïc2 , and the higher mass partners of the Ï, f0 (980), and K â . The addition of a resonance to the model that is not present in the data can be just as problematic as any exclusion of a resonance that is present. The problem is exacerbated if a blind fit is being performed. One technique is to use the log-likelihood reported by a particular model fitted to the data or to calculate a Ï2 statistic based on the number of events predicted from a fit and the number of real events in a bin in the Dalitz Plot. The statistical significance of the presence of a component can be estimated by evaluating the diï¬erence Î ln L between the negative log-likelihood of the nominal fit and that of a fit where the amplitude and ACP is set to zero. This is then used to evaluate a p value which is the integral from 2Î ln L to infinity of the p.d.f. of the Ï2 distribution. An important goal of the Dalitz Plot analysis is the extraction of CP asymmetries either from a time-integrated or time-dependent analysis. Consequently, the resonances are parameterized not just in terms of their widths and masses but as functions of the decay dynamics, angular"
81,6,0.986,The Price of Uncertainty in Present-Biased Planning,"In Sect. 4 we generalize our notion of uncertainty to individuals whose present bias Î² may change arbitrarily over time within the set B. This model is inspired by work of Gravin et al. [7], except that we do not rely on the assumption that Î² is drawn independently from a fixed probability distribution. Instead, our goal is to design penalty fees that work well for all possible sequences of Î² over time. We believe this to be an interesting extension of the fixed parameter case as the"
332,11,0.986,"Media Resistance : Protest, Dislike, Abstention","Also resistance to communication technology tends to be explained in psychological terms. Bauer (1995b) shows how a conï¬ned body of literature in the 1980s and 1990s âemploys âanxietyâ and âphobiaâ as core concepts for understanding resistance to computers at school, at work and at homeâ (97). Resistance is seen âas a structural and personal deï¬cit,â it is âirrational, morally bad, or at best, understandable but futileâ (Bauer 1995a, 2, see also Selwyn 2003, 103). I will show examples of positions that both ï¬t and do not ï¬t the popular image of a Luddite, but will not attempt to determine whether protesters really are Luddites. Instead, the discussion will show how accusations of Luddism inï¬uence the way writers and activists frame their arguments and how they try to distance themselves from assumptions that they are simplistic and anti-technology. While it is of course, legitimate to discuss whether an argument is technologically determinist or anti-technology, this might not be the most interesting aspect of a text expressing media resistance. In this book, I attempt instead to understand such texts as sense-making efforts, drawing on an eclectic mix of perspectives and ideas in order to warn about, or explain, potential damage resulting from mediaâs presence. The label of âlaggardâ is also used about those who resist the media (see Selwyn 2003, 105). The theoretical deï¬nition of laggards comes from diffusion theory and the classical work Diffusion of innovation from 1962 (1995), which divides adopters into ï¬ve ideal types (263â266). While the ï¬rst three: innovators, early adopters and early majority, are described in positive terms, the two last: late majority and laggards, are described in negative terms. Laggards are described as backward looking: Laggards are the last in a social system to adopt an innovation. They possess almost no opinion leadership. . . . The point of reference for the laggard is the past . . . . Laggards tend to be suspicious of innovations and of change agents (265)."
49,438,0.986,Artificial Intelligence and Cognitive Science IV,"As fuzzy inference rules are composed of uncertain linguistic terms (values), which are described just by membership functions (MFs) Âµ, it is apparent that parametric learning should precede the structural one. However, this process is just reverse to a human learning process. At first a human learns a basic structure of a given problem and after that he/she searches parametric values. The consequence of this fact is that some parameters like for example number of linguistic values must be determined manually in advance. Hence basic learning methods are not fully automatized and they require some manual interventions, too. Another division of adaptation approaches for FIS is whether its structure is preserved, i.e. the so-called direct or indirect adaptation, where the conventional mechanism of FIS is transformed into another structure as its part. Almost all approaches based on neural networks belong to indirect adaptation because FIS is transformed to a special neural network, which is functionally equivalent to a FIS [19]. In this case it can be difficult to extract a KB for a FIS but mostly the indirect adaptation is quick and some methods enable to design a KB incrementally and on-line. Other approaches represent mostly the group of the direct adaptation, whose basic structure can be depicted as it is seen in fig. 3. There is no need of a special method for knowledge extraction but these methods work only off-line. We see that the structures of FIS as well as KB are preserved and the adaptation"
269,41,0.986,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"(Dussauge and Kaiser 2012), the people and publications assembled around âcritical neuroscienceâ (e.g. Choudhury, Nagel, and Slaby 2009; Choudhury and Slaby 2012; Critical Neuroscience n.d.), the field of neuropsychoanalysis (Solms and Turnbull 2011; Fotopoulou, Pfaff and Conway 2012), the Association of Neuroesthetics (Association of Neuroesthetics 2015), the BIOS centre at the London School of Economics (BIOS 2015), as well as various âNeurohumanitiesâ programmes in prestigious universities (Hagouel 2012). We conjecture that that moment is no longer alive in quite the same way. It is certainly not that the avenues for interdisciplinary research have suddenly narrowed, but rather that the horizon of interdisciplinary possibility, in 2015â2016, is not quite the same as it was in, say, 2008â 2009. Our hunch is that the sense of possibility that we witnessed has moved to other, currently âsexierâ, biosocial fields. We think here, for example, of epigenetics (Landecker and Panofsky 2013) or metabolomics (Levin 2014). It is noteworthy that neither the ENSN nor the European Platform exists any longer (although a group of junior researchers, based primarily at Kingâs College London, is attempting to resuscitate a âNeuroscience and Society Networkâ â see Mahfoud and Maclean 2015). In our particular neck of the woods, it is noticeable that there now appear to be more large, grant-funded interdisciplinary projects that address the terrain of the brain and the mind (e.g. Hearing the Voice and Hubbub), and fewer specially designed âplatformsâ or programmes to train cohorts of researchers competent in at least two disciplines (though note graduate programmes, such as the Berlin School of Mind and Brain, committed to training interdisciplinary researchers). None of which is at all to say that such opportunities are now entirely foreclosed â or that nothing remains for the would-be junior collaborator save the unenviable labour of shaving the rough edges from an earlier, open, precarious moment, now sadly passed. But there is a temporality, as well as a momentum, of openness and closure around specific interdisciplinary fields. For those interested in collaboration around the brain and mind, itâs worth asking the question: What kind of moment are we actually in now in the relationship between the neurosciences and the social sciences? How might the specific dynamics and constellations of that moment shape how interdisciplinarity is imagined and practised?"
84,124,0.986,Eye Tracking Methodology,"that saccade destinations are preprogrammed. That is, once the saccadic movement to the next desired fixation location has been calculated (programming latencies of about 200 ms have been reported), saccades cannot be altered. One reason behind this presumption is that, during saccade execution, there is insufficient time for visual feedback to guide the eye to its final position (Carpenter 1977). One the other hand, a saccadic feedback system is plausible if it is assumed that instead of visual feedback, an internal copy of head, eye, and target position is used to guide the eyes during a"
208,168,0.986,Actors and the Art of Performance,"The event of the performative in the acting process is made up, as we have seen, of the conscious absorption of a critical reworking of oneâs own archive, the historical and the personal archive. The responsibility and the ethos of the actor must be to embrace this pathos, this passion, this passio â to be its physiological witness. He owes this to his talent, to promise himself to that which is existential within repetition, as a category of the future, a possibility that is always becoming, not as a promise of a tomorrow that never comes, but of one that can, and does indeed, arrive in the moment of a felicitous, providential performance. Against the spirit of our epoch, it might be time to reinstate beauty, felicity, and fortune in the canon of art. Lâavenir du bonheur! Lâavenir de la beautÃ©!"
132,176,0.986,AiREAS: Sustainocracy for a Healthy City: Phase 3: Civilian Participation â Including the Global Health Deal Proposition,"(matter) that human life needs to support itself, it keeps developing ups and downs without the harmony that nature always experiences, simply because it combines various value systems at once. The focus on a single currency with no other collateral than debt makes the human and ecological drama even larger. In nature, a diversity of lifeforms mingles in permanent pursuit of harmony, using different resources to develop. During a workshop at the Zoo of Emmen, the imagination of entrepreneurship was triggered through looking at the diversity of butterflies living together in a relatively small space, and peacefully at that, due to the non-competitive differentiation of size, food, reproduction, etc. Creating an ecosystem in economies can overcome problems of current models that live through single cyclic patterns. This also has its logic in the ï¬eld of human complexities. Not everyone goes through a crisis at the same time or in the same phase of their lives. Many people do this on their own and develop awareness and innovative patterns ahead of the mainstream. If their leadership is blocked by a formalized mainstream, then progress is blocked out of the systemâs self-interest. Negative tension then gradually builds up. However, if the leadership receives freedom to deploy itself, it generates a positive tension between the robustness of the mainstream and the argumentation of renewal. When we deployed AiREAS, we addressed the awareness level of human beings at different levels of society ï¬rst. When asking a deeply aware human being about the need for core human values, hardly any resistance is felt. If this is seconded by the proven vulnerability that builds up in the institutions, then the professional position of that same human being involves making a choice: contribute to the core values through the authority of the position, or negate awareness by supporting the mission of the institution, even if it proves damaging to the core values. Awareness and guts are human factors that become decisive for taking individual entrepreneurial action, but when these factors are combined in a multidisciplinary, awareness-driven co-creation, change is a fact. The human being comes ï¬rst, awareness places the core values as a permanent goal, and leadership produces the required change for harmony. We use our institutions, knowledge and technology as instruments for progress, rather than submitting to them in dependence. We have been attempting to prove this by going through our own value-driven cycles. Every exercise in AiREAS has been developed through this method of combined entrepreneurial approach by bringing people and authorities together behind the awareness switch. A whole new dialogue appeared, including new vocabulary to express ourselves without continuous disputes about the meaning of words in the different contexts."
82,213,0.986,Fading Foundations : Probability and The Regress Problem,"Abstract A probabilistic regress, if benign, is characterized by the feature of fading foundations: the effect of the foundational term in a finite chain diminishes as the chain becomes longer, and completely dies away in the limit. This feature implies that in an infinite chain the justification of the target arises exclusively from the joint intermediate links; a foundation or ground is not needed. The phenomenon of fading foundations sheds light on the difference between propositional and doxastic justification, and it helps us settle the question whether justification is transmitted from one link in the chain to another, as foundationalists claim, or whether it emerges from a chain or network as a whole, as is maintained by coherentists and infinitists."
124,248,0.986,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"The argument has the form of (v Out), so it is classically valid. The premises appear indisputable, since adopting classical logic gives us Excluded Middle and (Fact 1) was proven for ||PL||. It appears to follow that there is no room in ||PL|| for any unsettled sentences, for when it is settled that not-A, that is, v(âA) = T, we have v(A) = F, so that the conclusion of the argument asserts that A must be settled true or settled false, hence settled. The problem with this reasoning is that it does not take proper care in distinguishing the object language from the metalanguage. Therefore, the English renderings"
360,454,0.986,Compositionality and Concepts in Linguistics and Psychology,"1 Introduction The goal of this paper is to confront and explore the larger implications of a problem that we have repeatedly observed in our ongoing work on the semantics of modification within noun phrases, which is one instantiation of concept combination. L. McNally (â) â G. Boleda Department of Translation and Language Sciences, Universitat Pompeu Fabra, Barcelona, Spain e-mail: louise.mcnally@upf.edu G. Boleda e-mail: gemma.boleda@upf.edu Â© The Author(s) 2017 J.A. Hampton and Y. Winter (eds.), Compositionality and Concepts in Linguistics and Psychology, Language, Cognition, and Mind 3, DOI 10.1007/978-3-319-45977-6_10"
257,25,0.986,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","The second technique we explore is the use of concolic execution to derive a path condition, which is a formula over secret values that is consistent with a query result. By performing model counting to estimate the number of solutions to this formula, which are an underapproximation of the true size of the distribution, we can safely boost the lower bound of s. This approach is depicted to the left of the arrow in Fig. 3(b). The depicted shapes represent discovered path conditionâs disjuncts, whose size sums to 63. This is a better lower bound on s and improves the vulnerability estimate to 0.032. These techniques can be used together to further increase precision. In particular, we can first perform concolic execution, and then sample from the area not covered by this underapproximation. Importantly, Sect. 8 shows that using our techniques with the interval-based analysis yields an orders of magnitude performance improvement over using polyhedra-based analysis alone, while achieving similar levels of precision, with high confidence."
82,223,0.986,Fading Foundations : Probability and The Regress Problem,"iar washing out of the prior in Bayesian reasoning. In Bayesian updating, the prior probability becomes less and less important under the influence of new pieces of information coming in, until it washes out completely. Although this looks rather like the phenomenon of fading foundations, where the influence of p similarly diminishes, the two phenomena are actually quite different, as we explain in Appendix"
117,148,0.986,Care in Healthcare : Reflections On Theory and Practice,"contract does not concern goods, but instead a person who is vulnerable and non-exchangeable in their uniqueness. The relationship between the surrogate and the parents-to-be is the third relationship of special importance in surrogacy arrangements. Little attention is paid to this topic in scientific discourse, but for this analysis it is crucial to show that the surrogate and the commissioning parents are not just contract partners, but also interrelated in an ethical manner. Levinasâ concept of âthe thirdâ offers an interesting insight for the analysis of this special relationship, as it shatters the private relationship between the self and the Other and introduces a different, although still ethical, quality. As StÃ©phane MosÃ¨s points out, the third is different from the Other in the sense of proximity, quantity and its selection: The third is further afar than the Other, it is numerous instead of unique, and it is the only one in an ethical relationship that is freely chosen (MosÃ¨s 1993). The surrogates meet the criteria: they are usually miles away from the commissioning parents, it does not matter to them exactly which surrogate carries their child to term, and it is they who choose to enter a surrogacy arrangement and involve a third party in their family planning. However, for Levinas, the third does not need to be conceived as a visible empirical human being. Instead, it is best interpreted as reminder that other people who are not part of a personal relationship and differ from the self in terms of ethnicity, sex, status or religion must be considered as well. The third interferes with the relationship of the self and the Other and thereby challenges the privileged position of the Other. Thus, it opens up the frontiers of thinking. The relationship to the third is not personal anymore, but refers to the sphere of justice and equality.9 Therefore, the third is also allied with institutions and universal laws instead of the particularity and context-sensitivity that is part of personal relationships. Levinas comments on the difference between the Other and the third as being a difference in thinking: [â¦] what seems to me very important, is that there are not only two of us in the world. But I think that everything begins as if we were only two. It is important to recognize that the idea of justice always supposes that there is a third. But, initially, in principle, I am concerned about justice because the other has a face. (Levinas et al. 2005, p. 170)"
360,154,0.986,Compositionality and Concepts in Linguistics and Psychology,"One presumes that lexical verbs also correspond to mental concepts in this overall picture, and that since verb phrases can conjoin (eats, drinks, and falls asleep as well as marries, works, breeds, and dies); so presumably the corresponding verboriented concepts can conjoin. But as I will try to demonstrate in this section, it seems reasonable to suppose it will be a different type of concept combination than in the nominal side of things, because of the particular differences there are between the noun-concepts and the verb-concepts, as well as the higher degree of complexity that verb-concepts display. Conceptual combination seems to have been designed to deal with what might be called âconjunctive (or: intersective) conceptsâ, even though the resulting feature lists (or whatever used to keep track of properties) is not a simple conjunction of the feature lists of the subparts. A part of the rationale for concentrating on this could be that English allows easily for conjoined noun phrases (Bob and Sally, Men and women, Both small dogs and short snakes, All baseball players but only some Olympic high-jumpers, etc.) And also that the interpretation of adjective-noun combinations has historically been seen as conjunctive (although modern formal semantics treats them otherwise). But it is not so clear to me how any similar technique will work with what might be called âdisjunctive conceptsâ. After all, English pretty easily allows for disjunctive noun phrases: Juan or Alice, Men or women, Either Americans or Canadians, etc.48 As remarked just above, verb phrases easily conjoin just as much as noun phrases do, but I claimed that the conceptual items corresponding to verbal items will require some different sort of combining method from the ones in the noun realm. Verb phrases can disjoin, like noun phrases can, for linguistic phrases such as is either swimming or eating. Another place where noun concepts and verb concepts appear to be different is in the case of linguistic conjunction. But given that the conceptual combining method for verb-concepts is different from that of noun-concepts, due to the inherently more complex nature of verbs, it seems likely that the method for forming disjunctive verb-concepts would have to be different from that of forming disjunctive noun-concepts. A special difficulty for theories of concepts attends to those concepts that correspond to non-monadic verbs. That is, to relational verbsâones that take direct and indirect objects.) For example, the verb kick requires a direct object, and so any corresponding concept will require something corresponding to this. As with any verb-concept, there will need to be some notion of a subject of the kicking-concept. But in the case of transitive verbs, the corresponding concept will also require the notion of a recipient of the kicking. It seems that some as-yet unspecified notion of conceptual combination is required for this. (And a further unspecified one for"
192,181,0.986,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"Although the novel first and foremost revolves around the intricacies of university discourse, as we have seen, the discourse of the hysteric (in the Lacanian sense of the term) occasionally flares up as well, both on the individual and on the collective level, for instance when qualms of consciousness can no longer be contained or repressed. In the latter case, the hystericâs discourse gives rise to a boisterous attitude, challenging the authorities, criticising those in power: $ â S1 on the upper level. In the full version:"
192,236,0.986,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"he wants to control, but which actually controls him, up to the point of impairment ($ in the upper-right position). His craving is to transform it into a predictable tool, to domesticate it into a normal object, so that university discourse can function smoothly again. A working vaccine would have been a perfect outcome in this disruptive struggle for power between the researcher and his virus ($ â a). In that case, the phage would have ended his discontent, his anonymity, by making him famous, turning his life into a success story after all. But the idea of medical benefits functions as a faÃ§ade, a pretext for his urge to dominate nature in her most elementary (viral and microbial) dimensions, working himself into a state of neurasthenia â in accordance with Dostoyevskyâs insistence that intellectual activity is, in final instance, a âdiseaseâ (1864/1972, p. 18). In conclusion, we are not confronted with a moral dilemma in the sense of a problem that can be solved by developing or abiding to rules and regulations, such as the codes of conduct and ethical principles of research with human subjects. Although such practices (developed by professional ethicists) may help to subdue the tension, making microbial research more manageable no doubt, they will not abolish the basic divide. The desire to control life may become addictive precisely because it is driven by a disruptive compulsion. As a science novel, Arrowsmith opens up and dramatizes this basic rupture, thus furthering our awareness of the gap between what basic research produces and what clinical medicine basically needs. And this divide not only affects the knowledge dimension, but the normative dimension (the level of morality and of the Self) as well. According to Arrowsmith, what is considered integrity in the realm of basic research is regarded as misconduct in medical practice, and vice versa. But this may not the final word. Arrowsmith is the first real science novel, as we have seen. In the next sections we will explore how these tensions are addressed in other novels, notably in Chap. 7 (Cantorâs dilemma) and Chap. 9 (Intuition)."
192,143,0.986,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"The upper level refers to the actual achievements of modern science (since the beginning of modernity): the scientific breakthroughs. But Husserl rather focusses on the forces at work beneath the bar. Rather than being neutral, Husserl argues, scientific research had always been driven (albeit tacitly) by a normative ideal (S1 in the lower-left position). A latent truth, a basic conviction, a moral vocation had always been at work in modern research practices. But now, during the first decades of the twentieth century, physicists (S2 in the upper-left position, as agents) are confronted with enigmatic entities (elementary particles: a in the upper-right position), which not only open-up previously unknown dimensions of reality (the subatomic realm), but also unleash unprecedented forms of power, resulting in an existential crisis, as by-product of university discourse ($ in the lower-right position). The transition leading from quantum physics to nuclear warfare has been amply documented. In this section I will reread this history from a Lacanian perspective, focussing on the case of Robert Oppenheimer, whose life story became the subject matter of a novel (by Haakon Chevalier 1959), a play (by Heinar Kipphardt 1964) and an opera (by John Adams 2005). A strict (technocratic) definition of misconduct focusses on fabrication, falsification and plagiarism of data (FFP), as we have seen, so that questions such as whether it is condonable for scientists to contribute to the development of a genocidal bomb (explicitly developed for massive destruction of urban areas) falls outside the scope of the integrity discourse sensu stricto. But the case of Robert Oppenheimer cannot be ignored for various reasons, ranging from questions concerning sensitive data management (the physicist as a âsecurity riskâ) up to the function of the death drive, of which his atomic bomb seems the very embodiment. But the death drive also functions on the micro-scale, as a suicidal component (a tendency towards intellectual âsuicideâ) discernible in many cases of misconduct, even in the FFP sense of the term. Starting point for the analysis is The American Prometheus, the 719-page Oppenheimer biography written by Kai Bird and Martin Sherwin (2005/2006)."
363,161,0.986,History and Cultural Memory in Neo-Victorian Fiction,"âThe alluring patina of lossâ: Photography, Memory, and Memory Texts in Sixty Lights and Afterimage [I long] to have such a memorial of every being dear to me in the world. It is not merely the likeness which is precious in such cases â but the association and the sense of nearness involved in the thing â¦ the fact of the very shadow of the person lying there fixed forever! It is the very sanctification of portraits I think â and it is not at all monstrous in me to say, what my brothers cry out against so vehemently, that I would rather have such a memorial of one I dearly loved, than the noblest artistâs work ever produced. (Elizabeth Barrett Browning in a letter to Mary Russell Mitford, 1843) Photography is an elegiac art, a twilight zone â¦ All photographs are memento mori. To take a photograph is to participate in another personâs (or thingâs) mortality. Precisely by slicing out this moment and freezing it, all photographâs testify to timeâs relentless melt. (Susan Sontag, On Photography, 1977) Writing about historical recollection and material culture, Elizabeth Edwards asserts that âphotographs are perhaps the most ubiquitous and insistent focus of nineteenth- and twentieth-century memoryâ (Edwards, 1999: 221). It is fitting, then, that many contemporary historical novelists return to the Victorian origins of photography to explore history, memory and the Victorian era.1 They dramatise the value that attaches to photography as a memorial medium, its promise, as Elizabeth Barrett Browning suggests in the epigraph above, to erase distance, to"
363,176,0.986,History and Cultural Memory in Neo-Victorian Fiction,"After she shares the story with him, this image of this road to and from nowhere haunts her employer, Eldon, too. Indeed, it haunts the novel itself as a symbol of dislocation of time and place, the disruption of a linear sense of time, the continuum of past, present and future. In a novel obsessed with âdistance. Position. How to find your way back when where you are depends on where everything else isâ (47), the road to and from nowhere symbolises disorientation and displacement. With no connection to her past, cut off from her origin in the maternal body, Annie is as a traveller on this road that offers no hope of return. Moreover, Annie reflects that as a maid, she is unlikely to ever marry or have children, stunting, afresh, the maternal line: âthe future is more of the same. No, the future is less, and the sameâ (85). Disconnected from her past, Annie also has no future. Cut off from the maternal body, but desirous of a reunion with it, indeed haunted by it, Sixty Lights and Afterimage explore the inherence of the past in the present via a series of images of, and metaphors for, haunting. Whereas in Afterimage the use of haunting is restricted to Annieâs dreams, Sixty Lights explores the notion of ghostly haunting more fully. Lucy is jealously convinced that her brother Thomas, who sleepwalks, âotherworldly and implacably absent â¦ communes with ghostsâ (105), particularly those of her parents. Thomasâ feeling about these visions captures the characteristic indeterminacy of the spectral figure: âit was something that would follow him all his life, like having the wrong personâs shadow, like carrying an aberration of presenceâ (95). The liminal figure of the ghost, which exists in a space between presence and absence, and is perhaps only a trick of the light or of the mind, is an aberration of presence. It is not the restoration of what is lost. The trope of the ghost is a familiar one in contemporary fictional returns to the Victorian era. Indeed Rosario Arias Doblas argues that the prevalence of the use of ghostliness and hauntings as a metaphor for the presence of the past makes the âspectralâ novel a âsubsetâ of the neo-Victorian novel (Doblas, 2005: 88). In Liz Jensenâs Ark Baby (1998) a Victorian ghost literally inhabits the same space as the contemporary characters. Having lived in the house herself, when alive, she now haunts it in death ( Jensen, 1998). Christian Gutleben observes that the ghost literally interacts with the modern characters, thus establishing a sort of hyphen between the past and presentâ (Gutleben, 2001: 190). However, the figure of the ghost is actually a disruption of linear time. The link it establishes between past and present is not so much a hyphen, bridge, or other linear form, but is rather a repetition. Or, more precisely, it is repetition with a difference. As Nick Peim suggests, âthe spectre is"
235,33,0.986,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","1.5 Extrinsic Observers When it comes to the perception of systems â physical and virtual alike â there exist two modes of observations: The first, extrinsic mode, peeks at the system without interfering with the system. In terms of interfaces, there is only a one-way flow from the object toward the observer; nothing is exchanged in the other direction. This situation is depicted in Fig. 1.3."
232,495,0.986,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"the item related to the utterance, and the second is whether the utterance is an influence or not. If it is considered by the researcher as an influence, then it would be decided whether it is a rational or non-rational influence. One example of rational influence is as follows: for the item oxygen, someone says, âThe amount 2.5 kg is not sufï¬cientâ. The logical reason may be strong or weak but at least the participant shows a fact-based argument. On the other hand, an example of non-rational influence is as follows: for the item pistol, someone says, âPistol is really unimportantâ, without providing any argument. The statement that says whether an item is important or not is the expected decision, therefore when someone mention their preference about a decision without any argument, it is considered as a non-rational influence. The result of the analysis towards group 5_6 is shown in Fig. 3. In the ï¬gure, one dot represents one utterance. This group mostly discuss about compass and oxygen, 30 and 32 utterances respectively. For compass, 17 utterances were rational while for oxygen it was 24. The group was focusing on those two items for most of its rational influence. When looking at the distribution of IIR for each item in that group, oxygen on the other hand is the item with the least diverse IIR. Meanwhile, the item with the most diverse IIR, the heating unit, was discussed only in 5 utterances and 2 of them are rational influence. As can be seen from Fig. 3, in the ï¬rst half of the discussion they intensively used rational influence, while in the latter half non-rational influence was more dominant. One might think that the second half is probably the decision stage after they discussed the matters in the beginning. However, most of the other items appeared in the second half. It means that for these items they conformed to each other with few rational influences or even not at all. This happened regardless of the difference of IIR of these items. One possible factor is the time pressure. Reaching the latter half they realized that they do not have much time to make a decision. At that moment, they started to use the heuristic approach by conforming or influencing other member even without a logical reason. Most of the time in the latter half, some of the members mentioned their rank for some items and then other people conformed to it, or provided alternative rank without any argument. However, a different pattern was found in group 3_3. In the beginning, rational influence was not so intensive. When the content of the discussion was checked, it is found out that they began intensively using rational influence only after they found a difference of opinion against a certain rank or item. Another fact that was observed from the discussion is the domination of the discussion. In group 5_6, the discussion was dominated by two of the members. One other member almost never spoke or gave any opinion. When their IIRs were"
130,178,0.986,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 1,"Most of the design patterns, even ones devised later, can be categorized in the mentioned way. This organization is rather rough, but gives initial clue of the purpose of each pattern. This view is supported by the presentation in most catalogs, which relies on UML class models of sample pattern instances. Design patterns foster tactical thinking, to use the pattern at will, where relevant problem is to be solved."
84,603,0.986,Eye Tracking Methodology,"24.1.4 Indirect Eye-Based Interaction Gaze-based communication systems such as those featuring eye typing offer certain (sometimes obvious) advantages but are also problematic. Gaze may provide an often faster pointing modality than a mouse or other pointing device, especially if the targets are sufficiently large. However, gaze is not as accurate as a mouse because the fovea limits the accuracy of the measured point of regard to about 0.5â¦ visual angle. Another significant problem is accuracy of the eye tracker. Following initial calibration, eye tracker accuracy may exhibit significant drift, where the measured point of regard gradually falls off from the actual point of gaze. Together with the Midas Touch problem, drift remains a significant problem for gaze input. Zhai et al. (1999) take another approach to gaze-based interaction and test the use of gaze as a sort of predictive pointing aid rather than a direct effector of selection. This is a particularly interesting and significant departure from âeye pointingâ 1 See http://www.cogain.org. 2 See http://www.cs.uoregon.edu/research/cm-hci/EyeDraw/."
289,1985,0.986,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","[4,11,14]). Particularly modular is the work by Neis et al. [31] on a verified compiler for an ML-like imperative source language. The main distinguishing feature of our work is that we start from a set of higher-order recursion equations with pattern matching on the left-hand side rather than a lambda calculus with pattern matching on the right-hand side. On the other hand we stand on the shoulders of CakeML which allows us to bypass all complications of machine code generation. Note that much of our compiler is not specific to CakeML and that it would be possible to retarget it to, for example, Pilsner abstract syntax with moderate effort. Finally, Fallenstein and Kumar [13] have presented a model of HOL inside HOL using large cardinals, including a reflection proof principle."
253,1095,0.986,"Autonomous Driving : Technical, Legal and Social Aspects","The acceptance subject of autonomous driving can currently be approximated, for example, by taking transport system users who will either passively or actively come up against autonomous driving in future. This covers all of those using the current road system, be it as car drivers, cyclists or pedestrians. Further relevant acceptance subjects include developers and engineers, politicians and businesspeople, or even public research institutes. Acceptance object Acceptance object does not necessarily imply a physical object as such, but rather refers to the adoption of something âon offer, available, or proposedâ ([13]: 89, translation by the authors). This may be engineering or technology, but it could also be artifacts of any type, or people, attitudes, opinions, arguments, actions, or even the values and norms behind such things. In turn, such an object acquires its signiï¬cance only from what individuals or society ascribe to itâthere is therefore no such thing as autonomous driving per se. Rather, the question is what speciï¬c functions autonomous driving can fulï¬ll, and what signiï¬cance individual people and society at large place in the technology. Behind this is the assumption that engineering and technology have no signiï¬cance in and of themselves; instead, this is only attained by the fulï¬lling of social functions, human actions, and their embedding into social structures (see [9]). Acceptance context The acceptance context refers to the environment in which an acceptance subject relates to an acceptance objectâand thus can only be viewed in relation to both. For example, the context of autonomous driving is determined by the current individual and social signiï¬cance of car usage: Why do people use cars? What attitudes, values, expectations, etc. inform (auto)mobile praxis? Does autonomous driving ï¬t in here seamlesslyâor will it change the meaning of (auto)mobility and its system of norms? In the copious literature on acceptance and acceptance research, various dimensions and levels are identiï¬ed where acceptance is visible and, above all, comprehensible. In the following, we shall take a closer look at the dimensions of attitudes, actions, and values."
82,230,0.986,Fading Foundations : Probability and The Regress Problem,"Our method differs however from what Klein and Rescher seem to have in mind. As we will explain in more detail in 5.3, where we argue for a view of justification as a kind of trade-off, the level of accuracy of the target can be decided upon in advance. Whether this level will be reached after we have arrived at proposition number three, four, sixteen, or more, depends on the structure of the series and on the chosen level. In no way does it depend on the question of how obvious proposition number three, four, sixteen, etc. is. Even if the proposition at issue is very obvious, and thus has a high probability, its contribution to the justification of the target might be small enough to be neglected. This is different from the contextualism of Klein and Rescher, according to which an agent stops when the next belief in the chain is sufficiently obvious and itself not in need of justification."
49,332,0.986,Artificial Intelligence and Cognitive Science IV,"The Fig. 9 shows the course of agentâs continuous learning by interaction with the environment. On the Y axis there is value of mean cumulative reward obtained by the agent from the environment, on the X axis is time in discrete steps. From the graph are clearly visible moments when the agent managed to discover new behavior. We can see that the hierarchy of actions was built consequently by the bottom-up approach: from the simpler behaviors towards the more complex strategy. Finally the agent learnt the stable behavior. This means that was able to successfully drink and eat when necessary. In a âfree timeâ the agent was training the fulfilling the goal of the âTreasure problemâ. 4.2 The Lights and Doors âSimplified Example of Designed Hybrid Planning The second experiment was concluded in order to test the agents ability to reuse an autonomously knowledge by the planning system, that is to get the set of abstract actions in a form of RL and autonomously create the world description (model) and a set of primitive actions. These (from the planners point of view) primitive actions can be then used for planning. In this experiment, the user needs to pass the hallway in order to reach the goal position. The requirements are that all doors on the path are opened and the lights are turned on. The only userâs a priori knowledge is that these systems can be controlled only from unknown and unreachable part of the map. So our agent is sent to find out how these systems can be controlled. The agent learns how to survive simultaneously, so after some time, the agent is physically there and able to fulfill relatively complex tasks, as, for example, âenable passing through the hallwayâ, which is composed of subtasks: ""turn on the lights"", ""open the door1"" and âopen the door2"". The desired state of the map is depicted in the Fig. 10, however, before the task is specified to the agent, the both doors are closed and lights are turned off. The user does not have any prior knowledge about the problem, his only knowledge is that the controls to the necessary properties are somewhere in the unknown sector of the map (left part). Our agent is sent to autonomously learn these principles, while he has only two physiological needs predefined and a set of the following primitive actions: move in four directions, eat, drink and press buttons."
375,43,0.986,Musical Haptics,"2.5 Implications of a Coupled Dynamics Perspective on Learning to Play an Instrument At the outset of this chapter, we proposed that successful acoustic instruments are those which are well matched, in terms of their mechanical impedance, to the capabilities of our bodies. In other words, for an experienced musician, the amount of work they need to do to produce a desired sound is within a range that will not exhaust their muscles on the one hand but which will provide sufficient push-back to support control on the other. But what about the case for someone learning an instrument? What role does the dynamic behavior of the instrument play in the process of learning? Even if we do not play an instrument ourselves, we are probably all familiar with the torturous sound of someone learning to bow a violin, or with our own exhausting attempts to get a note out of a garden hose. This is what it sounds and feels like to struggle with the coupled dynamics of our bodies and an instrument whose dynamical behavior we have not yet mastered. And yet violins can be played, and hoses can produce notes, so the question is how does someone learn to master these behaviors? Musical instruments represent a very special class of objects. They are designed to be manipulated and to respond, through sound, to the finest nuances of movement. As examples of tools that require fine motor control, they are hard to beat. And, as with any tool requiring fine motor control, a musician must be sensitive to how the instrument responds to an alteration in applied action with the tiniest changes in sound and the tiniest changes in haptic feedback. Indeed, a large part of acquiring skill as a musician is being able to predict, for a given set of movements and responses, the sound that the instrument will make and to adjust movements, in anticipation or in real time, when these expectations are not met. The issue, as Bernstein points out, is that there are often many ways of achieving the same movement goal [20]. In terms of biomechanics, joints and muscles can be organized to achieve an infinite number of angles, velocities, and movement trajectories, while at the neurophysiological level, many motorneurons can synapse onto a single muscle and, conversely, many muscle fibers can be controlled by one motor unit (see Sect. 3.2 for more details concerning the hand). This results in a"
32,234,0.986,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","10.4.3 Linear Gravity Transportation Model From Figs. 10.1 and 10.2, we know that the amount of trades, degrees and GDPs have positive relations. Thus, we model these relations as a transportation model. In this model, we think the number of degrees P directly affects the amount of trade flow, where in-degree is defined as kj D i aij . And we calculate the GDPs as a result of the distributions of trades flows using gravity relation. We call this model as a LGTM, which is conceptually depicted in Fig. 10.3."
147,125,0.986,Trailblazing in Entrepreneurship : Creating New Paths For Understanding The Field,"(described above), a number of important research questions and opportunities remain. Dimensions of ACAP and the mutual adjustment process of potential opportunities. Rather than think of ACAP as an aggregation of the four dimensions (or of two groupsâPACAP and RACAP), perhaps it is more beneficial to think about the extent of mutual adjustment based on the individualâs lowest capability. In other words, if acquisition feeds into assimilation, which feeds into transformation, which feeds into exploitation, and this process is recycled based on engaging a community of inquiry, then it is likely that the process can only proceed as effectively as its lowest capabilityâit is not the average strength of the links that determines the effectiveness of a chain but the strength of its weakest link. However, perhaps some dimensions are more critical in opportunity refinement than others. For example, after a community of inquiry has been engaged, the ability to acquire information may take on less importance than it did in initial idea generation (perhaps because the source of the information is more apparent), or perhaps transformation, while still important, is not as important when the mind is refining a potential opportunity as opposed to generating one in the first place. How does the importance of the different dimensions of ACAP change throughout the entrepreneurial process, especially as it relates to the mutual adjustment process of opportunity refinement? Potential opportunity and transforming ACAP. As ACAP enables the entrepreneur to refine the potential opportunity, the potential opportunity itself transforms the mindâthat is, the idea refined by a community of inquiry transforms the mind of the ideaâs generator. This transformation is likely to be reflected in the dimensions of ACAP, such as an enhanced ability to acquire, assimilate, transform, and exploit the new knowledge generated by the community of inquiry. Therefore, not only does ACAP facilitate opportunity refinement (and transformation of the community of inquiry), but it also changes as a result of the process. Capturing how the mind is transformed in terms of its ability to acquire, assimilate, transform and exploit knowledge is a critical issue for innovation and entrepreneurship scholars to address. Again, exploring differences in the dimensions of ACAP adds the potential for scope and depth in understanding how a refined opportunity (by the community of inquiry) transforms its originatorâs mind. The dynamism of ACAP as a dynamic capability. To stretch our own minds, we reflected on Zahra and Georgeâs (2002) notion that"
346,163,0.986,"History Education and Conflict Transformation : Social Psychological Theories, History Teaching and Reconciliation","competition by escalating conflict and distrust. Contrary to what is often argued by the heritage, nationalist or romantic approach to nation building, the sense of self created is not one of security but one of a threatened and fragile self which is what Bar-Tal and Teichman (2005) called a âsiege mentalityâ. Identity construction on the basis of conflict narratives is thus not liberating as it might be argued by theoretical approaches or politicians who support the heritage approach. On the contrary, it is about constructing a fragile and threatened self which is distrustful of those with whom they need to co-operate to end violence, division or conflict. Thinking about the ramifications of the present findings for history teaching, what could in fact be liberating is reflection on the structure and function of historical conflict narratives as social representations by both teachers and students. In this way, children, youth and adults can understand the consequences of the internalization of master narratives for conflict transformation. To conclude, the heritage or the romantic identity building approach in history teaching can thus be criticised on all four grounds: pedagogical, epistemological, moral and political. Pedagogically, it is based on an outdated model of a transmission metaphor given that it is mostly delivered by educators as a communication type that Moscovici described as propaganda (kello and wagner (Chap. 8)). Epistemologically, it is based on naÃ¯ve realism since it promotes the single truth of the nation, which is an outdated epistemological stance. Morally, the idea of manipulating, silencing or hiding parts of the past from students is unacceptable. Politically, it reinforces conflict instead of resolving it. As Barton and Levstick (2004) argue, students have to examine the impact of telling any particular narrative, or any set of narratives, as well as the consequences of studentsâ narrative simplifications. for the disciplinary approach, there is an important take-home message from the present findings: history teachers need to familiarize themselves with relevant social psychological research and have in their âtoolboxâ the main findings of research such as the present one. Given the well-established findings that master narratives pose a threat to the cultivation of the historical thinking of students (Carretero 2011; Lopez et al. 2012), the present should be read as adding support to the idea of moving from the disciplinary to an interdisciplinary approach (see Psaltis et al. 2017) to the study of historical culture and consciousness in the history classroom. The cultivation of a critical historical and reflective consciousness that recognizes the socially"
395,232,0.986,Beyond Safety Training : Embedding Safety in Professional Skills,"The behaviour change approach described in this paper should be differentiated from the âsafe behaviour,â âbehaviour modiï¬cationâ or âbehavioural safetyâ approaches described by Hopkins (2006). Whilst behaviour and how to change it is at the heart of both approaches, âbehavioural safetyâ programmes are more narrow in focus and deal primarily with downstream causes of accidents. Theorists have criticized the behavioural safety approaches for falling foul of the âfallacy of monocausalityâ, which is the idea that there is often a single root causeâin this case, behaviourâof an event. Since human factors are often implicated at some point during the causal chain of events leading up to an industrial accident,"
346,314,0.986,"History Education and Conflict Transformation : Social Psychological Theories, History Teaching and Reconciliation","why group members act under the impact of heavy political constraints. This is an experience member of both sides share. Mutually listening to life stories brings about empathy and furthers trust and recognition of the âotherâ. whereas Bar-Onâs approach is influenced by psychoanalytical thinking, experimental social psychological research has also underscored the positive effects of empathy and described perspective taking exactly as what has taken place in Bar-Onâs groups, namely to âincrease the perception that a common humanity and destiny is shared with the other groupâ (Brown and Hewstone 2005:293). Biographical storytelling is just a means to make people ready for what Pettigrew has defined as âself-disclosureâ: to open oneself to others (Pettigrew 1998). Biographical storytelling stands at the beginning of Bar-Onâs projects, accompanies them at each phase and paves the way for the proper historical and pedagogical work on the teaching material. Bar-On has called this approach TRT (= To Reflect and Trust) (Bar-On et al. 2000). The TRT concept is not easily applicable because it presupposes the involvement of psychologically trained moderators, prolongs the groupâs work and claims from the participants to bring in their own personality, to present their personal experiences to persons who are regarded as enemies by the majority of oneâs own society. Usually, its application is restricted to small groups, and it is mostly used by NGO-driven projects. It can hardly be applied in official expert commissions. Besides Bar-Onâs special approach, w. fisher and others have formed a more general theory of narrative communication as a counter-model to binary argumentative logic (fisher 1984). The storiesâ coherence and the sincerity of the narration and the narrator create confidence and allow the listeners to relate themselves to the experiences of the âotherâ. whereas the biographical approach strives to generate empathy and positive feelings within a project from the very beginning and to take out politics as long as possible, J. Rothman (1997) has developed a contrasting model of dialogical identity conflict resolution containing four phases. It starts expressly with a confrontational âantagonism phaseâ during which participants exchange their conflicting political views. when participants come to the conclusion that a continuation of the political debate would not produce any common results, they are asked to rationally define their own positions and interests, to compare them with the ones of the adversary and so develop step by step an understanding of possible common goals and to work on solutions. He calls his model according to the four steps ARIA (= Antagonism, Resonance,"
134,41,0.985,Curriculum Reform in The European Schools : Towards a 21St Century Vision,"their mastery of this knowledge or skill is not a determination of their competence, but of whether they have successfully understood how to rework their capacity to fit the demands of the examination technology. As a result teaching to the test occurs and the curriculum is narrowed to accommodate those learning outcomes that can more easily be assessed. There is some evidence of this occurring in the European Baccalaureate. The reason for separating out learning approaches from assessment approaches is now clear. If these assessment approaches are the same as learning approaches, then this is likely to have a detrimental and reductionist effect on the curriculum and more importantly on the type and content of learning that takes place. However, there are different needs within a system of education, and one of these is that, at set points in time, supra-national (such as the European Commission), national and local educational bodies need to have information about how well the system is doing. This is a very different process from improving learning with an individual learner. However, there must be some connecting link between learning and reporting, so that the latter doesnât distort the former, and this is the role of learning aims and objectives. Learning and assessment practices on a programme of study, such as a curriculum, can be regarded as formative if evidence of a learnerâs achievements in relation to knowledge and skill acquisition is collected and used by the teacher, the individual student, and their fellow students, with the specific intention of deciding on their subsequent programme of learning. As a result, assessment is used formatively when it directly influences the learnerâs cognition. Curriculum developers consequently need to make a clear distinction between summative and formative assessment. If these two functions are combined, then the potential impact of the curriculum is weakened. There are two principles which structure the choice and order of content within a curriculum: a spiral element or a re-visiting of concepts, skills or dispositions at a higher level of intensity and at a later point in the programme of study, and theory transfer from theory to practice and from sites of learning to sites of application. The first of these is the need to incorporate a spiral element into the curriculum, i.e. a set of ideas or operations, once introduced, is revisited and reconstructed in a more formal or operational way, at different stages in the learning programme (cf."
51,44,0.985,How Generations Remember,"ferent historico-political periods, that of the Last Yugoslavs is the oscillation between different discourses, even opposing ones; and the discursive tactics of the Post-Yugoslavs are characterised by distancing and dissociating their personal histories from the experiences of the wider nation. Although the concept of discursive tactic used in this study relates to de Certeauâs concept of tactic, it is understood in a somewhat different way. Tactic as de Certeau describes it is more closely linked to resistance than the way tactic is used here. Relating tactics closely to resistance would suggest that the narratives of my interlocutors represent âcounter-memoriesâ or âalternative historiesâ, and that we can draw a clear line between âofficialâ and âpopularâ representations of the past, between history and memory. This is not the case, as I will outline in the following paragraphs."
249,30,0.985,Advances in Proof-Theoretic Semantics (Volume 43.0),"where the exhibited inference binds assumptions in the part A 3 of the form A(a) marked (1) as well as variables a that are free in A 3 . The inference can be seen as representing an application of mathematical induction, where N stands for ânatural numberâ and s is the successor operation. We keep open what forms of sentences are used in an argument structure in order to make the notion sufficiently general. However, when making comparisons with BHK-proofs of sentences in a first order language, we restrict ourselves to such languages. It is assumed that for each form of compound sentences there are associated inferences of a certain kind called introductions, for which we retain the condition from natural deduction that for some measure of complexity, the premisses of the inference and the assumptions bound by the inference are of lower complexity than that of the conclusion. For instance, we could allow the pathological operator tonk proposed by Prior and associate it with the introduction rule that he proposed. We shall say that an argument structure is canonical or in canonical form if its last inference is an introduction."
223,356,0.985,Knowledge and Action (Volume 9.0),"Theoretical Framework Some work has been done in this direction (Baldry, 1999; Edenius & Yakhlef, 2007; Ford & Harding, 2004; Friedman, 2011; Kornberger & Clegg, 2004; Meusburger, 2009; Taylor & Spicer, 2007; Woodward & Ellison, 2010). A review of the growing literature on space in organization studies found the field fragmented but identified three principal streams, each with interesting contributions and shortcomings (Taylor & Spicer, 2007). In one stream scholars conceive of space in terms of distance and proximity between points and have convincingly demonstrated how space makes a difference for important issues at the micro-, meso-, and macrolevels. However, they are âunable to account for the ways in which actors attribute meaning and significance to a space â¦ [and] not able to explain the role which perceptions or experiences of distances and proximity playâ (p. 329). In another stream researchers compensate for this weakness by focusing on the materialization of powerâbut it is questionable âwhether all spaces are necessarily manifestations of powerâ (p. 332). Furthermore, such a focus implies a âsystematic disregard of the ways that space may actually be the product of inhabitantsâ ongoing experience and understandingsâ (p. 333). Scholars in the third stream attend to this gap by exploring âhow spaces are produced and manifest in the experiences of those who inhabit themâ (p. 333). The inherent disadvantages are that power relations are overlooked and that the emphasis on perception undervalues the material, physical aspects of space. Logically, therefore, Taylor and Spicer argue for an integrated approach that addresses all three dimensions by building on the ideas of Lefebvre (1974/1991), who sought to bring together mental, physical, and social modalities of space (see also Ford and Harding 2004, p. 817). Although we agree that an integrated approach is needed, this particular proposal does not take some essential concepts into account. Strikingly absent from the organizational literature on space is the work on social space by two of the twentieth centuryâs most influential, and nonconventional, social scientists, the psychologist Kurt Lewin (1936, 1948/1997, 1951/1997) and the sociologist Pierre Bourdieu (1985, 1989, 1993, 1998; Bourdieu & Wacquant, 1992). Both placed social space as the cornerstone of their theoretical and methodological work, turning to the philosophical work of Ernst Cassirer (1923/1953, 1944, 1961), who conceived of space in rather relative terms as the positional quality of the material world. Cassirer, Lewin, and Bourdieu adopted the view that there is no empty space, only spaces that are formed by and between objects, and they applied this concept to the creation of social reality rather than to the physical world. At the heart of social space is a relational logic of social reality, which focuses neither on the individual nor the group as the unit of analysis but rather on the processes through which individuals, in interaction with others, construct their social spaces and identities (Friedman, 2011). These interactions are causal loops that link the ways people bring their thinking and feeling into the world through action, to"
310,89,0.985,Online Hate Speech in The European Union : a Discourse-Analytic Perspective,"To begin with, the commenter here does draw the line at parents forcibly sending their children to NARTH (National Association for Research & Therapy of Homosexuality), an organisation that offers gay conversion therapy, thus, recognising the danger that such therapy can bring on LGBTIQ youth. However, the language used in the rest of the comment indicates a strong negative stance toward homosexuality and a profound ignorance of LGBTIQ issues. Firstly, the user refers to non-straight sexual desires as âunwantedâ¦urgesâ. Both these words have negative connotations and imply something unfavourable. Moreover, by way of the modal verb âshouldâ, the user offers a suggestion for people with such âurgesâ to âget helpâ. In doing so, the user implies that heteronormative values are hegemonic and any deviation from them creates an urgent need for the individual to seek help. Finally, the use of the inclusive âorâ in the sentence âWhether those urges are for men or young boys, it shouldnât matter.â strongly implicates that, in the mind of the commenter, homosexuality is on a par with paedophilia and thus warrants âtreatment.â In view of all this, then, the ï¬nal, positive, statement that parents should not force their children to undergo gay conversion therapy is overshadowed by the overall negative stance the user has towards the LGBTIQ community. Similarly, the comment in (32), which was made in response to a newspaper article about the civil union of a gay couple, might not seem at ï¬rst to be overly discriminatory. Yet, if we break it down into its component parts and discern the meaning beneath the allusions being used therein, we might form a different opinion. (32) people marry because they fall in love, and although itâs a choice, it was meant to be like that even in the animal kingdom, for example swans mate for life, male and female, not male and male.38 The user that posted this comment may posit the idea that marriage is a choice, but frames the relevant clause with the conjunction âalthoughâ (a conventional implicature Ã  la Grice), which is generally used to present two contrastive arguments, thus indicating that even though marriage is a choice for people who fall in love, it is also a choice that comes with restrictions. By bringing in a comparison with mating in the animal kingdom, whereby all swan relationships are described as being heterosexual, the commenter subscribes to heteronormative ideals, implying in this way that any deviation from the heterosexual norm is unnatural. So, even though this comment concedes that marriage is a choice, somewhat echoing the main argument of most gay rights movements on the matter, it still exhibits a negative attitude towards the members of the LGBTIQ community. Clearly, this short section cannot do justice to the far-reaching implications that that the study of indirectness can have for our understanding of discrimination in language use. What we hope to have achieved through this discussion of some online reactions to news items in the Maltese press is to have justiï¬ed the need for going beyond the explicitly expressed and overtly communicated meaning when it"
228,542,0.985,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"system so it seems to be a kind of recursion, where only the lowest level relates directly to the FIBs. Still, despite the high complexity of the model, relations are described linguistically at various stages. However, although the idea of FIBs seems to be quite suitable for the linguistic modeling of complex relations, some problems may arise. The classical methods for fuzzy models often produce fuzzy answer sets, which are quite fragmented, not normal, and not convex. Such results are formally still fuzzy sets, but their forward processing without defuzzification could be questionable. Such problems do not occur if we use the OFN model. Its important advantage here is that we get a kind of fuzzy number at each stage of data processing. Thus the further use of such a result is easy and smooth without the need for defuzzification and then fuzzification again."
78,313,0.985,The Onlife Manifesto : Being Human in a Hyperconnected Era,"Much has been written about the shrinking of the private, the blurring of the public/ private divide and, for instance, the loss of privacy in public (notably Nissenbaum 1997). Such shrinking, loss and blurring have been attributed to either the lure of self-publication in web 2.0 (Cohen 2012), or to the secretive trading with and spying on our behavioural data in the course of pervasive computing (Cohen 2012; Hildebrandt 2012). Maybe we should return to Arendt (1958), when she spoke of the private as a sphere of necessity (the household), the public as the space for freedom (political action) and âthe socialâ as the emergence of mass society (bureaucracy, individual self-interest and conformity). Her understanding of âthe socialâ or what she called âsocietyâ is not altogether positive, to put it mildly. Is the rise of web 2.0 antithetical to âthe socialâ, because it concerns communication of one-2one, one-2-many as well as many-2-many, rather than many-2-one? Or does the processing of Big Data present us with âthe socialâ come true, where âthe socialâ is constituted by machine-readable bits and pieces that allow for the ultimate version of what Heidegger (1996) called âdas rechnende Denkenâ (calculative thinking)? I am not sure, and I believe the jury is still out. The answer will depend on empirical evidence of how âthe socialâ continues to evolve in smart environments. I do think that Arendtâs understanding of the private and the public might save us from dichotomous thinking, as well as from the glorification of âprivate lifeâ as a sphere of uncontroversial freedom. Simultaneously, we must come to terms with the fact that her glorification of the public sphere has little connection with present day politics, which rather fall within the scope of her depiction of âthe socialâ. We should also note that her glorification of politics as a âtheatre of debateâ (other than the realm of household economics) is rooted in an appreciation of privacy as âsome darker ground which must remain hidden if it is not to lose its depth in a very real, non-subjective senseâ (Arendt 1958, S. 71). To speak and act âin publicâ one must leave the security of oneâs home. But to distinguish oneself and to take the risk of being refuted, requires courage, daring and a place to hide. To recuperate from the tyranny of public opinion (Mill 1859) we need a measure of opacity to re-constitute the self, far from the social pressures that could turn us into obedient self-disciplined subjects (Hutton et al. 1988). In fact I would agree with Butler (2005), where"
235,71,0.985,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","9.3 Contemporary Representations Let us, for the sake of exposing an extreme position, contemplate on an infinite universe consisting of random bits â that is, these collection of bits are not only âlawlessâ in the sense that there does not exist any algorithm generating them, but they are, in a strictly formal way [103, 133, 355], also algorithmically incompressible. That is, its behaviour cannot be âcompressedâ by any algorithm or rule. One model of such a universe would be a single random real. We assume that the algorithmic incompressibility of encoded microphysical structures might be a quite appropriate formalization of primordial chaos. There are two ways how pseudo-lawfulness might be ârevealedâ to intrinsic observers: (i) Lawful substructures: It might be the case that these observers might have only restricted operational access to the entire random string, and merely perceive an orderly partial sequence (string) â that is, they accidentally live in a substructure of the random real which appears to be algorithmically compressible. Any such compression might be interpreted as a âlawâ governing this particular section of the universe. Calude, Meyerstein and Salomaa discuss universes which are lawless [106, 114] and mention the possibility that we might be riding on a huge but finite segment of a random string which, to its inhabitants, appears to be lawful: âAs our direct information refers to finite experiments, it is not out of question to discover local rules, functioning on large, but finite scales, even if the global behaviour of the process is truly randomâ [106, p. 1077]. These considerations are based on the finding that, âalmost all real numbers, when expressed in any base, contain every possible digit or possible string of digitsâ [103, Theorem 6.1, p. 148] â even entire deterministic universes. There appear âspurious correlationsâ in the following sense: âvery large databases have to contain arbitrary correlations. These correlations appear only due to the size, not the nature, of dataâ [113]. Yanofsky [581] has discussed related scenarios, and has heuristically investigated the âextracted order that can be found in the chaosâ by considering large matrices and finding patterns therein: Suppose, instead of a matrix, a long string (one might say a 1 Ã n matrix) whose entries are filled randomly and independently with decimal digits. The expected number of times any particular substring of m digits, say â123 . . . m,â occurs within this larger string of length n is (n â m + 1)(1/10)m . (ii) Emergence: The laws of nature might actually be âemergentâ in a Ramsey-theory type way. Because just as âone cannot not communicateâ [562, Sect. 2.24, p. 51] Ramsey theory [248, 327, 476] reveals that there exist properties and correlations for any kind of data, which do not depend on the way these date are generated or structured. This would also (but is not limited to) include cÂ£oj; that is, universes which are not âlawfulâ and not generated by intent; and consisting of data which cannot be algorithmically compressed. Such inevitable correlations might be"
360,209,0.985,Compositionality and Concepts in Linguistics and Psychology,"the more creative participants, in response to the requirement to ï¬nd a bird that was also a kitchen utensil. In this case a woodpecker has been trained to whisk eggs using its powerful head movements. Qualitative analysis of the results of this ï¬rst pilot study showed some very interesting principles were involved in combining concepts. First, there was evidence for instantiation of superordinate categories to basic level categoriesâbirds became woodpeckers, fruit bananas, and furniture couches. Second, people would align properties and functions between the two concepts. The example given here shows how the need to ï¬nd a function for a kitchen utensil is met by ï¬nding a behavior of a particular bird that can serve the function of a particular utensil. Third, there was evidence for simulation processes, in line with Barsalouâs suggestions about the role of concepts in achieving goals (Barsalou 1991). Commentary provided by the creator of the woodpecker whisk pointed out that it would not need electrical power (good for camping trips) but would on the other hand be unhygienic. The use of simulation takes the speciï¬cation of the combined concept and develops it in totally non-compositional directions, as the new concept is adapted to real world knowledge. Finally, many solutions identiï¬ed conflicting properties in the newly combined concept, and offered new emergent features to resolve them. For example, when a participant solved âA fruit that is also a kind of furnitureâ by proposing a banana couch, they went on to specify that the banana had been modiï¬ed to grow very large and to ripen very slowly. Extensional accounts of meaning focus on the sets of exemplars in the world. As such they can have nothing to say about concepts which have no members. (The problem of empty and ï¬ctional names is well known, Braun 2005). Thus when we think about counter-factual or hypothetical objects, the extension has to be taken to refer not to the actual world but to a virtual or âpossibleâ world. Computer teacups do not currently exist, but it is possible to imagine an alternative world in which they do, and we can speculate about their properties. In fact this process of conceptual combination may be a key factor in innovation and creativity. In a recent paper (Gibbert et al. 2012) my colleagues and I investigated creativity in the forming of concepts of hybrid productsâartifacts that serve more than one traditional function. In direct agreement with the processes postulated by the Composite Prototype Model we showed how ï¬rst attempts to imagine (say) a pillow that was also a telephone would simply aggregate the features of the two concepts into a composite. However when the two concepts being combined were sufï¬ciently dissimilar, then a second attempt at combining the concepts would generate integrative solutions in which features of one concept would be aligned with those of the other to provide emergent functionality. The telephone could be programmed to provide gentle sounds to help people get to sleep, or the pillow could gently move to alert the user that a phone call was arriving. A less kind proposal was that the pillow would allow one to nap while listening to oneâs mother on the phone. When people were able to generate integrative solutions, these were consistently judged to be more likely to succeed as marketable products."
223,421,0.985,Knowledge and Action (Volume 9.0),"question. As a first step it is important to have an idea of what the social context looks like in this case. Decision-making in a forager group such as the San of Namibia does not follow quasi-legal or rigid procedures. Instead, participants and observers alike can derive decisions only from the continuous discourse that allows them to make decisions based on plausibility. Their conversational and interactional style is a particular one of repetitions, overlaps, and echoing in everyday talk. Consensus is achieved as the interlocutors repeat and echo some opinions or arguments and leave out others. This kind of exchange enables people to make intelligent guesses about what they and others will be doing next. The strategy requires that everyone be allowed to join in the conversation while avoiding prominence (and exposure) as an individual voice of authority. Similar strategies for achieving consensus have been observed elsewhere, as in Aboriginal Australia (Liberman, 1985, p. 104). Taken together, they differ not only from the dominant western-style conversation and interaction but also from the aggressive and self-assertive style found in many societies, including âBig Man societiesâ in Melanesia or segmentary systems in sub-Saharan Africa. The following excerpt is one of the best known examples from the !Kung San, who are neighbors of the Hai//om San with whom I have worked and who have a similar interactional practice of overlapping and echoing talk: ââYesterday,â âeh,â âat Deboragu,â âeh,â âI saw old/Gaishay.â âYou saw old/Gaishay.â âeh, eh.â âHe said that he had seen the great python under the bank.â âEH!â âThe python!â âHe wants us,â âeh, eh, eh,â âto help him catch itââ (Marshall, 1976, p. 290). Among the San, people often talk in parallel, and there is no formal conclusion to this talk. Instead, it is made up largely of âtopographical gossipâ which invokes places and movements but without any formal decisions (Widlok, 1997, p. 321). Apart from this feature of particular conversational forms, the reasoning involved allows for unpredictable events in that nonhuman and apparently nonanimate features of the environment are expected to come in as well, influencing the direction that a decision may take. When people in this community refrain from long-term planning, it is not that they are incapable of doing so but rather that they allow the state of the environment or of other persons to prompt or trigger their decisions at certain stages of the process. Detailed studies on the process of tracking animals have shown that anticipating and predicting the movement of an animal that one is pursuing involves a continuous creation of new hypotheses in the light of new information added to the incomplete picture of tracks and other signs on the ground. This activity also involves a constant dialogue between trackers who are allowed to maintain their diverging views as events unfold (Liebenberg, 1990, p. 108). Making decisions about moving (or indeed any other decision) entails a similar process of encouraging heterodoxy in views, keeping the decision open until very late in the process and ultimately always allowing individuals to maintain their own diverging view. In residential mobility this tolerance of diverging views is facilitated by the fact that packing up oneâs belongings is easy; it allows for fast and flexible reactions either to join a party that leaves or simply stay put. Having briefly described the mode of reasoning ethnographically, one may now ask whether there is a more general model that can help reintegrate these observations"
152,40,0.985,interdisciplinary Perspectives On Mortality and Its Timings : When Is Death?,"Globe Theatre in recent years demonstrates how dancing provides a sense of release for actors and audience, an emotional and aesthetic complement to the catharsis of watching tragedy. The question âwhen is death?â finds a particularly nuanced response in two of Shakespeareâs plays: Hamlet and King Lear. Both tragedies are invested not only in death, but in the process of becoming dead, and do so through their male protagonists. I have suggested that the phrase âYouâre deadâ seems paradoxical, and we hear an equivalent one on the Renaissance stage: Hamlet, thou art slain. No medâcine in the world can do thee good. In thee there is not half an hour of life; (Hamlet, 5.2.266â268)"
245,837,0.985,The European Higher Education Area : Between Critical Reflections and Future Policies,"The simple idea behind the âdifference-in-differencesâ strategy is that it takes the difference between the treatment and control groups, before and after the treatment. More formally, the quantity of interest (the causal effect) looks as follows: EÂ½Y1 jD Â¼ 1Â"
249,332,0.985,Advances in Proof-Theoretic Semantics (Volume 43.0),"3.1 The Functional Closure The idea of function closure (in the realm of monotone inductive definitions) is that we have some things a, b, . . . given and also some functions f, g, . . .. We then define a notion X by saying that â¢ a, b, . . . is an X , â¢ if x is an X , then f (x), g(x), . . . is an X . Implicitly this means that X is defined by these clauses and nothing else. From an intensional and foundational point of view in âgeneratingâ X the things that f, g, . . . act on in X are not given, besides the initial things a, b, . . ., they are introduced as we build X . Once defined, X is then the smallest collection of things including a, b, . . . and being closed under f, g, . . .. Similarly the idea of a functional closure is that we have some things a, b, . . . and functions f, g, . . . given and also functionals F, G, . . .. Analogously, from an intensional and foundational point of view, the functions âinâ X that F, G, . . . act on, i.e., functions that X is closed under, are not given, but introduced as we build X . In both cases we take for granted certain things as primitive notions. In the first case some given objects and functions and in the second case some given objects (not necessary in all cases), some functions and functionals. In both cases what we rely on is, so to speak, inscribed in fundamental circles of reasoning. The objects we generate in building up the function closure are of course given in an abstract manner of speaking. The same thing holds for the functions we generate in building up the functional closure: â¢ a, b, . . . is an X , â¢ if x is an X , then f (x), g(x), . . . is an X , â¢ if X is closed under f , then F( f ), G( f ), . . . is an X . In non-foundational and mathematically precise definitions we assume there is given a universe of objects, a function space and some functions and functionals defined on this universe/function space."
360,370,0.985,Compositionality and Concepts in Linguistics and Psychology,"The type-shifting hypothesis is, at its core, a hypothesis about the lexical semantics of so-called âcoercion verbsâ and the conceptual entities that composition involving such verbs is sensitive to. We propose that this hypothesis as formulated faces at least three challenges which have direct implications for how the experimental results are interpreted. The ï¬rst challenge has to do with the underlying linguistic pattern. For at least aspectual verbs, the most representative subset of coercion verbs, the eventive interpretation is not obligatory in the presence of an entity-denoting complement. In their simple transitive uses, the context in which these verbs have been tested, aspectual verbs do not exclusively select for eventive complements and agentive subject-referents. Sentences (4)â(6), found in the Corpus of Contemporary American English (COCA), exemplify such cases. For example, there is no construal of (4) in which the book is coerced into an event of which the new autobiographical memoir is an agentive participant. The same is observed in (5); the sentence does not give rise to an eventive interpretation though the âcoercion verbâ is followed by an entity-denoting complement (the genealogy of the kings of England).2 (4) Although this is mostly a collection of previously published essays, it is notable because of the new autobiographical memoir that begins the book. (5) This image begins the genealogy of the kings of England and flows into materials speciï¬cally written for St. Albans. (6) This column continues the review of the new PSA Club Services Website."
294,249,0.985,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"equation model, NnC1 D rNn .1 Nn =M /, which allows oscillatory solutions and those are observed in animal populations. The problem with large t is that it just leads to wrong mathematics â and two wrongs donât make a right in terms of a relevant model.)"
78,343,0.985,The Onlife Manifesto : Being Human in a Hyperconnected Era,"In âthe Vita activa and the Modern Ageâ section of The Human Condition (Arendt 1959) , Arendt explains how the invention of the telescope changed the relationship between truth and appearance. âTruth was no longer supposed to appearâ¦to the mental eye of a beholderâ (Arendt 1959, p. 263). Indeed, as the telescope has demonstrated that we are fooled by our senses, ânothing could be less trustworthy for acquiring knowledge and approaching truth than passive observation or mere contemplation. In order to be certain, one had to make sure, and in order to know, one had to doâ (Arendt 1959, p. 263). The telescope has undermined deeply and for centuries our epistemological confidence in what we perceive without instruments, either by our senses or by mere thinking and contemplation. As a result, âin modern philosophy and thought, doubt occupies much the same central position as that occupied for all the centuries before by the Greek thaumazein, the wonder at everything that is as it isâ (Arendt 1959, p. 249). This has had a great effectiveness in the relationship with nature and the universe. Without the Cartesian âde omnibus dubitandum estâ (âeverything should be doubtedâ), we would not have taken the same technological path nor landed on the moon. The Cartesian doubt has shaped the relationship of men to nature in terms of questions to be answered through"
42,272,0.985,S-BPM in the Production Industry : A Stakeholder Approach,"The activities carried out throughout the case have been driven by a people-centred methodology based on the assumption that the force and power of the individual as a person can be understood according to three meanings (Cesaro 2016): â¢ The opportunities that the individual recognizes in himself which are the basis of his life plan â¢ The capabilities and potentialities the individual can exploit â¢ The possibility in the sense of giving oneself hope (it is possible that â¦) The starting point of this methodology is the historic philosophic thought about the relationship between human being and machine and peopleâs alienation. This situation especially occurs when there are daily needs that have to meet product-related process automation of economies of scale, and people need to respect timing and methodologies imposed by the machine. The fundamental question has always been whether to maintain the human/machine/human relation or rather the most frequent machine/human/machine requested by productive needs and by the technology domain over humans. This methodology puts at the ï¬rst place the relationships among people (human/human) taking into consideration that hierarchies, and the need of making decisions must ï¬nd a balance between power exercise and a positive organizational climate. The second point of attention is related to the workplace life quality, particularly focusing on the actual measurement of those parameters that could be"
311,861,0.985,The Physics of the B Factories,"We begin our discussion with b â cuÌd transitions. The case of b â cuÌs is completely analogous. Examples for these transitions are shown in Fig. 17.3.1. A popular and useful approach to calculate decay rates (especially for two-body B decays) is the factorization ansatz. To understand this technique, consider the decays that are shown in Fig. 17.3.2. In this ï¬gure only the electroweak contributions to the decay amplitudes are shown. A naÄ±Ìve attempt to calculate the decay rate would write the matrix element in terms of the usual currents, e.g., cÎ³u (1 â Î³5 )b. However, this is clearly a drastic approximation as it neglects the all important role of gluons in the production of the ï¬nal state hadrons. Nevertheless, at this early stage of calculation an important distinction becomes apparent. The decay B + â D0 Ï + can proceed through two amplitudes as shown in Figs 17.3.2a) and b). Since all ï¬nal state particles must be color singlets, diagram b) will be suppressed due to color matching relative to a) by 1/Nc , with Nc the number of colors. Amplitudes such as Fig. 17.3.2 a) are known as âcolor-allowedâ while an amplitude such as Fig. 17.3.2 b) is often called âcolor-suppressedâ. The decay"
192,76,0.985,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"Although in modern science the Masterâs discourse has been replaced by university discourse, in which an (allegedly autonomous) subject focusses on an (allegedly domesticated) object, this process is nonetheless spurred on and guided by latent basic concepts and convictions (âphilosophemesâ), by an unconscious metaphysics as it were (S1 in the lower-left position). In terms of Lacanâs symbolic algebra, the relationship between scientific discourse (S2) and its guiding imperatives or instructions (S1) can be represented as: S2/S1. The guiding imperatives remain implicit, are pushed beneath the bar, so that they cannot be explicitly articulated within the emerging flow of normal scientific texts. This was what Heidegger referred to when he claimed that science does not think (1954, p. 4). On the manifest level, academics are involved in various processes of text production: they speak and think continuously. But, as Heidegger argued, genuine thinking basically means to be addressed, namely by the voice of the Other, whose revelatory thoughts present themselves as genuinely questionable (1954, p. 1). Academics publish in journals and contribute to academic conferences (Heidegger 1954, p. 2). Thus, they are involved in what Lacan refers to as university discourse. But in normal science they are barred from addressing that which is genuinely questionable (S1 in the lower-left position), namely their basic Begriff, their answer to basic questions such as: what is nature, what is life, what is science, what is truth? S2 builds on certain basic categories or premises, but is at the same time barred from explicitly addressing these apodictic claims (S1), even though they actually guide the knowledge production process (S2). In principle it is possible to present such basic imperatives in a top-down, apodictic, authoritative and ex cathedra fashion. In that case, S1 is posited at the topside of the bar (upper-left position), resulting in what Lacan (1969â1970/1991) refers to as the Masterâs discourse. An authoritative voice (Hippocrates or Aristotle, for instance) is regarded as infallible. His instructions and imperatives provide"
8,435,0.985,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","This is then also the probability of finding two specified particles. They may be the initial (elastic scattering) or some definite other ones, e.g., p C p ! A C B. Between Eq. (19.30) and the differential cross-section come, of course, some further considerations (flux factors, centrality condition, influence of the actual masses of A and B), which have been treated in another paper [4]. The main point is that, if we compare Eq. (19.30) with the numerical result of Eq. (19.2) which fits the observed large angle scattering well [4], we find that T0 should have a value such that 1=T0 D 6:2. Thus in this case, T0 D 1:1m D 151 MeV :"
228,507,0.985,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"17.2.4 Fuzzy-Based Tool for Gait Assessment The subject of the research-the quality of gait-is a difficult term for formal precise definition. It depends on the general public and varies depending on the group we consider to be the norm in terms of gait. If the precise model is out of reach, we can use the tools for imprecise information processing: fuzzy systems. Their main advantage is the flexibility, intuitiveness, and clarity of rules that are easy to describe linguistically. The concrete proposition here is the multicriteria fuzzy evaluator of gait (MuFEG) defined for the purposes of the study presented in this chapter. This tool evolved from the multicriteria fuzzy evaluator proposed in [21], the purpose of which was evaluation of multicast routing algorithms. The measure of proper gait is presented as a percentage, where is an ideal quality. Defining MuFEGâs basic model for a good quality of gait, assumptions have been made that the quality of gait in people with no stroke, that is, a reference group of respondents, cannot be lower than. The presented results were achieved by combining three different descriptors of a gait. In the version of MuFEG used in the present studies, these descriptors are represented by three fuzzy Mamdani-type systems. A general idea of such fuzzy systems and precise presentation can be found in many books, for example, [1, 15, 16]. Finally, their results are aggregated to one normalized outcome. Two concurrent MuFEG approaches are used. Their working names are 1b-25 and KFNc-25. Both are based on the same assumptions and the same rules. The first, 1b-25, is the classical fuzzy system with: â¢ Singleton fuzzification. â¢ Implication operator, MIN. â¢ Defuzzification is realized as implications and the middle of maxima (MOM) method defuzzification. The fuzzy set representing the ideal value for each gait parameter is constructed from the data given for the reference group, people without stroke. Each of them is a triangular fuzzy set (see L-R fuzzy sets notation in [6]) and is determined on all the available data. For example, letâs look at set good gait velocity (good-GV):"
32,47,0.985,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","sometimes miss some price changes or repeatedly count the same price changes (see bottom of Fig. 2.1). This is good if we are interested in the most recent price signals and in financial markets this is the case. It also reflects the dynamical nature of the time series, as the inter-arrival times may vary from day to day or between equities no new Ä±t needs to be defined, it will always use only the most recent information in both the source and the target time series. The most significant shortcoming is that this TE assumes there is no information being carried by the inter-arrival time interval and it is not clear that some of the theoretical foundations on which the original TE is based necessarily hold, from this point of view this method of calculating the TE is currently only a heuristic and the results presented here are for the moment qualitative in nature."
249,55,0.985,Advances in Proof-Theoretic Semantics (Volume 43.0),"The standard reductions in natural deduction are all transformations of a given deduction by two kinds of very simple effective operations, possibly combined with each other. One kind consists of operations Ï such that Ï(D ) is a sub-deduction of D . The other kind consists of operations Ï such that Ï(D ) is the result of substituting in D an individual term occurring in a sentence of D for a free variable occurring in a sentence of D or substituting in a sub-deduction of D for a free assumption (in that sub-deduction) another sub-deduction of D . Also the reduction associated with mathematical induction (Sect. 5.2) is a transformation built up of these two kinds of operations. By applying operations of these two kinds to a deduction or an argument structure one obtains an argument structure that is contained in the given deduction or argument structure; in case substitutions have been carried out, we should perhaps say that the result is implicitly contained. A reduction of this kind associated to an inference constitutes a justification of the inference in a much stronger sense than the reductions that have been considered in connection with argument structures: Given that the arguments for the premisses are acceptable, there is an acceptable argument for the conclusion, because an argument for the conclusion is already contained, at least implicitly, in the arguments for the premisses taken together. This is actually the kind of justification of Gentzenâs elimination rules that I have labelled the inversion principle, using a term from Lorenzen, and have presented as the intuition behind the normalization theorem for natural deductions [16]. An argument structure that is valid with respect to a justification that assigns such operations to occurrences of inferences would in itself have an epistemic force. Perhaps one could say that the function of the justifications would then be to verify that they have such a force, whereas valid arguments as they have been defined here often get their entire epistemic force from the justifications. A notion of valid argument based on justifications of this kind would be a quite different concept from the variants of valid argument that have been dealt with in this paper. It would also be different from the notion of BHK-proof, it seems. Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
213,271,0.985,Collider Physics Within The Standard Model : a Primer,"In the SM, the non-vanishing of the N parameter [related to the phase ' in (3.66) and (3.67)] is the only source of CP violation in the quark sector (we shall see that new sources of CP violation very likely arise from sector). Unitarity of Pthe neutrino D Ä±bc . the CKM matrix V implies relations of the form a Vba Vca In most cases these relations do not imply particularly instructive constraints on the Wolfenstein parameters. But when the three terms in the sum are of comparable magnitude, we get interesting information. The three numbers which must add to zero form a closed triangle in the complex plane (unitarity triangle), with sides of comparable length. This is the case for the tâu triangle shown in Fig. 3.7 (or, what is equivalent to a first approximation, for the dâb triangle): Vtd Vud C Vts Vus C Vtb Vub"
289,441,0.985,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","Our starting point is the Guarded Higher-Order Logic [1] (Guarded HOL) inspired by the topos of trees. In addition to the usual constructs of HOL to reason about lambda terms, this logic features the â² and  modalities to reason about infinite terms, in particular streams. The â² modality is used to reason about objects that will be available in the future, such as tails of streams. For instance, suppose we want to define an All(s, Ï) predicate, expressing that all elements of a stream s â¡ n::xs satisfy a property Ï. This can be axiomatized as follows: â(xs : â² StrN )(n : N).Ï n â â² [s â xs] . All(s, x.Ï) â All(n::xs, x.Ï) We use x.Ï to denote that the formula Ï depends on a free variable x, which will get replaced by the first argument of All. We have two antecedents. The first one states that the head n satisfies Ï. The second one, â² [s â xs] . All(s, x.Ï), states that all elements of xs satisfy Ï. Formally, xs is the tail of the stream and will be available in the future, so it has type â² StrN . The delayed substitution â²[s â xs] replaces s of type StrN with xs of type â² StrN inside All and shifts the whole formula one step into the future. In other words, â² [s â xs] . All(s, x.Ï) states that All(â, x.Ï) will be satisfied by xs in the future, once it is available."
213,198,0.985,Collider Physics Within The Standard Model : a Primer,"pT distribution of b quarks observed at CDF and D0 appeared in excess of the prediction, up to the highest measured values of pT [83, 124].pBut this is a complicated problem, with different scales present at the same time: s, pT , mb . The discrepancy was finally explained by more carefully taking into account a number of small effects from resummation of large logarithms, the difference between b hadrons and b partons, the inclusion of better fragmentation functions, etc. [125]. At present the LHC data on b production are in satisfactory agreement with the theoretical predictions (Fig. 2.27 [67]). The top quark is really special: its mass is of the order of the Higgs VEV or its Yukawa coupling is of order 1, and in this sense, it is the only ânormalâ case among all quarks and charged leptons. Due to its heavy mass, it decays so fast that it has no time to be bound in a hadron: thus it can be studied as a quark. It is very important to determine its mass and couplings for different precision predictions of the SM. The top quark may be particularly sensitive to new heavy states or have a connection to the Higgs sector when we go beyond the SM theories. Top quark physics has thus attracted much attention, both from the experimental side, at hadron colliders, and from the theoretical point of view. In particular, the topâantitop inclusive cross-section has been measured in pNp collisions at the Tevatron [15], and now in pp collisions at the LHC [339, 346]. The QCD prediction is at present completely known at NNLO [150]. Soft gluon resummation has also been performed at NNLL [127]. The agreement between theory and experiment is good for the best available parton density functions together with the values of Ës"
337,31,0.985,Understanding Society and Natural Resources : Forging New Strands of Integration Across the Social Sciences,"2. A vision should be judged by the clarity of its values, not the clarity of its implementation path. Holding to the vision and being flexible about the path is often the only way to find the path. 3. Responsible vision must acknowledge, but not be crushed by, the physical constraints of the real world. 4. It is critical for visions to be shared because only shared visions can be responsible. 5. Vision must be flexible and evolving. This chapter represents a step in the ongoing process of creating a shared vision of the future of science. It lays out a personal vision of the kind of science I would really want to see in the future and why this new vision of science would be an improvement over what we now have. The paper itself is an attempt to share that vision, without getting bogged down in speculation about how the vision might be achieved or impediments to itâs achievement. Hopefully, the ideas presented here will generate a dialogue culminating in a shared vision of the future of science that can motivate movement in the direction of the vision."
198,448,0.985,Evaluating Climate Change Action for Sustainable Development,"Here, a theory of change as a whole is categorically treated as CMOâs âMâ. In developing this table, the authors have referred to the way Pawson in his work illustrated, e.g. in Chapter 5 of Pawson and Tilley (1997). However the authors are of the view that the identity of so-called âgenerative mechanismâ is the essence of programme theory; thus a theory of change itself is not the same as âMâ, the mechanism. A similar argument is developed by Blamey and Mackenzie (2007)"
360,262,0.985,Compositionality and Concepts in Linguistics and Psychology,"and error rate (e.g. Rosch 1973; Smith et al. 1974; Rosch and Mervis 1975). Moreover, I follow Hampton (2007) in assuming that an instanceâs category membership and its typicality as an instance of that category are two related behavioral measurements, based on one and the same underlying variable. For example, there is a correlation between binary membership measures for sparrow (1), ostrich (1), bat (0) and crocodile (0) on the one hand, and their typicality rating on the other hand (sparrow > ostrich > bat > crocodile). Hampton assumes a so-called threshold model, according to which there is a threshold somewhere along a typicality function that makes a binary distinction between members (sparrow, ostrich) and non-members (bat, crocodile). For the current purposes, taking into account typicality means extending the aspects of meaning that the SMH is sensitive to from deï¬nitional to prototypical, thus fleshing out what constitutes context.3 Incorporating such typicality effects on reasoning was ï¬rst proposed as a solution for reciprocal sentences, in the shape of the Maximal Typicality Hypothesis (Kerem et al. 2009; Poortman et al. 2017). Firstly, this hypothesis assumes that typicality effects also exist for verb concepts like the binary predicate concept pinch, i.e. it assumes that subjects can consistently rank some instances of pinching as more typical than others. The difference with noun concepts like bird is thus merely a matter of the type of things that are being categorized, namely events instead of objects. Secondly, it predicts that these typicality effects for verb concepts systematically affect the logical interpretation of the reciprocal expression that they combine with. Speciï¬cally, the Maximal Typicality Hypothesis (MTH) predicts the core situation for a reciprocal sentence to be the maximal one among those that are most typical for the predicate concept in the sentence (for an elaborate discussion see Poortman et al. 2017). In this chapter, I extend the same logic to plural sentences with predicate conjunction. I view the MTH as a general principle of meaning composition that systematically governs vagueness in plural sentences. The MTH as such a general mechanism surfaces whenever a graded concept such as reciprocity (in reciprocal sentences) or distributivity (in predicate conjunction sentences)4 combines with a natural concept such as a verb or an adjectiveâwhich each have their own typicality structure. Accordingly, I claim that typicality also affects interpretation in"
82,391,0.985,Fading Foundations : Probability and The Regress Problem,"There are now two ways in which we could extend our investigation and go beyond one-dimensional chains. The first is to keep the one-dimensionality, but to look at loops rather than chains: this would take us to the second horn of Sellarsâs dilemma, where knowledge is pictured as Kundalini swallowing its own tail. The other way is to give up one-dimensionality altogether and to study multi-dimensional networks. This would take us to the coherentist caucus in epistemology, or rather to an infinitist version of it, in which ultimately the network stretches out indefinitely in infinitely many directions. It might seem that such a version will be especially vulnerable to the standard objection to coherentism, according to which coherentist networks of knowledge hang in the air without making contact with the world. Indeed, as Richard Fumerton noted, if we worry about âthe possibility of completing one infinitely long chain of reasoning, [we] should be downright depressed about the possibility of completing an infinite number of infinitely long chains of reasoningâ.2 Remarkably enough however, the opposite is the case. Since the connections between the propositions in the network are probabilistic in character, we are dealing with conditional probabilities. As we explained in Section 4.4, the conditional probabilities together carry the empirical thrust, and this is even more so in a multi-dimensional system than in a structure of only one dimension, for the simple reason that now there are more conditional probabilities that may be linked to the world. Extending the chains to networks thus enables us to catch it all: to develop a form of coherentism which not only is infinitist, but also acknowledges the foundationalist maxim that a body of knowledge worthy of the name must somehow make contact with the world.3 We start in Section 8.2 by discussing one-dimensional loops. We will see that, if justification is interpreted probabilistically, then it is in general un2 Fumerton 1995, 57. 3 Thus we do not have many quibbles with William Roche when he argues that"
365,868,0.985,Climate Smart Agriculture : Building Resilience To Climate Change,"Finally, we investigate the âtransitional heterogeneityâ (TH), that is whether the effect of adapting to climate change is larger or smaller for the adapters or for the non-adapters in the counterfactual case that they did adapt, that is the difference between Eqs. (7 and 8), i.e., (TT) and (TU)."
49,422,0.985,Artificial Intelligence and Cognitive Science IV,"revision: sometimes new beliefs have to be added which contradict some of the previous beliefs. Now the problem is to restore consistency. Which beliefs is it best to abandon? The belief revision theory offers an interesting answer, which is, however, beside the scope of this chapter. A different model of information dynamics is the public announcement logic (see van Ditmarsch, van der Hoek and Kooi [9, ch. 4]. This is an extension of the basic epistemic logic of section 5.3. Importantly, the basic epistemic language is extended by a modality [A] for any formula A. Now the formula [A]B is read âAfter a truthful public announcement of A, B is the case.â Semantically, [A]B is true at a point x iff A is true at x and B is true at x with respect to a model where every point that does not make A true has been deleted. Public announcement logic is a simple formal model of communication and public observation with many interesting applications. For more information on the formal models of epistemic dynamics, see van Benthem [5]."
228,78,0.985,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"A well-defined fuzzy rule base should be complete, consistent, and continuous [31]. The completeness means that for each value from the input space at least one rule is activated, that is âi=1,2,...,I Î¼A(i) (x) = 0. The knowledge base is consistent if there are no rules with the same antecedent but different consequents. And finally, the knowledge base is continuous if there are no neighboring rules, for which the result of intersection of fuzzy sets in their consequents is an empty set. The knowledge base is constructed first by acquiring knowledge about the modeled phenomenon, and next by representing it in a form of fuzzy conditional rules. In practice, there are three basic methods to create a fuzzy rule base [16]:"
214,105,0.985,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"land surface has water evaporate from it and plants that take up water. The amount of water in the soil column (a box of the land surface) is a result of precipitation falling from the atmosphere, runoff at the surface, and the motion of water in the soil column. These boxes also then exchange their properties with other boxes, such as water ï¬ltering deeper into the soil, or runoff going to an adjacent piece of the land surface. In addition, exchanges can occur with other pieces of the system: The precipitation falls onto the land from the atmosphere, evaporation goes into the atmosphere, and runoff goes into the ocean. These interactions can all be described at a particular time, and the effects can be calculated and used to update the state of the system. Key to this system are the descriptions of each process. Some examples of processes are the condensation of water vapor to form clouds, carbon dioxide uptake by plants, or the force on the ocean from the near-surface wind. Each of these processes introduces a forcing on the climate system. As we will learn, many of these processes are hard to describe completely, particularly for processes that occur at scales much less than the typical size of one grid box in a model. A climate model usually has one value for each substance (like water) or the wind speed in each large location, and it has to represent some average of the process, often by approximating key parameters. Parameterization is a concept used in many aspects of climate models (see box). The basic concept is like that of modeling itself: to represent a process as well as we can by approximations that flow from physical laws. Many of the approximations are required because of the small-scale nature of the processes. The goal of a parameterization is not to represent the process exactly. Instead, it is to represent the effect of that process at the grid scale of the model: to generate the appropriate forcing terms for the rest of the system and the rest of the processes. Parameterization Representing complex physical processes (clouds, chemistry, trees) in large-scale models is in some sense impossible. The French mathematician Laplace articulated a thesis of the reductionist worldview in the early 19th century: If one could have complete knowledge of every particle in the universe and the laws governing them, the future could simply be calculated. Of course, we cannot do that, so we seek to represent what we know about the behavior of particles, based on physical laws and empirical observation. For some processes, we can refer to the basic physical laws, which often have little uncertainty in them. The laws of how photons from the sun move through a well-mixed gas such as air are an example. Other processes are more complex, or variable on small scales. It is hard to derive laws from these processes. For instance, the flow of low-energy photons from the earth through air is somewhat uncertain because the laws governing how the energy interacts with water vapor are very complex. In the case of water vapor, the way the molecule is constructed it can absorb and release energy at many different wavelengths. For these processes, we often must use statistical"
311,938,0.985,The Physics of the B Factories,"quite possible that power corrections are O(1) eï¬ects relative to the perturbative calculation, preventing a reliable quantitative estimate. However, the direct CP asymmetry calculations are still LO calculations, contrary to the branching fractions, so the final verdict + [0.049 + 0.051i ]NLOsp + [0.067]tw3 must await the completion of the NLO asymmetry cal+0.217 +0.115 culation. Contrary to direct CP asymmetries, the S pa+ (â0.077â0.078 )i . (17.4.9) = 0.240â0.125 rameter that appears in time-dependent CP asymmeHere 0.220 represents the naÄ±Ìve factorization value. tries is predicted more reliably, since it does not require Loop corrections to the form-factor-like term in the the computation of a strong phase. This is exploited first line of Eq. 17.4.6 and the first two lines of Eq. 17.4.9 in computations of the diï¬erence between sin 2Ï1 from almost cancel this number, but generate a sizable imagb â s penguin dominated and b â ccs tree decays (Beinary part, i.e. scattering phase. The real part of the neke, 2005; Cheng, Chua, and Soni, 2005b). amplitude is regenerated by spectator-scattering in the 5. Polarization in B â V V decays was expected to be second line of Eq. 17.4.6 and the third and fourth line predominantly longitudinal, since the transverse helicof Eq. 17.4.9. It is evident that the strong interaction ity amplitudes are Î/mb suppressed due to the V-A dynamics of the color-suppressed tree amplitude is far structure of the weak interaction and helicity conserfrom the naÄ±Ìve factorization picture, and is governed vation in short-distance QCD. While this is parametby quantum eï¬ects. The theoretical uncertainty is corrically true (with one exception (Beneke, Rohrer, and respondingly large. Yang, 2006)), a closer inspection shows that the para3. The QCD penguin amplitude P that governs branchmetric suppression is hardly realized in practice for the ing fractions of decays to final states such as ÏK and penguin amplitudes (Beneke, Rohrer, and Yang, 2007; its vector-meson relatives is certainly underestimated Kagan, 2004). This leads to the qualitative prediction in leading order in the heavy-quark expansion. The (or rather, in this case, postdiction) that the longipower-suppressed but chirally-enhanced scalar penguin tudinal polarization fraction should be close to 1 in amplitude, and perhaps a (diï¬cult to disentangle) weak tree-dominated decays, but can be much less, even less annihilation contribution, is required to explain the than 0.5, in penguin-dominated decays, as is indeed penguin-dominated PP final states. While the scalar observed. However, quantitative predictions of polarpenguin amplitude is calculable, some uncertainty reization fractions for penguin-dominated decays must mains. An important observation is the smaller size be taken with a grain of salt, since they rely on modelof the PV, VP and VV penguin amplitudes as comdependent or universality-inspired assumptions of the pared to PP final states, which can be inferred from non-factorizing transverse helicity amplitudes. the measured branching fractions of hadronic b â s The remainder of this chapter is devoted to an overview transitions. This is a clear indication of the relevance of factorization, which predicts this pattern as a conse- of experimental techniques of importance to charmless B quence of the quantum numbers of the operators Qi . If decay measurements and provides a summary of two-body the penguin amplitude were entirely non-perturbative, and three-body final state data collected by the BABAR no pattern of this form would be expected. A similar and Belle experiments. A detailed comparison and interstatement applies to the Î· (â²) K (â) final states, where pretation of the data in the light of theoretical approaches factorization explains naturally the strikingly large dif- as discussed above is beyond the scope of this review. For ferences in branching fractions, including the large Î· â² K this reason we will generally refrain from making reference branching fraction, through the interference of penguin to specific theoretical papers in the following. amplitudes, although sizeable theoretical uncertainties remain. A flavor-singlet penguin amplitude seems to 17.4.3 Experimental techniques play a sub-ordinate role in these decays. 4. The situation is much less clear for the strong phases and direct CP asymmetries. A generic qualitative pre- The decays of B mesons to final states with two or three diction is that the strong phases are small, since they hadrons without a charm quark are loosely broken down arise through either loop eï¬ects (Î±S (mb )) or power cor- into âtwo-bodyâ, âquasi-two-bodyâ and âthree-bodyâ derections (Î/mb ). Enhancements may arise, when the cays. The âtwo-bodyâ analyses concentrate on long-lived leading-order term is suppressed, for instance by small final states such as ÏÏ, KÏ, KK, etc. As these modes can Wilson coeï¬cients. This pattern is indeed observed. be used to access the CKM angle Ï2 , they are covered Quantitative predictions have met only partial suc- in Chapter 17.7; only the observation of direct CP violacess. The observed direct CP asymmetry in the de- tion is discussed here. The âquasi-two-bodyâ category incay to Ï + Ï â , and the asymmetry diï¬erence in the de- cludes decays where one or both of the decay products is a cays to Ï 0 K + and Ï â K + are prominently larger than resonance. Final state particles that have been measured predicted. A comparison of all CP asymmetry results include scalar (S) particles (a0 (980), f0 (980), f0 (1370), shows a pattern of quantitative agreements and dis- f0 (1500), K0â (1430)); pseudoscalar (P) particles (K Â± , K 0 , agreements that are not presently understood. Since Ï Â± ,Ï 0 , Î·, Î· â² ); vector (V) particles (Ï, Ï, Ï, K â ); tensor Î±S (mb ) and Î/mb are roughly of the same order, it is (T) particles (K2â (1430), f2 (1270)); and axial-vector (A)"
303,116,0.985,Multiculturalism and Conflict Reconciliation in the Asia-Pacific,"among the users, while those attempting to portray the hybrid nature of identities through their defense of localized English contend that the idea of varieties of English is essential in constituting democracy in the newly emerging cosmopolitan culture. What permeates both positions, however, is a subjectivity constructed prior to the confrontational encounter between the two sides of the World Englishes dispute, and the subjectivity that each side focuses on is presumably constructed by socio-political factors and elements elsewhere. Some may argue against the statement that the hybrid forms of identities are not set a priori, in the sense that their subjectivities are constituted through the practice of crossing over cultural boundaries. This is correct, and this is precisely the reason why I contend that their identities are pre-set. Their subjectivities are assumed to exist before the dispute. What is missing in this argument is the awareness that subjectivity is constituted and discovered through the World Englishes dispute over who owns the language. In other words, the presumed dialectical relationship between the core and periphery misses the point of the construction of subjectivity through the investigation of relationality. It is not local history, heritage, and the experience of crossing over boundaries that perform an essential role in the construction of subjectivity. Rather, one can say that those engaged in the dispute in search of these elements discover these subjectivities, which account for their peculiar identities that then must be distinguished from the âother.â In this sense, the concept of World Englishes clarifies a system of relationality, which includes the subjectivity of the disputants in the World Englishes dispute, where the emergence of relationships constructs the subjects. Therefore, the important issues here are how relationships shape and engender the subject, and how this process of subjectivity production ensures the emergence of an inclusive public domain in world affairs. The idea of relationality, which constitutes subjectivity, is relatively visible and is often understood as common sense in the peripheries of the contemporary hegemony. Perhaps one of the archetypal examples in this context is Nishida Kitaroâs philosophical concept of the âplace of nothingness.â Nishida, one of the most prominent Japanese philosophers, claimed that individuals do not exist prior to experience, but, rather, experiences construct individuals (Nishida, 1947, p. 4). Thus, individual identity relies entirely on its experiences. In society, the experiences that produce individual identities are, by definition, social, and therefore relational. This means that the relationality of subjects becomes the central focus of inquiry into socio-politics."
269,130,0.985,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"emission tomography (PET), electroencephalography (EEG), and other technologies are of course also possible.) The cognitive neuroscientist should be on hand to conduct the experiment and process the data; the psychologist should take charge of fine-tuning the protocol (sometimes the neuroscientist and the cognitive psychologist are subsumed within one person); the bioethicist should be there to soothe our consciences about any ethical implications of the study; the social scientist should be there to help assess the suitability of the sample and to comment on the generalizability of the findings; the philosopher should be there to ensure rigorous parsing and application of constructs (and to make the tea). The most important data to emerge are those produced through the coming together of the human experimental subject(s) and the scanner; these should be published in a neuroscientific journal (though ancillary publications that address other disciplinesâ concerns are of course welcomed). In short: what emerges here is a spatial imaginary centred on the model of a multi- (rather than inter-) disciplinary mix, a mix that will establish, it is hoped, a methodological and conceptual âlayer cakeâ. (We are employing here the usual distinction between the multi-disciplinary, in which disciplines line up alongside one another, from the interdisciplinary, which usually carries some commitment towards creating emergent, and novel forms of knowledge different from those contained within any one discipline.) Within this layer cake, each disciplinary layer has dominion over a particular kind of expertise, particular methods, and particular objects of knowledge. You can see this play out in numerous published interdisciplinary books and studies that address the mind and brain (see e.g. Goldman 2006; Slaby and Gallagher 2015). Joseph Dumit, we should note, has provided a powerful analysis of how cognitive psychology was brought into contact with PET to produce the interdisciplinary layer cake that became the foundation of the interdisciplinary field of cognitive neuroscience (Dumit 2004). We should not be unduly surprised, therefore, that the layer cake functions as the buttressing logic for other interdisciplinary endeavours involving the brain and mind. There are a few examples that depart from the layer cake genre. Roepstorff and Frith, for example, in elaborating a model of experimental anthropology âas a method, as an object of study and as a research aestheticâ, argue that joint engagement (doing things together) in research projects across the disciplines leads to the need for researchers of all stripes then to âbe sensitive both to the type of facts and the types of contexts produced by going experimentalâ (Roepstorff and Frith 2012,"
124,203,0.985,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"9 The Making of an Agent Typical propositional systems of the logic of action, constructed along Belnapian lines, focus on agency and the truth conditions for agentive sentences, rather than (Footnote 16 continued) the ontology, but not active at any moment in any history, and thus to be for all practical purposes non-existent. Itâs a bit difficult to see how to interpret such a situation. This perhaps argues for the introduction of such a constraint, particularly for the usual systems whose models are single Belnapian worlds. It is also possible that an agent should be active only in some histories and not in others, e.g. that there might be some histories in which the agent was, and others in which she was not, born."
363,30,0.985,History and Cultural Memory in Neo-Victorian Fiction,"If language and narrative are the âother sourcesâ of history and fiction alike, and if historyâs referent is not âout thereâ waiting to be discovered and recorded, but is rather constructed, the notion that historical narratives have privileged access to the real is undermined. Indeed, no longer guaranteeing unproblematic access to the past, the historianâs narrative is at a double remove from the past âas it really happenedâ. The primary sources have not simply mediated the past but have always already interpreted it, and the historianâs narrative, constructed from these textual remains, is itself an interpretation of them. History is no longer a stable entity, the assurance of an extra-textual reality or context against which literature can be understood. Nor is it a stable context against which historical fiction can be judged as true or false. As Paul Hamilton observes, the new historicism ârecasts history as a battle over fictionsâ (Hamilton, 1996: 171). Indeed, the reconfigured relationship between history and fiction forged by the new historicist emphasis upon the historicity of texts and the textuality of history seems to suggest that the writer of fiction can share the role of the historian. Martha Tuck Rozett links the publication of Umberto Ecoâs The Name of the Rose (1984) to the influence of the new historicist school of literary criticism. She claims that âthis dazzling mixture of thick historical research and popular detective fiction invited its readers to view historical fiction as an academically respectable genre and a vehicle for recovering and reimagining the past in unconventional waysâ (Rozett, 1995: 145). In his Postscript to The Name of the Rose (1988), Umberto Eco emphasises the importance of historical research in the construction of a detailed world, and in the creation of characters that truly belong to that time and place. More specifically, he suggests that the novelist can present a past âthat history books have never told us so clearly [and] make history, what happened, more comprehensible â¦ identify in the past the causes of what came later, but also trace the process through which those causes began slowly to produce their effectsâ (Eco, 1984: 75, 76). The delineation of the postmodern historical novelistâs task in this way recalls that which Ferris attributes to Scott, in writing his fiction. His or her role is to illuminate history, including making the pattern of history comprehensible. Whereas for Scott this meant the depiction of a Hegelian dialectical development, or evolution, effected by the clash of civilisations, for Eco it means a Foucauldian genealogy or archaeological descent, the process of historical inquiry, not a process of purposive history.3 What is significant about Ecoâs account of his own project as a postmodern historical novelist is that he still foregrounds a strong engagement with"
249,133,0.985,Advances in Proof-Theoretic Semantics (Volume 43.0),"Here n should be understood as abbreviating a numeral of the form sn (0) for some fixed n â N, which may in turn be understood as the GÃ¶del number of a proof in T. And on this model, it seems reasonable to think of p in A as abbreviating some (possibly complex) closed term in the language of T (or a similar theory) which is intended to denote a particular constructive proof. Note, however, that the principle ExpRfn which is used in the derivation of the Kreisel-Goodman paradox differs from ExpRfnR and ExpRfnPr not only in that it is formulated in terms of the derivability relation â¢T of the Theory of Constructions, but also in that it may be used in the case where t is a variable of the theory.33 But note that the free variable instances in ExpRfnR and ExpRfnPrâi.e. R(A, x) â A / FV(A) and x â / FV(Ï))âare and ProofT (x, Ï) â Ï (where we assume x â equivalent over intuitionistic first-order logic to the following âimplicitâ reflection principles: (RfnR) âx R(A, x) â A (RfnPrT ) âxProofT (x, Ï) â Ï The contrast between ExpRfnPrT and RfnPr is likely to be familiar: (i) all instances of ExpRfnPr are both true in the standard model of arithmetic and provable in T â Q; (ii) but while all instances of RfnPrT are true in the standard model, in light of LÃ¶bâs theorem for T, the only instances of RfnPrT which will be provable in T (provided it is consistent) are those for which T â¢ Ï. Moreover, although arithmetical theories T â Q will satisfy an analog of the rule Intâi.e. if T â¢ Ï, then T â¢ âxProofT (x, Ï)âthe result of closing a theory Tâ² which already proves all instances of RfnPrTâ² will be inconsistent in light of Montagueâs paradox. A related observation is that not only will instances of ây(ProofT (y, âxProofT (x, Ï) â Ï) be unprovable in T when T  Ï, they will in fact be false in the standard model in light of the formalized version of LÃ¶bâs theorem. As the foregoing observations pertain to formal provability in the arithmetical theory T, it is not immediately clear what (if any morals) can be read off about the status of ExpRfn or RfnR on their intended interpretations.34 What they do suggest, however, is that when the term t in ExpRfn is allowed to contain free variables, the effect of including this principle in a theory such as T may be closer to the effect of adding RfnR rather than ExpRfnR. For as is exemplified by the derivation of the Kreisel-Goodman paradox, the free variables of T (in conjunction with the relevant form of substitution principle) function very much like universally bound variables in first-order logic. And thus although the Theory of Constructions contains neither quantifiers nor implication in its object language, the instance of ExpRfn with t = x can be understood as expressing for all proofs x, if x is a proof of s, then s is true."
84,81,0.985,Eye Tracking Methodology,"with the naked eye by looking âoff the fovea.â Because the periphery is much more sensitive to dim stimulus, faint stars are much more easily seen out of the âcornerâ of oneâs eye than when they are viewed centrally. Thus the high-level component of vision may be thought of as a covert component, or a component which is not easily detectable by external observation. This is a well-known problem for eye tracking researchers. An eye tracker can only track the overt movements of the eyes, however, it cannot track the covert movement of visual attention. Thus, in all eye tracking work, a tacit but very important assumption is usually accepted: we assume that attention is linked to foveal gaze direction, but we acknowledge that it may not always be so."
271,494,0.985,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"To illustrate these points, let us take research on families as an example. At first sight one might consider the family a rather straightforward and easily definable field as well as an easily definable figuration. However, under closer scrutiny, defining what makes a family is not an easy task. As pointed out in The Blackwell companion to the sociology of families (Scott et al. 2004) we can observe a pluralization from family to families. The editors state: âOur title acknowledges the plurality of family forms and, by implication, the dynamic process of family formation and dissolution across timeâ (Scott et al. 2004: xvii). Recent research on families (see for example Jamieson et al. 2014) has acknowledged the complexities of families. Transnational family networks, as they have been researched by Madianou and Miller (2012) and Beck-Gernsheim (2014), are but one example of the growing diversity of families. In addition, these studies have shown that the complexities increase with the recent development in media change. In line with the broader definition of family in academia, research participants might similarly have shifting understandings of who belongs to their family, depending on circumstances. In a recent action research project with teenage refugees based in Bremen (Volmerg et al. 2016), participants were creating family-like relations with each other as well as with at least one of the researchers involved. Definitions of the family are certainly dependent on historical and cultural contexts. As Morgan (2014) points out, the changing definition of family is partly thanks to changing circumstances and the fluidity of social life, and partly to the changing perceptions of what can constitute a family. From a methodological point of view, the difficulty of defining families at the point of data gathering might seriously limit the data gathering process by imposing a set definition of families from the outside. By conceptualizing families as figurations, researchers can consider their actor constellations, relevance frames and practices and thereby construct families through a multi-perspective lens. The concept of communicative figurations can therefore be used to sharpen the understanding of what âthe fieldâ one wants to research is, by not neglecting fluidities within a figuration but by illuminating them. Another requirement when applying a figurational approach to a field of study is that it allows for scalability. As Couldry and Hepp (2017: 70) point out:"
378,167,0.985,The Great Mindshift : How a New Economic Paradigm and Sustainability Transformations Go Hand in Hand,"The following summary is thus also the conclusion of this chapter. It briefly runs through the four big ideas listed in Table 3.1 and why they fall short when we seek to understand how to satisfy human needs while respecting natureâs law. The discussions above have shown that they are scientiï¬cally flawed. Yet, through their materialization in todayâs market and government structures, these ideas have become very real in their impact on people, their decision-making and their freedom to do things differently and more sustainably. The reflexive ontology behind this book sees the opening up of worldviews and belief systems as the ï¬rst step in system innovation strategies: identify which arguments, practices or laws are built around flawed assumptions and ideas and understand how they hamper more sustainable developments. Engaging in transforming such path dependencies will of course always be a highly political, contested and power-ridden process whose outcome no one can predict. But shedding vested intellectual interests often comes before shedding economic vested interests: sense and legitimacy of the status quo start crumbling and alternative practices multiply, inspired by an emerging imaginary that there are indeed alternatives. Trying to understand the world by dividing it into pieces creates an imaginary in which the relationships and generative rules underlying system dynamics get lost behind numbers and detailed descriptions of the individual pieces. This atomistic view of mechanical systems suggests that the single items remain unchanged and between them one can detect and thus manage linear and reproducible causalities in an additive or subtractive approach. System dynamics are viewed as predictable and controllable as long as the properties of the individual parts are understood well. There are no time delays or feedback loops that allow one to anticipate the fact that stopping a particular cause will no longer stop a particular reaction once system dynamics have reached tipping points. General ignorance of the Tyranny of Small Decisions is related to this: large output changes need large input changes. It is this âparticleâ worldview or paradigm that leads to the common juxtaposition of incremental versus radical change. But applying this view to complex systems ï¬lled with humans is not helpful, as the ï¬nancial crises show. Using it to inform strategies of human need satisfaction offers very little insight about the matter it is supposed to address: utility or happiness is a relative and context-dependent experience and not a thing that can be privately held and hoarded. Meanwhile, natural capital is a web of life and not a"
271,487,0.985,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"The ânatural settingâ in which television viewing takes place has diversified over the past decades. Given the many ways in which television is consumed nowadays, considering one show in one medium through one type of methodological approach gives a limited perspective on social life, and can therefore only serve to consider one specific aspect of the lifeworlds of individuals. Of course there have been methodological developments which have in some way or other answered David Morleyâs call for a more nuanced style of research. One such example is the volume put forward by Berker et al. (2006). Relating to and building on the domestication approach, the collection takes a wider view of the home and the interrelation between media and technologies in the home. They focus on âthe continuity of routines and patterns of everyday life, but also consider the breaking of routines and the discontinuity of some processesâ (Berker et al. 2006: 3). If we apply the figurational approach to Angâs study, we might ask how Dallas relates to other media consumed and appropriated, who the different actors in the family home are, how they relate to a particular programme and how they communicate with each other and with other actors about the programme, a specific episode, and the specific themes it raises. In the current state of deep mediatization, these questions should be broadened to take into account other mediated interactions that take place in relation to the viewing of a programme. If we identify the media ensembles and communication practices of different actors within a"
8,302,0.985,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","16.2 Highest Temperature D The Boiling Point of Primordial Matter? I claim that it is not surprising that the temperature seen in high energy collisions is that highâin fact, one would have expected it to be much higher and in particular that it should grow with the energy of the colliding particles. Namely, as one knows from the black body radiation lawâand that is what we are dealing with hereâ temperature should grow at about the fourth root of the energy. Instead, it remains a simple constant, apart from some not yet quite understood exceptions. More precisely, as the particle collision energy grows, the temperature T0 approaches a finite limit of 1:8  1012 K corresponding to 160 MeV. It appears that this fact is extremely significant indicating that in the decomposition of matter, we have reached an unexpected end, which is, nevertheless, not an end."
393,402,0.985,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 2,"We have presented a survey of concepts, techniques, and topics constituting the core of the research and engineering area of Parallel Discrete Event Simulation. Of course, not all methods or approaches to individual problems can be captured in such survey, nor every single problem arising in this area can be mentioned. In spite of that, we believe that the document gives a reasonable overall insight into the area as a whole, and, at the same time, a more detailed description of some issues which draw the most of our attention."
346,279,0.985,"History Education and Conflict Transformation : Social Psychological Theories, History Teaching and Reconciliation","without explicit effort and without addressing any particular group. In fact, this is the way identity-related aims are addressed in Estonian national curriculum: âIn the process of solving historical problems [...] [the studentsâ] national and cultural identity, tolerance and positive attitude to the values of democracy developâ (Estonian Government 2002, similarly 1996, 2011, 2014). As can be seen, the curriculum mentions these things rather in passing, cautiously, so there is a lot of space for individual interpretations by the teachersâmore space than in the case of the more cognitively oriented aims. This explanation is also supported by the teachersâ comments on the statement The current teaching of history is too âself-centredââconcentrating on the Estonian and European past produces young people with narrow worldviews. Although some of the teachers admitted that current history teaching in Estonia was too self-centred (ethno- or Euro-centric), most of them did not oppose the focus, either supporting Estonia- and Europe-centred history teaching with pedagogical or ideological arguments (thus negating excessive self-centredness), or considering such a state inevitable. The âself-centrednessâ was justified by the necessarily limited teaching time and, thus, the inevitability of choosing some kind of a focus in history teaching, as well as with reference to the pedagogical principle that teaching should commence with what was closest to the student. Connected to this was the argument that history teaching is first and foremost about understanding oneself and learning about oneself and that in support of this goal Estonian and European history is the most important (see more in kello and Masso 2012). On what else should we concentrate? [â¦] if you donât know about your own countryâs history, then what sense does it make to talk about anything else. (Jaanika)"
390,578,0.985,The Hidden Language of Computer Hardware and SW,"ur lives are full of repetition. We count the days through the natural rhythms of the rotation of the earth, the revolution of the moon around the earth, and of the earth around the sun. Each day is different, but our lives are often structured by standard routines that are similar from day to day. In a sense, repetition is also the essence of computing. Nobody needs a computer to add two numbers together. (Letâs hope not, anyway!) But adding a thousand or a million numbers together? Thatâs a job for a computer. This relationship of computing and repetition was obvious early on. In Ada Lovelaceâs famous 1843 discussion of Charles Babbageâs Analytical Engine, she wrote: Both for brevity and for distinctness, a recurring group is called a cycle. A cycle of operations, then, must be understood to signify any set of operations which is repeated more than once. It is equally a cycle, whether it be repeated twice only, or an indefinite number of times; for it is the fact of a repetition occurring at all that constitutes it such. In many cases of analysis there is a recurring group of one or more cycles; that is, a cycle of a cycle, or a cycle of cycles."
306,59,0.985,"Early Geometrical Thinking in the Environment of Patterns, Mosaics and Isometries","important condition was the preservation of equal distance between the elements. Some examples are shown in Fig. 33. b. Axial symmetry There were also situations where a child created a symmetrical, limited construction. Its creation proceeded floor by floor. The whole work then had only one symmetrical axis (Fig. 34). Table 2 presents a quantitative analysis of the presence of isometries in the Italian pupilsâ protocols. Table 2 reveals that the rate of application of the more complex isometries grows with the pupilsâ age, even if the children in the sample did not receive any formal geometry teaching. We can interpret this data by observing that the enhancement of manipulation ability obtained by school teaching is probably also of beneï¬t for improving geometrical and spatial intuitions. Marchini et al. (2008) treats these and other interesting geometrical features discovered in protocols in depth. It is possible to observe a low number of symmetries, even if the drawings on the tiles could suggest their use. This supports the ideas that the âmetaphor of equilibriumâ (NÃ¹Ã±ez et al. 1999) does not influence the pupilsâ performance and that symmetry is not an embodied cognition but mathematical knowledge that must be constructed by learning (Swoboda 2007; Bulf 2010; Bulf et al. 2013; Bulf et al. 2014a; Bulf et al. 2014b)."
269,132,0.985,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"Both of us were at the workshop at which a highly regarded interdisciplinary researcher conveyed this strong account of appropriate interdisciplinary practice â one that defended disciplinary turfs and disciplinary expertise. The sentence above was spoken after one of us (FC) had countered what she interpreted as the academicâs territorialized landscape of interdisciplinarity with a question about the untoward relationalities unfurled by postcolonial studies â as well as by other interdisciplinary theoretical domains that have been committed to queering the pitch of orthodox histories and geographies. In many ways, we understood this researcherâs response. In calling for interdisciplinarity, no one wants to be misunderstood as championing sloppiness, and few, we imagine, want to be interpellated as the inevitable jack of all trades and master of none. We certainly would not want to find ourselves endorsing, say, an analytical philosopher who planned to get by in an interdisciplinary project, untrained, as a sociologist; we do not wish to champion an anthropologist who imagines she can, in a trice, become a conceptual artist. But in this chapter we want, nonetheless, to challenge the spatial logic of fences and neighbours â and the specific forms of territory it brings into being. Here is discipline thought through the figure of the happy householder â willing to reach over the fence, for sure, but still very keen to maintain a proper sense of where the boundaries lie. Indeed, there is an important claim embedded in this metaphor (and one hears it all the time) of good fences, insofar as it dramatizes what is still sometimes an unquestioned assumption about the primacy of the private space of disciplinary training. Whatever our â and your â feelings about private property might be, it strikes us that there is a pernicious notion, embedded here, that maps intellectual inquiry on to the taken-for-granted boundaries of relationships between public and private space. (In this regard, we find it intriguing that many writings that address cross-disciplinarity employ language that conjures private property relations. We might turn, for example, to the philosopher Brian Massumi, who discusses the âpoachingâ of a scientific concept (which, moreover, on his account, does not result in âprevent[ing] it from continuing to function in its home environmentâ, and who maintains that if he âwere a concept, [he] could emigrate and stay behind in [his] home countryâ [italics in original] (Massumi 2002, 21)); or to Thomas Osborne, who describes interdisciplinary movements via languages of poaching and trespass (Osborne 2013).) What would change, for the kinds of projects that we are trying to bring into"
152,28,0.985,interdisciplinary Perspectives On Mortality and Its Timings : When Is Death?,"empathy when they imagine the final minutes and seconds of a personâs life. Surveying examples from literature, philosophy, and the history of crime, RÃ©e finds that the evolution of execution narratives has a lot to do with social attitudes about capital punishment and, in particular, an urge to think about our own last moments of life. Finally, in Chap. 10, Thomas Laqueurâs afterword focuses on the possibility of future breathsâdetermined by the apnea test in brain death situationsâas a way to think about the end of life. Brain dead people may subsist for decades attached to ventilators and participate in the same biological milestones as everyone else (puberty, pregnancy, death). Despite the expanding chronologies of the âliving deadâ through science and technology, however, Laqueur argues that the âwhenâ of death starts at the time when it is shown that a person will never breathe again without artificial assistance. This particular death sentence, of course, does not discount the reality that becoming dead also takes time in other, non-biological ways of thinking. In its movement from history and literature, to philosophy and ethics, the contributions in this book attest to a pervasive dynamic between finality and continuance, between death as a concrete biological event and death as a social negotiation. The question we have addressed is inherently interdisciplinary. It will continue to fascinate scholarly and lay audiences alike, because death timings allow us to make sense of who we are as individuals and societies in the midst of time, shorn between long memories and imagined futures on the one hand, and a single irrevocable destiny on the other."
249,131,0.985,Advances in Proof-Theoretic Semantics (Volume 43.0),"5.4 Reflection The explicit reflection principle ExpRfn formalizes the principle that if p is a construction proving A, then A is true. Like decidability, such a principle may plausibly be regarded as part of the intended interpretation of the proof relation. To the best of our knowledge, no one has ever argued explicitly that ExpRfn should be given up in the face of the Kreisel-Goodman paradox.32 But although we do not wish to challenge this consensus, we will now adduce several considerations which suggest that finding an appropriate formulation of reflection in the Theory of Constructions may not be as straightforward as it might appear. The central difficulty is most readily appreciated by again invoking the analogy between the proof relation R(A, p) and the arithmetical proof predicate ProofT (x, y). If we continue to assume that the system in terms of which we reason about the former contains intuitionistic first-order logic, than one might at first think that the relevant analogs of ExpRfn would take the forms (ExpRfnR) R(A, p) â A (ExpRfnPrT ) ProofT (n, Ï) â Ï 31 A similar reaction is voiced by Sundholm [40, p. 16]: âSince [the second clauses] had been"
153,236,0.985,Solving Pdes in Python : The Fenics Tutorial I,"This is similar to the Expression subclass we defined above, but we make use of the member function eval_cell in place of the regular eval function. This version of the evaluation function has an additional cell argument which we can use to check on which cell we are currently evaluating the function. We also defined the special function __init__ (the constructor) so that we can pass all data to the Expression when it is created. Since we make use of geometric tests to define the two SubDomains for â¦0 and â¦1 , the MeshFunction method may seem like an unnecessary complication of the simple method using an Expression with an if-test. However, in general the definition of subdomains may be available as a MeshFunction (from a data file), perhaps generated as part of the mesh generation process, and not as a simple geometric test. In such cases the method demonstrated here is the recommended way to work with subdomains."
271,542,0.985,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"this is too much for me. I donât have time for that. And then I realizeâand this was an aspect you also mentioned at the beginning: digital world. I believe everything we accelerate: transcribing, typing with the computer and so on. All this always returns as a problem in the form of this flood of possibilities. (P3: 1712â1721) Possibly, the interviewee would have mentioned this irritation in her conduct of life irrespective of the impulse to reflect upon a changing media environment and her own media repertoire. However, there are indicators that the implicitly stated research interest stimulated the consideration of this. The reference to the impulse given in the introduction to the interview implies that this inspired the respondent to make the connection between her reflections and the changing media environment. At the same time, the implicitly set stimulus did not overshadow her own relevance structure, which is supported by the fact that the topic came up in the course of her statement and was not triggered by a direct enquiry. Consequently, the implicit naming of the media-related research interest can produce an adequate balance. With this third interviewing strategy, we further observed that the enquiry about media-related topics only at the end of the interview came as a surprise for the interviewees, who were attuned to the announced five sets of questions, and tended to be tired by the end of the interview. Thus, it is likely that they answered the respective questions in a less elaborate and committed way than previous ones. Moreover, it can be assumed that at the end, the respondents were no longer aware of, or were not able to recollect, all aspects of the previously discussed life spheres and, hence, responded in a way that does not allow for separate conclusions for the different life spheres under study. Consequently, the interviewees might neglect crucial aspects, meaning that the retrospective questions yield less detail. Interviewing strategy 4 implicitly stating the media-related research interest plus enquiries after each set of questions dealing with one of the life spheres Applying the fourth interviewing strategy, we stated our interest in the role of changing media environments and repertoires for the respondentsâ conduct of life only implicitly using the same formulation as in strategy 3 (see above)."
28,148,0.985,A History of Self-Harm in Britain,"These descriptions are folksy and idiosyncratic, but draw upon Asherâs well-established interest in psychology. The intent in these cases is articulated through common-sense ideas of communication: âappeal valueâ and âshowing offâ. Despite the casual tone, the practices used to elicit these objects are remarkably labour-intensive. The information used to construct the above case histories is only fully obtained âafter carefully, and sometimes repeatedly, questioning patients and their relativesâ.23 Thus at Guyâs and the Central Middlesex in London, in Gateshead and in Birmingham, âattempted suicideâ emerges. Referral enables a series of transfers between separated therapeutic regimes. In Asherâs case, it is his eclectic (boundary-crossing) interests that are crucial. The object appears with increasing frequency, and yet the irregular, impermanent nature of the practices negotiating the split makes these clinical objects seem like so many miscellaneous, disconnected occurrences. There is certainly not much sense from the articles surveyed that attempted suicide is a national problem. The potential for an epidemic is clearly there, but it requires more high-level coordination and intervention to be fully realised. As the provision of mental-healthcare is rethought and reconstructed in the late 1950s, new objects appear. Too great a fixation on 1959 is unhelpful because the act removes restrictions to mental treatment. These are largely irrelevant, in one sense because this particular phenomenon presents first as physical injury. The 1959 act does not enact integration, it merely removes legal obstacles. Whilst the 1961 retraction of the law from suicide and attempted suicide is similar in one sense, the government is much more pro-active, prescriptive and practical, so the Suicide Act repays this kind of closer scrutiny."
275,222,0.985,Foundations of Trusted Autonomy,"7.2 TA Game AI One long-standing, and apparently obvious area where Trusted Autonomy could make a true impact in game execution and game environments is AI - Artificial Intelligence - opponents, team-mates and characters to play along-side-of, against, and to inhabit the imaginary game worlds. A superficial glance at the intersections of computational intelligence and renowned cultural games of the intellect such as Chess or Go would seem to indicate that the âAI challenge is solvedâ. In particular IBMâs Deep Blue triumph over chess grandmaster Gary Kasparov in 1997 [12], and most recently Googleâs AlphaGo triumph over Go grandmaster Lee Sedol in early 2016 [13] mark turning points for computational intelligence; showing its ability to exceed the highest levels of human performance in abstract games of reasoning. While there is not yet consensus within the academic community about the scope, range, type and enablers of human intelligence; there does seem to be broad agreement that human intelligence is much more than simply logical and mathematical with visual/spatial, inter-personal (emotional), linguistic (language), and kinesthetics"
275,539,0.985,Foundations of Trusted Autonomy,"17.3.3 Autonomy The term âautonomousâ is now routinely ascribed to various artifacts that are based on computing machines. Unfortunately, such ascriptions are â as of the typing of the present sentence in late 2016 â issued in the absence of a formal definition of what autonomy is.10 What might a formal definition of autonomy look like? Presumably such an account would be developed along one or both of two trajectories. On the one hand, autonomy might be cashed out as a formalization of the kernel that agent a is autonomous at a given time t just in case, at that time, a can (perhaps at some immediate-successor time t â² ) perform some action Î±1 or some incompatible action Î±2 . In keeping with this intuitive picture, if the past tense is used, and accordingly the definiendum is âa autonomously performed action Î±1 at time t,â then the idea would be that, at t, or perhaps at an immediate preceding time t â²â² , s could have, unto itself, performed alternative action Î±2 . (There may of course be many alternatives.) Of course, all of this is quite informal. This picture is an intuitive springboard for deploying formal logic to work out matters in sufficient detail to allow meaningful and substantive conjectures to be devised, and either confirmed (proof) or refuted (disproof). Doing this in the present chapter is well outside our purposes here."
8,502,0.985,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","where .E/ is the number of states between E and E C dE of the fireball considered. As for this fireball E D m (we stay in its rest frame), we can say as well that we have for our âmainâ fireball .m/dm states in the mass interval fm; dmg. Now .m/ is the number of hadron states in the interval fm; dmg and if our postulate (20.1) above is applied, it follows that asymptotically .m/ and .m/ must somehow become the same. A detailed discussion (see Part I of [1]) reveals that one cannot require more than that log .m/ ! 1 ; log .m/ m!1"
277,547,0.985,integrating Immigrants in Europe : Research-Policy Dialogues,"utilisation of knowledge because of the relatively intense EU level interactions that now occur between national level actors. However, it is also the case that the EMN tends to serve as a forum for the representation of member statesâ policy concerns rather than for the transmission of EU priorities. While it would be wrong to argue that the flow of ideas is entirely one way because it is necessarily more complex, it is the case that the EMN tends to be used as an arena for the substantiation of existing policy choices. In the cases of both the EMN and MIPEX we can see the production of knowledge, its utilisation and also the role played by dialogue structures. We can also see that this goes beyond the instrumental use of knowledge to also include knowledge-use as a form of institutional legitimation and as a way of substantiating existing policy choices. Finally, what is also very clear about the EU level dialogue is that it is relatively closed and specialist, with little evidence that EU action has spilled over into wider public debate. Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
8,287,0.985,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","what the events from high energy nucleus-nucleus interactions would look like. What, also, would the multiplicities of the secondaries be? And how would these be distributed in phase space? Furthermore, how could we recognize the existence of a QGP during the interaction and how valid would an enhancement of strangeparticle production be as a diagnostic tool for QGP? Which effects could distort the measurement and simulate a phase transition? At that point we went for advice to Hagedorn, who had studied those subjects [14]. He patiently discussed these matters with us and gave much useful advice. For the questions about strangeness, however, Hagedorn suggested that we contact directly Johann Rafelski who, he said, would be delighted to discuss that issue with us! This echoed the advice we got from LÃ©on Van Hove, a former CERN director general, also a strong supporter of the new research program. Indeed, Johann was delighted and this was the start of a long and friendly collaboration. Our first two experiments, WA85 and WA94, took data at 200 A GeV in a sulfur beam, using the ââbutterfly chambersâ. They were followed by two lead-beam experiments WA97 and NA57. The latter was a North-Area experiment with a new spectrometer and a new spokesperson: Federico Antinori. Both the latter two experiments made use of the Silicon Pixel telescope as their main tracking device. The experiments confirmed our hopes. We found that the abundances of multistrange baryons and anti-baryons produced in heavy-ion collisions were indeed enhanced [15]. Moreover, these enhancements increased with the strangeness content of the produced baryon [15, 16]. For example, in central lead-lead collisions, the rare Ë particles carrying three units of strangeness were enhanced by a factor twenty! A behavior expected to ensue from the appearance of a deconfined phase during the interaction [6]. Similar results were subsequently obtained by many other experiments. These results constituted one of the main pieces of evidence for the formation of a new state of matter at the CERN-SPS energies, which CERN announced in a press release in February 2000. More on this topic is reported in Chap. 33. Another interesting finding, suggesting a thermal production for s and sN quarks [17] was the similarity of the slopes of the transverse mass spectra between strange baryons and corresponding antibaryons [18]. An observation which did indeed please Hagedorn! With this last example, I conclude my brief review of the influence that Rolf Hagedorn, together with his disciples and continuators, had on the CERN heavy-ion program and on our physics choices."
311,1111,0.985,The Physics of the B Factories,"was feeble. Since Ç«K was measured in 1964, it took until 1973 before Kobayashi and Maskawa provided a real theEditors: ory for CP violation. It needed many years to demonstrate Chih-hsiang Cheng (BABAR) that the parameter Ç«â² was non-zero. Even before the unexYoshihide Sakai (Belle) pected âlongâ lifetime of B mesons was discovered, the B Ikaros Bigi (theory) meson system was recognized as the ideal testing ground for CP violation (Bigi and Sanda, 1981) and the decay Additional section writers: B â J/Ï KS0 as ideal for the purpose. Detection of the Tagir Aushev, Eli Ben-Haim, Adrian Bevan, Bob Cahn, final state is especially clean because the J/Ï decays to Chunhui Chen, Ryosuke Itoh, Alï¬o Lazzaro, Owen Long, lepton pairs and the KS0 is suï¬ciently long-lived to deFernando Martinez-Vidal, Vincent Poireau, Klaus Schu- cay into pairs of oppositely charged pions at a secondary vertex displaced from the interaction region. Unlike neutral kaons, the neutral B mesons start oscilPrecision measurement of the CP asymmetries in B â lating just after their production, since their mixing rate J/Ï KS0 decays was the principal motivation for building Îmd is comparable to their natural widths Î (see Secthe B Factories. With the accumulation of data samples tion 17.5). If we begin with a B 0 , at a later time the state larger than anticipated, the BABAR and Belle experiments will be a superposition of B 0 and B 0 . The decay to J/Ï K 0 at the B Factories are able to study CP asymmetries in a will occur through both components and the interference wide range of related channels. This section describes mea- pattern will depend on the relative phases between the surements of the Unitarity Triangle angle Ï1 , also known B 0 and B 0 components, which is directly calculable in as Î² in the literature. An overview of Ï1 measurements and the CKM model. The interference pattern depends on the their motivation is presented in Section 17.6.1, followed by two decay amplitudes to the final state. Because the fia review of the quark transitions and the formalism of Ï1 nal state is a CP eigenstate and because there is only one measurements in Section 17.6.2. The various channels for significant pathway to it from B 0 or B 0 , the two decay Ï1 measurement, and the B Factories results, are then amplitudes are identical, up to another calculable phase. described in Sections 17.6.3â17.6.7. Resolution of discrete As a result, the oscillation pattern can be predicted simply ambiguities is discussed in Section 17.6.8, and a summary in terms of the phases due to the CKM matrix without of Ï1 results is presented in Section 17.6.10. any dependence on hadronic physics. The time-dependent In the Standard Model, non-zero asymmetries mea- formalism required for the measurement of sin 2Ï1 can be sured in these analyses reflect violation of both the CP found in Chapter 10. and T symmetries. Performing the measurement in a way In order to test the CKM paradigm we need to know that directly demonstrates T violation, without assuming if we are starting with a B 0 or with a B 0 . The Î¥ (4S) is (for example) CP T symmetry, requires special care. Such an analysis has been performed at BABAR, and is presented very near the threshold for BB so if one B is observed, the from another B. Moreover, in Section 17.6.9. Tests of CP T symmetry are presented remaining particles must come symmetry, observed the other particle in Section 17.5. must be a B 0 at that instant, since the two mesons must be in an antisymmetric state to produce the unit of angular momentum carried by the Î¥ (4S). Thus âtaggingâ one B 17.6.1 Overview of Ï1 measurement at the B meson tells us both, when to start the clock and the type Factories of B at that time (see Chapter 8). The decay B 0 â J/Ï KS0 is just one of a large family of Initially, CP violation seemed isolated from the mainstream of particle physics. Since it was seen only in the KS -KL related decays due to a b â ccÌs transition. Of particular system, it was possible to imagine that it was due entirely interest is the decay to J/Ï KL because the final state has opposite eigenvalue, and we expect exactly the to a ÎS = 2 operator as postulated in the superweak theory (Wolfenstein, 1964). Two developments put CP vi- opposite oscillation. Other charmonia can take the place olation at center stage. The first was A. D. Sakharovâs of J/Ï , including Ï(2S), Î·c , and Ïc1 . The decay B â complex because spins final demonstration (Sakharov, 1967) that CP violation was one of the three requirements for the existence of the state particles can be combined to produce an overall spin baryon anti-baryon asymmetry of the universe (see Sec- equal to 0, 1, or 2, and correspondingly the orbital angular tion 16.2). The second was Kobayashiâs and Maskawaâs momentum will be 0, 1, or 2. This complexity has the demonstration that CP violation was natural if there were advantage that it can help resolve the ambiguity inherent three generations of quarks (see Chapter 16). With the in determining the angle Ï1 when only sin 2Ï1 is known. subsequent discovery of the last three quarks, testing the At first, the B Factories concentrated on measuring CKM model became urgent. time-dependent asymmetries in the so-called charmonium The KS0 -KL0 system was not suï¬cient by itself to test âgolden modesâ concentrating on B 0 â J/Ï KS0 , Ï(2S)KS0 , the CKM picture. The measured parameters, ÎmK , Ç«K Ïc1 KS0 , J/Ï KL0 , and J/Ï Ï 0 . However, it was understood and Ç«â²K , depended not just on the fundamentals of the that there were other ways to measure Ï1 . Once an unweak interactions, but on non-perturbative hadronic ma- derstanding of how to do these measurements started to trix elements. Moreover, CP violation in the kaon system develop, the experiments branched out to study similar"
132,120,0.985,AiREAS: Sustainocracy for a Healthy City: Phase 3: Civilian Participation â Including the Global Health Deal Proposition,"For personal exposure, we have chosen the ï¬ne dust with an aerodynamic diameter smaller than 2.5 micrometers, referred to as PM2.5. In the table below, established by the Ministry of Health, you can see that the concentration point that determines the PM2.5 transition from âGoodâ to âModerateâ is placed at 20 Âµg/m3. We keep these reference values during our research period. If the measurement of one or more airboxes shows values above those thresholds, then we can assume a situation of the worst air quality and hence a larger possible impact on your health. The qualiï¬cation of âGoodâ is relative too. Air pollution is always bad. Very sensitive people can also feel discomfort at lower levels of ï¬ne dust. Everything therefore depends on the particular situation of a person and environment. The table with value indicators was established by the RIVM (Ministry of Health) in cooperation with IRAS (Risk Assessment of the University Utrecht). Information can also be found on the AiREAS site: http://www.AiREAS.com."
360,132,0.985,Compositionality and Concepts in Linguistics and Psychology,"And finally, McGilvray tries to show how this âperspectiveâ allows for an internalist viewpoint on linguistic meaning which nonetheless has certain âtiesâ with the external world by means of when language is employed âin the world.â Chomskyâs view of [the semantic value of a linguistic item] as âa perspectiveâ was an adaptation for a general audience of the technical syntactically defined concept of [such a value] as âan interfaceâ. He points to a way in which [these values] are both rich and anthropocentric, and suggests a way to conceive of how they work cognitively in âinterpretingâ the world: [these values] focus attention on selected aspects of the world as it is taken to be by other cognitive systems [my emphasis], and provide intricate and highly specialized perspectives from which to view them, crucially involving human interests and concerns even in the simplest cases. I have in mind by richness the idea that [these values] provide very fine-grained media for all sorts of enterprises, not only describing and explaining the world (although not as a science that uses vocabulary apart from natural language would), but writing and reading literature, chatting with friends, and so on. (McGilvray 1998, p. 256)"
307,58,0.985,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"1.10 Results Many of the models, methods, and results described in these notes are well known in the literature. All the Markov models are taken from the literature and so are the stochastic differential equations and the models describing the probability density approach. Compared to earlier published models, we will often derive simplified models, but the ideas behind them are basically the same as those used by many authors. Concerning the modeling of mutations, we aim to consistently model the effect of mutations as simply as possible and preferably only by changing a single parameter: the mutation severity index. The novel part of these notes is that we attempt to systematically describe how to compute characterizations of drugs that are optimal in a specific sense and we do so for a number of applications. We almost exclusively address so-called gainof-function mutations. For such mutations, the open probability of the channel or receptor is too large, which can lead to severe difficulties for the cell and, ultimately, for large collections of such cells."
235,20,0.985,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","This chapter introduces some important epistemology. Without epistemology any inroad into the subject of (un)decidability and (in)determinism may result in confusion and incomprehensibility. Thereby, and although this book is mainly concerned with physics, we shall not restrict ourselves to the physical universe, but also consider virtual realities and simulations. After all, from a purely algorithmic perspective, is there any difference between physics and a simulacrum thereof?"
87,68,0.985,"Bioeconomy : Shaping The Transition To a Sustainable, Biobased Economy","In the academic literature, the concept of the green economy has a long history (see review by Loiseau et al. 2016). The question arises as to how the concept of bioeconomy is linked to the concept of the green economy. Ultimately, this is a matter of definition. One option is to consider the bioeconomy as an integral component of the green economy. According to this view, one may consider renewable energy sources that do not rely on biological resources, such as wind and solar energy, as part of the green economy but"
393,334,0.985,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 2,"Consider a space-parallel simulation model consisting of submodels, running on multiple processors. It is hardly feasible to assume anything about the relative speeds of the processors, and about the load of physical processors. The load of a processor depends on the number of submodels mapped to that processor, the âdensity of eventsâ on the time axis in different submodels which may vary during the simulation, the computational requirements of individual event handlers, etc. For a while, assume that the submodels are simulated independently, without coordination of the advancement of their local simulation times. Then it can easily happen that a submodel receives a message with timestamp less than its local simulation time. This situation is undesirable, because an event with smaller timestamp has the potential to change the state and thus affect events that occur later. Such violation of causality is called causality error. In sequential simulation, the single event list is a means to establish a total order of events and thereby avoid such errors. In the parallel case, the sufficient condition to avoid causality errors is that events within each submodel are processed in correct i.e. non-decreasing timestamp order. This condition is known in PDES literature as the"
289,1096,0.985,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","maximumâ of the function f , and is by construction monotonic. He shows that this notion satisfies all the desired properties, provided some of them are restricted by additional side conditions, such as monotonicity requirements. In this work, we go slightly further than Howell, in that we consider functions whose domain is an arbitrary filtered type A, rather than necessarily Nk . We give a standard definition of O and verify all of Howellâs properties, again restricted with certain side conditions. We find that we do not need OÌ, which is fortunate, as it seems difficult to define fË in the general case where f is a function of domain A. The monotonicity requirements that we impose are not exactly the same as Howellâs, but we believe that the details of these administrative conditions do not matter much, as all of the functions that we manipulate in practice are everywhere nonnegative and monotonic. Avigad and Donnelly [3] formalize the O notation in Isabelle/HOL. They consider functions of type A â B, where A is arbitrary and B is an ordered ring. Their definition of âf = O(g)â requires |f (x)| â¤ c|g(x)| for every x, as opposed to âwhen x is large enoughâ. Thus, they get away without equipping the type A with a filter. The price to pay is an overly restrictive notion of domination, except in the case where A is N, where both âx and Ux yield the same notion of dominationâthis is Brassard and Bratleyâs âthreshold ruleâ [7]. Avigad and Donnelly suggest defining âf = O(g) eventuallyâ as an abbreviation for âf â² , (f â² = O(g) â§ Ux.f (x) = f â² (x)). In our eyes, this is less elegant than parameterizing O with a filter in the first place. Eberl [13] formalizes the Akra-Bazzi method [1,21], a generalization of the well-known Master Theorem [12], in Isabelle/HOL. He creates a library of Landau symbols specifically for this purpose. Although his paper does not mention filters, his library in fact relies on filters, whose definition appears in Isabelleâs Complex library. Eberlâs definition of the O symbol is identical to ours. That said, because he is concerned with functions of type N â R or R â R, he does not define product filters, and does not prove any lemmas about domination in the multivariate case. Eberl sets up a decision procedure for domination goals, like x â O(x3 ), as well as a procedure that can simplify, say, O(x3 +x2 ) to O(x3 ). TiML [25] is a functional programming language where types carry time complexity annotations. Its type-checker generates proof obligations that are discharged by an SMT solver. The core type system, whose metatheory is formalized in Coq, employs concrete cost functions. The TiML implementation allows associating a O specification with each toplevel function. An unverified component recognizes certain classes of recurrence equations and automatically applies the Master Theorem. For instance, mergesort is recognized to be O(mn log n), where n is the input size and m is the cost of a comparison. The meaning of the O notation in the multivariate case is not spelled out; in particular, which filter is meant is not specified. Boldo et al. [4] use Coq to verify the correctness of a C program which implements a numerical scheme for the resolution of the one-dimensional acoustic wave equation. They define an ad hoc notion of âuniform Oâ for functions of type R2 â R, which we believe can in fact be viewed as an instance of our"
228,178,0.985,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"we postulate for a given method to be sensitive, the words âchange of direction can (not must) change the resultâ are a clue. It is worth noting that the basic arithmetic operations on the OFN model presented in Sect. 4.5 are generally sensitive to the direction. If the up-part and down-part of OFN A are not equal, then the reversal of direction operation (see Definition 4.2 from previous chapter) generates A|â = A. Therefore the result of an arithmetic operation will be different after reversal of the single input value. The purpose of this chapter is to propose a full set of methods and operations to define fuzzy systems based on OFNs that are sensitive to the direction feature. Therefore in the next sections, a general tool for processing a tendency of OFNs is presented. It is called the direction determinant (see also [24, 25]) as it is a kind of measure of direction for a given element of OFN support. Next the compatibility of OFNs as a result of statement A is B is proposed see [25] that uses the direction determinant. Finally a proposal of a technical inference method is presented that is meant to be a practical realization of the rule IF X is A THEN Y is B."
113,159,0.985,Pentecostalism and Witchcraft : Spiritual Warfare in Africa and Melanesia,"5. It is important to note at this stage that the notion of ndoki is also a relevant trope within Afro-American religions, albeit with a different meaning than in places like Angola (see e.g., Ballard 2005). 6. In fact, as FÃ¡tima Viegas and Jorge Varanda (2015) note, in Angola the understanding of nganga conflates, both in kikongo and kimbundo (language of the Ambundu), ideas of healer, diviner, and sorcerer (see also CalvÃ£o 2013 and Bahu 2014). 7. âReturneesâ, i.e., former Angolan Bakongo expatriates in the DR Congo who returned, over the past decades, to Angola. 8. All real names of my interlocutors in Luanda have been replaced for anonymity purposes. 9. In their case they are thinking about the events in Seattle in 1999. 10. I am referring to technologies devised in sci-fi productions, although obviously there are other, factual technologies that can be seen under the same framework of turning objects invisible: optical camouflage, stealth technology, etc. 11. Strong (this volume) and MacCarthy (this volume) also detect this centrality of sight or lack thereof in the witchcraft praxis in Melanesia. 12. That is, at least until the publication of Thomas Pikettyâs Capital in the Twenty-First Century (2014). 13. Bakongo âtraditionâ invokes perceptions of clanic (mvila) and lineage (kanda) logics (often referred to as ngenda or âcodeâ) linked to particular territorialities (see, e.g., Van Wing 1959). 14. By this I mean that the index is a number that is an indicator or interpreter of a given reality, through a set of calculations, but it also and simultaneously affects and defines that same reality by effecting upon the financial agents, creating expectation and trust or mistrust. 15. In the same vein of social structuring of economy (Bourdieu 2000), see, e.g., the work of Daniel Lopes (Lopes and Marques 2011; Lopes 2013) for a debate on credit and social relationships. 16. Since my first visits to Luanda in 2006 and 2007, I have visited numerous Christian and non-Christian churches, from prophetic, Pentecostal, and traditionalist backgrounds. My main focus, however, has been the Tokoist Church (Blanes 2014). 17. FÃ¡tima Viegas (2008; Viegas and Varanda 2015) also uses the concept of âneotraditional movementsâ to describe the spectrum of movements that encompass and intersect the categories I am using in this textâfrom evangelical to Pentecostal, traditional, prophetic, etc. 18. A relevantly analogous event can be found in the famous episode of the âchute na santaâ (kicking the saint) promoted by a Brazilian pentecostal pastor as a symbolic critique to the Catholic churchâs idolism (see Giumbelli and Birman 2003)."
166,311,0.985,Surveying Human Vulnerabilities across the Life Course (Volume 3.0),"outcomes as well their sequence can put people in a severe condition of risk affecting the reconstruction of resources needed to exit from vulnerability (Dannefer 2003, 2009). The temporal structure of LHC data is particularly suitable for accounting for these aspects and can be used for performing sequence analysis event-history analysis or other longitudinal models (Axinn and Barber 2001; Axinn et al. 1999). Last but not least the interpretation of events is at the same time a dependent and an independent variable in the study of vulnerability. The same event can assume very different meanings and affect peoplesâ lives in very different ways. For instance members of different social classes or members of the same class may not experience losing a job or being fired in the same way. Thus an event that for one person can start a dangerous domino effect may be a minor problem for another person. In addition the meaning attached to events is culturally and socially embedded. Losing a job in a period of economic crisis may be a different experience compared to losing it in a period of economic growth. On the other hand in a period of crisis the loss of the job becomes sadly common while in other historical periods it could represent a less likely and for this reason more stigmatizing experience (Oesch and Lipps 2012). The interpretation of events and their relationships with the socialhistorical contexts are tightly related and researchers can address them both easily in two ways with LHCs. Data in LHCs are both connected to temporalâi.e. the year in which events occurredâand spatial dimensionâi.e. where the respondent was located at the time the event occurred. This makes it possible to relate events to places and consequently to social and historical contexts. In addition LHCs can allow a retrospective evaluation of events providing information on individual interpretations of life trajectories. This is probably one of the less developed uses of LHCs although it is central to the study of vulnerability. In the next section we present some experiences with using LHCs to tap the three processes of diffusion accumulation and interpretation as examples of the use of LHCs to survey vulnerability."
264,111,0.985,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"Herbert Robbins. However, this title is a questionâwhat is Courant and Robbinsâ answer? Indeed, the book does not give an explicit deï¬nition of âWhat is Mathematics,â but the reader is supposed to get an idea from the presentation of a diverse collection of mathematical investigations. Mathematics is much bigger and much more diverse than the picture given by the CourantâRobbins exposition. The presentation in this section was also meant to demonstrate that we need a multi-facetted picture of mathematics: One answer is not enough, we need many."
32,82,0.985,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","for the origin of the phenomenon. Subsequently, we verify increments of the process has the long memory property. In general, there are three ways to verify a discrete time process has some property. The first one runs computer simulations. The second calculates the distribution of the discrete time process directly. And the third, which we use in this paper, is to show that the scaled discrete time process converges to a continuous process which has that property. There are various explanations for the origin of the long memory property of order sings [3]. A cogent one, which was proposed by Lillo et al. [9], is that investors tend to strategically split their large hidden orders into small pieces before execution to prevent the increase in the trading costs. Empirical findings partially support this explanation. A long memory phenomenon is found in a time series of order signs of transactions initiated by a single member of the stock market [3, 8]. Investors enter their orders into the market through one of its members. Assuming the size of hidden orders distributes as a power law, Lillo et al. [9] considered a discrete time mathematical model with this explanation. Under an additional technical assumption that the number of hidden orders is fixed, they showed rigorously the model has a long memory property. However, this technical assumption does not seem natural. Taking account of the bursty nature of human dynamics [1], Kuroda et al. [7] proposed another theoretical model with this explanation. They assumed that a time interval between order executions distributes as a power law, and that the power law exponent does not depend on the size of hidden order. Under an additional technical assumption that the size of hidden order is bounded above, they showed the scaled discrete time process converges to a superposition of a Brownian motion and a finite number of fractional Brownian motions with Hurst exponents greater than one-half. Moreover, the number of hidden orders is not fixed in their model, and it randomly varies. Although, the maximum Hurst exponent of obtained process depends on the largest hidden order. The Hurst exponent of order signs expected by the theory of splitting large hidden order is smaller than the value of the empirical study [3]. About stocks with high liquidity, the fluctuation of Hurst exponents of order signs is small by a stock and a period [6]. These findings suggest that there might be some other cause about the long memory of order signs. We can pay attention not only to large hidden orders but also to small hidden orders. VÃ¡zquez et al. found two universality class in human dynamics [13]. On the other hand, Zhou et al. observed that in an online movie rating site, a power law exponent of the time interval between userâs postings depends on userâs activity [15]. In this paper, we propose a new mathematical model with an explanation for the origin of long memory of order signs that investors split their hidden order of any size into small pieces before execution. We assumed that the power law exponent of distribution of a time interval between order executions depends on the size of hidden order. We showed the scaled discrete time process converges to a superposition of a Brownian motion and countably infinite number of fractional Brownian motions with Hurst exponents greater than one-half. We note that the number of hidden orders randomly varies, that Hurst exponents are not bounded"
311,3033,0.985,The Physics of the B Factories,"has a structure similar to the SM one. The most important feature is that there is a hierarchy similar to the one in the quark flavor sector of the SM. The first two generations are much lighter than the third generation. Also, the mixing between the third and the first two generations is much smaller than the one between the first two generations (i.e. Vcb , Vub âª Vus ). If this pattern is also present in NP with roughly the same hierarchies and directions of flavor breaking as in the SM, then FCNCs generated by NP will not be dangerously large. This insight can be formalized using symmetries and goes by the name of general MFV (GMFV). It is more general than cMFV, but coincides with the most general form of MFV, if Yukawa couplings are perturbative. Since GMFV is more general, there are also less correlations between observables. For instance, depending on which operators dominate, the new CP violating phases in Bd0 â B 0d and Bs0 â B 0s mixing are either exactly the same or there is a new CP violating phase only in Bs0 â B 0s mixing as we will see below. Supersymmetry The supersymmetric (SUSY) extensions of the SM are some of the most popular NP models. SUSY relates fermions and bosons. For example, the gauge bosons have their fermion superpartners and fermions have their scalar superpartners. SUSY at the TeV scale is motivated by the fact that it solves the SM hierarchy problem. The quantum corrections to the Higgs mass are quadratically divergent and would drive the Higgs mass to the Planck scale â¼ 1019 GeV, unless the contributions are canceled. In SUSY models they are canceled by the virtual corrections from the superpartners. The minimal SUSY extension of the SM refers to the scenario where all the SM fields obtain superpartners but there are no other additional fields. This is the Minimal Supersymmetric Standard Model (MSSM). SUSY cannot be an exact symmetry since in that case superpartners would have the same masses as the SM particles, in clear conflict with observations. Diï¬erent mechanisms of SUSY breaking have very diï¬erent consequences for flavor observables. In complete generality the MSSM has more than a hundred parameters, most of them coming from the so-called soft SUSY breaking terms (the SUSY breaking terms with dimensionful couplings, such as e.g. masses, so that the divergence is at most logarithmic). If superpartners exist at the TeV scale the most general form with O(1) flavor breaking coeï¬cients is excluded due to flavor constraints. This has been dubbed the SUSY flavor problem (or in general the NP flavor problem)."
228,489,0.985,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"16.4.1 Directed OFN as a Combustion Trend Linking the trend with the directing is associated with reaching the objective of a certain state. In the case when the water temperature of the stove is low, the OFN describing this variable should be positive: Fig. 16.10a, which seeks directing to the boundary point Âµ A ( p A ) = 0. Selecting âdownâ of the OFN emphasizes what value the OFN is going to achieve. For example, the stove water temperature can be a linguistic variable, as shown in Fig. 16.13. A fuzzy number A indicates that the"
291,23,0.985,Visualizing Mortality Dynamics in the Lexis Diagram (Volume 44.0),"Restate my assumptions: One, mathematics is the language of nature. Two, everything around us can be represented and understood through numbers. Three, if you graph the numbers of any system, patterns emerge. Sean Gullette as Maximilian Cohen in the movie  (1998)."
148,212,0.985,Anti-fragile ICT Systems (Volume 1.0),"According to the fail fast principle in Chap. 4, we need to learn from systemsâ abnormal behavior and downright failures to achieve anti-fragility to classes of negative events. The earlier we can detect problems, the smaller the negative consequences are and the faster we can start learning how to improve the systems. Since humans are not good at detecting anomalies, especially in streaming data from large cloud applications, a form of automatic anomaly detection is needed. There are many ways to detect anomalies, depending on which complex adaptive system we consider. For example, Internet banking solutions employ a rich set of heuristics to detect fraud [26]. This first chapter of Part IV introduces a general learning algorithm based on Hawkinsâs developing theory of how the brain learns, called hierarchical temporal memory (HTM) [27, 96]. The HTM learning algorithm, or just HTM, is used in the next chapter to detect anomalies in a systemâs behavior. HTM was earlier referred to as the cortical learning algorithm. The underlying basis for the HTM learning algorithm is not easy to understand and the algorithm itself is still being developed. To grasp HTMâs novelty and importance, the current chapter first discusses the approach to learning taken by traditional artificial intelligence (AI) research, as well as efforts to âtrainâ artificial neural networks to realize particular inputâoutput mappings defined by data âtraining sets.â Second, the chapter outlines why HTM is an improvement over these earlier approaches. Finally, it provides a fairly detailed description of the HTM learning algorithm (with some algorithmic details left out to ease understanding). While Hawkinsâ general theory on how the brain works is very interesting, this chapter only provides enough information to understand the major steps of the HTM learning algorithm. The reader wanting to know more about the theory behind HTM should study Hawkinsâ book On Intelligence written with Sandra Blakeslee [27]. More technical information on HTM is given in a white paper [96] by Numenta (http://numenta.com), which was set up to develop the algorithm for both commercial and scientific use. An open source project, called the Numenta platform for intelligent computing (NuPIC) (http://numenta.org), provides HTM program code and documentation. YouTube (see also http://numenta.com/learn) has a growing number Â© The Author(s) 2016 K.J. Hole, Anti-fragile ICT Systems, Simula SpringerBriefs on Computing 1, DOI 10.1007/978-3-319-30070-2_11"
294,38,0.985,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"y/x as input parameter or argument. The atan function takes one argument, and the computed value is returned from atan. This means that where we see atan(y/x), a computation is performed (tan 1 .y=x/) and the result âreplacesâ the text atan(y/x). This is actually no more magic than if we had written just y/x: then the computation of y/x would take place, and the result of that division would replace the text y/x. Thereafter, the result is assigned to the name angle on the left-hand side of =. Note that the trigonometric functions, such as atan, work with angles in radians. The return value of atan must hence be converted to degrees, and that is why we perform the computation (angle/pi)*180. Two things happen in the print statement: first, the computation of (angle/pi)*180 is performed, resulting in a real number, and second, print prints that real number. Again, we may think that the arithmetic expression is replaced by its results and then print starts working with that result. If we next execute ball_angle_first_try.py, we get an error message on the screen saying NameError: name âatanâ is not defined WARNING: Failure executing file: <ball_angle_first_try.py>"
336,17,0.985,"Early Algebra : Research into Its Nature, Its Learning, Its Teaching","Australian studentsâ understanding of the use of the equal sign found evidence of a certain persistence of narrow views. Warren (2002) also noted the difï¬culties that 8- and 9-year-olds experience in handling problems with unknowns. Van Ameron (2002) similarly reported from her study of Dutch students that nudging 11-year-olds to use symbolic formulas is not productive, not even if it is done in a tentative and well-considered way. While technology was not a major component of much of the early-days research in Early Algebra, there were a few exceptions. For example, the research of Ainley (1999) and Ainley et al. (1998) showed that spreadsheets, with their algebra-like notation and graphing facility, can be quite productive with 8- to 11-year-olds as a tool for emergent algebraic reasoning. Sutherlandâs (1993) research in the ANA Logo project with 11- and 12-year-olds reported studentsâ successful use of variables to express simple mathematical relationships within the context of teacher-developed and teacher-supported tasks in the Logo programming environment."
232,311,0.985,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"limited rationality; they then select a particular mode of action, and evaluate a provisional satisfactory solution before iterating the process if necessary [6]. The comparison of potential scenarios results in mathematical formulations, developed from an economic perspective. The choice is equivalent to the optimization of a function (the utility). A set of axioms expresses postulates about the properties of preferences (for a discussion of the various theories see [7]). If an objective assessment of the probabilities of the consequences of an act is not possible, the decision maker can use âsubjectiveâ probabilities. The probability distribution is therefore measured according to their knowledge of the possible states of the object they are interested in and upon which they wish to act. Nevertheless, the assessment of subjective probabilities can be arbitrary and, in many practical situations, the behaviour of agents does not reflect preferences that are consistent with these axioms, without being necessarily called an âerrorâ [1]. To account for the assumed ignorance of the decision maker of certain states and their aversion to uncertainty, Gilboa and Schmeidler [8] showed that decisions can sometimes be seen as the maximization of a form of expected utility that takes into account the worst case scenario, which is consistent with the âmaxminâ model. It is also possible to broaden the spectrum of reactions in uncertain situations [9], in order to model less paranoid attitudes [1]. From this perspective, the behaviour of the decision maker is interpreted according to a concept of ambiguity relative to their knowledge of the worldâhe could not be sure of the meaning of the few information he got regarding the state of the facilitiesâ, which is how the questions of the Japanese investigators should be understood (see Sect. 2). Faced with ambiguity about the state of Reactor 1, it could be argued that Yoshida violated expected utility theory by not deciding to immediately cool the core; or, alternatively, he demonstrated an aversion to uncertainty, by deciding to rely on information about the water level, while simultaneously preparing cooling mechanisms.5 The fact that he did not foresee hydrogen leaks can be interpreted in two ways using the âmaxminâ model: either he failed (cognitively) in his assessment based on all of the objective information available; or his attitude or imperfect knowledge led him to limit his choices to a subset of possible states. As for his decision to send staff to gather information on the state of the reactors before considering operations such as venting, this can be interpreted as an example of incomplete preference, where the status quo is maintained until such time as a conclusively better, new alternative appears. Can we conclude that Yoshida acted irrationally, or do the models provide an incomplete description of such decisions? Gilboa [10] argues that we qualify behaviour as âirrationalâ if whoever violates its precepts regrets their actions. The regret expressed by Yoshida about the conï¬dence he placed in Reactor 1âs water level indicators (see Sect. 2) would suggest that the decision was irrational."
108,14,0.985,Bordieuan Field Theory as an Instrument for Military Operational Analysis,"and actually still is need for theory regarding military operations. A number of the articles written attempted to capture the essence of the new term, which at the time was afforded a certain currency, with the aim of loading the term with old rehashed material so that more resources could be obtained for projects initiated earlier.9 An approach that resonates with the content of this paper is offered by the researchers Michael Callan and Michael Ryan: âEffects-Based Operations are the application of military and non-military capabilities to realise specific and desired strategic and operational outcomes in peace, tension, conflict and postconflict situationsâ. The later application of the term maintained a relatively general level, which was appropriate since the approach required a high degree of generalisation. The definition is also good because it can be applied to the border area between the use of force and more peaceful means, as described in the above text. The fact is that the definition works very well from a field theory perspective underlines the relevance of the field theory approach. The focus below is on those aspects of the JFCOM (United States Joint Forces Command) interpretation that stress the winning of peace.10 The aspects of both interpretations that focus on weapons effects are not relevant for the purposes of this paper.11 The theory presented in this text offers a practical approach that can be applied in situations where conventional warfare and its concepts are not appropriate. The issue is scientifically relevant, much research, for example, having been conducted into examining terms such as the now dearly departed EBO or the current strategic communication approach. This particular term is one for which researchers and other writers have created a number of definitions, the problem having been very much one of mastering the terminology. This resulted in a series of straggling"
257,192,0.985,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","low-level details of the language implementation (such as the choice of physical addresses when allocating a block), all of these could be relevant when defining the semantics of cast. The three frame theorems (1, 3, and 4) are thus lost, because the state of unreachable parts of the heap may influence integers observed by the program. An important consequence is that secrecy is weakened in this language: an attacker could exploit pointers as a side-channel to learn secrets about data it shouldnât access. Nevertheless, integrity is not affected: if a block is unreachable, its contents will not change at the end of the execution. (This result was also proved in Coq.) Theorem 7 (Integrity-only Noninterference). Let s1 , s2 , and sâ² be states and c a command such that vars(c) â vars(s1 ), ids(s1 ) # blocks(s2 ), and [[c]](s1 âª s2 ) = sâ² . Then we can find sâ²1 â S such that sâ² = sâ²1 âªs2 and ids(sâ²1 ) # blocks(s2 ). The stronger noninterference result of Corollary 1 showed that, if pointer-tointeger casts are prohibited, changing the contents of the unreachable portion s2 has no effect on the reachable portion, sâ²1 . In contrast, Theorem 7 allows changes in s2 to influence sâ²1 in arbitrary ways in the presence of these casts: not only can the contents of this final state change, but the execution can also loop forever or terminate in an error. To see why, suppose that the jpeg decoder of Sect. 1 is part of a web browser, but that it does not have the required pointers to learn the address that the user is currently visiting. Suppose that there is some relation between the memory consumption of the program and that website, and that there is some correlation between the memory consumption and the identifier assigned to a new block. Then, by allocating a block and converting its pointer to a integer, the decoder might be able to infer useful information about the visited website [22]. Thus, if s2 denoted the part of the state where that location is stored, changing its contents would have a nontrivial effect on sâ²1 , the part of the state that the decoder does have access to. We could speculate that, in a reasonable system, this channel can only reveal information about the layout of unreachable regions, and not their contents. Indeed, we conjecture this for the language of this subsection. Finally, it is worth noting that simply excluding casts might not suffice to prevent this sort of vulnerability. Recall that our language takes both offsets and identifiers into account for equality tests. For performance reasons, we could have chosen a different design that only compares physical addresses, completely discarding identifiers. If attackers know the address of a pointer in the programâ which could happen, for instance, if they have access to the code of the program and of the allocatorâthey can use pointer arithmetic (which is generally harmless and allowed in our language) to find the address of other pointers. If x holds the pointer they control, they can run, for instance, y â alloc(1); if x + 1729 = y then . . . else . . . , to learn the location assigned to y and draw conclusions about the global state."
8,770,0.985,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","26.5 Conclusions Despite a great number of well worked out partial theoretical models, we do not yet know enough to build a theory which describes coherently the whole of the relativistic heavy ion collision. Under these circumstances, even the detailed models cannot be adequately tested since we have not yet learned to disentangle the dozen or so different mechanisms mixed up in an the relativistic heavy ion collision. There exists, however, a fully worked out analogue computer programme based on the one and only true, complete theory: the relativistic heavy ion collision experiment. As we do not know the programme, but only its output for a given input, learning the theory from it is far from trivial. We should try to force it to give answers to the following unsettled questions:"
192,275,0.985,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"This type of discourse starts off with the question raised by Leah, who indeed plays the role of psychoanalyst in the novel,2 namely: Que voi? What do scientists want (p. 66)? Initially, their cupido sciendi is bent on symbolisation: replacing the physical, phenomenal suffering of cancer patients by the noumenal, structural formula, for instance concerning âsilveryâ arginine (C6H14N4O2). For Cantor, however, the ultimate trophy is something even more symbolical, namely the gold medal, the Nobel Prize (that which pulls him into action: a as agent, upper-left position). This alluring entity provokes him and transforms him (the allegedly self-composed professor) into a craving subject, suffering from hysteria chemicorum, but in a contemporary form: the tendency of researchers to forget or disavow their vocation, the constraints of their profession (S2 now in the lower-left position), so as to give in to âperverse incentivesâ (coming from a). His counterpart Stafford, perhaps disillusioned by the realities of laboratory life, moves in the opposite direction: from the noumenal, symbolical world of chemical formula and academic credits back into the ârealâ world of biomedical (evidence-based) health care. The by-product of the narrative is a normative lesson (S1 in the lower-left position). As the author argues in his Afterword, although the science in the novel is fictitious, the ethics (the questionable research practices, such as data trimming, data smoothing, etc.) is not (p. 229). And the primary goal of the discourse of the analyst is normative insight (finding out the truth concerning your desire)."
269,85,0.985,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"get off the ground than collaborations with, say, entomologists. We might nonetheless claim that Cacioppo and Berntson set the stage for a contemporary research landscape that â at least in certain corners â is attending carefully to the ways in which social adversity is a causal factor in mental disorder (European Network of National Networks studying Gene-Environment Interactions in Schizophrenia (EU-GEI) et al. 2014; Hyman 2009), in which the sociopolitics of stress and trauma are situated in accounts of specific psychopathologies (Galea 2011), in which specific brain regions are situated vis-Ã -vis the role they play in human social interaction (Amodio and Frith 2006), and so on. From the other direction, a range of positions in social theory have radically undercut the traditional scepticism that the social sciences (at least the qualitative social sciences) have displayed toward not only biological explanations â but any admixture of the biological and social per se. In his account of what might become of âThe Human Sciences in a Biological Ageâ, sociologist Nikolas Rose points out that, âNo longer are social theories thought progressive by virtue of their distance from the biological. Indeed the reverse assumption is common â it seems that âconstructivismâ is passÃ©, the linguistic turn has reached a dead end and a rhetoric of materiality is almost obligatoryâ (Rose 2013, 4). While retaining some critical distance from these obligations, Rose nonetheless casts a future for the social sciences in which an attention to vitality (understood not only as a philosophical commitment, but as a deep, abiding and collaborative attention to the biological stakes of human life) returns to the centre of the sociological enterprise. Similar claims have been made across a range of contemporary social theories (e.g. Braidotti 2010; Greco 2005; Grosz 2004). In our own efforts to think with and through bodies, we have been especially moved by arguments emerging from feminist science studies, and from that literature sometimes inadequately gathered together under the rubric of a âmaterial feminismâ â which has, at least since Donna Haraway first drew attention to the theoretical potency of the body positioned scientifically (Haraway 1985), pioneered such approaches. Put very broadly, a âmaterial feminismâ moves beyond an insistence on the construction of gendered bodies to ask whether feminist theory might, rather, seek resources in the materiality of the body, and in the thinking of the sciences that labour to bring that materiality into understanding. It describes a move of feminist theory beyond the âtextual, linguistic and discursiveâ, to refocus on the pains and pleasures of embodiment: âwe need a way to talk about these"
297,313,0.985,The R Book,"Inspection of the relationship suggests an exponential decay in numbers surviving, so we shall analyse a model in which log(survivors) is modelled as a function of time. There are lots of zeros at the end of the time series (once the last of the individuals was dead), so we shall use subset to leave out all of the zeros from the model. Let us try to do the regression analysis of log(survivors) against date:"
223,325,0.985,Knowledge and Action (Volume 9.0),"What Is Semantic Knowledge? What is it that you know when you know a language? Certainly, you know many words of the language (its lexicon), and you know how to put the words together in an appropriate way (the syntax). More important, you know the meaning of the words (the semantics of the language). If you do not master the meaning of the words you are using, there is no point in knowing the syntax (unless you are a parrot). You can communicate in a foreign language with some success just by knowing some words and without using any grammar. In this sense semantic knowledge precedes syntactic knowledge. This chapter focuses on an aspect of semantic knowledge that has not been well studied, its organization into domains. Children learn a language without effort and completely voluntarily. They learn new words miraculously fast. Teenagers master about 60,000 words of their mother tongue by the time they finish high school. In their speech and writing they may not actively use more than a subset of the words, but they understand all of them. A simple calculation reveals that they have learned an average of 9â10 words per day during childhood. A single example of how a word is used is often sufficient for learning its meaning. No other form of learning is so obvious or so efficient. Nevertheless, the semantic learning mechanisms show some strong asymmetries. For instance, why is it easier to explain to a 4-year-old the meaning of the color terms chartreuse and mauve than to explain monetary terms like inflation or mortgage? The difference is not a matter of word frequency: The monetary terms are more frequent, but the 4-year-old masters the semantic domain of colors and thereby knows the meaning of many color words. Adding new color terms is just a matter of learning the mapping between the new words and the color space."
311,3060,0.985,The Physics of the B Factories,"Integrating out NP particles we obtain corrections to the eï¬ective weak Hamiltonian. However, since NP is formally invariant under GF we know its flavor structure. We just need to construct an eï¬ective weak Hamiltonian that is GF invariant, and all the breaking comes from YU and YD . This then also fixes the allowed flavor breaking. The Yukawa coupling matrices YU and YD are not aligned and are diagonal in diï¬erent bases for QL . The misalignment leads to flavor changing charged currents, JCÎ¼ = uÌL Î³ Î¼ V dL , with V being the same as the CKM matrix. In this section we focus on a particular realization of MFV â the so-called constrained minimal flavor violation (cMFV) (Blanke, Buras, Guadagnoli, and Tarantino, 2006; Buras, Gambino, Gorbahn, Jager, and Silvestrini, 2001). The assumptions that underlie cMFV are (i) the SM fields are the only light degrees of freedom in the theory, (ii) there is only one light Higgs and (iii) the SM Yukawa couplings are the only sources of flavor violation. The NP eï¬ective Hamiltonian for qj â qi processes following from these assumptions thus has exactly the same CKM suppression and form of the eï¬ective operators as in the SM. This is sometimes taken to be the definition of cMFV (Blanke, Buras, Guadagnoli, and Tarantino, 2006; Buras, 2003; Buras, Gambino, Gorbahn, Jager, and Silvestrini, 2001). For instance, the eï¬ective Hamiltonian for the ÎF = 2 transitions is"
342,62,0.985,Semiotics in Mathematics Education,"(Greeno 1988). To assist students in learning the relation between mathematical graphs and the physical phenomena they are investigating, some textbooks layer different sign systems with the apparent intention of offering students a way to link particular aspects of one system to a corresponding aspect in the other (Roth et al. 2005). Using an example from a Korean science textbook, a graph exhibiting Boyleâs law relating the volume V and pressure of an ideal gas (V * 1/p) is overlaid by (a) the images of a glass beaker with different weights and (b) differently sized arrows (i.e., weight vectors) corresponding to the weights. The authors suggest that these complexes of sign systems may be difï¬cult to unpack because relationships emerge only when students structure each layer in a particular way so that the desired relationships then can be constructed."
360,129,0.985,Compositionality and Concepts in Linguistics and Psychology,"7.2 Subjectivist Two-Tiered Theories Subjectivist theories that attempt to also account for the âobjective realityâ aspect of language all have the feature that they replace the Objectivist slogan of language representing reality with the slogan that (some aspect of) peopleâs mental lifeâfor instance, their mental conceptsârepresents reality. It would seem to take some fancy footwork to justify the replacement of an unexplained notion of representation with another unexplained notion also to be called a representation. One way that some Subjectivists have responded to the challenge that they need to have two distinct tiers was to take the viewpoint that they can use the distinction between peopleâs judgments of membership in a category and their judgments of the typicality of membership in that category. The membership judgment could perhaps fill the role of the objective-tier membership in that category. But from the Objectivist point of view using the judgements of membership as a substitute for âreally being a member of the categoryâ is the sort of idea that will give rise to Lewisâs âconfusionâ. At the best it can be evidence for membership in the category, but is is clearly âcontaminatedâ (the Objectivist claims) by all sorts of other biases. We can see that this canât really be the Objectivistâs âfact about realityâ by considering some of the actual experimentation that tests this. For example, the fact that subjects will judge a middle-height person as âboth tall and not tallâ in strong preference both to âtallâ and to ânot tallâ (Alxatib and Pelletier 2011) does not in the least tend to support any theory about the reality of people manifesting contradictory properties."
113,186,0.985,Pentecostalism and Witchcraft : Spiritual Warfare in Africa and Melanesia,"has allowed people to make microphones, satellites, and even nuclear energy is diabolical. The only way scientists have obtained this knowledge is through an engagement with death.â Pastor DieudonnÃ© earned a License degree in physics (equivalent of an M.A.) from the University of Kinshasa (UNIKIN). After working as a teaching assistant at the university for a few years, he abandoned the academic world because he found it increasingly difficult to reconcile his faith with his work as a scientific researcher. For Pentecostal Christians, any knowledge that distracts the Christian from Godâs path is perceived as anti-Christian. Knowledge in itself is not diabolical or divine, rather the difference lies in the source of knowledge. Some knowledge is God-given (euti epay ya Nzambe). Most Christians have received this gift of knowledge (bwanya ya Nzambe), which allows them to understand the world, to interpret signs of the material world as metonyms of invisible powers and spirits, and act upon it (Pype 2012: 112â115). However, other kinds of knowledge are inspired by the devil (ya Satani). To understand whether certain knowledge is âChristianâ or not, one should look into its consequences: does it sow death, as, for instance, the atomic bomb? Or does it harm people, as do weapons, for example? The types of knowledge that do not âcome from Godâ are abstract and irrelevant for daily social life. Pastor DieudonnÃ© explained how he had refused to continue his postgraduate studies at the university. Having finished his License degree with a dissertation on metallurgy, he felt uncomfortable with the level of abstraction that studies in physics entail. The mysteries of abstract thinking undoubtedly lead to the devil. According to pastor DieudonnÃ©: There were certain demands that I could not perform. Especially things that you cannot grasp with your five senses. We have five senses, but some scientific experiments rely on another sense. As a Christian, my 6th sense is my faith. If I cannot feel, taste, hear, see, or smell something then I need to believe that God is there. But, I know that in those sciences, people do not rely on God."
244,981,0.985,"Advancing Human Assessment : The Methodological, Psychological and Policy Contributions of Ets","In light of the limited conception of both âverbal abilityâ and âwriting abilityâ at the time, Huddlestonâs conclusions appear, in retrospect, to be unnecessarily strong and overreaching. The evolving conception of âverbal abilityâ continues to this day, and it is only recently that even basic skills, like vocabulary knowledge, have become better understood (Nagy and Scott 2000); it was not by any means settled in the early 1950s. Importantly, readily available research at the time was clearly pointing to a more nuanced understanding of writing ability. Specifically, the importance of the role of âfluencyâ in writing was beginning to emerge (C. W. Taylor 1947) well within the psychometric camp. Today, the assessment of writing is informed by a view of writing as a âcomplex integrated skillâ (Deane et al. 2008; Sparks et al. 2014) with fluency as a key subskill. By todayâs standards, the scope of the concept of reliability was not fully developed in the 1930s and 1940s in the sense of understanding the components of unreliability. The conception of reliability emerged from Spearmanâs work (see Stanley 1971, pp. 370â372) and was focused on test-score reliability. If the assignment of a score from each component (item) is error free, because it is scored objectively, then the scoring does not contribute error to the total score, and in that case score reliability is a function of the number of items and their intercorrelations. In the case of constructed responses, the scoring is not error free since the scorer renders a judgment, which is a fallible process.6 Moreover, because items that require constructed responses require more time, typically, fewer of them can be administered which, other things"
297,553,0.985,The R Book,"Up to this point our response variable was shown as a scatter of data points. In many cases, however, we want to show the response as a smooth curve. The important tip is that to produce reasonably smooth-looking curves in R you should draw about 100 straight-line sections between the minimum and maximum values of your x axis. The Ricker curve is named after the famous Canadian ï¬sh biologist who introduced this two-parameter hump-shaped model for describing recruitment to a ï¬shery y as a function of the density of the parental stock, x. We wish to compare two Ricker curves with the following parameter values: yA = 482x eâ0.045x ,"
192,337,0.985,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"Solar quite convincingly explains how, as a young researcher, Michael Beard had been an isolated, introverted, highly committed, hyper-individual quantum physicist. As an ageing scientist, however, his situation has completely changed. A new arena of âconverging researchâ has emerged, in the intermediate zone between nano-technology, photovoltaics and climate politics. From the 1950s onwards, physicists (with their high-tech contrivances and advanced mathematics) migrated towards the life sciences, employing their powerful physical technologies to understand and mimic the basic processes of life. Artificial photosynthesis, as a sub-field of biomimesis (i.e. the use of biotechnology to mimic living nature on the molecular level), is an exemplification of this trend. Thus, the epistemological backdrop of the narrative is a transformation that is actually taking place in laboratories world-wide, where biotechnology is evolving into bio-mimesis, i.e. mimicking (âcopy-pastingâ) nature on a molecular scale (Church and Regis 2012; Zwart et al. 2015; Blok and Gremmen 2016). In principle, this biomimetic turn entails a positive ambition. The aim is to develop technologies which, although highly advanced, are nonetheless more sustainable and naturefriendly than the technologies which humankind managed to produce so far. Indeed, artificial photosynthesis basically aims to see plant leaves as biological factories from which human technology still has a lot to learn in terms of efficiency, sustainability and circularity. Nature is the paradigm, the teacher (natura artis magistra) for molecular life scientists and bioengineers, notably on the quantum or nanoscale. The down-side is that there is a lot of investment, prestige and politics involved in this type of research, so that it runs the risk of becoming tainted by privatisation, commercialisation and politicisation. This transformation (presented in Solar as an emerging scientific-industrial ârevolutionâ, p. 36, p. 211, p. 336; as a ânew chapter in the history of industrial civilisationâ, p. 293) is quite credibly reflected in the novel, and it is clear that author Ian McEwan has conducted a considerable amount of preparatory research.1 Although Beard is said to hold âan irrational prejudice against physicists who defected to biology, SchrÃ¶dinger, Crick and the likeâ (p. 121), he basically follows in their footsteps, moving from âpureâ quantum physics2 to âappliedâ molecular life sciences research. Yet, the most dramatic discontinuity in his career is not the shift from basic physics (studying photons and electrons) to biomimesis, but from original research"
140,154,0.985,R.J. Rummel : An Assessment of His Many Contributions,"of being able to test warring hypotheses from different contending schools of IR. There are differences between system and ï¬eld, in that a system has directed relations among its component parts, with feedback (e.g., A attacks B, B defeats A, B changes the system, establishing an international organization to try to prevent attacks; or A attacks B because of a certain systemic power distribution, then war changes the power distribution, affecting the probability of subsequent wars), whereas in ï¬eld theory there usually is not such a âcyberneticâ set of relationships and feedback loops. Instead, in a force ï¬eld, there can be continuous pair-wise repulsion or attraction. As explained in his ï¬ve-volume study, Understanding Conflict and War, Rummel (1975) organized his thinking around a certain type of ï¬eld theory. In practice, ï¬elds involve variables whose associations or correlations have an angular expression, which leads to Rummelâs (1970) most widely-cited publication, Applied Factor Analysis, about a statistical tool for determining the number of dimensions in a space we need to study, and telling us the coordinates of the variables and cases in the resulting or âreducedâ space. A ï¬eld can be a geographic grid, such as a farmerâs ï¬eld or the longitudes and latitudes of Earth; an abstract, mathematical expression of this, such as the Cartesian coordinates; or a force ï¬eld in such a spatial array, in which forces acting at a distance, such as electromagnetism or gravity, pull objects around. Rummel (1975) distinguishes âa region of spaceâ from âthe area in which a force operatesâ, and says the former undergirds spatial theory, the latter, ï¬eld theory. He is concerned with âintentionalâ ï¬elds that one would ï¬nd in psychological and social work (the latter including Wright, 1955). This orientation became central to Rummelâs DON project. Rummel earned his Ph.D. at Northwestern University, where he developed DON for his dissertation while under the tutelage of Harold Guetzkow, one of the pioneers in the behavioral revolution in international relations. Guetkow was searching for cross-national data for his Inter-Nation Simulation, and this led him to imagine what became the DON Project. In that crucible, âthe earliest event-data project of major proportions began in 1962 â¦ [when] Rummel assembled data â¦ originally for use in Harold Guetzkowâs Inter-Nation Simulationâ (Merritt, Muncaster & Zinnes, 1993: 4). By contrast, Singerâs COW Project, launched the next year, was Singerâs own idea from the outset, and as such would become a more central and hence more persistent part of Singerâs professional identity and life work. After Rummel had dealt with missing data by an iterative processâbeginning with variables that had little missing data, and then gradually ï¬lling in estimates of values for the other variables, a phase one of the project emerged, with nations placed in a multidimensional space of numbered factors. There was a lack of association between domestic instability and foreign conflict. At the start of his career, however, Rummel (1966) indicated where he was going with this null ï¬nding; his words anticipated DONâs phase two, the dyadic studies: Foreign conflict behavior is not internally derived. Its genesis lies outside the nation. It is a relational phenomenon depending on the degree of economic, social,"
342,60,0.985,Semiotics in Mathematics Education,"language and perception. In the case of the translation of statements in natural language into the standard algebraic symbolism, Radford (2002b) noticed that the ï¬rst algebraic statements are not only imbued with the meanings of colloquial language, but also colloquial language lends a speciï¬c mode of designation of objects that conflicts with the mode of designation of objects of algebraic symbolism. He discusses a mathematical activity that was based on the following short story: âKelly has 2 more candies than Manuel. JosÃ©e has 5 more candies than Manuel. All together they have 37 candies.â During the mathematical activity, in Problem 1, the students were invited to designate Manuelâs number of candies by x, to elaborate a symbolic expression for Kelly and JosÃ©e, and, then, to write and solve an equation corresponding to the short story. In Problem 2, the students were invited to designate Kellyâs number of candies by x while in Problem 3 the students were invited to designate JosÃ©eâs number of candies by x. Radford suggests that one of the difï¬culties in dealing with problems involving comparative phrases like âKelly has 2 more candies than Manuelâ is being able to derive non-comparative, assertive phrases of the type: âA (or B) has Câ. If, say, Manuel has 4 candies, the assertive phrase would take the form Â«Kelly (Subject) has (Verb) 6 (Adjective) candies (Noun)Â». In the case of algebra, the adjective is not known (one does not know how many candies A has). As a result, the adjective has to be referred to in some way. In using a letter like âxâ (or another device) a new semiotic space is opened. In this space, the story problem has to be re-told, leading to what has been usually termed (although in a rather simplistic way) the âtranslationâ of the problem into an equation. Radford suggests the term symbolic narrative, arguing that what is âtranslatedâ still tells us a story but in mathematical symbols. (Radford 2002b). He shows that some of the difï¬culties that the students have in operating with the symbols are precisely related to the requirement of producing a collapse in the original stated story. This he terms the collapse of narratives, adding The collection of similar terms means a rupture with their original meaning. All the efforts that were made at the level of the designation of objects to build the symbolic narrative have to be put into brackets. The whole symbolic narrative now has to collapse. There is no corresponding segment in the story-problem that could be correlated with the result of the collection [addition] of similar terms. (Radford 2002b, Vol. 4, p. 87)"
82,414,0.985,Fading Foundations : Probability and The Regress Problem,"The black area contains the points that belong to the Mandelbrot set. Each point corresponds to a complex number, c, being the ordered pair of the Cartesian coordinates, (x, y). The edge of the Mandelbrot set forms the boundary between those values of c that are members of the set and those that are not. It is this boundary, the âMandelbrot fractalâ, that has the wellknown property of being infinitely structured in a remarkable way: no matter how far you zoom in on it, you will always find a new structure that is similar to, although not completely identical with the Mandelbrot set itself. Our aim in this section is to demonstrate that, on condition that Î± + Î² = Î³ + Î´ , the quadratic relation (8.9) is equivalent to the Mandelbrot iteration (8.10). As it turns out, c will be a function of the conditional probabilities Î± , Î² , Î³ and Î´ alone, and will thus be a known quantity. The qâs, on the other hand, will be directly related to the unconditional probabilities; these are unknown and their values are to be determined through the iteration. It will prove convenient first to define Îµ as the average of the conditional probabilities Î³ and Î´ , that is"
107,111,0.985,"Symbiotic interaction : 5Th international Workshop, Symbiotic 2016, Padua, Italy, September 29â30, 2016, Revised Selected Papers","and grammatical violations on speech listening [2]. However, a recent research path moves from the measurements and the relative understanding of the basic characteristic of the sensory perception towards the more complex way in which we, as specie, perceive and enjoy the reading and listening of stories [3]. In this attempt, researchers have started to measures cerebral and emotional activities manipulating some factors that could alter or increase the pleasure in the story perception [4, 5]. Furthermore, the knowledge of the features modulating the emotional involvement in the reader could be also implemented in designing e-books [6], also potentially based on a symbiotic interâ action with the user. Additionally, the neurophysiological approach could provide measâ urable information on the emotional content of a text, as evidenced by the lowering of the heart rate and the variation of the breathing and the electroencephalographic (EEG) alpha rhythm asymmetry [7]. In this scenario, neuroaesthetics is an emerging disciplines investigating the biologâ ical underpinnings of aesthetic experiences, and since its formal birth in 90s by Semir Zeki (for a review [8]) it constitutes a ï¬eld receiving growing interest by the scientiï¬c community. Although traditional topic of neuroaesthetic investigation are visual art and music [9, 10], an enlargement in the horizon of artistic items to be investigated is beginâ ning and literature and poetry are experiencing a renewed scientiï¬c interest from the neuroaesthetics perspective [3]. That is, the question we would like address here is âWhich biological underpinnings make classical authors cognitively and emotionally engaging?â. Two aspects of poetry may contribute to the emotional responses it may elicit: its lexical content and its structural features (i.e., poetic form). Meter and rhyme constitute the key features in poetry [11], additionally they have been found to be signiï¬cant contributors to the aesthetic and emotional perception of poetry [4], but it is diï¬cult to investigate the contribution of the content âper seâ due to the needing to compare diï¬erent kinds of texts. A possibility to overcome this limitation is oï¬ered by the famous Italian XIV century poem the âDivina Commediaâ by Dante Alighieri (1265â 1321). The book is characterized by a repetitive and constant structure, as it is composed by three parts (canticas: Inferno âHellâ, Purgatorio âPurgatory- and Paradiso âParaâ dise-), each part is composed by thirty-three cantos, for an average length of 142 verses. The verse scheme used, âterza rimaâ, is hendecasyllabic (lines of eleven syllables), with the lines composing tercets relying to the rhyme scheme xyx yzy z. Each cantica, despite the conservation of the metric structure is characterized by a diï¬erent content. The reader follows Danteâs journey through Hell, Purgatory and Paradise, where the poet respecâ tively meets damned souls, then souls expiating their sins and ï¬nally enjoys the vision of God. Studying the reaction to the exposure to stanzas belonging to each of the three canticas would enable to test the eï¬ect of the content of the excerpts, maintaining the meter and the rhyme scheme constant. Furthermore, the comparison among diï¬erent canticas would enable to assess the eï¬ect of a potentially diï¬erential previous knowledge of the peculiar cantica. In this work, we described the results of a pilot study that used neurometric indexes during the listening of a selected pieces of the Divina Commedia in a reduced sample of voluntary participants. Noteworthy, half of the participants had a literary formation in their advanced studies (Humanist; they are students of literature at the University) while the other half of the sample are attending other university courses (Not Humanist)."
233,61,0.985,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"Human Emotional Responses to the Natural World It is also claimed that biodiversity is valuable because the psychological makeup of human beings causes them to feel an intimate connection with the natural world which might be expressed variously in emotions such as love of, or respect for, nature. The idea that such emotional responses are a result of our evolved psychology was promoted by Wilson (1984) and Kellert and Wilson (1993). We note that the so-called Biophilia Hypothesis has received limited support in the literature (Simaika and Samways 2010 p. 903), but let us assume for the moment that we do share a common innate love of nature. There are two important problems with grounding conservation in common emotional responses. Firstly, such responses are not always reliable guides to rational action. There is after all some fundamental fact about human beings that also causes them to see cigarettes as valuable. We donât think that this implies that we should âconserveâ cigarettes, because we donât think that this common emotional response is adaptive. Human beings feel positively disposed toward all sorts of things that are not actually good for us. But if we must then judge the adaptiveness of our feelings toward biodiversity, it seems that conservation justiï¬ed thereby would not be a consequence of our feelings towards biodiversity, but rather of the utility of biodiversity to human populations (to which we turn shortly). Secondly, people clearly differ a great deal in the extent to which they feel positive emotions toward biodiversity (Einarsson 1993). If a general measure of biodiversity is to be inferred from emotional responses to biodiversity, then it seems that we will either have to discount the responses of outliers or average across a relatively large range of responses. Finally, this style of justiï¬cation for conservation suffers from the same problems as conservation based on intrinsic value. Even if it were true that almost everyone attached the same equally strong positive emotion to the conservation of the biosphere, it is hard to see how we could turn universal love of nature into a practically applicable general measure of biodiversity. For these reasons, we think it"
232,304,0.985,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"The word âdecisionâ typically refers to the âend of the deliberation in a voluntary act that results in the choice of an actionâ.1 The meaning given to each of these terms varies according to different schools of thought. In Aristotleâs Nicomachean Ethics, the decision relates to the available resources needed to reach a given, desirable, conclusion, while Cartesians (e.g. RenÃ© Descartes in his Metaphysical Meditations, 1641) would argue that without Circeâs advice, Odysseus was taking a decision that was beyond his ability to understand and he therefore did not choose. Leibniz argues in his Theodicy (1710) that understanding is not a necessityâit only provides a guideâand that the decision exists only through the effort of action. In terms of expected utility theory [1], Odysseus rationally opted for the path that minimized the maximum damage. Beyond these considerations, if the sacriï¬ce of companions can be made acceptable, it must be integrated into a social and symbolic universe that gives it meaning. It is the will of the gods that allows Odysseus to return to his kingdom, which places him above other men. Of course, nowadays Man tends to be emancipated from the gods and can think for himself. He decides after careful reflection on causes, which âmust always be mixed with chance in order to form a basis for reasoningâ.2 However, despite this distancing that is at the heart of Technology, the human being must still ï¬nd meaning in their actions. In his testimony [2], the Director of the Fukushima Daiichi power plant shows how operators, who were obliged to decide between the survival of some and the sacriï¬ce of others, gave meaning to such decisions. Some of their critical decisions are set out below (Sect. 2). The inability of current theories to account for the magnitude of such decisions (Sect. 3) leads us to introduce the concept of âprojected timeâ (Sect. 4), and to explore mechanisms for the development of meaning in extreme situations (Sect. 5)."
214,468,0.985,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"simpliï¬es the range of choices important for a particular problem. Many planning and management activities have small spatial scales and timescales of the next three to ï¬ve decades; thus, the choice of emissions scenario is less important and the representation of internal variability is more important. Large model uncertainty argues strongly for intensive speciï¬c evaluation of model uncertainty (e.g., if the issue is snowpack, is the current temperature correct in the model?) or even the use of multiple models (multiple-model ensembles). Scenario uncertainty is also associated with the impacts side of the problem. Recall the example of the impact of tropical cyclones on a coastline. That impact will change over time with the built environment, as well as with changes in cyclones. So the scenario for practitioners may also include aspects of the human or natural system impacts that are not treated by the climate model. One difï¬culty is consistency: If impact scenarios are estimated, like the built environment on a coastline, they should be consistent with the climate scenarios. The human side of the problem may matter between different scenarios, even if the climate projection is similar, as in the case for the example of the built environment around a coastline. Even if cyclones do not signiï¬cantly change in 30â50 years, the built environment might very well change. The human narrative in scenarios for climate prediction is evolving in 2015, and the current versions of climate scenarios (beyond the RCPs that predict emissions) are called Shared Socioeconomic Pathways (SSPs).7 These scenarios contain not just emissions, but also the growth assumptions used to estimate the emissions, and a narrative describing the assumptions about the future of society. Initial condition uncertainty is most important when the goal of the simulation is to represent actual âclimate forecastsâ rather than representative âclimate projections.â Projections are often conditional: Given a set of emissions, the expected result is a speciï¬c climate. But a forecast is more speciï¬c. What will happen to the climate in 2020, or 2055 (the latter is more of a projection). The deterministic weather forecast problem is a classic example of a problem that relies strongly on accurate and complete initial conditions. In climate applications, the early decades of a simulation depend strongly of the initialization of the ocean. It is possible for the same model to determine quite different climates from different initializations, which is a motivation for ensemble results. Even if the initializations were (impossibly) perfect, model errors would lead to imperfect forecasts. There are methods for mathematical quantiï¬cation of uncertainty. These methods for uncertainty quantiï¬cation involve understanding how perturbations to the different sources of uncertainty change the results. The computational demands of climate models as well as the complexity make brute-force methods of uncertainty quantiï¬cation impractical. The uncertainty in a few parameters can be assessed explicitly, by varying different parameters over a range. This is hard to do with"
108,9,0.985,Bordieuan Field Theory as an Instrument for Military Operational Analysis,"This was a young researcher still strictly empirical, creating the foundation for his theories. Just recently his book Algerian Sketches was released post-mortem where he writes about Algeria, the Algerian conflict and his theories. Later on, Bourdieu found that much of the social mechanisms he had found among the Kabyle were present in all manners of other social contexts. Bourdieu was one of the foremost French thinkers of the late 1900s and had great influence on many humanist and social science disciplinesâmainly in Europe. Those who have not read his work consider his thinking to be post-modern, but strictly speaking, he is not of the deconstructive school. He saw himself as post-structuralist, which was partly an acknowledgement of structuralism, but which also showed that it was not as embedded in his thinking with the degree of rigidity normally imposed by structuralism. He has, for example, done extensive research in the fields of art, literature and even studied his peers, the professors in Paris as empirical material. Bourdieu might at times be hard to understand, but his theories are based strictly on empirical material which he then has generalised to theoretical systems. With the passage of time, he introduced new terms, for example the term field, which we will return to later on. For more than a decade, I myself have used and expanded Bourdieuâs theories and found that there is a case to state that they are valid for studies and practice regarding Military Operations as well. This text should be seen as an introduction of the theories into the military field, especially regarding the field of intelligence analysis. In military context, the theories can be used to make social patterns visible. If you have good empirical material and a good analysis, then you will be able to see how actors (collective as well as individual) relate to each other. In the end, one can be both able to predict what will happen (with a certain degree of certainty) and able to make the other part (enemy or not) behave in a certain way, without them knowing that you are manipulating them. I would like to underscore that this text is primarily focused on the theory and not the empirical aspect. It only serves as a way to exemplify the theory and show its usefulness in operational analysis. I recommend the bibliography at the end for those interested in the empirical events briefly explained in this text. It should be stressed that field theory is more of an object than a tool in the text. The text is primarily a tool for training, inspire primarily military officers how to use field analysis. The overall aim of this text is to present an alternative approach to"
78,104,0.985,The Onlife Manifesto : Being Human in a Hyperconnected Era,"There can be no doubt that the information and communication technologies (ICT) deeply impact the human society. The difficulty in appraising their effect and anticipating the concomitant changes lies in the depth of that impact. In an attempt to understand the present evolutions, we propose to uncover the underlying structure of this new world by revisiting its dependencies on the hyper-connectivity on which it is grounded, and the consequences of this hyper-connectivity, in modifying profoundly the network of inter-individual relations. Where we used to have ten to fifty close friends living near us, with whom we shared convivial relations, we may now have hundreds of acquaintances living on other continents, with whom we currently exchange specialized information about our main fields of interest that can be professional, artistic or related to any kind of hobby. It naturally follows from these major changes in the scale and nature of individual relationships, that the social fabric is dramatically evolving. Therefore, to quote Aristotle, since âman is by nature a social animal,â humanity is changing because society is changing. But, how are humans and society changing? And, what does it mean to be human, in this new society? These are the questions we would like to address. Besides, the hyper-connected world is also a world of hyper-memorisability, where all the information is stored in huge databases and accessible anytime from anywhere, without any oblivion. And, it is a world of hyper-reproducibility and hyper-diffusibility, where all the knowledge, and more generally, all the works of the mind, i.e. all the music, all the paintings, all the movies, etc. can be freely and massively reproduced and diffused. So, both the way in which individuals access knowledge and their internal memories are deeply modified, which transforms human cognitive abilities."
165,132,0.985,New Methods for Measuring and Analyzing Segregation,"the degree to which the rescaling exaggerates quantitative differences on p is greater when groups are unequal in size as seen in the White-Asian comparison. Accordingly, the G-S and D-S discrepancies can be especially large in such comparisons. This raises the question, âWhy is it appropriate to score y in a way that dramatically amplifies group differences in contact with Whites as observed in this example?â Relatedly, âIn what way is the exaggerated difference of 59.1 points on y scored for G and 100.0 points for y scored for D more sociological meaningful than the smaller difference of 23.8 points for y scored for S?â Perhaps compelling answers to these questions can be given. For now, however, the measurement literature does not provide a ready answer and I am skeptical that a compelling answer can be advanced. Regardless, it will remain the case that in these segregation comparisons examining S and its component terms reveals important information that would be missed if one looked only at G and D. Specifically, S documents that White-Asian segregation does not involve group residential separation and neighborhood polarization whereas White-Latino segregation and especially White-Black segregation do. The practical implication is straightforward; one cannot safely assume that high values of G and D indicate a prototypical pattern of segregation. One must also examine S to draw a safe conclusion on this issue."
311,1215,0.985,The Physics of the B Factories,"Ï1 with B â J/ÏKS0 . The theoretical issues associated with this approach are described in Section 17.7.1.1, and the corresponding experimental treatment is summarized in Section 17.7.3.1. It is worth noting that new physics (NP) could enhance penguin contributions significantly. Experimentally one could identify such contributions by observing a significant diï¬erence between values of Ï2 obtained using diï¬erent decay modes. The impact of penguin amplitudes in general is diï¬erent for diï¬erent final states, and as bd â uudd decays can be used to measure Ï2 , it became necessary to explore experimentally and theoretically more diï¬cult scenarios in the hope that nature was kind enough to permit measurement of this angle in one or another way. Having determined that the measurement of Ï2 via B 0 â ÏÏ would be less sensitive than anticipated, the B Factories approached the problem using a rather diï¬erent technique: a timedependent analysis of the Dalitz plot of B 0 â Ï + Ï â Ï 0 . The theoretical issues related to this measurement are introduced in Section 17.7.1.2, while the corresponding experimental discussion can be found in Section 17.7.4. The resulting constraints obtained from BABAR and Belle data do not add a significant amount of information to improve the accuracy of the SM solution for Ï2 , however they suppress the discrete ambiguities arising from the interpretation of other measurements. For the future higher statistics experiments, the decay B 0 â Ï + Ï â Ï 0 is expected to dominate the experimental determination of Ï2 and to provide a sensitive probe for the impact of NP and its features as a non-leading source of CP violation. After several years of data taking it became apparent that extraction of Ï2 from the B Factories is a diï¬cult enterprise. Thus it was realized that one has to think about other final states and BABAR started to investigate other related options such as B â ÏÏ decays. They were previously dismissed by the community as experimentally and theoretically too challenging to be a viable alternative compared with the already ambitious attempts to study B â ÏÏ and B â ÏÏ. When the experimental work commenced, the outcome of this endeavor was not entirely clear; however, there were hints that indicated these modes could be more promising than originally thought. The presence of two vector particles in the final state meant that one would have to perform a full angular analysis of the final state (see Chapter 12) in addition to constraining penguin contributions. However, it was possible to piece together suï¬cient information from various sources in order to motivate attempting the measurement of Ï2 with B â ÏÏ decays. Ultimately a full angular analysis was not required to constrain Ï2 as the fraction of longitudinal polarization in B â ÏÏ decays was found to almost completely dominate (Section 17.7.3.2). The result of this approach turned out to provide the most stringent constraint on Ï2 , where the eï¬orts of BABAR and Belle are summarized in Section 17.7.3.2. The time-dependent analysis of B 0 â Ï0 Ï0 promises to help resolve some of the discrete ambiguities inherent in the isospin analysis and is discussed in Section 17.7.3.3. An additional cross-check"
305,162,0.985,Quantum Computing for Everyone,"we can add a little delay by changing a straight-line path to one like the one depicted in figure 6.21. By putting mirrors in the appropriate places and adding delays, we can construct the gate so that the outputs are lined up with the inputs and when balls enter at the same time they leave at the same time. (This is depicted in figure 6.22.) We can then form circuits that contain more than one Fredkin gate.* Since the Fredkin gate is universal, it can be used to construct any boolean circuit. Consequently, any boolean circuit can be constructed using just billiard balls and mirrors. Fredkin believes that the universe is a computer. He didnât convince Feynman of this, but the billiard ball computer did impress him. As they both realized, any slight error in the position or velocity of a ball would result in an error that would propagate and get amplified. Collisions are never perfectly elastic; there is always friction and heat is lost. The billiard ball computer is clearly just a theoretical machine, not something that can be constructed in practice. But this machine does conjure images of atoms bouncing off one another, and it led Feynman to consider gates based on quantum mechanics rather than classical mechanics. We look at this idea in the next chapter. * There is a great animation showing this gate with balls entering and leaving on the website http://www.bubblycloud.com/billiard/fredkin-from-switches.html."
95,50,0.985,Elements of Robotics,"Chapter 4 Finite State Machines A robot can be in different states, where its reaction to input from its sensors depends not only on these values but also on the current state. Finite state machines are a formalism for describing states and the transitions between them that depend on the occurrence of events. Chapter 5 Robotic Motion and Odometry Autonomous robots explore their environment, performing actions. Hardly a day goes by without a report on experience with self-driving cars. This chapter reviews concepts related to motion (distance, time, velocity, acceleration), and then presents odometry, the fundamental method that a robot uses to move from one position to another. Odometry is subject to significant errors and it is important to understand their nature. The second part of the chapter gives an overview of advanced concepts of robotic motion: wheel encoders and inertial navigation systems that can improve the accuracy of odometry, and degrees of freedom and holonomy that affect the planning of robotic motion. Chapter 6 Control An autonomous robot is a closed loop control system because input from its sensors affects its behavior which in turn affects what is measured by the sensors. For example, a self-driving car approaching a traffic light can brake harder as it gets close to the light. This chapter describes the mathematics of control systems that ensure optimal behavior: the car actually does stop at the light and the braking is gradual and smooth. An autonomous mobile robot must somehow navigate from a start position to a goal position, for example, to bring medications from the pharmacy in a hospital to the patient. Navigation is a fundamental problem in robotics that is difficult to solve. The following four chapters present navigation algorithms in various contexts. Chapter 7 Local Navigation: Obstacle Avoidance The most basic requirement from a mobile robot is that it does not crash into walls, people and other obstacles. This is called local navigation because it deals with the immediate vicinity of the robot and not with goals that the robot is trying to reach. The chapter starts with wall following algorithms that enable a robot to move around an obstacle; these algorithms are similar to algorithms for navigating a maze. The chapter describes a probabilistic algorithm that simulates the navigation by a colony of ants searching for a food source. Chapter 8 Localization Once upon a time before every smartphone included GPS navigation, we used to navigate with maps printed on paper. A difficult problem is localization: can you determine your current position on the map? Mobile robots must solve the same localization problem, often without the benefit of vision. The chapter describes localization by trigonometric calculations from known positions. This is followed by sections on probabilistic localization: A robot can detect a landmark but there may be many similar landmarks on the map. By assigning probabilities and updating them as the robot moves through the environment, it can eventually determine its position with relative certainty. Chapter 9 Mapping But where does the map come from? Accurate street maps are readily available, but a robotic vacuum cleaner does not have a map of your apartment. An undersea robot is used to explore an unknown environment. To perform"
270,82,0.985,The Huawei and Snowden Questions : Can Electronic Equipment From Untrusted Vendors Be Verified? Can An Untrusted Vendor Build Trust into Electronic Equipment? (Volume 4.0),"5.1 GÃ¶del and the Liarâs Paradox The easiest accessible pathway into decidability is through the liarâs paradox. Although we intuitively think that a statement is either true or false, it is possible to make an expression that is inconsistent if it is true and equally absurd if it is false. The liarâs paradox is such an example: consider the statement, âThis statement is false.â If it is true, then it has to be false and, if it is false, then it has to be true; thus it can be neither true nor false. The liarâs paradox has been subject to long philosophical discussions throughout history. Its first application of relevance to our case was by the logician and mathematician Kurt GÃ¶del [6]. GÃ¶del used a slightly modified version of the liarâs paradox to prove his first incompleteness theorem. This theorem states that no theory with a countable number of theorems is able to prove all truths about the relation of Â© The Author(s) 2018 O. Lysne, The Huawei and Snowden Questions, Simula SpringerBriefs on Computing 4, https://doi.org/10.1007/978-3-319-74950-1_5"
289,744,0.985,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","SLR assertions are interpreted by (sets of) resources, which represent permissions to write to a certain location and/or to obtain further resources by reading a certain message from memory. As is common in semantics of separation logics, the resources form a partial commutative monoid, and SLRâs separating conjunction is interpreted as the composition operation of the monoid. When defining the meaning of a Hoare triple {P } s {Q}, we think of the promise machine as if it were manipulating resources: each thread owns some resources and operates using them. The intuitive description of the Hoare triple semantics is that every run of the program s starting from a state containing the resources described by the precondition, P , will be âcorrectâ and, if it terminates, will finish in a state containing the resources described by the postcondition, Q. The notion of a program running correctly can be described in terms of threads ârespectingâ the resources they own; for example, if a thread is executing a write or fulfilling a promise, it should own a resource representing the write permission."
140,98,0.985,R.J. Rummel : An Assessment of His Many Contributions,"Burnes & Cook (2013) have written an extensive review of âï¬eld theoryâ research by that name. A broader framework, however, placing ï¬eld theory in a larger social science context, would be highly desirable. Only a few generations ago, about all that existed of âpolitical scienceâ was what we now call leadership theory, public policy and administration, and political philosophy. Psychology was dominated by the psychoanalysis schools inspired by Freud and behavior conditioning theories of Pavlov and Skinner. Crowd psychology, especially mob psychology (Le Bon) hinted at a crude form of ï¬eld theory. But so far as I can see presently, it was the insight of Adam Smith and the dynamic âinvisible handâ of the market place (the idea that the behavior of individuals and communities in interaction had a dynamic of their own that needed to be understood as distinct from individual psychology or crowd dynamics), that ushered in a third âlevel of analysis,â the system level. Today, of course, there is wide acceptance of the insights by Singer (1961) and othersâ that at least three analytic levels (individual, organizational or bureaucratic, and systemic) are needed in international relations theory, each with their own structures and functions. But even today, there is no systematic effort to integrate them into a multi-level theory, not to mention to follow the work of Richardson to examine longer term multi-nation dynamics. In this context of inchoate theory development, Rudyâs ï¬eld theory transcends earlier efforts in relative clarity and comprehensiveness."
249,27,0.985,Advances in Proof-Theoretic Semantics (Volume 43.0),"paired with a set of reductions is called an argument, and we define what it is for an argument to be valid by essentially the same three clauses that defined the notion of valid deduction. I shall develop two new notions of validity, called weak and strong validity. They are variants of notions of valid arguments that have been proposed earlier,8 and will be shown to have distinct features that are especially important when it comes to compare valid arguments and BHK-proofs. At the end of the paper, I reflect upon the fact that all the variants of valid arguments considered so far deviate in one important respect from the intuitions connected with Gentzenâs approach as described above, and point to how the notion of justification may be developed in another way that stays closer to the original ideas."
228,175,0.985,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"Abstract It was already mentioned in previous sections that the Ordered Fuzzy Number (OFN) model can represent a kind of tendency or direction. However, for a real practical use of this feature the tools for processing it are also needed. Of course some kind of quantitative processing is provided by the definitions of calculations, but there is much more potential for this feature apart from arithmetic operations. This part presents the idea of a property of processing data called sensitivity to the direction. The main focus here is placed on the proposition of a direction determinant parameter that can be understood as a kind of measure of a direction. This determinant is a basis for the definition of such elements as the compatibility between two OFNs and also for an inference operator for a rule where the OFNs were used. The propositions of such operations are the important part of these sections of the book."
28,171,0.985,A History of Self-Harm in Britain,"of self-harm with complex intent. This contrasts starkly with todayâs clinical concern with self-cutting, which is based upon internal, and sometimes neurochemical triggers. Both acts of Parliament involve the removal or significant retraction of the law around the field of mental disorder (with suicidal behaviour securely, though not inevitably, entrenched as part of this field). This enables a more fluid interaction between mental and general medicine, altering the kinds of clinical objects likely to emerge. The Suicide Act, in removing the legal sanctions around attempted suicide does not necessarily change practices very much in one (empirical) sense; people are not being convicted very much during the 1950s. However, reform arguments have a resonant connection with ambiguous suicidal intent, and decriminalisation alters the terms of the debate through which attempted suicide is conceptualised, prompting formal intervention by the Ministry of Health. Because of the high level of psychiatric scrutiny required to produce complex, communicative intent around a presenting physical injury, HM(61)91 does not enable a huge number of studies by itself. The lack of extra resources is significant, but perhaps even more significant is the vastly increased potential for the object to flourish in a number of different sites, if increased resources become available. This is another important step for the progress of a clinical object â from an observation ward curiosity to one inscribed in a nationally consistent manner. The epidemic â and the broad, homogenising administrative machinery required for a multi-site epidemic â emerges through wider integration promoted through a retraction of the law in the areas of suicide and mental health more broadly. Returning to the notions of incidence broached discussing Stengelâs attitude to the hospital memorandum, we can see that as the potential for this clinical object becomes more and more widespread and more visible, the behaviour potentially becomes more and more available. Ian Hacking observes: Cynics about one thing or another ... say the epidemics are made by copycats. But even if there was a lot of copying, there is also a logical aspect to âepidemicsâ of this type. In each case ... new possibilities for action, actions under new descriptions, come into being or become current ... to use one popular phrasing, a culturally sanctioned way of expressing distress.76 Hacking shows, in his example of multiple personality disorder, that this logic of epidemics is a powerful and useful way to understand"
118,791,0.985,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"24.3.3 Cognitive Model of Team Performance The drawbacks of first-generation HRA are attributable to its basic assumption of the decomposition principle that a human task can be decomposed into elementary task units. It is equivalent to the assumption in the linear system that the whole is the sum of its parts. It will be shown in this subsection that this assumption does not apply to team performance. Since teamwork is used in most business settings, the reliability of team performance must be assessed in PRA, and some model of team performance is required to do so. The simplest approach is to combine multiple models of individual performance and this approach was actually taken in the early stage of development. A team, however, is a nonlinear system so that team performance is greater than the simple sum of individual performance. The cognitive processes of team performance can be effectively described by the concept of mutual beliefs. Tuomela and Miller introduced a notion of âWe-Intentionsâ to describe the cognitive mechanism in a cooperating team as"
289,293,0.985,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","Observation Failure and Assertion Failure. In our PPL, observe(flip( 12 )); earlier exceptions bypass later exceptions, as illustrated assert(flip( 21 )); in Listing 8. However, because we are operating in a Listing 10. Observaprobabilistic language, exceptions can occur probabilis- tion or assertion failure tically. Listing 10 shows a program that may run into an observation failure, or into an assertion failure, or neither. If it runs into an observation failure (with probability 12 ), it bypasses the rest of the program, resulting in  with probability 12 and in â¥ with probability 14 . Conditioning on the absence of observation failures, the probability of â¥ is 21 . An important observation is that reordering the two statements of Listing 10 will result in a different behavior. This is the case, even though there is no obvious data-flow between the two statements. This is in sharp contrast to the semantics in [34], which guarantee (in the absence of exceptions) that only data flow is relevant and that expressions can be reordered. Our semantics illustrate that even if there is no explicit data-dependency, some seemingly obvious properties (like commutativity) may not hold in the presence of exceptions. Some languages either cannot express Listing 10 ([23,33] lack observations), cannot distinguish observation failure from assertion failure [24] or cannot handle exceptions implicitly [34,35]. Summary. In this section, we showed examples of probabilistic programs that exhibit non-termination, observation failures and errors. Then, we provided examples that show how these exceptions can interact, and explained how existing semantics handle these interactions."
360,335,0.985,Compositionality and Concepts in Linguistics and Psychology,"As we may naturally expect, the concept PINCH shows preference for situations where an agent only pinches one patient at a time (this was experimentally verified by Kerem et al. and Poortman et al.). However, the reciprocity concept for each other prefers as many relations as possible (Dalrymple et al. 1998; Sabato and Winter 2012). In sentence (8a) this leads to a conflict between the preferences of the two concepts. By contrast, in sentence (8b) there is no conflict in preferences between the verb and the reciprocal expression: a person may know many people, without clear typicality preferences between different numbers of acquaintances. Intuitively, this contrast points to a possible guppy effect in sentence (8a) when compared to (8b). Indeed, Kerem et al. and Poortman et al. experimentally showed that there is a substantial difference in the interpretations of sentences like (8a) and (8b). Poortman et al. tested truth-value judgements on Dutch versions of these sentences (and similar ones) for I3 situations as in Fig. 1 above: three agents, each of them acting on another agent. While 88% of the participants accepted (8a) as true in this I3 situation, only 36% accepted (8b) in the same situation. Kerem et al. and Poortman et al. explain such differences as a guppy effect, using a principle that they call the Maximal Typicality Hypothesis (MTH). As an instance of the more general principle in (7), the MTH is analyzed as responsible for the guppy effect. For example, with the concept PINCH in (8a), the I3 situation is the most typical situation for the reciprocity concept that is consistent with the preference that one person does not pinch more than two people at the same time. If we try to add more relations to I3 in order to satisfy better the preferences of the reciprocal concept, we get atypical situations for the PINCH concept, As a result, I3 is the critical typicality point for the verb phrase pinch each other in sentence (8a). By contrast, in sentence (8b), with the concept KNOW , there is no substantial typicality difference between I3 and configurations containing more relations. Consequently, the preferences of the reciprocal concept are free to take over, and I3 is not a CT point for the expression know each other in (8b). Rather, in this case the CT point is the I6 situation, where every one of the three people knows every other person. Figure 4a, b summarizes the typicality considerations for sentences (8a) and (8b) with I3 and I6. For contrast, these figures also include the I2 situation, with only two pinching relations between the three people. In Kerem et al.âs and Poortman et al.âs experiments, I2 situations consistently showed very low acceptability for reciprocal sentences with all tested verbs. In (9) below we apply principle (7) to the case of the complex concept for pinch each other, where PINCH is the head concept HC and RECIP is the gradable concept (9) Let LM-P be the set of local typicality maxima for the concept PINCH. In formula: LM-P = arg maxx TYPPINCH (x). The CT point(s) for pinch each other is defined by: arg maxxâLM-P TYPRECIP (x)."
8,964,0.985,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","How Did the Name âQGPâ Come into Use? Quite often in physics names attached to important insights appear late and even sometimes attribute the discovery to the wrong person. The situation is similar with the naming of hot interacting quark-gluon matter as QGP: we call QGP today what appeared in many early articles under a different name âquark matterâ, while yesterday QGP used to denote something else, a Feynman parton gas. In my memory, the use of âQGPâ to describe the strongly interacting quark-gluon interacting thermal equilibrium matter was adopted following the title of a paper by Kalashnikov and Klimov [5] of July 1979. However, let me stress that the work by Kalashnikov-Klimov [5] did not invent QGP, neither in the content, and the name already existed â¢ We see the key results of Kalashnikov-Klimov in a year earlier, July 1978, work of Chin [6] presented under the name âHot Quark Matterâ and including hot gluons and their interaction with quarks and with themselves, which is the important pivotal element missing in many other papers. â¢ Kalashnikov-Klimov may have borrowed the term from another work, of March 1978, by Shuryak [7]. Shuryak at that time also used âQGPâ in his title addressing pp collisions as a source of photons, dileptons and charmonium. With time one notices Shuryakâs pp work cited in the modern AA QGP meaning context. This was also done in some of our citations both by Hagedorn and myself."
213,20,0.985,Collider Physics Within The Standard Model : a Primer,"That is, at high energies the weak interactions are no longer so weak. The range rW of weak interactions is very short: it was only with the experimental discovery of the W and Z gauge bosons that it could be demonstrated that rW is nonvanishing. Now we know that rW D"
8,411,0.985,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","where kD1 Ck and kD1 Ëk stand for the mean values of the squared matrix element with respect to the invariant or non-invariant phase space, respectively, and Fstat takes into account spin and isospin weight and contains 1=ni Å  when ni particles of type i are present. It was then observed by Fermi that a statistical model starting from Eq. (19.1) fits smoothly into a thermodynamical model once the energy E and thus the particle number become large enough. Thus, in discussing high-energy limits, one will use conveniently the methods of statistical thermodynamics which are far easier to handle than the expressions in Eq. (19.1)."
123,203,0.985,Fallibility At Work : Rethinking Excellence and Error in organizations,"through, so either somebody makes an impulsive and discreet intervention, or they follow the conductorâs directions into sour singing of Mozart. This critical moment in a concert hall is a miniature example of how human fallibility can affect the quality of what people are trying to achieve together. Excellence in this performance depends on the detection of error, and an initiative to halt the course of events it sets in motion. Musicologist and conductor Mette Kaaby sees it as a nightmare situation, one that should not happen but may nevertheless be a reality. âA performance of Mozartâs Requiem is all about collective precision. The choir and their conductor has put down hundreds of hours of practice together, to get the details exactly right. They are supposed to breathe, move, and sing together as one entity. The conductor needs to be sensitive to what happens among the choir members, and should be able to note signs among them that something is wrong. When that does not happen, it can create a musical crisis.â (Kaaby, 2016). The narrative about the conductor and the choir builds on an event that actually took place in a concert hall. What happened next was that one singer saved the day by discreetly correcting the tone from the conductor. The other singers started from that new tone instead, and the performance went well. There was a barrier system in place to stop the conductorâs initial mistake from developing into a collective breakdown in the form of bad singing. The narrative also highlights other dimensions of coping with fallibility at work, as will emerge in the concluding reflections below. This book has addressed how individuals, groups, and organizations can handle fallibility at work. It has highlighted how mistakes are not necessarily bad, since they can generate breakthroughs in innovative processes. Kaaby explains how there can be musical contexts where starting from a mishit tone can generate unexpected new dynamics among musicians. You open the wrong door, and explore what you find there, rather than turn around and insist on opening the door you were planning to open in the first place (Kaaby, 2016). Even in cases where mistakes lead to a bad outcome, there can be important learnings to"
192,333,0.985,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"ments, in response to their queries, which somehow should produce the object a (lower-right position): instances of fraud as by-products of biomedical research. In other words, Cliff as a scientific subject now finds himself in the role of target (S2 in the upper-right position). ORIS is allegedly fuelled by normative imperatives (the fight against fraud, S1 in the lower-left position), but other disavowed motives (the lust for power, resentment against scientists, etc.) may also play a role. The jouissance involved in this inquisitive practice (from the ORIS perspective, that is), is the pleasure of science-bashing, for instance by exposing and red-listing apparently irrelevant research projects. This is even more obvious when the focus of attention shifts from ORIS to the Redfield Subcommittee on Science and Technology. The integrity of the lab is now seriously questioned. Cliffâs notes (allegedly containing the object a) are subjected to a forensic ink analysis. The authorities hope to uncover a âwhole culture of scientific finessing and fraudâ by subjecting the scientists to a formal hearing. When Redfield refers to the institute as a âtotalitarian systemâ, an âoppressive regimeâ, this is not completely besides the truth of course as we have seen, but it also seems self-referential. In the post-truth era, science as such now seems to be on trial (p. 291). The Master (the Senator, S1) is having his revenge on the emancipated scientists (S2). But in the end the tables are turned again, when yet another panel calls for âan external review of the structures and processes used for ethical oversight at NIHâ itself (p. 329)."
32,697,0.985,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","30.4 Conclusion One of the important factors ignored in the past analyses in e-marketing is âcolorsâ of products. This is so because it is difficult to define a color of a product, which typically consists of many different colors. The purpose of this research is to fill this gap by developing an algorithmic procedure for identifying the dominating color of a product by analyzing a digital image of the product. Since humans tend to clearly distinguish RED from GREEN as well as YELLOW from BLUE, the Euclidean distance in CIE-L a b is more consistent with the sensuous feeling of human for colors than the Euclidean distance in RGB. Accordingly, for analyzing color preferences of consumers in e-marketing, CIE-L a b is more appropriate than RGB. Based on this idea, we proposed the CCPV (Color-Class Profile Vector) which represents the overall impression of a digital image containing a product. Since each product has its color in the data base, these vectors can be utilized to"
169,460,0.985,Riverine Ecosystem Management : Science For Governing Towards a Sustainable Future (Volume 8.0),"The importance of goals and mental models to decision-making becomes clear if the AM cycle is expanded to try to account for the complexity of the policy world (Fig. 16.3). In the assessment phase, all factors in Fig. 16.2 are used to deï¬ne the problem (usually as a trend) and the pattern of likely causes. From the outset this is complicated by the fact that the nature of the problem itself is in question. Problems may have multiple interpretations, based on conï¬icting values and goals (Weick 1995). Even if the basis of a problem is relatively simple and clear, the goals and mental models of individuals and of the surrounding culture and politics will dominate how it is deï¬ned and interpreted. This is because goals and mental models act like a ï¬lter that determines what information is selected and how it is measured in evaluating policy performance or in decisions as to how policy is implemented. One example of this is the physics of increasing the concentration of CO2 in the atmosphere, which is not complex. It can only cause more heat to be retained, and this reliably predicts why the years since 2000 have exhibited the hottest atmospheric temperatures ever measured. Yet many deny the evidence (that climate change exists at all) or the theory that society is driving it by generating more greenhouse gases. Political conservatives, who often ascribe to hierarchist or individualist paradigms, tend to deny climate change theory or evidence. Political liberals, who tend to subscribe to paradigms centered on community (communitarian) and social equality (egalitarian), tend to accept the science supporting climate change theory (Kahan 2008, 2013). However, both conservatives and liberals are equally likely to interpret data ideologically. There is a third, social and political, aspect involved. As Kahan (2013) concludes: â. . .ideologically motivated cognition . . .[is] a form of information processing that promotes individualsâ interests in forming and maintaining beliefs that signify their loyalty to important afï¬nity groups.â Thus, while the perceived political and economic consequences of climate"
235,194,0.985,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","n-atomic contexts (blocks, maximal observables with n outcomes, Boolean subalgebras 2n , orthonormal bases with n elements) â for instance, as Greechie orthogonality diagram of quantum logics â is larger than n, then there cannot be any globally consistent two-valued state (truth value assignment) obeying adjacency (aka admissibility). Likewise, if no two-valued states on a logic which is a pasting of n-atomic contexts exist, then, by reduction, no global consistent coloring with n different colors exists. Therefore, the KochenâSpecker theorem proves that the chromatic number of the graph corresponding to the unit sphere with adjacency defined as orthogonality must be higher than three. Based on Godsil and Zaks finding that the chromatic number of rational points on the unit sphere S 3 â© Q3 is three [245, Lemma 1.2] â thereby constructing a twovalued measure on the rational unit sphere by identifying one color with â1â and the two remaining colors with â0â â there exist âexoticâ options to circumvent Kochenâ Specker type constructions which have been quite aggressively (Cabello has referred to this as the second contextuality war [94]) marketed by allegedly ânullifyingâ [369] the respective theorems under the umbrella of âfinite precision measurementsâ [32, 75, 76, 146, 306, 366]: the support of vectors spanning the one-dimensional subspaces associated with atomic propositions could be âdilutedâ yet dense, so much so that the intertwines of contexts (blocks, maximal observables, Boolean subalgebras, orthonormal bases) break up; and the contexts themselves become âfree and isolated.â Under such circumstances the logics decay into horizontal sums; and the Greechie orthogonality diagrams are just disconnected stacks of previously intertwined contexts. As can be expected, proofs of Gleason- or KochenâSpecker-type theorems do no longer exist, as the necessary intertwines are missing. The ânullificationâ claim and subsequent ones triggered a lot of papers, some cited in [32]; mostly critical â of course, not to the results of Godsil and Zaksâs finding (ii); how could they? â but to their physical applicability. Peres even wrote a parody by arguing that âfinite precision measurement nullifies Euclidâs postulatesâ [392], so that ânullificationâ of the KochenâSpecker theorem might have to be our least concern. Exploring Value Indefiniteness Maybe one could, with all due respect, speak of âextensionsâ of the KochenâSpecker theorem by looking at situations in which a system is prepared in a state |xx| along direction |x and measured along a non-orthogonal, non-collinear projection |yy| along direction |y. Those extensions yield what may be called [286, 401] indeterminacy. Indeterminacy may be just another word for contextuality; but, as has been suggested by the realist Bell, the latter term implicitly implies that there âis something (rather than nothing) out there,â some âpre-existing observableâ which, however, needs to depend on the context of the measurement. To avoid such implicit assumption we shall henceforth use indeterminacy rather than contextuality. Pitowskyâs logical indeterminacy principle [401, Theorem 6, p. 226] states that, given two linearly independent non-orthogonal unit vectors |x and |y in R3 , there is a finite set of unit vectors Î (|x, |y) containing |x and |y for which the following statements hold:"
192,292,0.985,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"language; â as indicated by one of his most famous phrase, put forward on several occasions: Die Sprache spricht (âlanguage speaksâ). Lacanâs downplaying of intellectual property may sound radical but, similar to Perlmannâs Silence, he does challenge us to explicitly consider a concept which is too easily taken for granted in mainstream integrity discourse (which increasingly revolves around a neoliberal framing of the scientist as a textual entrepreneur, scoring citations on the discursive stock market of citation indexes, known as academic publishing). Lacan challenges us to question the P of FFP. In dialectical terms, we initially start from an understanding of intellectual property and plagiarism which seems self-evident (M1). If I am the first person to publish about something (a concept, a formula, an equation, a discovery, a syndrome, a technical innovation, a personality test, etc.), I may rightfully claim it to be âmineâ, so that others should at least cite me as the owner. But further reflection will convince us that we always stand on the shoulders of others, and that every novelty presupposes terms, approaches, techniques, etc., developed by others, so that it seems artificial to consider my contribution as something which belongs exclusively to me, as my âpropertyâ even. In other words, the initial concept of intellectual property is challenged or even negated when the original concept is exposed to actual research practices, as fleshed out in science novels for instance (M2). This is an important experience, for it reveals that mainstream understandings of intellectual property actually build on questionable (neoliberal) framings of scientific productivity, staging scientists as a textual entrepreneurs, scoring citations on the discursive stock market of citation indexes, which allegedly has become the basic objective of academic publishing (which, according to this neoliberal logic, is neither about making discoveries not about working for the benefit of humankind, but about harvesting citations and boosting oneâs h-score). The various instances of plagiarism presented in novels (as literary case histories) expose this symptomatic misunderstanding and the subsequent subversion of neoliberal interpretations of intellectual property in misconduct novels forces us to critically reconsider the original concept and to actively work through the experiences which these novels describe. I will come back to this discussion in the final section, and also in Chap. 10."
75,469,0.985,"Opening Science : The Evolving Guide on How the Internet is Changing Research, Collaboration and Scholarly Publishing","Seen as a whole, this section serves to explain why publishing scientists have an interest in being quoted as frequently as possible. It is equally conceivable that there are sufficient reasons for manipulating citations, always with the aim of being cited more often by others. A number of scientists managed to solve this problem in the past by means of citation cartels, and in case this is controversial, let it be said that it definitely applies to some operators of websites at least, because it constituted a problem for search machines for a long time. The following section addresses feasible aspects of the Social Internet for Open Science, always against the backdrop of possible manipulation and the ensuing consequences in practice. This list, like the whole book, is an incomplete preview of a topic that is still under development but will revolutionize science."
124,204,0.985,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"on the agents themselves. A non-empty set of agents is postulated, and the choice function indicates what choices each will have at each juncture in branching time. But little else is normally said about the nature of the agents and about what distinguishes one agent from another other than brute non-identity. Belnap has remarked17 that in a branching spacetime system, it is possible to associate each agent with a unique set of point events, the set of those at which the agent is present. This is made possible by the fact that distinct agents cannot occupy the same place at the same time. Unfortunately, if we look merely at branching time, with no basis for discussion of spatial dimensions, no such simple account of the identity of agents is possible. However, the usual constraints imposed on the choice function, including in particular the constraint independence of agents, do make it possible for us to look at agents in new ways. In particular, we can give some formal substance to, and gain some new insight into, the view that an agent is the sum of the choices she makes and thus that an agent is a work in progress, existentially shaping her character and her very identity through her choices. In a Belnapian world, the constraint independence of agents assures that, strictly speaking, no two agents will be presented with the same choices at a given moment in time. Of course at the restaurant both may be choosing between having the scallops and having the mussels, but that is only to say that the choices facing one may be descriptively like those facing the other. For one thing, even if they both choose the scallops, for example, they will not get the same scallops. But more significantly, agent a is choosing what a will order (and, presumably, eat), not what agent b will order. So even confining attention to a single moment, agents are normally18 differentiated from one another by the choices they face. If we shift attention to the choices agents make, not just the choices they face, this becomes even clearer: even at a single moment, provided only that the agents are active, they are differentiated by their activity, which is to say: by the choices they make. Widening our perspective to scan a history within which a moment falls, we find the agent making a succession of choices which cumulatively help define that very history, setting it apart, choice by choice, from others that had been available. The totality of those choices will be absolutely unique to a given agent, no matter whether we focus on the menu of choices the agent faces or on the choices the agent selects from that menu. Seeing this accumulation of choices along the history as uniquely associated with one agent, we can begin to consider it as constituting that agent, in which case as we survey the history we now get a strong sense of what it might mean to say that the agent is creating herself by her choices. In a different history, pursued by making different choices, she would have become a different person. 17 At the Î±EONâ10 Conference, Fiesole, Italy, 2010. 18 There is one kind of exception to this generalization: at a given moment two agents might be"
117,132,0.985,Care in Healthcare : Reflections On Theory and Practice,"asymmetry of power, and both these characteristics of life require the care of another person. Being in a relationship with someone is therefore the first condition for being in the world. An ethics which emerges from such an image of human contingency and dependency represents an alternative to the model that regards people as âself-interested strangersâ (Held 2006, p. 77) who simply enter into a contract with each other. It highlights responsibilities which exceed contractual models of reciprocity. Furthermore, the motherâchild relationship sheds light on the special characteristic of ethical relations: In the eyes of the mother, her child is special. Because of the fact of natalityâwhich plays a crucial role both in the ethics of care and for Levinasâthe concept of humankind starts with an emphasis on the particularity of every person and every situation. Just as the child is special to the mother, all people are of importance to someone. They are unique and irreplaceable in their meaning to someone else. Finally, both ethics underline the importance of the attitude of being responsive to the Other and the world. Being responsive is not something one can really choose to be. Levinas uses the image of âbeing held hostageâ to describe the phenomenon of dependency. In pregnancy, this dependency becomes obvious. Having a baby limits the freedom of the motherâshe is not supposed to drink or eat what she wants, her body changes enormously and feeling physically sick is often part of pregnancy. It is not unusual for women to wish for their âcustomary bodyâ back (Staehler 2016, p. 31), that is the ability to perform everyday activities again as usual. A motherâs love for her child is not affected by these constraints, however. According to Levinas, the same is true for the relationship with the Other: Being in a relationship with the Other represents a challenge for the self. This relationship is not freely chosen in its conditions, but is based on unconditional responsiveness and responsibility towards the Other. Being responsive and responding to the needs of someone else are thus central to both Levinas and the ethics of care. While Levinas primarily foregrounds the needs of the Other, an ethics of care also asks to what extent the self can fulfil the needs of the Other. According to the wellknown definition by the care-ethicist Joan Tronto (Tronto 1993, 2013), care is best understood as attitude and as practice. While this differentiation can be made methodologically, in daily life the phases of care often"
264,63,0.985,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"Introduction I am thinking back to 1984, when I was a secondary mathematics teacher in New Zealand, and I attended my ï¬rst ICME conference in Adelaide. The very ï¬rst session I attended was Ubiratan DâAmbrosioâs talk (DâAmbrosio, 1985). I remember being completely blown away by this. Here was Ubiratan DâAmbrosio bringing to my very small world in New Zealand a vision of a caring world society in mathematics education. And that changed my life, so it is a very great honour to be invited to come here at the end of my career and be able to present some thoughts about where that agenda has gone. First, let us remember the pleasure within mathematics. Remember that this beauty is accessible to all, and exists even in the most elementary mathematics. An example is the visceral pleasure of a visual proof of a mathematical idea. And there can be pleasure in all the mathematics of any curriculum. My personal favourite topic to teach was trigonometric equalities because I could talk about how the equation of the sum of sines helps us to understand the common knowledge of surfers that big ways come in threes, or that every seventh wave is a big one. Waves B. Barton (&) University of Auckland, Auckland, New Zealand e-mail: b.barton@auckland.ac.nz Â© The Author(s) 2017 G. Kaiser (ed.), Proceedings of the 13th International Congress on Mathematical Education, ICME-13 Monographs, DOI 10.1007/978-3-319-62597-3_3"
395,287,0.985,Beyond Safety Training : Embedding Safety in Professional Skills,"16.2.1 The âGood Professionalâ A good professional would be better equipped to make the most appropriate choices in any situationâone which might impact safety as well as other performancesâ taking into consideration various constraints. But what is a âgood professionalâ? The criteria differ depending on who is determining it. Although from the peersâ viewpoint being a good professional has something to do with the identity of the trade, the identity of the work collective, from the viewpoint of the organization, professionalism is deï¬ned in a much more top-down manner. As an example, the"
275,140,0.985,Foundations of Trusted Autonomy,"There are several parts to the compilation â in this section we describe just the two most important: encoding consistent belief update; and encoding the perspective of other agents when the planning agent is unsure whether they witnessed an event. These both extend a base encoding, which strips away epistemic fluents are replaces them with propositional fluents suitable for our (non-epistemic) multi-agent planner. Technical details about this encoding can be found in Muise et al. [13]. In this section, we simply provide the intuition behind these via some examples. Base Encoding. The base encoding describes a simple multi-agent planning problem that is not equivalent to the original problem. This encoding is then extended to deal with belief update and uncertain firing of events. Put simply, the encoded problem takes the original problem and compiles it to an alternative problem such that each epistemic fluent in the action models, initial state, and goal is encoded into a proposition; that is, fluents of the form [?a]p are compiled to a_p. Thus, a_p represents the agent a believing p as a proposition. This replacement is nested for nested beliefs; for example, [?a][?b][?c]p is encoded as a_b_c_p. Negations of the form not([?a]p) are encoded as not_a_p. Belief Update. In classical planning, belief update is straightforward: when a proposition becomes true, it is no longer false, and vice versa. However, in epistemic planning, the problem is not so simple. Consider the Grapevine example described in Example 1, in which agent 1 learns secret s, modelled as the epistemic fluent [?a]s. The propositional fluent a_s models this, however, we must also consider that if [?a]s is true, then so is not([?a] not(s)) â if agent a believes s, then is should not believe the negation of s. Thus, for every compiled action in which a_s becomes true, so too must not_a_not_s. This counters for epistemic actions in which not([?a] not(s)) is a precondition for example. If we add only a_s to the state, then not_a_not_s will not be true when that precondition is evaluated for another action. As such, the encoded model would not be equivalent without this modified belief update."
381,794,0.985,The Dynamics of Opportunity in America : Evidence and Perspectives,"All-American: Equal Opportunity as Egalitarian Individualism The volume in your hands (or perhaps, more likely on your screen) is one of thousands with the word âopportunityâ in its title. Especially in America, opportunity is a term redolent of optimism, progress, and freedom. It is, in short, impossible to be against. The danger is that opportunity becomes a protean term, meaning almost anything, or something different to different people in different contexts. Some specificity is therefore required in order to move beyond rhetoric and into action. I will shortly argue for a specific concept of opportunity, namely relative intergenerational income mobility. But first I will attempt to define equal opportunity as a distinctly American kind of fairness. In his second inaugural address in 2013, Obama declared: âWe are true to our creed when a little girl born into the bleakest poverty knows that she has the same chance to succeed as anybody else, because she is an American; she is free, and she is equal, not just in the eyes of God but also in our own.â So: the âsame chance to succeed,â even though âborn into the bleakest poverty.â This is the utopian ideal of American fairness, in which the inequalities of birth do not dictate the inequalities of life. While Obama, like most politicians, focused on upward mobility out of poverty, the equal opportunity ideal reaches all the way up the distribution. It is about the chance for a middle class kid to join the elite, as well as for a poor kid to join the middle class. The ideal also goes deeper than political rhetoric. Equality of opportunity is in Americaâs DNA. The moral claim that each individual has the right to succeed is even implicit in the proclamation of Declaration of Independence that âAll men are created equal.â In his first draft of that historic document, Thomas Jefferson in fact wrote that all were created âequal and independent.â This is the distinctly American formulaâequality plus independence adds up to the promise of upward mobility. Equal opportunity reconciles individual libertyâ the freedom to get ahead and âmake something of yourselfââwith societal equality. It is how the ideal of natural equalityââborn equalâ is fused with the ideal of individualismââborn independent.â It is a philosophy of egalitarian individualism.1"
8,300,0.985,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Black Body Radiation I now need to introduce another concept that has played an influential role by undermining classical physics. This idea forced Planck to postulate the quantum hypothesis initiating a radical conceptual change which culminated in the formulation of quantum theory. Arguably, there has not been anything of comparable importance discovered since. I present to you âBlack Body Radiationâ. If you place a completely empty boxâa cavityâin a heat bath of temperature T, it does not remain empty; it fills with electromagnetic radiation, whose spectral distribution, i.e. the composition of different wavelengths (radio waves, heat, light, ultraviolet light, X-rays), is described accurately by Planckâs radiation law. This spectral distribution is a function of temperature; in fact, we measure temperature of very hot and/or far and distant bodies (stars), by studying the radiation spectral distribution. Aside of the spectral distribution dependence on the temperature, the intensity of the radiation is also temperature dependent. Namely, the total radiated energy is proportional to T 4 . Or said differently, the way I prefer: the temperature is proportional to the fourth root of the radiation energy content. When the temperature just doubles, the radiation energy is increased 16 times. From daily life experience, by and large, (that is, apart from chemical and phase changes, such as melting, boiling), we are accustomed to thermal energy being approximately proportional to temperature increases; that is, 16 times the thermal energy also means 16 times the temperature. This is because heat is nothing more than the random motion of molecules and that, as their number (usually) remains constant, all energy supplied again finds itself as heat and the temperature increases proportionally: temperature is defined as a measure of the average kinetic energy per molecule. However, in the radiation fieldâalso called photon gasâthe number of âmolecules,â that is to say, the number of photons, is not at all constant: ever more and more of them are created as the temperature is increased, as I supply ever more energy. This larger number of photons, many more than were originally available, must share the newly supplied energy; therefore each photon takes only a minor portion for itself, than it would have received, had their number been constant. The temperature = average energy per photon rises more slowly than in the case of constant particle number; in consideration that a large part has just been invested in the creation of new photons. In a more careful evaluation we find the Stefanâ Boltzmann law which I introduced, the p temperature is proportional to the fourth root of energy density: T D Const:  4 E What does this have to do with our indestructible nucleons and the newly created particles? All we need is to generalize the concept of black body radiation: who says that the radiation must consist only of photons? There is no law in physics prohibiting material particles forming from radiation. In fact, relativity and quantum theory claim it outright: if E  mc2 , a particle of mass m can arise spontaneously (there are certain constraining conservation laws, but in principle this detail changes nothing). So if we increase the temperature of our box on and on, it is inevitable"
311,2234,0.985,The Physics of the B Factories,"material of the detector.135 Such subtle eï¬ects are not described to a suï¬cient accuracy in the simulated data samples â note that as explained in Section 19.2.1.3 in the search for the CP violation in the charm sector one is interested in eï¬ects of the order of 10â3 â and hence they must be estimated using data control samples. To explain ideas used in the corrections for the non-CP violating asymmetries mentioned above let us first consider an example of a charged D meson decay, DÂ± â XhÂ± , where X denotes a neutral hadronic system which is self-conjugated (and hence the same for the D+ and Dâ ) and hÂ± represents a charged hadron. The experimentally determined asymmetry is Arec ="
363,7,0.985,History and Cultural Memory in Neo-Victorian Fiction,"a category to which Hutcheon opposes historiographic metafiction. It suggests that the historical novel has always been invested in historical recollection and aware of the partial, provisional nature of such representations. It also links contemporary historical fiction to the burgeoning interest in a broadly conceived âhistorical imaginaryâ (DeGroot, 2009) in order to disrupt a hierarchical approach that privileges history and marginalises historical fiction. I suggest that the emergence of memory discourse in the late twentieth century, and the increasing interest in non-academic forms of history, enables us to think through the contribution neo-Victorian fiction makes to the way we remember the nineteenth-century past in ways that resist privileging historyâs non-fictional discourse, on the one hand, and postmodernismâs problematisation of representation on the other. Approaching neo-Victorian fiction as memory texts provides a larger framework for examining the sheer diversity of modes, motivations and effects of their engagement with the past, particularly one which moves beyond dismissing affect. As Mieke Bal suggests, âthe memorial presence of the past takes many forms and serves many purposes, ranging from conscious recall to unreflected re-emergence, from nostalgic longing for what was lost to polemical use of the past to shape the presentâ (Bal, 1999: vii). And, I would suggest, these multiple forms and purposes are often simultaneously present in the one text. Moreover, âmemory is active and it is situated in the presentâ (ibid.: viii). Positioning neo-Victorian novels as acts of memory provides a means to critically evaluate their investment in historical recollection as an act in the present; as a means to address the needs or speak to the desires of particular groups now. I resist a popular and academically persuasive use of ânostalgiaâ as the opposite of critical historical inquiry, This opposition is evident in Hutcheonâs suggestion that in The French Lieutenantâs Women âthe past is always placed critically â and not nostalgically â in relation to the presentâ (Hutcheon, 1988: 45). Indeed she is at pains to distinguish postmodernismâs approach to the past from ârecuperation or nostalgia or revivalismâ (ibid.: 93). Nostalgia is, for Hutcheon, an encumbrance from which postmodernism, and its historiographic metafictions, frees itself for its âcritical, dialogical reviewing of the forms, contexts, and values of the pastâ (ibid.: 89). Here, a conservative, even naÃ¯ve, nostalgia is contrasted with a somehow more authentic, because critical, attitude toward the past. David Lowenthal, too, asserts that ânostalgic dreamsâ of retrieving the past âhave become almost habitual, if not epidemicâ in recent years. He finds nostalgia expressive of âmodern malaiseâ, calling it todayâs âuniversal catchword for looking backâ (Lowenthal, 1985: 4)."
99,15,0.985,Social Innovations in the Urban Context,"ic method, but simultaneously characterised by an indeterminate quality that makes it adaptable to a variety of situations and flexible enough to follow the twists and turns of policy, that everyday politics sometimes make necessaryâ (European Commission 2013, p. 16). Indeed, it has achieved the status of a buzzword in national and European policy circles. US President Obama established no less than two offices for social innovation. The EU has used the term to fund several initiatives, including the research project upon which this book is based. It is then little wonder that the meaning has diluted, sometimes referring to anything that is considered new and that is not technical. Although as an academic concept, it is less wide-ranging, there still remains a broad range of interpretations. Some posit simply that it must constitute a new approach to a particular kind of problem. The Stanford Centre for Social Innovation, for example, describes it as âthe process of inventing, securing support for, and implementing novel solutions to social needs and problemsâ (Phillis et al. 2008, p. 34). This is a conveniently flexible interpretation, yet one could argue that, according to this definition, there is little that does not qualify as a social innovation. Other scholars are more specific in circumscribing the nature of innovation. For example, the SOCIAL POLIS project defined it as âthe satisfaction of alienated human needs through the transformation of social relations: transformations which âimproveâ the governance systems that guide and regulate the allocation of goods and services meant to satisfy those needs, and which establish new governance structures and organizations (discussion forums, political decision-making systems, etc.)â (Moulaert 2010, 2013). This implies not only that an innovation must be radical (transformative), but also that it changes the power structure within the system where it is introduced. The problem with this kind of definition is less with its normative character, but with its essentialist nature. It is true that innovations are about new ideas and purposes deriving from established paths and patterns getting practical; however, it must be likewise considered that they are about processes and ways of development under conditions and in contexts where interaction is not determined and foreseeable ex ante. Innovatory effects of a new product, strategy or service can be path breaking to different degrees. Thinking this way it becomes clear that what is needed is a concept of social innovation as a complex societal process, rather than a mere classificatory definition of an action or product. For the purposes of the WILCO project, we defined social innovation as both products and processes; ideas translated into practical approaches; new in the context where they appear. It was important for us to use such a definition, rather than a more specific one, because one cannot clearly predict what comes out of even a very promising innovation in the course of its development. The problem with defining social innovation resides less in âinnovationâ and much more in the meaning one attributes to âsocialâ. Studying the current literature on conceptualising and defining social innovations, one finds that âsocialâ is mainly equated with âimprovementâ (Phillis 2008), finding better answers to basic needs and more satisfying social relations (Moulaert 2010), and a range of other âgood thingsâ. One way of challenging such an interpretation of âsocialâ has been proposed by Johnson in his essay asking âWhere good ideas come fromâ (2010). He argues that there are four different environments that create new ideas, processes and things:"
124,412,0.985,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"â¤x =def â¤ â =x A strong argument could be made that, for modelling purposes, this would be a useful and natural extension. It is easy to accommodate but I will not do so in the rest of this chapter. It would not fit so well in the stit-framework since that would require relating actions/choices across moments in different, incompatible histories which does not seem so natural. One final remark: I am thinking here of âmovingâ as a basic, simple kind of act, such as moving an arm while it grasps the vase or pushing the vase in one movement from one location to another. I am not thinking of âmovingâ as an extended process of some kind requiring the vase to be packed up, transported somehow to the new location, and unpacked (say). In the latter case, the transitions in the diagrams would correspond to executions of this more elaborate âmovingâ process. In that case we might well not want to say that Ï1 â¤a Ï1â² , since the moving process might be different if it happens to be raining as the vase reaches the out location. Indeed there might be many different âmovingâ transitions between in and out, each corresponding to a different combination of actions by a. We will return to this point later under discussions of granularity of representations."
360,88,0.985,Compositionality and Concepts in Linguistics and Psychology,"5.3 (Some) Essentialists A large number of contemporary Essentialists who follow Chomskyâs teaching on this matter claim that semantics and pragmatics are not a part of the study of language. [T]he study of meaning and reference and of the use of language should be excluded from the field of linguistics . . . [G]iven a linguistic theory, the concepts of grammar are constructed (so it seems) on the basis of primitive notions that are not semantic (where the grammar contains the phonology and syntax), but that the linguistic theory itself must be chosen so as to provide the best possible explanation of semantic phenomena, as well as others. (Chomsky 1977, p. 139) It seems that other cognitive systems â in particular, our system of beliefs concerning things in the world and their behavior â play an essential part in our judgments of meaning and reference, in an extremely intricate manner, and it is not at all clear that much will remain if we try to separate the purely linguistic components of what in informal usage or even in technical discussion we call âthe meaning of linguistic expression.â (Chomsky 1977, p. 142)"
297,1216,0.985,The R Book,"There is a real paradox about analysis of variance, which often stands in the way of a clear understanding of exactly what is going on. The idea of ANOVA is to compare several means, but it does this by comparing variances. How can that work? The R Book, Second Edition. Michael J. Crawley. Â© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd."
378,222,0.985,The Great Mindshift : How a New Economic Paradigm and Sustainability Transformations Go Hand in Hand,"One scholar I have found particularly insightful for an understanding of the governing role of a dominant paradigm or common sense is Gramsci (1891â1937). An Italian political economist of the twentieth century, he wrote most of his work while imprisoned by Benito Mussoliniâs fascist regime in the 1930s. His quest was to ï¬nd an answer to the phenomenon of democratic orders whose citizenship rights were presumed to be democratic, but in which a small number of people enjoyed far more privilege than the majority (Gramsci 1971: 377). For Gramsci, shaping or changing regime structurations or governing institutions requires a âcollective willâ that can mobilize sufï¬cient support, either in numbers of people or in politico-economic power. This collective will is represented by a group of people whose political program âpresupposes the attainment of a âcultural-socialâ unity through which a multiplicity of dispersed wills with heterogeneous aims are welded together with a single aim, on the basis of an equal and common conception of the worldâ (Gramsci 1971: 349). The essential ingredient is the common conception of the world, and from it springs the agreement that a particular program, its aim and its underpinning values and norms are of general interest for the given communityâor at least the best possible solution for it. Of course here we ï¬nd a resemblance to what I discussed as paradigms or worldviews, and the role of narratives in generating collective action. Gramsci uses the term âaimâ for what I formerly described as the declared purpose of systems or strategies. Successful aims would have the quality of a âsocial myth,â which is âa political ideology expressed neither in the form of a cold utopia nor as learned theorizing, but rather by a creation of concrete fantasy which acts on a dispersed and shattered people to arouse and organize its collective willâ (Gramsci 1971: 126). This âconcrete fantasyâ is close to what I used the term âimaginaryâ for in this book, and both are essential for the emergence of the will to act and change. In successful transformation processes, slowly but surely, the originally new aim or purpose becomes the norm or basis of consent that I have called the âdefault.â It is produced and reproduced through a set of institutions, social relations, and ideas that live in science or canonized knowledge as much as culture. Here we ï¬nd the analogy to the changing yet objectiï¬ed role of old ideas in todayâs path dependent systems, taking the shape of infrastructures, technologies, political regulation, market patterns, sociocultural norms, and mind-sets (Fig. 2.5). Gramsci puts most emphasis on the sociocultural aspects when seeking to understand why capitalism, despite its clearly unequal distribution of beneï¬ts and power, was not leading to the revolution that Marxists had predicted. This is what he coined the term âhegemonyâ for: it is the soft or invisible factors like values, ideas, knowledge, and norms about what is good and right that ï¬lter which solutions appear to be of general interest or acceptable. A successful collective willâs narrative thus provides ânot only a unison of economic and political aims, but also"
261,302,0.985,The Poetics and Politics of Alzheimerâs Disease Life-Writing,"As Alzheimerâs disease embodies the loss of personhood, writing about it becomes a searching for meaning-making substitutes, where metaphorical language turns into expressions and assertions of the self as much as the narrative itself.27 As Arno Geiger tells us in the quote offered at the beginning of this chapter, the confrontation with death brings narratives by patients and caregivers together. The title of this work anticipated it: dementia narratives are a form of life-writing. The condition is perceived as threatening the mind and, thus, personal identity with such intensity that it cannot simply remain a crisis and, thus, a chapter within (the musings on) a life.28 Instead, it must become the central focus of a narrative that explores the full existential implications of the condition and asserts that cognitive abilities and consciousness are not the lone identiï¬ers of a person. In the light of the ever-aging population, Alzheimerâs disease is the most feared neurological condition today; and it may remain so for several more decades, given that a complete understanding of its cause continues to be elusive and its treatment therefore only symptom-based and, currently, of limited efï¬cacy. This book has explored how individuals affected by the condition â caregivers as well as individuals with dementia themselves â engage with and aspire to shape the culturally dominant dementia narrative. It has done so by reading their narratives against what have become Health Humanities canonical texts, only alluding to psychoanalytical and phenomenological implications for example in the context of sonsâ lifewriting.29 One of the core aims of this study was to amplify the voices of those whose texts it critiqued. As such, the discussion has concentrated on the immediate relevance for policy changes and healthcare approaches of the author-narratorsâ poetic choices and political concerns. Concurrently, it has become clear that their intentions and needs have shifted over the thirty-year period covered in this analysis, raising perhaps the question as to a more expansive chronological contextualisation of these"
78,423,0.985,The Onlife Manifesto : Being Human in a Hyperconnected Era,"Judgment is one among three cognitive faculties, the other two being theoretical and speculative reason (along with sensibility and understanding) in the First Critique, and pure practical reason in the Second Critique. Kantâs own focal point in his treatment of judgment is taste and the sublime, and applies first and foremost to art, as distinguished from nature (pure reason) and freedom (practical reason). As such, judgment primarily concerns the aesthetic domain of feelings of pleasure and displeasure, as opposed to the faculties of cognition and desire. As such, pleasure and displeasure can never make claims to objective necessity or a priori validity: As with all empirical judgments, [pleasure or displeasure] is, consequently, unable to announce objective necessity or lay claim to a priori validity. â¦ [J]udgement of taste in fact only lays claim â¦ to be valid for every one. â¦ [O]ne who feels pleasure in simple reflection on the form of an object â¦ rightly lays claim to the agreement of everyone, although this judgment is empirical â¦.2"
253,694,0.985,"Autonomous Driving : Technical, Legal and Social Aspects","external sensor measurements is the yaw rate or yaw angle of other road users. Without vehicle-to-vehicle communication, these variables can only be determined reliably for oneâs own vehicle. However, for the subsequent situation evaluation and situation prediction, not only the physical measurement of the objects is required, but also information about what class of object is involved. For example, a pedestrian and a motorcyclist differ in terms of their possible degrees of freedom of movement and also their possible movement dynamic. Also, depending on the context and constellation, road surface markings can have different meanings. Therefore, it is necessary also to determine the semantic meaning of the objects detected from the sensor data, or from other information sources such as a digital map. In the context of the machine perception, this operation is known as a classiï¬cation step, but it is a component of the machine perception. While humans are able to assign a semantic meaning to the visual perceptions very quickly and nearly without errors, this is still a comparatively difï¬cult task for the machine perception with the current state of the technology. The known classiï¬cation algorithms are always based on more or less complex models of expected object classes, which are either learned automatically from examples or are speciï¬ed manually. These models then display, as discriminately as possible, characteristics that can be captured with the available sensors, so that a distinction can be made between the object classes that occur. However, it also becomes clear that object classes that are not trained in advance cannot be identiï¬ed semantically with the methods known at present. Due to their signiï¬cantly greater capabilities, learning classiï¬cation algorithms have become widely accepted. A machine perception with semantic information is only technically possible in the context of driver assistance systems and automated driving because the driving area is well structured and limited to a few object classes. Additionally, only a rough class differentiation is relevant for situation recognition and situation prediction. With the current state of the technology, it is sufï¬cient to be able to distinguish between the pedestrian, cyclist, passenger car and truck or bus classes with respect to moving objects. Additionally, there are stationary obstacles, but these are usually assigned to a residue class along with the non-classiï¬able objects. For the correct assignment of the classiï¬ed objects to the trafï¬c infrastructure, it is also necessary to be able to identify reliably, with the correct semantic meaning, road surface markings, blocked areas, stop lines, trafï¬c light systems and trafï¬c signs. As this complex classiï¬cation task is not yet possible with the required degree of reliability, highly accurate and comprehensively attributed digital maps are used as a support, based on the state of the technology. Knowing its own position, the automated vehicle can use these maps to identify the stationary objects and markings expected in the sensorsâ ï¬eld of vision, together with their semantic meaning. The sensors then only have to verify that the objects are present."
124,470,0.985,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"we might wonder: Must there really have been ancestor-less living beings? Yes, wherever in the vague morning haze of evolution the horizon of life may be hidden. For suppose, BTA C5 were not true. Then there would be at least one topologically infinite lineage of living beings, which, since every reproductive step takes some finite time and there is some lower bound to the length of such a time,16 would also be metrically infinite. So there would have been living beings before big bang, which is absurd. BTA C5 comes naturally along with BTA C7: There is no infinity of ancestor-less living beings. They neither presuppose nor preclude one single ancestor-less living being, the one and only primordial cell. It is probable that the primordial soup was boiling on more than one stove. Life grew together.17 The least obvious principle is BTA C4: Every living being has either no descendants or is an ancestor of some living being that has no descendants. This makes BTA structures topologically finite towards the reproductive future. One might motivate this by the fateful certainty of the sun running out of fuel some day in the future, or by big crunch or big chill scenarios. Large-scale modeling can be depressing. My reason for BTA C4 is rather pragmatic: let us model life up to now. Or up to any time in the past we choose (as long as it contains life). Remember, by the way, that the overwhelming majority of all living beings that ever existed never reproduced, which already makes for lots of endpoints in a BTA structure. MDSs of BTA structures, on the biological interpretation, are just the set of all ancestors of some descendant-less living being. No temporal sequence is modeled directly. No life-spans are modeled. There is no way to express the fact that individual lives are finite. It is true that remote ancestors will not coexist with their remote descendants. But there isnât even so much simultaneity modeled in a BTA structure to express this."
84,572,0.985,Eye Tracking Methodology,"the two idealized models. That is, when a cumulative distribution of human visual search performance is plotted, it should appear somewhere between the two curves, as indicated by the dashed curve in Fig. 22.4. Using this methodology, it is then possible to gauge the efficiency of an individual inspector: the closer the inspectorâs curve is to the idealized systematic model representation, the better the inspectorâs performance. This information can then be used to either rate the inspector, or perhaps to train the inspector to improve her or his performance. Tracking eye movements during visual inspection may lead to similar predictive analyses, if certain recurring patterns or statistics can be found in collected scanpaths. For example, an expert inspectorâs eye movements may clearly exhibit a systematic pattern. If so, then this pattern may be one that can be used to train novice inspectors. In the study of visual inspection of integrated circuit chips, Schoonard et al. (1973) found that good inspectors are characterized by relatively high accuracy and relatively high speed, and make many brief eye fixations (as opposed to fewer longer ones) during the time they have to inspect. In a survey of eye movements in industrial inspection, Megaw and Richardson (1979), identify the following relevant eye movement parameters. â¢ Fixation times. The authors refer to the importance of recording mean fixation times (perhaps fixation durations would be more appropriate). Megaw and Richardson state that for most inspection tasks, the expected average fixation duration is about 300 ms. Longer fixation times are associated with the confirmation of the presence of a potential target and with tasks where the search times are short. â¢ Number of fixations. The number of fixations is a much more critical parameter in determining search times than fixation times and is sensitive to both task and individual variables. â¢ Spatial distribution of fixations. The coverage given to the stimulus material can be found by measuring the frequency of fixations falling in the elements of a grid superimposed over the display or by finding the frequency of fixations falling on specific features of the display. In both cases these frequencies correlate with the informativeness of the respective parts of the display as revealed by subjective estimates made by the searchers. â¢ Interfixation distances. With static displays this measure is equivalent to the amplitude of the saccadic movements. It is possible that when a comparatively systematic strategy is being employed, interfixation distances may reflect the size of the useful field of view (visual lobe). â¢ Direction of eye movements. Horizontal saccades may occur more frequently than vertical ones, which may reflect the elliptical shape of the effective useful field of view (visual lobe). â¢ Sequential indices. The most popular of these is the scanpath. In their survey, the authors review previous inspection studies where eye movements were recorded. These include inspection of sheet metal, empty bottles, integrated circuits, and tapered roller bearings. In an inspection study of tin-plated cans, the authors report that experienced inspectors exhibited smaller numbers of fixations and that each inspector used the same basic scanpath from one trial to the next although"
271,593,0.985,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"There are a couple of studies in this volume which are insightful in their own terms, but which I have some difficulty in seeing as entirely felicitous analyses of communicative figurations. Perhaps there is a risk at this point that communicative figuration research is encountering a âbandwagon effectâ: the terminological inventory that comes with communicative figurations offers a systematic, heuristic vocabulary that many are tempted to associate their research with, although strictly speaking the theoretical and analytical tools of the figurational approach are not fully compatible with the research question. For instance, in their interesting study of the attempts in the 1950s of Hamburg and Leipzig to brand themselves as urban spaces of identity, in a process of âurban collectivity buildingâ, in Chap. 7, Yvonne Robel and Inge Marszolek recognize that â[â¦] the concept of figuration exhibits a strong bias to the investigation of communicative practices. However, not only individuals are involved in these communicative practices but also collectivities and organizations.â On this basis, they argue that it is justified to ask about the role of the citiesâ media organization for the âcollective processes of identity buildingâ, and their analysis accordingly applies a cross-media critical discourse analysis of the two citiesâ broadcasting and electronic media content, considering significant metaphors which position the cities in different ways as a âbridge to the worldâ (Leipzig) and âgateway to the worldâ (Hamburg). However, my problem is that we are not presented with an actor constellation whose negotiations, contestations and concerted efforts can be said to have resulted in the âcommunicative practicesâ (or simply âcontentsâ) disseminated by the"
153,196,0.985,Solving Pdes in Python : The Fenics Tutorial I,"It is important that this space is exactly the same as the space we used for the velocity field in the NavierâStokes solver. To read the values for the velocity field, we use a TimeSeries: timeseries_w = TimeSeries(ânavier_stokes_cylinder/velocity_seriesâ)"
84,115,0.985,Eye Tracking Methodology,"encouraging, color discrimination was tested at limited peripheral eccentricities (within the central 30â¦ ). In the second, Ancman (1991) tested color discrimination at much greater eccentricities, up to about 80â¦ visual angle. She found that subjects wrongly identified the color of a peripherally located 1.3â¦ circle displayed on a CRT 5% of the time if it was blue, 63% of the time if red, and 62% of the time if green. Furthermore, blue could not be seen farther than 83.1â¦ off the fovea (along the x-axis); red had to be closer than 76.3â¦ and green nearer than 74.3â¦ before subjects could identify the color. There is much yet to be learned about peripheral color vision. Being able to verify a subjectâs direction of gaze during peripheral testing would be of significant benefit to these experiments. This type of psychophysical testing is but one of several research areas where eye tracking studies could play an important supporting role."
269,207,0.985,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"A: Our answer is going to sound like advice that is appropriate to any group of people working collaboratively together: how do temperament and discipline combine and diverge within and across individuals? How might one regard an eruption of affect â or the collection of affect around a particular person â as diagnostic of some of the things that are going on in and across the wider collaboration? Tracking a groupâs interdisciplinary processes, here, might DOI: 10.1057/9781137407962.0010"
130,326,0.985,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 1,"OCL is a static language in the meaning that has been proposed in Section 6.2.1. This means that OCL is unable to describe dynamic change of object structure. Following example comes from (NavarÄik, 2005) where the OCL is used as alternative way of"
101,306,0.985,Finite Difference Computing with Exponential Decay Models (Volume 110.0),"corresponds to a sudden change in the environment at t D t  . Choose C0 D 2T0 , C1 D 12 T0 , and t  D 3=k. Plot the solution T .t/ and explain why it seems physically reasonable. d) We know from the ODE u0 D au that the CrankâNicolson scheme can give non-physical oscillations for t > 2=a. In the present problem, this results indicates that the CrankâNicolson scheme give undesired oscillations for t > 2=k. Discuss if this a potential problem in the physical case from c). e) Find an expression for the exact solution of T 0 D k.T  Ts .t//, T .0/ D T0 . Construct a test case and compare the numerical and exact solution in a plot. Find a value of the time step t such that the two solution curves cannot (visually) be distinguished from each other. Many scientists will claim that such a plot provides evidence for a correct implementation, but point out why there still may be errors in the code. Can you introduce bugs in the cooling function and still achieve visually coinciding curves? Hint The exact solution can be derived by multiplying (4.8) by the integrating factor e k t . f) Implement a test function for checking that the solution returned by the cooling function is identical to the exact numerical solution of the problem (to machine precision) when Ts is constant. Hint The exact solution of the discrete equations in the case Ts is a constant can be found by introducing u D T  Ts to get a problem u0 D ku, u.0/ D T0  Ts . The solution of the discrete equations is then of the form un D .T0  Ts /An for"
234,224,0.985,Mobile Professional Voluntarism and international Development : Killing Me Softly?,"we are doing here is attempting to measure the measurable (as information or âdataâ) rather than the meaningful (as knowledge): Simplistically conceptualising knowledge as information makes its valuation and trade measurable but loses most of the originality of the empirical phenomenon. By contrast when scholars conceptualise knowledge as complex capabilities embodied in people and organisations, it no longer ï¬ts into the concept of an economic good that can be valued, traded, and accumulated, and its exact measurement becomes an impossibility. (Gluckler et al. 2013: 6)"
153,259,0.985,Solving Pdes in Python : The Fenics Tutorial I,"Calls to the inside method In the code snippet above, we call the inside method for each coordinate of the mesh. We could also place a printout inside the inside method. Then it will be surprising to see that this method is called not only for the points assoicated with degrees of freedom. For P1 elements the method is also called for each midpoint on each facet of the cells. This is because a Dirichlet condition is by default set only if the entire facet can be said to be subject to the condition defining the boundary."
281,63,0.985,Stochastics of Environmental and Financial Economics (Volume 138.0),"However, this notion of solution turns out to be unsuitable to deal with all significant examples. As a matter of fact, if we consider the path-dependent PDE arising in the hedging problem of lookback contingent claims, we can not expect too much regularity of the solution (this example is studied in detail in Sect. 3.2). Therefore, we are led to consider a weaker notion of solution. In particular, we are interested in a viscosity-type solution, namely a solution which is not required to be differentiable. The issue of providing a suitable definition of viscosity solutions for pathdependent PDEs has attracted a great interest, see Peng [33] and Tang and Zhang [42], Ekren et al. [18â20], Ren et al. [34]. In particular, the definition of viscosity solution provided by [18â20, 34] is characterized by the fact that the classical minimum/maximum property, which appears in the standard definition of viscosity solution, is replaced with an optimal stopping problem under nonlinear expectation [21]. Then, probability plays an essential role in this latter definition, which can not be considered as a purely analytic object as the classical definition of viscosity solution is; it is, more properly, a probabilistic version of the classical definition of viscosity solution. We also emphasize that a similar notion of solution, called stochastic weak solution, has been introduced in the recent paper [29] in the context of variational inequalities for the Snell envelope associated to a non-Markovian continuous process X . Those authors also revisit functional ItÃ´ calculus, making use of stopping times. This approach seems very promising. Instead, our aim is to provide a definition of viscosity type solution, which has the peculiarity to be a purely analytic object; this will be called a strong-viscosity solution to distinguish it from the classical notion of viscosity solution. A strong-viscosity solution to a path-dependent partial differential equation is defined, in a few words, as the pointwise limit of strict solutions to perturbed equations. We notice that the definition of strong-viscosity solution is similar in spirit to the vanishing viscosity method, which represents one of the primitive ideas leading to the conception of the modern definition of viscosity solution. Moreover, it has also some similarities with the definition of good solution, which turned out to be equivalent to the definition of L p -viscosity solution for certain fully nonlinear partial differential equations, see, e.g., [3, 11, 27, 28]. Finally, our definition is likewise inspired by the notion of strong solution (which justifies the first word in the name of our solution), as defined for example in [2, 24, 25], even though strong solutions are required to be more regular (this regularity is usually required to prove uniqueness of strong solutions, which for example in [24, 25] is based on a Fukushima-Dirichlet decomposition). Instead, our definition of strong-viscosity solution to the path-dependent semilinear Kolmogorov equation is not required to be continuous, as in the spirit of viscosity solutions. The term viscosity in the name of our solution is also justified by the fact that in the finite dimensional case we have an equivalence result between the notion of strong-viscosity solution and that of viscosity solution, see Theorem 3.7 in [8]. We prove a uniqueness theorem for strong-viscosity solutions using the theory of backward stochastic differential equations and we provide an existence result. We refer to [10] for more general results (when the path-dependent equation is not the path-dependent heat equation) and also for the application of strong-viscosity solutions to standard semilinear parabolic PDEs."
81,5,0.985,The Price of Uncertainty in Present-Biased Planning,"across studies and individuals, the difficulty of predicting a personâs temporal discount function becomes apparent. Clearly, this poses a serious challenge for the design of reliable incentives. After all, Alice and Bobâs scenario demonstrates how arbitrarily small changes in the present bias can cause significant changes in a personâs behavior. In this work we address the effects of incomplete information about a personâs present bias in two different notions of uncertainty. In Sect. 3 we consider naive individuals whose exponential discount rate is Î´ = 1, but whose present bias Î² is unknown. The only prior information we have about Î² is its membership in some larger set B. Our goal is to construct incentives that are robust with respect to the uncertainty induced by B. More precisely, we are interested in incentives that work well for any present bias contained in B. An alternative perspective is that we try to construct incentives which are not limited to a single person, but serve an entire population of individuals with different present bias values. A simple instance of this problem in which a single task must be partitioned and stretched over a longer period of time has been studied by Kleinberg and Oren [8]. But like most research on incentivizing heterogeneous populations, see e.g. [12], Kleinberg and Orenâs results are restricted to a very specific setting. They themselves suggest the design of more general incentives as a major research direction for the graphical framework [8]. Using penalty fees as our incentive of choice and a fixed reward to keep people motivated, we present the first results in this area. Our contribution is twofold. On the one hand, we try to quantify the conceptual loss of efficiency caused by incomplete knowledge of Î². For this purpose we introduce a novel concept called price of uncertainty, which denotes the smallest ratio between the reward required by an incentive that accommodates all Î² â B and the reward required by an incentive designed for a specific Î² â B. We present an elegant algorithmic argument to prove that the price of uncertainty is at most 2. Remarkably, this bound holds true independent of the underlying graph G and present bias set B. To complement our result, we construct a family of graphs G and present bias sets B for which the price of uncertainty converges to a value strictly greater than 1. On the other hand, we consider the computational problem of constructing penalty fees that work for all Î² â B, but require as little reward as possible. Drawing on the same algorithmic ideas we used to bound the price of uncertainty yields a polynomial time 2-approximation. Furthermore, we present a non-trivial proof to show that the decision version of the problem is contained in NP. Since all hardness results of [3] also apply under uncertainty, we know that there is no 1.08192-approximation unless P = NP."
103,409,0.985,Solar Particle Radiation Storms Forecasting and Analysis : The Hesperia Horizon 2020 Project and Beyond (Volume 444.0),"The goodness of fit describes how well the model predictions fit a set of observations. One way to evaluate the goodness of the fit, in case the measurement errors are known, is to construct a weighted sum of squared residuals (for details see Agueda 2008). The 2 estimator does not work very well for SEP events because during impulsive events the maximum intensities can be several orders of magnitude higher than the intensities observed during the decay phase, thus emphasizing the peak period. Therefore a better goodness-of-fit estimator is provided by the sum of the squared logarithmic differences between the observational and the modeled data. This estimator gives an equal weight of all relative residuals instead of just emphasizing the goodness of fit at the time of maximum. By evaluating the goodness of the fit under different interplanetary transport conditions (different values of ), one can objectively discern the âbest fitâ scenario (-value and associated injection profile) by minimizing the values of the goodness-of-fit estimator."
253,1088,0.985,"Autonomous Driving : Technical, Legal and Social Aspects","Autonomous driving should not be viewed as a wholly new technology appearing out of the blue, as it were. Rather, it always builds on what is already there, for example the current everyday practice of using (individual) vehicles. In light of this, Eva Fraedrich and Barbara Lenz tackle the topic of Taking a Drive, Hitching a Ride: Autonomous Driving and Car Usage. Based on an empirical examination of the needs, perceptions, and experiences of car users, they map out the complexity of attitudes towards automating driving. In particular, they place the reservations held on autonomous driving in context. They also demonstrate that the way autonomous driving is currently assessed depends fundamentally on two things: ï¬rst, the social group addressed; second, the speciï¬c use case or scenario in question. The acceptance or non-acceptance of autonomous driving will sooner or later manifest itself in the (private or commercial) purchase or non-purchase of an autonomous vehicle. How does this impact the signiï¬cance of automobile brands, given the fact that the ânewâ car must be able to do more than simply drive? In his article What Drives Consumersâ Purchase Intentions of Automated Driving Technologies? An Examination of Use Cases and Branding Strategies, David WoisetschlÃ¤ger examines the question of whatâfrom the perspective of todayâs car usersâthe relevant acceptance criteria might be in future: the experience of the auto manufacturers or that of software producers. He shows that there currently is still little readiness to purchase autonomous vehicles, and that this is largely irrespective of which sector the vehicle has been developed inâit is more important that the brand which markets the vehicle is a highly trusted one. With his article on individual consumer acceptance, WoisetschlÃ¤ger concludes this last section of the book, which spans all aspects of acceptance from the general to the individual."
62,57,0.985,"Agile Processes in Software Engineering and Extreme Programming: 17th International Conference, XP 2016, Edinburgh, UK, May 24-27, 2016, Proceedings (Volume 251.0)","Few or no questions for measuring a practice. A reason for not being able to calculate the correlation of the tools is that they cover slightly or even not at all some of the practices. An example of this is the Smaller and Frequent Product Releases practice. OPS includes four questions, while on the other hand, PAM and TAA have a single question each. Furthermore, Appropriate Distribution of Expertise is not covered at all by PAM. In case the single question gets a low score, this will affect how effectively the tool will measure an agile practice. On the contrary, multiple questions can better cover the practice by examining more factors that affect it. The same practice is measured differently. Something interesting that came up during the data analysis was that although the tools cover the same practices, they do it in different ways, leading to different results. An example of this is the practice of Refactoring. PAM checks whether there are enough unit tests and automated system tests to allow the safe code refactoring. In case the course unit/system tests are not developed by a team, the respondents will give low scores to the question, as the team members in Company A did. Nevertheless, this does not mean that the team never refactors the software or does it with bad results. All teams in Company A choose to refactor when it adds value to the system, but the level of unit tests is very low and they exist only for specific teams. On the other hand, TAA and OPS check how often the teams refactor, among other aspects. The same practice is measured in opposite questions. The Continuous Integration practice has a unique paradox among TAA, PAM and OPS. The first two tools include a question about the members of the team having synchronized to the latest code, while OPS checks for the exact opposite. According to Soundararajan [18], it is preferable for the teams not to share the same code in order to measure the practice. Questions phrasing. Although the tools might cover the same areas for each practice, the results could differ because of how a question is structured. An example of this is the Test Driven Development practice. Both TAA and PAM ask about automated code coverage, while OPS just asks about the existence of code coverage. Furthermore, TAA focuses on 100 % automation, while PAM does not. Thus, if a team has code coverage but it is not automated, then the score of the respective question should be low. In case of TAA, if the code coverage is not fully automated, its score should be even lower. It is evident that the abstraction level of a question has a great impact. The more specific it is, the more a reply to it will differ, resulting in possible low scores. Better understanding of agile concepts. In pre-post studies there is a possibility of the subjects becoming more aware of a problem in the second test due to the first test [26]. Although the testing threat, as it is called, does not directly apply here, the similar surveys on consecutive weeks could have enabled the respondents to take a deeper look into the agile concepts, resulting in better understanding of them, and consequently, providing different answers to the surveysâ questions."
135,542,0.985,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 4,"This is probably the most simple form of user modeling. For example, a great number of educational systems assume that a new student knows nothing (or that all users have some standard prior knowledge) about the domain and thus initialize the knowledge on all domain concepts to one pre-deï¬ned value [61]). This category encompasses also the stereotype-based approach. According to [34] (as cited in [61]) a stereotype consists of three main components: 1. a set of trigger conditions; 2. a set of retraction conditions; 3. a set of stereotype inferences. Trigger and retraction conditions are boolean expressions that activate or deactivate an active stereotype. Stereotype inferences of the particular stereotype serve as default assumption for the user, used for adaptation. However, stereotype-based approach is often considered static, meaning that the stereotype is assigned at the beginning of the userâs work and is not changing ever after (in fact, the stereotype can be âforgottenâ when it is used only for initialization of an overlay user model)."
84,525,0.985,Eye Tracking Methodology,"occurred; it is entirely possible that subjects were simply holding their eyes steady throughout all tachistoscopic flashes. From an eye tracking experiment, Loftus draws the argument that given more places to look at in the picture, more information can be acquired from the picture. Additional (tachistoscopic) flashes are only useful insofar as they permit acquisition of information from additional portions of the picture. Information pertinent to subsequent recognition memory seems to be acquired only from the small 2â¦ Ã 3â¦ foveal region during a given fixation, and a fixation is useful only to the degree that it falls on a novel place in the picture. The fixational perceptual span in scene perception mirrors that for reading with one important difference: meaningful information can be extracted much farther from fixation in scenes than in text (Rayner 1998). Objects located within about 2.6â¦ from fixation are generally recognized, but recognition depends to some extent on the characteristics of the object. Qualitatively different information is acquired from the region 1.5â¦ around fixation than from any region farther from fixation. At high eccentricities, severely degraded information yields normal performance. This suggests that low-resolution information is processed in the more peripheral parts of the visual field, whereas high-resolution information is processed in foveal vision. High spatial frequency information is more useful in parafoveal and peripheral vision than low spatial frequency information. Rayner and Pollatsek (1992) concede that much of the global information about the scene background or setting is extracted on the initial fixation. Some information about objects or details throughout the scene can be extracted far from fixation. However, if identification of an object is important, it is usually fixated. The work discussed in Rayner and Pollatsekâs paper indicates that this foveal identification is aided significantly by the information extracted extrafoveally. Rayner and Pollatsek conclude that it is necessary to study eye movements to achieve a full understanding of scene perception. They argue that if the question of interest is how people process scenes in the real world, understanding the pattern of eye movements will be an important part of the answer. Rayner (1998) summarizes a number of other findings and claims, some controversial, eventually asserting that given the existing data, there is fairly good evidence that information is abstracted throughout the time course of viewing a scene. Rayner concludes that whereas the gist of the scene is obtained early in viewing, useful information from the scene is obtained after the initial fixations. According to Henderson and Hollingworth (1998), there are at least three important reasons to understand eye movements in scene viewing. First, eye movements are critical for the efficient and timely acquisition of visual information during complex visual-cognitive tasks, and the manner in which eye movements are controlled to service information acquisition is a critical question. Second, how we acquire, represent, and store information about the visual environment is a critical question in the study of perception and cognition. The study of eye movement patterns during scene viewing contributes to an understanding of how information in the visual environment is dynamically acquired and represented. Third, eye movement data provide an unobtrusive, on-line measure of visual and cognitive information processing. Henderson and Hollingworth list two important issues for understanding eye movement control"
270,120,0.985,The Huawei and Snowden Questions : Can Electronic Equipment From Untrusted Vendors Be Verified? Can An Untrusted Vendor Build Trust into Electronic Equipment? (Volume 4.0),"each other out. An example of the latter is two instructions that push and pop the same variable on a stack. Another obfuscation technique is to exchange the usage of variables or registers between instances of the same malware. The semantics of the malware would be the same, but a signature that detects one instance will not necessarily detect the other. More advanced methods will make more profound changes to the code. A key observation is that, in many situations, multiple instructions will have the same effect. An example is when you want to initialize a register to zeros only: you could do so by explicitly assigning a value to it or by XOR-ing it with itself. In addition, one can also alter the malware by scattering code around and maintaining the control flow through jump instructions. The most advanced obfuscations techniques are the so-called virtualization obfuscators [16]. Malware using this technique programs malicious actions in a randomly chosen programming language. The malware contains an interpreter for this language and thus performs the malicious acts through the interpreter. In parallel with the development of obfuscation techniques, we have seen an abundance of suggestions for deobfuscators. These are tasked with transforming the obfuscated code into a representation that is recognizable to either humans or a malware detector equipped with a signature. For some of the obfuscation techniques above, deobfuscators are easy to create and efficient to use. The successes of these techniques unfortunately diminish when obfuscators replace instructions with semantically identical instructions where the semantic identity is dependent on the actual program state or when the control flow of the program is manipulated with conditional branches that are also dependent on the program state. This should, however, not come as a surprise. We learned in Chap. 5 that whether two programs are behaviourally identical is undecidable. Perfect deobfuscators are therefore impossible to design. The hardest challenge in deobfuscation is to extract the meaning of code that has been through virtualization obfuscation. The first step in doing this would have to be to reverse engineer the virtual machine, to get hold of the programming language that was used in the writing of the malicious code. The complexity of this task becomes clear when we consider the following two facts. First, the virtual machine may itself have been obfuscated through any or all of the mechanisms mentioned above. Second, many different programming paradigms have strength of expression equal to that of a Turing machine. Logic programming, functional programming, and imperative programming are all considered in Sect. 9.3âbut, in addition, we have algebraic programming [6] and Petri nets [13], to mention two of the more important. All of these paradigms can be implemented in a programming language in many different ways. Analysing the virtual machine itself is a task that can be made arbitrarily complex and the analysis must be completed before one can start analysing the operational part of the malware. This is a clear indication that we have a long way to go before the static analysis of programming code can help us against a malicious equipment vendor."
257,623,0.985,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","Example 2. Consider the tree from Fig. 1. The term corresponding to the subtree rooted in the via ATM node is   ANDp ORp Cp eavesdrop, Co (coverKey, camera) , force , stealCard, withdrawCash , where the labels of basic actions have been shortened for better readability. We denote the set of trees generated by grammar (1) with T. In order to analyze possible attacks in an attackâdefense tree, in particular, determine cheapest ones, or the ones that require the least amount of time to execute, one needs to decide what is considered to be an attack. This can be achieved with the help of semantics that provide formal interpretations for attackâdefense trees. Several semantics for attackâdefense trees have been proposed in [18]. Below, we recall two ways of interpreting attackâdefense trees and the notions of attack they entail. Definition 1. The propositional semantics for attackâdefense trees is a function P that assigns to each attackâdefense tree a propositional formula, in a recursive way, as follows P(ORs (T1s , . . . , Tks )) = P(T1s ) â¨ Â· Â· Â· â¨ P(Tks ), P(b) = xb , P(C (T1 , T2 )) = P(T1 ) â§ Â¬P(T2 ), P(ANDs (T1s , . . . , Tks )) = P(T1s ) â§ Â· Â· Â· â§ P(Tks ), where b â B, and xb is the corresponding propositional variable. Two attackâ defense trees are equivalent wrt P if their interpretations are equivalent propositional formulÃ¦. Deï¬nition 1 formalizes one of the most intuitive and widely used ways of interpreting attackâdefense trees, where every basic action is assigned a propositional variable indicating whether or not the action is satisï¬able. In the light of the propositional semantics, an attack in an attackâdefense tree T is any assignment of values to the propositional variables, such that the formula P(T ) evaluates to true. We note that this natural approach is often used without invoking the propositional semantics explicitly (e.g., in [2] or [8]). Observe also that due to the idempotency of the logical operators â¨ and â§, and the fact that every basic action is assigned a single variable, when the propositional semantics is used, cloned actions are indeed treated as the same instance of the same action. In particular, this implies that the trees ANDp (b, ORp (b, bâ² )) and b are equivalent under the propositional interpretation. Such approach might not always be desirable, especially when we do not only want to know whether attacks are possible, but actually how they can be achieved. To accommodate this point of view, the set semantics has recently been introduced in [5]. We brieï¬y recall its construction below."
192,163,0.985,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"Initially, Bloch is a high-trained physicists who becomes recruited as a nuclear physics expert (S2 in the upper-left position) to investigate how U-235 can (the object a) can be used to unleash a nuclear chain reaction. But before long, this object a (the gadget, the contraption) becomes an obsession. During the years of the Bolt, Sebastian had to ârule out everything that was tangential to his overriding taskâ (p. 332). As a result, he becomes a divided subject ($ in the lower-right position), divided between his former loyalties and his present obsession, so that he falls victim to the matheme of desire ($ â a). Bloch is now obsessed with his gadget, and Valhalla is basically machinery which allows this impossible object a to come into existence, providing Bloch the privilege of access and interaction. In the next section, the vicissitudes of university discourse will be analysed in more detail."
385,696,0.985,Advanced R,"In this section, Iâll explore three trade-oï¬s that limit the performance of the R-language: extreme dynamism, name lookup with mutable environments, and lazy evaluation of function arguments. Iâll illustrate each trade-oï¬ with a microbenchmark, showing how it slows GNU-R down. I benchmark GNU-R because you canât benchmark the R-language (it canât run code). This means that the results are only suggestive of the cost of these design decisions, but are nevertheless useful. Iâve picked these three examples to illustrate some of the trade-oï¬s that are key to language design: the designer must balance speed, ï¬exibility, and ease of implementation. If youâd like to learn more about the performance characteristics of the Rlanguage and how they aï¬ect real code, I highly recommend âEvaluating the Design of the R Languageâ (https://www.cs.purdue.edu/homes/jv/ pubs/ecoop12.pdf) by Floreal Morandat, Brandon Hill, Leo Osvald, and Jan Vitek. It uses a powerful methodology that combines a modiï¬ed R interpreter and a wide set of code found in the wild."
311,3022,0.985,The Physics of the B Factories,"order contributions to CP violation in the quark sector that might be manifest in a hypothetical new physics scenario are constrained by the results discussed in this section: i.e. CP violation in the quark sector beyond the SM cannot be O(1). In the absence of experimental indications of a departure from the CKM picture the particle physics community chooses to explore the space of possible new physics models. Given the variety of such models, one has to focus on the predictions and behavior of specific benchmark models, a number of which are discussed in Section 25.2."
311,3083,0.985,The Physics of the B Factories,"D0 â D0 mixing amplitudes. The weak phase in the mixing can still be large (and would be even much larger if the Ç«K constraint is omitted). It can lead to eï¬ects in D0 â D0 mixing that are of several percent, e.g. â0.02  SDâKS0 Ï  +0.01 (Bigi, Blanke, Buras, and Recksiegel, 2009). 25.2.3 Summary Flavor physics has a significant potential to discover new physics by its sensitivity to high energy scales through virtual eï¬ects. At present, there is no solid experimental hint of an eï¬ect beyond the SM. Lacking any preferred theoretical foundation for the observed flavor structure, an analysis of the new physics eï¬ects in low energy precision observables must thus make use of well defined and commonly agreed benchmark models. While clearly it is not possible to cover all the possibilities, a large enough set of representative benchmark models gives a picture of what kind of eï¬ects are possible. Many of the currently discussed scenarios are already highly constrained or can be strongly constrained at the currently planned experiments. One of the main motivations to extend the Standard Model with new particles with TeV masses is to solve the hierarchy problem â to stabilize the electroweak scale against radiative corrections. The flavor structure in most cases is not fixed by the rationale behind the model and hence remains mostly arbitrary from the theoretical considerations. Experimentally, on the other hand, the flavor structure in the new physics sector is tightly constrained. The legacy of the B Factories program is that in low energy flavor violating processes the dominant contributions are from the SM. The low energy eï¬ects of a viable new physics model have to be minimally flavor violating, or at least have to be close to this limit. In fact, mainly due to the data of the two B Factories, the corners of phase space for non-MFV eï¬ects in low energy processes have become very sparse. Both BABAR and Belle have performed a test of the flavor structure, in many cases at a precision level. In this respect the two experiments have performed a similar task in the flavor sector as LEP did for the gauge couplings; still we do not have any substantial hint for a crack in the structure of the SM, neither in the gauge nor in the flavor sector. Future experiments at both the energy as well as the intensity frontier will have an extended reach and a larger sensitivity. In particular, super flavor factories will refine many of the measurements performed at BABAR and Belle and thus improve the reach for new physics. Complementary to this, there will be measurements of leptonic processes at dedicated experiments, focusing especially on lepton-number and lepton-flavor violating processes. These eï¬orts will be augmented further by experiments at the energy frontier, which will be mainly the LHC experiments ATLAS and CMS for the next decade. A direct discovery of new degrees of freedom at the energy frontier â beyond the discovery of a single Higgs particle â will"
208,109,0.985,Actors and the Art of Performance,"In the end, the most powerful theatrical moments are maybe even enactments of the âdeath of the subject.â And perhaps this staging, the pinnacle of what theater can be, is exactly what Heiner MÃ¼ller means by death in transformation, for him a core element of theater that unites audience and actors in their fear of this transformation â because it is, at least, a fear we can count on."
360,449,0.985,Compositionality and Concepts in Linguistics and Psychology,"5 Conclusion The current experimental results are consistent with a view of the modiï¬cation effect in which the primary driver is the expectation of similarity and contrast between category and subcategory, as suggested by (GagnÃ© and Spalding 2011, 2014b; Spalding and GagnÃ© 2015). In particular, and contrary to what would be expected given feature-based theories of conceptual combination, the modiï¬cation effect seems to be insensitive to the normative force with which a property is predicated of the head noun concept, though the overall level of attribution of the property is quite sensitive to the force of the predication. This view, in turn, is consistent with an approach to concepts deriving from the Aristotelian-Thomistic tradition (Spalding and GagnÃ© 2013). We argue that taking the A-T view seriously has much to offer current research on concepts and might provide a fruitful basis for the development of a psychological theory of conceptual combination and of concept processing, in general."
393,370,0.985,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 2,"Roughly speaking, lookahead is the ability of a LP or submodel to predict its simulation future. More precisely, as mentioned in the the discussion on conservative synchronization protocols, a logical process at local simulation time t having lookahead L will not sent any message (real or null) with timestamp less than t+L. In general, the lookahead may vary in the course of simulation, so it is more appropriate to express it as L(t), and the earliest timestamp as t+L(t). Generally, this ability is achieved through sending messages ahead of simulation time. It can be nicely illustrated using the notation of event graphs. Consider a submodel modeling a non-preemptive G/G/1 queue with arbitrary queuing discipline depicted in Figure 7-7."
77,277,0.985,A Life Course Perspective on Health Trajectories and Transitions (Volume 4.0),"the effect of degree of satisfaction with personal relationship and change therein on self-rated health. Then, we might define a multivariate LCM with satisfaction assessments from 2004 to 2007 and health assessments from 2008 to 2011. We could then estimate the effects of the intercept and slope of the former (and temporally preceding) construct on the intercept and slope of the latter. Another important multivariate extension of the LCM stems from its implementation within the structural equation modeling framework, where it is possible to define latent variables based on the common, shared variance of a chosen set of observed variables via a common factor model (Spearman 1904). Assuming the common factor can be defined at each wave of measurement of a longitudinal study, the outcome of interest is no longer an observed variable, but the latent variable itself. A LCM (with growth factors of higher order) can hence be specified to assess the growth of the common factor (McArdle called this extension a Curve of Factors Model; McArdle 1988). For instance, rather than relying on a single health question, multiple questions (and/or objective health measurements) could be assessed, to define a common health factor. The LCM would then study the change trajectory not of the single health assessments, but of the common health factor. Specific multivariate extensions of the LMEM/LCM are particularly useful when trying to assess causality relations between multiple outcomes. For instance, the LCM can be modified to define multiple slope factors, each acting between two adjacent i-1 and i assessments. Then, each assessment at time i-1 can influence the immediately upcoming change between i-1 and i. In a bivariate setting it is then possible to estimate, for instance, the influence of variable A at time i-1 on the change in variable B between i-1 and i, and vice-versa. This analysis would allow determining whether satisfaction with personal relationships drives changes in self-assessed health, or vice-versa, or both variables influence each otherâs change. Similar extensions are possible within the multivariate LMEM, with the inclusion of instrumental variables to predict time-invariant and/or time-varying confounders (e.g., Skrondal and Rabe-Hesketh 2004). However, for this extension, as well as for all models discussed here, it is important to bear in mind that they cannot be considered as proofs of causal relationships, because it is not conceivable to assure that no other construct or variable is causing such relationships."
271,563,0.985,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"Understanding digital traces as the sequence of âdigital footprintsâ which are left by the use of digital media and services represents quite a new area of media and communication research. At the same time, we can refer this back to more prolonged discussions about whether ânewâ media also require ânewâ methods of research (see for example Golding and Splichal 2013; Hutchinson 2016), and have to contextualize it in the much more far-reaching discussion surrounding âdigital humanitiesâ and its methods (Baum and StÃ¤cker 2015; Gardiner and Musto 2015). As a phenomenon, digital traces have evoked a sophisticated but also controversial methodological discussion (Kitchin 2014). In this respect, we can notice a multiple complexity of the phenomenon. First of all, it is important to be aware that they are more than just (big) data. As âbig dataâ is used as âa catch-all, amorphous phraseâ (Kitchin and McArdle 2016), it provokes substantial discussions about its capacity. Heavily criticized by one group of scholars (boyd and Crawford 2012; Andrejevic 2014), it is regarded as the future of empirical research by others (Mayer-SchÃ¶nberger and Cukier 2013; Townsend 2013). Hence, we follow a different direction while discussing some questions of big data later more in detail. Digital traces are a kind of digital data which become meaningful because this sequence of âdigital footprintsâ is in a technical procedure of construction related to a certain actor or action, typically an individual but in principle also a collectivity or an organization. By such procedures of connecting data with entities of the social world they become meaningful information, and this is the reason why companies and other organizations of data processing are highly interested in this kind of data aggregation in relation to ârealâ people. For the purpose of empirical research, a good starting point is to define digital traces as numerically produced correlations of disparate kinds of data that are generated by practices of individual, collective and corporative actors in a digitalized media environment.1 The complexity of digital traces is reasoned by the variety of their production, but also the variety of possible correlations. Recently, digital traces and related possibilities of data generation became an issue of fundamental critique of social science methods; one that we do not share in detail but have to be aware of. The argument at this point is that with increasing datafication, methods of social sciences increasingly entered a âcrisisâ as digital traces seem to be a much more"
87,135,0.985,"Bioeconomy : Shaping The Transition To a Sustainable, Biobased Economy","emerge such that larger entities hold properties the individual elements do not exhibit (âthe system is more than the sum of its partâ). This phenomenon is usually referred to as emergence. Thus, systems thinking provides a huge potential for transdisciplinary research as it offers options to connect phenomena of different kinds. Usually, this connection implies a hierarchy in the sense that systems are constituted by elements, which are of a different kind. The connection is referred to as âstructural couplingâ. Emergent systems are structurally coupled with the entities, on which they are built. Structural coupling describes a nondeterministic relationship, in which the emergent system does not recognise the existence of the lower-order entities. For example, the human consciousness and cognitive abilities are based on neurobiological processes. However, what we think is independent from the neurobiological processes (nondeterminism) and, at the same time, our consciousness is unable to observe that the neurons of our brain are working (Fig. 4.2). For the study of wicked problems in bioeconomy, such a system understanding is relevant as it enables people to connect the material phenomena related to bio-based technologies (e.g. bioinformatics resulting in the possibility"
232,313,0.985,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"However, the superintendent immediately qualiï¬es his comments by stating the impossibility of conceiving an alternative solution. Through this remark, Yoshida integrates his decision into a wider context of action. This is to be compared with observations from current research in Natural Decision Making (NDM) that aims to account for decision making in the presence of changing conditions, ill-deï¬ned tasks, time pressure and signiï¬cant personal risks in the case of error [11]. In these conditions, the decision makerâs accounts of their decision making âdo not ï¬t into a decision-tree frameworkâ; they are not âmaking choicesâ, âconsidering alternativesâ, or âassessing probabilitiesâ, but they see themselves as acting and reacting on the basis of prior experience, generating and modifying plans to meet the needs of the situation [12]. The decision maker acts on the basis of heuristics, then develops a mental simulation to assess the feasibility of the proposed response. These studies are consistent with those of Gilboa and Schmeidler [8], who axiomatized a Case-Based Decision Theory, which postulates that the decision maker acts by comparing the current situation to one already experienced.6 Coordination and leadership modalities also change when tasks are unpredictable and interdependent, as is the case in an emergency context [13]. However, if the context for the intervention of ï¬reï¬ghters or emergency room surgeons is sometimes called an âextreme situationâ [14], in practice these âdynamicâ situations constitute the predictable working environment of the decision maker. The problem relates more to the deï¬nition of the case in question, than the solution once the diagnosis has been made. Ultimately, the âextremeâ nature of a situation is assessed differently by different researchers and does not necessarily imply that the decision maker is completely overwhelmed or out of their depth [15]. Such individuals have substantial resources at their disposal, a well-established set of procedures and the impact of their actions is limited at the scale of society. In addition, whether they focus on decision making processes based on scenarios or on more empirical approaches, investigations of the influence of stress [16], a hostile physical or social environment [17], or the formal organization [18] on the performance of the decision maker are simplistic. They lead the analysis to be focused on the physical or emotional factors that could have led Yoshida to make errors (for example his decision to evacuate the site). This cognitive approach is indicative of the common sense meaning of âemotionâ, i.e. a complex state of consciousness, usually sudden and momentary, accompanied by physiological disorders.7 However, this perspective largely ignores the role played by emotions in decision making [19]. Ellis [20] considers that emotions and values are necessary components of a decision, which does not mean that the decision becomes âirrationalâ. This assertion is illustrated by the way in which the plantâs staff decided to return to the ï¬eld following the explosion at Reactor 3 on 15 March. According to"
8,278,0.985,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","15.1 Many Years Ago In 1964, as a CERN Fellow I started doing research on hadron physics, using at first bubble chambers and then electronic detectors. Like many other Fellows I was able to benefit from the vigorous CERN academic training program and from its teachers, all of whom were excellent physicists. There I met Rolf Hagedorn for the first time and enjoyed his lectures as well as his âYellow Reportsâ. His lectures were deep and clear. His reasoning was precise and very rigorous, yet he was patient with us and had a sense of humor. For example, once at the beginning of a lecture, he told us about a competition between ethologists of various nationalities for the best essay about âthe elephantâ. While all the others described some facet of the elephantâs personality, such as its character, its mental and physical capabilities as well as its elegance or its love-life, the German competitorâs essay was entitled: âOn the definition of the elephantâ. Hagedorn then continued: âat the end of this lecture, you will not have the slightest doubt about my nationality!â . Fifteen years later, during a discussion on a possible heavy ion experiment, I reminded him of the elephantâs joke; he smiled and forgave a somewhat imprecise definition of mine. In the 1960s, Hagedorn developed a statistical approach to describe particle production which led to the concept of a finite limiting temperature for hadronic matterâthe Hagedorn temperatureâand to the formulation of the statistical bootstrap model [1] in which the exponentially rising hadron mass spectrum occurred naturally. This major discovery, however, had to wait a few years before being fully appreciated, since at the time there was no fundamental theory of the strong interactionâand no consensus on how to construct one."
118,324,0.985,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"Regarding autonomy, Landes also describes why, within Europe, the Industrial Revolution took place first in Britain. Here too, quoting Landes: Britain, moreover, was not just any nationâ¦ Remember that the salient characteristics of such a society is the ability to transform itself and adapt to new things and ways, so that the content of âmodernâ and âindustrialâ is always changing. One key area of change: the increasing freedom and security of the people. To this day, ironically, the British term themselves subjects of the crown, although they have longâlonger than anywhereâbeen citizens."
311,760,0.985,The Physics of the B Factories,"dm2X m2n .(17.1.41) X = ÎE>Ecut E>Ecut Here, dÎ/dm2X is the diï¬erential width as a function of the mass squared of the hadronic system X. For both types, n is the order of the moment. For n > 1, the moments can also be deï¬ned relative to Eâ  and m2X , respectively, in which case they are called central moments. The OPE cannot be expected to converge in regions of phase space where the momentum of the ï¬nal hadronic state is O(ÎQCD ) and where perturbation theory has singularities. This is because what actually controls the expansion is not mb but the energy release, which is O(ÎQCD ) in those cases. The OPE is therefore valid only for sufï¬ciently inclusive measurements and in general cannot describe diï¬erential distributions. The lepton energy moments can be measured very precisely, while the hadronic mass moments are directly sensitive to higher dimensional matrix elements such as Î¼2Ï and Ï3D . In most cases, one has to take into account an experimental lower threshold on the lepton momentum. The leptonic and hadronic moments give information on the quark masses and on the non-perturbative OPE matrix elements, while the total rate allows for the extraction of |Vcb |. The reliability of the inclusive method rests on our ability to control the higher order contributions in the double series and to constrain quark-hadron duality violation, i.e. eï¬ects beyond the OPE, which exist but are expected"
192,54,0.985,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"Initially, Freud discerns the Gestalt of an attractive young Victorian woman (i.e. the imaginary), and in response to her attractiveness he tries to lure her into a corner, but something seems far from perfect, something is troubling her: an anomaly has occurred. Freud therefore asks her to open her mouth, as doctors tend to do, and is terrified by what he sees. The open mouth is like the intrusion of the real: it is as if the backside of Irmaâs face suddenly stares at him (Lacan 1978, p. 186). That which should remain hidden, is suddenly too close. Her mouth is tainted, moreover, by a disconcerting white spot: a condensation of the Real, something which Lacan refers to as the âobject aâ. To ward off this disconcerting, disturbing anomaly (a), the assistance of the symbolical is called in, literally: in the form of a chemical formula for trimethylamine â N(CH3)3 â depicted above. But this chemical âsolutionâ (both in the literal and in the figurative sense of the term) does not really âsolveâ the problem. Rather, it reminds Freud of his failure to live up to the expectations and demands of professional standards involved in the treatment of such patients, standards which pre-structure the landscape of medical practice in a normative way."
32,291,0.985,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","negligible in the case where the degree is sufficiently large and this supports the results obtained by Nadakuditi and Newman [20]. A method for achieving the ultimate detectability threshold with the spectral method has already been proposed by Krzakala et al. [25]. They proposed using a matrix called the non-backtracking matrix, which avoids the elements of eigenvectors to be localized at a few vertices. A question about this formalism is: to what extent is the gap in the detectability in fact closed by the non-backtracking matrix as compared to the Laplacians? Our estimate gives a clue to the answer to this question. In order to gain further insight, we need to analyze the case of graphs with degree fluctuation. In that case, the methods using the unnormalized Laplacian and the normalized Laplacian will no longer be equivalent. Moreover, it is important to verify the effect of the localization of eigenvectors on the detectability. These problems remain as future work. Acknowledgements This work was supported by JSPS KAKENHI Nos. 26011023 (TK), 25120013 (YK), and the JSPS Core-to-Core Program âNon-equilibrium dynamics of soft matter and information.â Open Access This book is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
311,1658,0.985,The Physics of the B Factories,"â VS (r) S . (18.1.10) The singlet potential VS (r) is a series in the expansion in the inverse of the quark masses; the terms up to 1/m2 have been calculated long ago (Brambilla, Pineda, Soto, and Vairo, 2001; Pineda and Vairo, 2001). They involve NRQCD matching coeï¬cients (containing the contribution from the hard scale) and low-energy non-perturbative parts given in terms of static Wilson loops and field strength insertions in the static Wilson loop (containing the contribution from the soft scale). In this regime, from pNRQCD we recover the quark potential singlet model. However, here the potentials are obtained from QCD by non-perturbative matching and they often appear to have a diï¬erent form with respect to phenomenological potential models. Their evaluation requires calculations on the lattice or in QCD vacuum models. Recent progress includes new precise lattice calculations of these potentials (Koma, Koma, and Wittig, 2008; Koma and Koma, 2010). Using these potentials, all the masses for heavy quarkonia away from threshold can be obtained by the solution of the SchroÌdinger equation. A trivial example of application of this method is the mass of the hc . The lattice data show a vanishing longrange component of the spin-spin potential so that the potential appears to be entirely dominated by its shortrange, delta-like part. This suggests that the 1P1 state should be close to the center-of-gravity of the 3PJ system. Indeed, the measurements show consistency between data and this expected value (see experimental results in Table 18.2.1). LpNRQCD ="
82,53,0.985,Fading Foundations : Probability and The Regress Problem,"Thomas Aquinas pointed out that Aristotleâs picture of a causal regress appears to be too simple. There are at least two different causal regresses, each of them covering Aristotleâs four causes, one being vicious and one being benign. Aquinas and other scholastics refer to the distinction as a causal series per se versus a causal series per accidens. The difference should not be confused with the distinction we mentioned in Section 1.1 between knowing a proposition per se and knowing it per demonstrationem. Nor should it be simply put on a par with the distinction between necessary and accidental properties. Causal series per accidens and per se are about the ways in which its members are ordered, i.e. the way in which the causes in the series are linked. A particular cause can have necessary properties but be linked to other causes in an accidental way. Conversely, a cause may have accidental properties, but be part of a series of which the members are ordered in an essential way. In a causal series per se each intermediate member (that is each member except the first and the last) exerts causal power on its successor by virtue of the causal power exerted on this member by its predecessor. Aristotleâs stonestick-man example in the above citation involves such an essential ordering of causes. The stick causes the stone to move by virtue of the fact that the man causes the stick to move. This series consists of three elements, of which only the second (the stick) exerts causal power on its successor (the stone) by virtue of the causal power exerted on it by its predecessor (the man). Of course there will be more intermediate members if the essential ordering is longer. If for example the stone were to move a pebble, the stone would cause the pebble to move by virtue of the fact that it was moved by the stick. The salient point is that the intermediate members depend for their causing on their being caused. Things are different in a causal series per accidens. Here each member (except the last) exerts power on its successor, but not by virtue of the causal power exerted on it by its predecessor. The standard example is Jacob, who was begotten by Isaac, who in turn was begotten by Abraham. Again we have a series of three elements, but none of them, not even the second one, causes by virtue of the fact that it is caused. Isaac fathers Jacob not because of the fact that he was fathered by Abraham, but because of having had intercourse with Rebecca. A stick needs a hand to move the stone, but Isaac does not need Abraham to sleep with Rebecca. Of course, Isaac needs Abraham for his existence: if Abraham had not existed, then Isaac would not have existed either. But neither Abraham nor Abrahamâs intercourse with Sarah is the cause of Isaac begetting Jacob. As Patterson Brown formulates it in his outstanding paper on infinite causal regressions:"
251,375,0.985,inter-Group Relations and Migrant integration in European Cities : Changing Neighbourhoods,"interactive processes, confirming once again that ethnic conflict is often far from being rooted in actual cultural differences8 (Koopmans et al. 2005). This does not mean that the conflict does not exist but, as the German cases clearly show, it may assume the form of âmeta-conflictâ or âconflict over the nature of conflictâ (Horowitz 1991), a struggle over the framing and narrative used to label and explain the conflict. As we said at the beginning of this section, the symbolic capital held by neighbourhood policy communities gives them a role in this process. We can thus conclude that the ability of any given local policy community to construct its own narrative is crucial to making a neighbourhood resilient against disruptive external narratives and thus to building a more cohesive local society."
306,31,0.985,"Early Geometrical Thinking in the Environment of Patterns, Mosaics and Isometries","Various activities related to the manipulation of shapes affect their better recognition, and the need to determine the properties common for a âfamilyâ affects a better familiarity with shapes. This situation forces the use of language, which supports the transition to the descriptive level. In this way studies show that for children from the age of 5â7 years old in an active environment, evolution of the visualization is accessible for the students, although it does not seem to be used spontaneously (Coutat and Vendeira 2016). Situations that involve naming of particular shapes and the influence of names to deal with ï¬gures have appeared in many publications. There have been observations that show that the fact that the word triangle can refer to many objects of everyday life can be an obstacle to the construction of the concept of triangle (Vighi 2003a). Coutat and Vendeira (2016) have noted that children tend to give shapesâ both geometric and non-geometricânames that are associated with objects from the real world. Naming objects in this way makes it easier to deal with shapes as such. Another problem that has often been undertaken relating to the understanding of geometric ï¬gures at a slightly higher level has been the study of the understanding of the relationship between triangles and quadrangles (i.e., classiï¬cation of quadrangles). This problem can be placed at the transition between the descriptive and relational level. Research has focused on this problem due to the belief that students at this educational stage need not only know geometric properties but also need to understand relationships between properties and shapes. The aim of such studies has been to determine the ability to describe the ï¬gures and to observe attempts to create deï¬nitions. Studies have repeatedly been carried out that use models and ï¬gures that students have to group according to properties they choose themselves. Quadrilaterals and the relationships among them have often been a part of elementary school mathematics curricula. Research has suggested that students initially focus on visual characteristics of ï¬gures instead of their properties (Mack 2007). One of the reasons for having students operate with prototypical ï¬gures is their static understanding in the typical position: considering quadrilaterals to be static ï¬gures with certain properties (e.g., a trapezium is a ï¬gure with one pair of parallel sides, one of which is parallel to the bottom edge of the paper). This has been considered to be the main reason for students not being able to conceptualize the interrelationships among different quadrilaterals (Walcott et al. 2009). Some of this research has analysed these skills while examining the impact of teaching style. A constructivist approach to teaching has developed the concept of inquiry-learning through self-exploration. Researchers have been trying to determine to what extent questioning may influence an increase in competence in describing ï¬gures (Lee 2016). The conclusion that has come from these studies suggests that when students are faced with the need to answer teachersâ âwhyâ questions they are forced into deeper analysis of the properties of ï¬gures. However, this generalization may be wrong. It is necessary to take into account the different needs of students, including different ethnographically embedded learning styles and possible different interpretations of questions. This has been a particularly signiï¬cant problem in certain Asian countries (Hsu and Lin 2009)."
327,0,0.985,Bottom-Up Fabrication of Atomically Precise Graphene Nanoribbons,"Abstract Graphene nanoribbons (GNRs) make up an extremely interesting class of materials. On the one hand GNRs share many of the superlative properties of graphene, while on the other hand they display an exceptional degree of tunability of their optoelectronic properties. The presence or absence of correlated low-dimensional magnetism, or of a widely tunable band gap, is determined by the boundary conditions imposed by the width, crystallographic symmetry and edge structure of the nanoribbons. In combination with additional controllable parameters like the presence of heteroatoms, tailored strain, or the formation of heterostructures, the possibilities to shape the electronic properties of GNRs according to our needs are fantastic. However, to really beneï¬t from that tunability and harness the opportunities offered by GNRs, atomic precision is strictly required in their synthesis. This can be achieved through an on-surface synthesis approach, in which one lets appropriately designed precursor molecules to react in a selective way that ends up forming GNRs. In this chapter we review the structure-property relations inherent to GNRs, the synthesis approach and the ways in which the varied properties of the resulting ribbons have been probed, ï¬nalizing with selected examples of demonstrated GNR applications."
334,222,0.985,Protest Movements in Asylum and Deportation,"purposes, however, we define success exclusively with regard to the protestersâ central goal of preventing a particular deportation. The comparison of the âsuccessfulâ and âunsuccessfulâ cases, thus defined (see Table 6.1), at first glance reveals no striking patterns: The two groups of cases do not differ significantly with regard to any fundamental characteristic. A closer look at the beneficiariesâ initial situation, the various aspects of the local context and, above all, the protestersâ interventions, reveals that certain protest strategies tend to be effective in certain contextsâan issue which we will refer to in the following as mechanisms. First and foremost, the initial situation of the beneficiaries, especially with regard to legal aspects and their personal backgrounds, largely determines the kind of protest activities that develop. The concept of deportability (De Genova 2002, 438), signifying the possibility of being deported, can indeed materialize in different ways: Subjects become âdeportableâ for different reasons, such as the material or formal (Dublin) rejection of an asylum claim, the discovery of a situation of irregularity, and so forth. Deportation must be understood as a process which can be halted, disrupted, or blocked at different points in time. The initial situation thus determines the possibilities of intervention or, in other words, the kinds of strategies that have the potential to be successful in preventing deportation. This corresponds to Giugniâs and Kolbâs insights that the success of movement strategies can only be"
363,185,0.985,History and Cultural Memory in Neo-Victorian Fiction,"Her diary would compel attentiveness. Would claim these images. Would set her formally agapeâ (178). Like the shards of a mirror in the opening pages of Sixty Lights, which continue, in the face of death, loss and grief, to hold the world, offering it up, not whole and complete, but as slices (3â4), memory and its surrogates, photography and writing, do retrieve images from the melt of time, but only in fragments. âThis was memory as an asterix [sic]. The glory of the glimpse. The retrieval of just enough lit knowing to see [the] way forwardâ (115). While Lucy learns to celebrate these moments if not of retrieval then at least of truncated remembrance, of diffuse light, the use of the word âasteriskâ, which is most often used to mark omissions or footnotes in a text, signals here all that cannot be brought into vision. An asterisk is as the ghostly trace, âthe present mark of an absent presenceâ (Peim, 2005 79). For each memory there is a shadow archive of lost moments and forgotten features. Having posited both stories and the body as fragmented, imperfect, media of memory, Sixty Lights and Afterimage link these more explicitly by suggesting that the novel itself can stand in the place of, or embody, memory. Following Lucyâs death, Thomas feels âsuspended in a kind of absent-minded griefâ (248) and wonders if it is possible âto summon as an after-image on the surface of the retina some image-memory that has lain, pristine and packed away, unglimpsed since early adulthood?â (58). Yet this summoning of a photograph-like image, stored as memory, is not granted him within the novel. Rather, it is his re-reading of Great Expectations, âsaturated by memoriesâ (249) of reading it with his uncle and sister nearly a decade earlier that finally unlocks Thomasâ grief (249). Re-reading Great Expectations enables Thomas to re-member the sister and uncle he once read it with. Re-reading it is his act of devotion to them, consolidating and celebrating the geometry of connections that still joins them. In the absence of both Neville and Lucy, Great Expectations holds, or embodies, this, their collective memory. This idea of the story as a medium, channelling the past and forming a geometry of connections with present, is also dramatised by Afterimage. Eldon lends Annie books, which become a shared imagerepertoire between them, a collective memory, or shared experience, siphoned from the accounts they read. Annie reads with a âfeverish passion for wordsâ that recalls Rolandâs epiphanic reading at the end of Possession. She reads âwith the same bursts of clandestine intensity that one would use to pursue an illicit encounterâ (17). When Annie reads Eldonâs books about John Franklinâs expedition to the Arctic, they stand"
311,987,0.985,The Physics of the B Factories,"ÎE changes significantly, this can be compensated for by using a derived observable such as ÎE/Ï(ÎE). For similar reasons, multivariate discriminants need to be carefully constructed from variables that are as independent as possible from the position of the event in the Dalitz Plot. As in quasi-two-body analyses, care must be taken with charm mesons that either decay to the same final state or are mis-reconstructed e.g. where a lepton is mistaken for a pion or kaon. This is particularly important in searches for highly suppressed modes such as B â â K + Ï â Ï â Aubert (2008aw). The charm background can usually be much reduced by applying mass range criteria about known resonances such as D mesons, J/Ï and Ï(2S). This will result in empty bands in the Dalitz Plot that must be carefully considered when calculating eï¬ciencies and migrations. Alternatively, some charm decays are deliberately kept in the Dalitz Plot. A motivation for this comes from resonances such as the Ïc0 that have no weak phase and so can be used in an interference analysis to extract the weak phase from the Dalitz Plot. Unfortunately, the branching fraction for B â Ïc0 h is too small to be useful currently. When the Dalitz Plot is represented as a Cartesian coordinate system, with the square of the mass of pairs of final state particles as the x and y axes, the phase space is roughly triangular in shape. Figure 17.4.17 illustrates the distribution of events extracted from data in the decay of B 0 â KS0 Ï + Ï â . The distribution of events on the Dalitz Plot is plotted after applying a constraint on the B meson mass (mES = mB ). This improves the resolution and ensures that all events fall within the kinematic boundaries of the Dalitz Plot. An alternative often used is a âsquareâ Dalitz Plot where one of the axes is transformed into a âhelicity-likeâ variable e.g. (Aubert, 2007v) or see Chapter 13. Although this transforms the distribution of resonances from simple bands parallel to the axes to more complex hyperboloids, the âsquareâ Dalitz Plot has a number of benefits. It can expand the region near areas where large variations are occurring such as in narrow resonances like the Ï. Bands near the Dalitz Plot edges also"
50,122,0.985,"Seeing Ourselves Through Technology : How We Use Selfies, Blogs and Wearable Devices To See and Shape Ourselves","What we cannot measure The sex tracking app SpreadSheets offers a striking example of how little our devices can really measure. Spreadsheets is an iPhone app that promises to measure and quantify our sexual activity. Similar to one of its forerunners, Bedposted.com, its purpose is to create a log of each time you have sex, but while Bedposted.com required you to enter the information yourself (J.W. Rettberg 2014, 87â8), Spreadsheets monitors your sex life automatically. That is, Spreadsheets tracks every aspect of sex that an iPhone can automatically track when placed on a bed: frequency of thrusts, total duration of thrusting activity and the decibel levels of the participants in the act. Thatâs really all an iPhone can automatically measure about sex: motion, sound and when that motion and sound begins and ends. As Whitney Erin Boesel (2013) points out in a blog post to Cyborgology, that means that this app can only measure a very heteronormative idea of sex as thrusting penetration. The Spreadsheets app applies a technological filter to its representation of sex. The representation is constrained by what an iPhone can measure. Interestingly enough, though, the way a machine â or specifically a smartphone in the early twenty-first century â can understand or perceive sex is very close to a strong cultural understanding of sex that we are familiar with from traditional pornography. Sex seen through this cultural filter is all about thrusting hard and fast, screaming loudly DOI: 10.1057/9781137476661.0007"
257,285,0.985,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","In this section we present our framework for reasoning about information leakage, extending the notion of information leakage games proposed in [4] from only simultaneous games with hidden choice to both simultaneous and sequential games, with either hidden or visible choice. In an information leakage game the defender tries to minimize the leakage of information from the system, while the attacker tries to maximize it. In this basic scenario, their goals are just opposite (zero-sum). Both of them can influence the execution and the observable behavior of the system via a specific set of actions. We assume players to be rational (i.e., they are able to figure out what is the best strategy to maximize their expected payoff), and that the set of actions and the payoff function are common knowledge. Players choose their own strategy, which in general may be mixed (i.e. probabilistic), and choose their action by a random draw according to that strategy. After both players have performed their actions, the system runs and produces some output value which is visible to the attacker and may leak some information about the secret. The amount of leakage constitutes the attackerâs gain, and the defenderâs loss. To quantify the leakage we model the system as an information-theoretic channel (cf. Sect. 2.2). We recall that leakage is defined as the difference (additive leakage) or the ratio (multiplicative leakage) between posterior and prior vulnerability. Since we are only interested in comparing the leakage of different channels for a given prior, we will define the payoff just as the posterior vulnerability, as the value of prior vulnerability will be the same for every channel."
192,334,0.985,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"From the perspective of university discourse, the most challenging disaster is the intrusion of the real. Suddenly, there is a recurrence of the tumour in the mice, which puts Cliffâs data into question. Recurrence could be an interesting finding in itself, revealing something about the complexity of the interplay between virus, immune system and cancerous cells, and initially Cliff is fascinated by the phenomenon. But in view of the pressures, it implies the failure of his R-7 project. To make matters even worse, replication tests conducted in other labs are not getting the expected results. They fail to confirm Cliffâs claims. Marion concludes that they published too opportunistically, too soon, and decides to retract the Nature paper. According to Bouter (2015) there are only losers in this novel, but this does not seem completely true. In the aftermath of the crisis, Sandy accepts a new position, as head of a new private cancer facility in Wellesley, a position that is bound to make him an even richer man. But Cliff also seems to recover from the trauma. During the denouement or catharsis stage (the final chapters of the novel), when the discourse of the analyst takes the floor, Cliff realises that he will be able to work again with a clear name. He has lost 2 years of work, but is more experienced now, because of all the turmoil, and resolved to make a better start. He still loves science, the slow, exhausting work, the rush of discovery, and will never give that up. He continues to be susceptible to the quest for knowledge, coming from a promising new object of"
223,116,0.985,Knowledge and Action (Volume 9.0),"before they erected them in reality, a wonderful illustration of how the abstractness of Platonic forms is turned into the concreteness of visible objects. Seemingly a textbook application of practical reason, the intentionality of the plan preserved in the materiality of physical structures. Only seemingly, though, for in the case of the early Zuiderzee polders the impact of technological change was seriously underestimated. To make a long and complicated story short and simple, the constructed places proved to be too many, too small, and too closely packed, the location of the brick-built stores, schools, police stations, and hospitals obsolete before the mortar had dried. In addition, and because roads and houses are costly both to build and to tear down, the spatial nonoptimality tends to stick. The Achilles-heal of every optimizing location theory. LÃ¶schâs remarks about the comparison between rational theory and faulty reality come readily to mind, Hegelâs epistemology of self-conscious reevaluation as well. The reason is that the planning of the later polders, especially the Oostelijk and the Zuidelijks Flevolands, has become increasingly sophisticated. But that development rather heightens than lessens my surprise that Christallerâs static, deterministic, and inelegant theory was used at all. The only excuse I can think of is that we are all children of our own time and place, the Dutchmen of 1930 as much as I at eighty. And what a happy circumstance that is. For what saved the subjects of the great polder project was not the machinations of social engineering but the circumstance that Holland is an open society, its citizens free to design their honeycombs as their fancy fancies. In that respect the Third Reich was obviously different. But what richer pasture could the likes of Konrad Meyer and Walter Christaller have wished for than the newly conquered Lebensraum (living space) of Eastern Europe, a vast area showered down on them as a gift from the FÃ¼hrerâs heaven. Like the Dutch polders, also the territory that the Reichskommissariat fÃ¼r die Festigung deutschen Volkstums (Reich Commission for German Resettlement and Population Policy) was commissioned to settle had the characteristics of a homogeneous plain, the techniques of ethnic cleansing as merciless in Poland, Lithuania, Belarus, and Ukraine as anywhere else. The clearing of sufficient living space in the East was de facto the cornerstone of Nazi foreign policy, the very precondition for the Germanization that was meant to follow, the Entfernung (removal) of foreign elements setting the stage as effectively as the digging of the dikes in Holland. As an amateur artist, Hitler surely knew that without a properly prepared canvas there will never be any painting. And as the Leader of a populist movement he was well aware that no political battle is more decisive than that about the boundary between identity and difference, one and many, us and them, me and you. Such was consequently also the purpose of the charts, tables, and maps that came out of Meyerâs office: the utopia of a totalitarian Herrschaft (rule) projected into optimally located settlements, everything and everyone in its proper place. Seventy-five years later the whole affair strikes me as a Dadaesque blend of Kandinskyâs Bauhaus, Malevichâs suprematism, Picassoâs cubism, Ernstâs surrealism. The irony of the entartete Kunst (degenerate art) in its proper perspective."
257,259,0.985,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","shown that when defender and adversary act simultaneously, the adversaryâs optimal strategy also requires randomization [4]. â We use our framework in a detailed case study of a password-checking protocol. The naive program, which checks the password bit by bit and stops when it finds a mismatch, is clearly very insecure, because it reveals at each attempt the maximum correct prefix. On the other hand, if we continue checking until the end of the string (time padding), the program becomes very inefficient. We show that, by using probabilistic choice instead, we can obtain a good trade-off between security and efficiency. Plan of the Paper. The remaining of the paper is organized as follows. In Sect. 2 we review some basic notions of game theory and quantitative information flow. In Sect. 3 we introduce our running example. In Sect. 4 we define the visible and hidden choice operators and demonstrate their algebraic properties. In Sect. 5, the core of the paper, we examine various scenarios for leakage games. In Sect. 6 we show an application of our framework to a password checker. In Sect. 7 we discuss related work and, finally, in Sect. 8 we conclude."
311,1747,0.985,The Physics of the B Factories,"In addition, before the panorama of states is fully clarified, there is always the lurking possibility that some of the observed states are misinterpretations of threshold effects: a given amplitude might be enhanced when new hadronic final states become energetically possible, even in the absence of resonances."
228,372,0.985,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"and insects living in colonies [44]. We have finally got successful experiments and methods based on ant or termite colony observation [13, 20, 43, 45]. Observations of birds in V-formation inspired many researchers to create and to develop the concept of particle swarm optimization. [24]. Those studies in the field of AI were also inspired by information obtained from marine biologists on the collective intelligence of a shoal of fish or plankton. Other sources of inspiration stemmed from the development of industry, in particular the automotive industry in that case. Particle swarm optimization was created thanks to studies on, among others, sandblasting of a car body or other corroded metal parts. Hence, generally, this branch of AI has been called swarm intelligence [11, 14, 25, 38]. Conversion of those intelligence mechanisms prevailing among simple individuals into the field of computer systems resulted in creation of the current sometimes called computational swarm intelligence. It exists parallel to the branch of science called multiagent systems and those two fields often overlap one another. Although they are often not directly based on associations with colonies of living organisms, they are often similar in their rules of operation. They enable creation of interesting implementations in the domain of parallel computing. The development of swarm intelligence was preceded by the development of multiple-valued logic, in particular, fuzzy logic. The author of fuzzy logic is an American professor at Columbia University in New York City and Berkeley University in California, Lotfi A. Zadeh, who published the paper entitled âFuzzy Setsâ in the journal, Information and Control, in 1965 [5]. He defined the term of fuzzy set there, thanks to which imprecise data could be described using values from the interval (0,1). The number assigned to them represents their degree of membership in this set. It is worth mentioning that in his theory Zadeh used the article on three-valued logic published 45 years before by a Pole, A. Janukasiewicz [6]. That is why many scientists in the world regard this Pole as the ""father"" of fuzzy logic. The next decades saw the rapid development of fuzzy logic. As the next milestones in the history of that discipline one should necessarily mention L-R representation of fuzzy numbers proposed by D. Dubois and H. Prade [7, 8], which enjoys great success today. Coming back to the original analogy, an observer can see a trend, that is, a general increase during a rising tide or decrease during low tide, regardless of momentary fluctuations of the water surface level. This resembles a number of macro- and micro-economic mechanisms where trends and time series can be observed. The most obvious example of that seems to be the bull and bear markets on stock exchanges, which indicate the general trend, while shares of individual companies may temporarily fall or rise. The aim is to capture the environmental context of changes in the economy or another limited part of reality. Changes in an object described using fuzzy logic [30, 32] seem to be thoroughly studied in many papers. But it is not necessarily the case as regards linking those changes with a trend [39, 41, 42]. This might be the opportunity to apply generalizations of fuzzy logic which are, in the opinion of authors of that concept, W. KosinÌski [9â11] and his team [12, 13], Ordered Fuzzy Number (OFN) [28, 33, 40]. There are already interesting studies available published by well-known scientists [1, 18] that present successful implementation of fuzzy logic to swarm intelligence methods, including methods inspired by ant and termite colonies. However, according to the best knowledge of"
249,129,0.985,Advances in Proof-Theoretic Semantics (Volume 43.0),"5.3 Decidability Although the strategies of Kreisel and Goodman may be sufficient for obtaining a consistent version of the Theory of Constructions, their approaches are not clearly grounded in considerations which follow directly from the BHK interpretation itself. As such, it seems reasonable to consider the status of the other principles which figure in the Kreisel-Goodman paradox. We will begin by considering the role of the decidability of the proof relation. As we have seen, this is formalized within the system T by the rule Dec, which may in turn be understood to ensure that terms of the Ï st are always defined.30 We assertion to the objects pertaining to some level L n in the hierarchy of constructions (i.e. one which might make reference to proofs of yet higher level) to a proof which is present at level L n+1 . Such an assumption plays an important instrumental role in Goodmanâs formulation of the clause (P2â ) in T Ï as it allows him to replace the quantifier over all constructive proofs with one which only ranges over the level one higher than that of the term interpreting A â B. To justify this he writes âIt seems to us essential to the intuitionistic position that given a fixed assertion A about a well-defined domain, there is an a priori upper bound to the complexity of possible proofs of Aâ [17, p. 111]. But as Weinstein [49] observes, it is not at all clear whether there is anything implicit in the BHK interpretation itself which justifies this assumption. 29 This is at least true of the formulations given by Heyting [20, pp. 13â15] and Kolmogorov [24, pp. 329â330]. Martin-LÃ¶f [30, p. 128] claims that typing is already implicit in clause (Pâ ) if we additionally accept that every function must have a type as its domain. But it is unclear what necessitates that we adopt such an assumption. 30 For reasons discussed in footnote 18 the same effect is also formally achieved by either reasoning about the proof relation in intuitionistic first-order logic or by adopting Kreiselâs [26] proposal to base the Theory of Constructions on the calculus Î»Î² â (wherein all terms always reduce to normal form)."
223,11,0.985,Knowledge and Action (Volume 9.0),"and space be documented by which indicators and empirical methods? How much are the spatial conditions of actions exposed to historical transformation? What exactly is the role and importance of spatial representations for the construction of sociocultural realities in the past, present and future? How does the digital revolution change the historically established societyâspace relations? What are the spatial implications for the formation of knowledge? Is the term environment an abstract category, a social macrophenomenon, a local cluster of individual factors of influence, or a localized culture? How can one measure an environmentâs impact on action and knowledge production (Meusburger, 2015a)? These and other questions indicate that relations between knowledge, action, and space are not as simple as some people might assume or as some decision and risk models or traditional rational choice theories suggest. The questions simultaneously underscore the urgent need to explore the interdependencies of knowledge, action, and space from different disciplinary angles, scales of analysis,3 time dimensions,4 and ontologies. The main ambition of this book is to contribute to the clarification of the linkages between knowledge, action and space beyond the well-established models. To redeem this claim it is first necessary to overcome the problematic legacy of homo oeconomicus and traditional rational choice theories and discuss some of the reasons why the spatial dimension was neglected or played only a marginal role in action-centered social theories. If we want to deepen the insights into the relations between action, knowledge and space, then the spatial dimension needs as much theoretical inquiry as the relations between knowledge and action (see the chapters by Werlen (Chap. 2), Ernste (Chap. 3), Olsson (Chap. 4), GardenfÃ¶rs (Chap. 12), and Berthoin Antal and Friedman (Chap. 13) in this volume)."
315,65,0.985,Situating Children of Migrants Across Borders and origins : a Methodological Overview,"Another important component of comparative research on the children of immigrants concerns the geographical levels (local, national, etc.) at which comparisons are made and the types of groups that are used in such comparisons. Choosing the level of analysis has major conceptual implications and suits different research questions. This is an issue that has been ever present in recent migration research. In 1991, Bovenkerk et al. concluded that defining the correct level of comparison in European migration research is difficult, as research at the time either âmirroredâ European findings to that in one particular country or lacked a meaningful comparison point at all and took a too general approach. Given the short supply of general theories that can be or are applied, Bovenkerk and colleagues argued that comparisons in this sense run the risk in being solely descriptive and not really add to the explanation of phenomena. They concluded that the choice between generalization and specificity requires different levels of abstraction (Bovenkerk et al. 1991). With an outlook toward more general migration research, Green (1994) outlined three different models of comparison that are often used when thinking about the choice between level and groups: the linear model, the convergent model, and the divergent model. Given that the linear model, which focuses on following immigrants from origin to destination, does not necessarily relate to the experiences of the children of immigrants, it will not be explored in this section.8 Convergent modIt can nonetheless be the case that following the parents and their migration motives and patterns is important for understanding the outcomes of their children."
289,1077,0.985,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","Furthermore, for this definition of cost, the domination assertion (B) holds as well. The proof relies on the fact the functions g and gÌ, where gÌ is Î»n. max0â¤nâ² â¤n g(nâ² ) [19], dominate each other. Although this approach seems viable, and does not require the function g to be monotonic, it is a bit more complicated than we would like."
211,217,0.985,Entrepreneurial Cognition : Exploring The Mindset of Entrepreneurs,"trials, individuals test new notions of the self (created through play) by projecting them into the relatively near future. Without the constraints of distal goals, the failed entrepreneur can freely generate and strive for near goals, such as creating and trying on provisional identities developed through identity play. Finally, while fantasy in a cognitively deconstructed state may be detrimental (Baumeister 1990), as part of identity play it may be very useful. Identity play âgenerally unfolds at the threshold between fantasy and reality, or the boundary between dreams (i.e., the possible selves in our heads) and reality (i.e., concrete possibilities available in the given world at any given time)â (Ibarra and Petriglieri 2010: 15). Thus, fantasy is insufficient for identity play as it requires flirtations across the boundary between dream and reality. Fantasy in a cognitively deconstructed state is problematic because it is free of any reality, thus making it rather ineffective in generating identity alternatives. However, when individuals play out identity fantasies, they are able to creatively explore (Brown and Starkey 2000) or flirt with ideas of a provisional future self that actually have meaning in reality, which can improve the chance of forming an identity that is positive (Shepherd and Williams 2018). For instance, the failed entrepreneur may play out his or her fantasy of working in a non-profit organization as an alternative new identity by working with a local non-profit for two weeks."
311,943,0.985,The Physics of the B Factories,"such that only a few events can be expected to appear in the data. Higher mass resonances that peak outside the invariant mass selection region can still feed-down to the signal region because: they have a large width, such as f0 (1370); through reflection, where a daughter particle is mis-identified, such as in B â ÏÏ + ; or where a resonance has a long range component e.g. the S-wave component of the K0â (1430). These backgrounds are often treated in a separate analysis that looks in the mass region above the resonance under consideration (since this is still blinded) and performs a ML fit to the higher mass region using mES , ÎE, the multivariate discriminant, and the reconstructed mass. Once the yield is extracted, the number of events in the resonance signal region is estimated by extrapolating the fitted mass p.d.f. (or a fit to the extracted s W eights, see Chapter 4) down to the low mass region and integrating. A further category of background occurs when the B meson decays to the same final state without passing through a resonance, such as B + â Ï + Ï â Ï + when looking for B + â Ï0 Ï + or B 0 â Ï + Ï â Ï0 when looking for B 0 â Ï0 Ï0 . These backgrounds also become important when D mesons are used as calibration channels as these ânon-resonantâ decays can be responsible for a significant number of the events underneath the calibration channel of interest. Strictly speaking, ânon-resonantâ means a decay in a Dalitz Plot that is uniformly distributed in phase space (see later and Chapter 13). However, this distinction is generally ignored and any final state which cannot be represented by a peaking structure is usually categorized as non-resonant. This has practical benefits when performing a fit as it is often diï¬cult to identify the source of smoothly varying distributions. A fit which uses more than one such distribution is likely to find that the background events flow between the diï¬erent distributions without aï¬ecting the significance of the signal. As a result, some papers will report a non-resonant measurement while others will simply consider it as part of the background. The signal modes, mis-reconstructed signal modes (if used), continuum background and BB backgrounds distributions are used in a ML fit to extract the signal yield, branching fraction, ACP , and longitudinal polarization fL . The observables used are usually mES , ÎE, the multivariate discriminant and the intermediate resonance masses. If an angular analysis is required, the helicity cos Î¸H of the resonances is also used. In this later case, the reconstruction eï¬ciency as a function of cos Î¸H must be taken into account, often by multiplying the expected true distribution by a polynomial of a suitable order. The eï¬ciency for the other variables is usually treated as uniform. The observables used in the p.d.f.s are usually assumed to be uncorrelated and the total p.d.f. is taken to be the product of the separate individual p.d.f.s. However, in some cases this assumption is invalid and the correlations need to be taken into account explicitly. If the correlation only exists between two observables and is reasonably linear, then the correlation can be reduced by using rotated"
247,58,0.985,Humanities World Report 2015,"can communicate insights from their research through similar media, as well as book reviews. That the humanities have such a role may seem obvious, but it turns out to be a disputed area. Academic research in literary criticism, art history and music has certainly had an aesthetic function in the past. Nowadays, there is no shortage of critics outside academia, writing and talking in the media, who aim to guide the general public in its appreciation of different kinds of artwork. But is this something modern-day academics do in the humanities? In the case of literature, it may actually be controversial to attribute such a role to researchers, perhaps because of trends such as postmodernism or, more generally, the âdemocratisationâ of public life, and hence the demise of academic expertise in matters of aesthetic appreciation.31 It was notable that very few of our respondents mentioned aesthetic appreciation as a value of the humanities. Here are two exceptions, both from North America: NA11: I would remind [an impatient and potentially hostile audience] first, that the way they live their lives and the pleasure they get from the world, some high percentage of that comes from their education in the humanities. Learning how to distinguish between good [and bad] forms of communication ... , between canned and serious things, between superficial things and profound things. And this doesnât just go for aesthetic experience, but just being an intelligent consumer of media, politics, business and sciences. Again, I know this sounds old-fashioned but it helps people think broadly and deeply with discrimination. If they donât care about that, then thereâs not really much to say. You canât convince them. NA14: Iâd prioritise aesthetic appreciation, i.e. the way research can make possible new and sophisticated forms of aesthetic pleasure. This is bound up with the way it shows how aesthetic pleasure has changed over time. But note that in the first of these quotes aesthetic value is mentioned only briefly and is considered âold-fashionedâ."
213,135,0.985,Collider Physics Within The Standard Model : a Primer,"which we have alreadyP introduced. R is dimensionless and is given in perturbation theory by1 R D NC i Q2i F.t; Ës /, where F D 1 C O.Ës /. We have already mentioned that for this process the âanomalous dimensionâ function vanishes, i.e., .Ës / D 0, because of electric charge non-renormalization by strong interactions. Let us recall how this happens in detail. The diagrams that are relevant for charge renormalization in QED at 1-loop are shown in Fig. 2.12. The Ward identity that follows from gauge invariance in QED requires the vertex (ZV ) and the self-energy (Zf ) renormalization factors to cancel, and the only divergence remains in Z , the vacuum polarization of the photon. Hence, the charge is only renormalized by the photon vacuum polarization blob, and it is thus universal (the same factor for all fermions, independent of their charge) and not affected by QCD at 1-loop. It is true that at higher orders the photon vacuum polarization diagram is affected by QCD (for example, at 2-loops we can exchange a gluon between the quarks in the loop), but the renormalization induced by the divergent logs from the vacuum polarization diagram remain independent of the nature of the fermion to which the photon line is attached. The gluon contributions to the vertex (ZV ) and to the self-energy (Zf ) cancel, because they have exactly the same structure as in QED, and there is no gluon contribution to the photon blob at 1-loop, so that .Ës / D 0. At the 1-loop level, the diagrams relevant for the computation of R are shown in Fig. 2.13. There are virtual diagrams and also real diagrams with one additional gluon in the final state. Infrared divergences cancel between the interference term of the virtual diagrams and the absolute square of the real diagrams, according to the"
58,253,0.985,Enabling Things to Talk,"The Virtual Entity and IoT Service FGs include functions that relate to interactions on the Virtual-Entity and IoT-Service abstraction levels, respectively. Figure 7.12 shows the abstraction levels and how they are related. On the left side of Fig. 7.12, the physical world is depicted. In the physical world, there are a number of Sensors and Actuators that capture and facilitate the change of certain aspects of the physical world. The Resources associated to the Sensors and Actuators are exposed as IoT Services on the IoT Service level. Example interactions between applications and the IoT system on this abstraction level are âGive me the value of Sensor 456â or âSet Actuator 867 to Onâ. Applications can only interact with these Services in a meaningful way, if they already know the semantics of the values, e.g. if Sensor 456 returns the value 20, the application has to be programmed or configured in such a way that it knows that this is the outdoor temperature of the car of interest, e.g. Car MXD â 123. So, on this level no semantics is encoded in the information itself, nor does the IoT system have this information, it has to be a-priori shared between the Sensor and the application. Whereas interaction on the IoT Service level is useful for a certain set of applications that are programmed or configured for a specific environment, there is another set of applications that wants to opportunistically use suitable Services in a possibly changing environment. For these types of applications, and especially the Human Users of such applications, the Virtual Entity level models higher-level aspects of the physical world, and these aspects can be used for discovering Services. Examples for interactions between applications and the IoT system on this abstraction level are âGive me the outdoor temperature of Car MXD â 123â or âSet lock of Car MXD â 123 to lockedâ. To support the interactions on the Virtual Entity level, the relation between IoT Services and Virtual Entities needs to be modelled, which is done in form of associations. For example, the association will contain the information that the outdoor temperature of Car MXD â 123 is provided by Sensor 456. Associations between Virtual Entities and IoT Services are modelled in the Information Model (Sect. 7.1.4)."
20,127,0.985,"Communicating, Networking: Interacting: The International Year of Global Understanding - IYGU","underpin each transactionâsmall and largeâare similar. However, this assumes the universality of the European and United States superpower discourse (Said 1994). In Saidâs words: Without signiï¬cant exception the universalizing discourses of modern Europe and the United States assume the silence, willing or otherwise, of the non-European world. There is incorporation, there is inclusion; there is direct rule; there is coercion. But there is only infrequently an acknowledgement that the colonised people should be heard from, their ideas known. (Said 1994, p. 50)"
217,178,0.985,Finite Difference Computing With Pdes : a Modern Software Approach,"1.10.6 Visualization The functions for visualizations differ significantly from those in the undamped case in the vib_undamped.py program because, in the present general case, we do not have an exact solution to include in the plots. Moreover, we have no good estimate of the periods of the oscillations as there will be one period determined by the system parameters, essentially the approximate frequency s 0 .0/=m for linear s and small damping, and one period dictated by F .t/ in case the excitation is periodic. This is, however, nothing that the program can depend on or make use of. Therefore, the user has to specify T and the window width to get a plot that moves with the graph and shows the most recent parts of it in long time simulations. The vib.py code contains several functions for analyzing the time series signal and for visualizing the solutions."
253,479,0.985,"Autonomous Driving : Technical, Legal and Social Aspects","most often is 1.1 s (see Fig. 15.3), with an average value of 1.4 s. If drivers complied with the legally stipulated speciï¬cations, trafï¬c on many roads would come to a standstill much earlier than is currently the case. This paper builds on the papers by Friedrich [2] and Pavone [3] in this book. While [2] describes the general effects of autonomous vehicles on the transportation system, this paper addresses the modeling of autonomous and human-driven vehicles as well as the effects of autonomous vehicles on trafï¬c management. Paper [3], by contrast, largely ignores questions of trafï¬c flow and trafï¬c control and focuses primarily on the optimal allocation of supply in relation to demand based on the premise that vehicles can be shared. We can quite rightly conclude at this point that a combination of these approaches, together with a correct description of the share of travelers who would opt for transportation via a robotic âmobility-on-demandâ system, allows the best possible appraisal of the potential of autonomous vehicles. The paper also does not consider effects that would result from a fundamentally different organization of transportation. One example of this would be the EUâs CityMobil project, in which such scenarios are discussed and examined in greater detail [4]. This paper will examine how autonomous vehicles affect typical trafï¬c management applications by looking at a few examples which have not been developed in all speciï¬cs. These examples, in order of increasing complexity, are the simulation of a single trafï¬c signal system (Sect. 15.4), simulation of an intersection controlled by an adaptive trafï¬c signal system (Sect. 15.5), simulation of a green wave (Sect. 15.6) and the simulation of an entire city (Sect. 15.7). Some of the questions to be considered here can draw on the effects of the introduction of intelligent speed control (autonomous intelligent speed controlâAIC) on trafï¬c flow on highways in particular [5]. There is a great deal of literature on this subject; the dissertation [5] and parts of the book [10] provide a more in-depth overview than is possible in this chapter. One such AIC scenario is highly similar to Use Case #1 âInterstate Pilot Using Driver for Extended Availabilityâ, which in turn (from a trafï¬c-flow standpoint) is a special variant of Use Case #3, âFull Automation Using Driver for Extended Availabilityâ. This is also the use case that plays the most important role in this chapter, notwithstanding the fact that it is rather irrelevant from the trafï¬c-flow standpoint whether the driver is available or not. The availability of the driver could be important if the impact of failures on trafï¬c flow was being examined, but this topic will not be addressed in this book. This would require detailed statistics regarding how frequently something of this sort occurs and under what circumstancesâinformation which is not available at the current stage of technology of autonomous vehicles. The Use Cases #2 (Autonomous Valet Parking) and #4 (Vehicle on Demand) play only a minor role in this chapter, although Use Case #4 should be treated like Use Case #3 from a trafï¬c-flow standpoint. Use Case #2 would be interesting because it has an influence on parking search trafï¬c and thus indirectly on trafï¬c demand and thereby also trafï¬c control, but on the trafï¬c management level it would require a signiï¬cantly more complex approach than can be achieved hereâit would"
85,164,0.985,Bayesian Methods in the Search for MH370,"predictions and the measured values. Each sample is weighted according to its overall likelihood and the combination gives the marginals in Fig. 10.7. The horizontal axis is again the measurement index and the vertical axis is residual error, dark points have high probability, and the error bars show the one-sigma limits. The figure also overlays the one-sigma lines for the assumed measurement noise. For BTO this varies with the message type, for BFO it is a fixed Â±7 Hz. There is no BTO measurement for the C channel communications that occurred at 18:39 and 23:14. The BFO has been discarded at 18:25 and 00:19 since these readings are thought to be unreliable. The plots only show residuals when a valid measurement is available. For the BTO residual there are two larger residuals on the final two measurements. These measurements are actually very close together in time: the first is an R600 message at 00:19:29 and the second is an R1200 message at 00:19:37. The reason why both of these measurements show large residuals is that they are not consistent with each other: the residuals have opposite signs, reflecting that one measurement is longer in range than the prediction and the other is shorter. In the absence of a reason to prefer one over the other, we use both measurements and let the filter find paths that are the best statistical fit. The BFO residuals are statistically consistent with the empirical error model."
49,292,0.985,Artificial Intelligence and Cognitive Science IV,"surprisingly simple and efficient together. We can mention the ant colonies as an example. This approach utilizes simple interactions between many small agents in order to solve complex problems. Systems inspired by this concept are now used to solve complex tasks, as is Travelling Salesman Problem (TSP) or some optimization tasks [1,2]. But mans strive to go further, to copy the Nature in her ability to create intelligent beings with similar qualities to the mankind reaches far into our history. Despite the many attempts, this old aim still has not been reached. The autonomous functioning of a robot in real environment meets many challenges. The control architecture must give the robot ability to react timely with respect to the local disturbances and uncertainties, while adapting to more persistent changes in environmental conditions and task requirements [3]. This adaptation occurs in such a way that the robot optimizes its behavior so as to minimize required effort and maximize its profit (i.e. gather maximum environment resources while consume minimum energy). The result is often called as rational behavior. Learning and adaptation should occur without outside intervention â unsupervised learning, which means that the agent itself must decide what, is good and what is not. The inherent problem in this area of research is that considerable work effort is required to equip robots with adequate means for sensing (sensors) and actuation (effectors). Recognition and transformation of data in noisy and voluminous environment poses an obstacle in the robot design. Thus, to study control architecture the research moved from real environments to virtual ones. The research of behavior no longer needs a physical robot; the virtual representation of a robot can provide the same level of embolisms as real one. For these virtual robots in analogy with the MultiAgent Systems (MAS), the term âagentâ started to be used [4]. These two fundamentally different approaches merge by the selection of common name âagentâ. MAS originally used the top-down approach, focused on planning, problem solving which we can consider as a high level function of some animals and also humans. On the contrary the bottom-up approach used in robotics and also by nature in the simple organisms is focused on reactions to the stimuli. This approach uses emergence as a tool for creating more complex and complicated behavior by chaining the most basic reactions together. By joining these two approaches together with a meaningful trade-off between theirs pros and cons proved to be a very interesting option. This option is called hybrid architecture. Also, according to our belief, the single kind of problem representation or approach to solving the problem is almost never sufficient. Each action made by living animals is a consequence of superposition of many different motivations, needs, emotions, intentions etc. We simulate this by connecting several of the decision and control blocks. Each of these blocks consists of one"
93,390,0.985,Nordic Mediation Research,"there is a decrease of 0.07% in the childrenâs experience of speaking freely. The horizontal line in Fig. 2 gives a graphic presentation of this.12 Table 2 below show the results of bivariate multilevel analysis where the correlation between the childrenâs assessments and each of the three predictorsâtype of mediation, conï¬ict level and worrisome conditionsâare assessed one by one. There is signiï¬cant correlation between conï¬ict level and the question âWere you able to say what you wanted to the mediator?â (t Â¼ 2.90; p < 0.05). There is a negative correlation between these two variables, which means that an increase in conï¬ict level gives a lower score on the experience variable, although the correlation is very small. There was no other signiï¬cant correlation between the variables, and the effects were generally quite small. In other words, the conï¬ict level in the family only affected the childrenâs experience of being able to say what they wanted in conversation with the mediator to a small degree. Through multivariate analysis where all the predictors (type of mediation, conï¬ict level and worrisome conditions) are assessed simultaneously, we were able to estimate the unique contribution of each variable to explain the childrenâs"
264,264,0.985,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"Issues of Values I have already briefly evoked this issue in a ï¬nal comment regarding the research on differential equations, but this fundamental issue of values certainly needs more than a small comment. Mathematics education, for better or for worse, is a ï¬eld in which science and values strongly intertwine. Some years ago, I was asked by UNESCO to pilot the realization of a document on the challenges in basic mathematics education (UNESCO, 2011). The group of experts involved agreed that the main challenge to be addressed was that of âquality mathematics education for all.â However, coming to an agreement on what was the exact meaning that should be given to this commonly used expression was another story. We had long discussions that reflected differences in perceptions and values. Of course, these also had an impact on the vision we each had of the types of actions to be promoted in order to progress towards this goal. Even within my own culture, even for theories with close epistemology, such as the theory of didactical situations and the anthropological theory of the didactic, there is no doubt that the forms of didactical engineering research developed are different. TDS relies on a constructivist vision of learning, which is not the case for ATD. The vision of didactic engineering in ATD, which expresses in terms of ï¬nalized and non-ï¬nalized study and research paths (Chevallard, 2015) with the role given in these to the dialectics between media and milieu and the opening of"
213,178,0.985,Collider Physics Within The Standard Model : a Primer,"of HERA data, and the full effect of the true small x asymptotics is only felt at much smaller values of x. The related effects are not very important for most processes at the LHC, but could become relevant for the next generation of hadron colliders."
235,60,0.985,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","6.2 Determinism Does Not Imply Predictability One immediate consequence of reduction is the fact that, at least for sufficiently complex systems allowing the implementation of Peano arithmetic or universal computation, determinism does not imply predictability [497, 499]. This may sound counterintuitive at first but is quite easy to understand in terms of the behaviour, the temporal evolution or phenomenology of a device or subsystem capable of universal computation. Let us, for the sake of a more explicit (but not formal and in a rather algorithmic way) demonstration what could happen, consider a supposedly and hypothetically universal predictor. We shall, by a proof by contradiction show, that the assumption of such a universal predictor (and some âinnocentâ side constructions) yields a complete"
378,38,0.985,The Great Mindshift : How a New Economic Paradigm and Sustainability Transformations Go Hand in Hand,"Not everyone within the transformation research community works with a transdisciplinary approach and reflexive paradigm. The community combines a wide array of scientiï¬c disciplines and is still sorting out where exactly paradigmatic agreements lie. So within this book I pulled together the work of leading scholars who do at least reject the positivist epistemology and ontology that one ï¬nds in the mainstream economic paradigm and its methodological individualism. In this paradigm, humans do not reflect on more than the costs and beneï¬ts of the choice set with which they are confronted. So each person in their economic system behaves similarly (representative actors), regardless of where they happen to live. This is very convenient because individual behavior assumptions are aggregated into extrapolations of how the system will work as a whole and what knock-on effects it will haveâe.g., the prediction that markets will balance themselves. However, even within allegedly objective, positivist/standard economics, it has been recognized that such additive approaches risk a fallacy of aggregation, ending in incorrect predictions. For example, American economist Alfred E. Kahn warned of The Tyranny of Small Decisions as early as 1966. He stressed that market equilibrium theory must remain cautious about the reliability of its methodological individualism: small decisions by rationally calculating actors may well lead to misallocation effects on the macro scale that produce outcomes which the same individuals would not choose (Kahn 1966: 23). One prime example of this tyrannical effect in natural sciences is the way that climate change results from the cumulative effect of what seem to be negligibly small entities of additional CO2 emissions made on the individual scale."
269,71,0.985,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"three of them. On the final day of the event, likely collaborators were more-or-less forced â via highly curated groups â into extended interactions with one another, and instructed to return with the outline of a collaborative experiment. Most of these conversations did not produce anything very concrete (and this was fine) â but a fascinating interdisciplinary collaboration, drawing together very different traditions of psychological and neuroscientific experimentation on introspection, did emerge from the conversation between SK, Russ Hurlburt, Charles Fernyhough (CF), and others (KÃ¼hn et al. 2014). For FC and DF, the key mode of interdisciplinary intervention, here, was to use this workshop as a vehicle to ensure that likely collaborators were pushed to work with one another â and, critically, for the two of them to get out of the way once this curatorial work was done, and they were no longer required. Interdisciplinarity, here, was located first in the curatorial labour of bringing SKâs, CFâs, and Russ Hurlburtâs different, psychologically-rooted research traditions together (and we are absolutely committed to the view that the joining of heterogeneous psychological and neuroscientific traditions, rooted in profoundly different epistemological histories, is an intensely interdisciplinary endeavour), and generating the space for an experimental conversation to take place. But there was also a significant interdisciplinary intervention in knowing when to get out of the way: a more traditionally-minded mode would have insisted that, for this to be a truly interdisciplinary effort, it must include some âperspectiveâ from the social sciences and/or humanities. Such a view not only flattens the historical and internal multiplicity of such a capacious discipline as psychology, but also makes invisible the vital interdisciplinary labour of simply bringing things together. We are insistent, by contrast, that the generation of space, the curatorial labour of choreographing encounters, and then the willingness to know when your disciplinary perspective is not necessarily adding anything important, remain crucial, if underappreciated, ways to do interdisciplinary collaboration."
186,117,0.985,Dignity in The 21St Century : Middle East and West,"This implies that human beings with their different ideas about a good life, in all their diversity, deserve respect as entities with dignity. Yet, this is very far from Kantâs understanding of respect for human dignity, âfor the criminal would argue on this ground against the judge who sentenced himâ (Kant 1997: 39 [4:430]). This is a dilemma, of course. The Formula of Humanity could enable criminals to argue that their sense of purpose and self-worth are being violated through, for instance, punishment. However, this is not at all in line with Kantâs spirit. In order to explain Kantâs response to the criminal, Kantâs distinction between the noumenal and the phenomenal world is important. The Greek words noein (to perceive through thought) and nous (mind) combine to form the term noumenon, an object or a world that cannot be perceived through the senses, but only through pure"
228,183,0.985,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"5.2.2 The Direction Determinant The direction of the OFN is an additional property in comparison with classical fuzzy numbers and its meaning is different from the degree of membership. Therefore, if we want to process the full information contained in the OFN, we need an additional parameter that will represent a new property."
95,166,0.985,Elements of Robotics,"where the value is negative or positive because the robot could move up to p% before or after the intended distance. Suppose now that there is an error p% in the heading of the robot, and, for simplicity, assume that there is no error in the distance moved. The geometry is:"
315,332,0.985,Situating Children of Migrants Across Borders and origins : a Methodological Overview,"The advantage of having access to biographical narratives at two points in time became clear as it allows for a comparative analysis of the changes or continuity in the biographical structure and the dominant patterns of interpretation. Our specific focus on the young peopleâs social networks in the biographical interviews turned out to be another advantage, as it provided us with a rich source of diverse information about the genesis and the relevance of social networks in the transition into adulthood. And the relatively large sample for our case-reconstructive project has proved to be an excellent base for further differentiation of our findings and to identify statements extending beyond the individual case. On a final note I would like to point out that working with biographical material always encourages us to question the ideas and standards used by the publicâand often also by researchersâto assess biographies and in particular the transition into adulthood. What seems to the outside world to be an easy transition into the workplace because it happens quickly and âquietlyâ does not always prove to be the transition that is subjectively experienced as the one that is most satisfactory to a given individual and that meets his or her requirements for an independent life. And the opposite is also true. Those biographies and transitions that seem to be very difficult and plagued by personal crisis can provide a great deal of subjective satisfaction and hold potential for social innovation. Acknowledgments This paper benefited from the support of the Swiss National Centre of Competence in Research LIVESâOvercoming Vulnerability: Life Course Perspectives, which is financed by the Swiss National Science Foundation (Grant number: 51NF40-160590)."
214,480,0.985,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"The key to using climate model output is understanding the credibility of the model, the legitimacy of the model, and the salience or relevance of the projection. Credibility comes from the type of model and the model development process. Legitimacy comes from a detailed understanding of uncertainty in all its dimensions for a particular problem. Salience (relevance) comes from understanding of a particular problem. Climate model output must be made relevant, and having translators or interpreters familiar with a particular application has been effective in many cases. Climate information in many cases is just one dimension of a problem, and it may not be the dominant dimension of uncertainty, particularly where the human sphere is involved. It may matter more how society changes than how climate changes to determine the load on a particular resource (e.g., water, land). So climate model output must be put into perspective, and uncertainty assessed against particular problems to determine salience. The use of interpreters and a focus on salience allows the dimensions of uncertainty to be reduced. These dimensions are different for particular problems. The prediction problem determines the timescale, and that determines the balance of scenario, initial condition, and model uncertainty. It may also help determine how to construct an ensemble of models for a particular problem. And the particular impacts determine what portions of model uncertainty are most important. Examples of particular aspects of model performance include intense summer convective precipitation over a region, or a particular mode of climate variability like tropical cyclones, the Asian monsoon, or blocking events. Focusing on a particular process allows a better assessment of uncertainty in a particular model, or an ensemble of models. This can be done with speciï¬c observations. It also helps to ï¬t the model output into a particular problem, and getting the particular data on the right spatial and time scales. We hope that this approach is useful in helping to frame the problem of assessment and use of climate models from broad uncertainties to speciï¬c and more tractable uncertainties. These uncertainties can be qualitative or, when narrowed sufï¬ciently, even made quantitative. For most problems, if framed in this way, it is not necessary to wait to use a future model with reduced uncertainty, and the âbestâ model or set of models may be different for different processes. Climate models provide a wealth of salient information that is ready to be interpreted and assessed to make speciï¬c projections."
223,254,0.985,Knowledge and Action (Volume 9.0),"food, the attractiveness of the serving staff, and any number of other concepts) will also activate associations of their own. Some of these associations will achieve sufficient activation to attain awareness, so a particularly delicious-looking muffin might prompt an automatic expectation of a good taste. The reflective system then categorizes and relates the activated concepts, the result being that the muffin is recognized as tasting good, and activates additional relevant content in the impulsive systemâsuch as health. This concept, in turn, changes the activation pattern in the impulsive system, so the associated concept of salad might become activated as well. This activation pattern is again categorized, and the process repeats until a decision or inference is reached. Such end results in the reflective system are driven by the principle of consistency of the propositions generated. For example health is good, taste is good, health is more important than taste might lead to the decision to select the healthy salad rather than the unhealthy one but also the tasty muffin, for this choice would be consistent with a greater number of propositions. However, the content of the propositions generated is necessarily limited by the activation pattern of the impulsive systemâalthough tennis is also healthy, its activation potential in the environment of the cafeteria during lunch hour is very low, so the reflective system will not include it in processing without any prior link or further relevant perceptual stimuli. Synergy between the systems occurs when the impulsive systemâs associations are valid and relevant to a consistent reflective solution. When the impulsive activation pattern is in synergy with reflective processing, concepts relevant to the focus of reflective processing become comparatively accessible, and cognitive effort is therefore reduced. The reflective system is not forced to perform extra categorizations and activations of concepts to achieve consistency, so subjective effort is lessened. This reduction may be accompanied by a feeling of flow (Winkielman, Huber, Kavanagh, & Schwarz, 2012), that is, ease of processing, which is then linked to positive affect. Therefore, when both systems are in accord, it feels easy and good to think and make decisions. As an example, if the only tasty option in the cafeteria were the healthy salad, people who ate there and cared deeply about their health (i.e., had a high accessibility of the concept health) would find it natural to choose the salad and, moreover, would feel good about how easy the choice was. A different picture emerges when the systems are at odds with each other, as when impulsive activation patterns present associations that are opposed to a consistent reflective conclusion and produce a feeling of conflict. It requires additional cognitive effort to activate new impulsive patterns and to form propositions that lead to a consistent end state. Once the muffin is added to the lunch options, the decision-maker must actively work against the temptation of the tasty dessert in order to generate the propositions about healthy eating that justify selecting the salad. This dependence of effortful processing on automatic activation has an interesting consequence: A fluently (synergistically) processed inference should have a higher truth value than a disfluently (antagonistically) processed inference does, unless the reflective system specifically corrects for the consequences of fluency (Allport & Lepkin, 1945; Begg, Anas, & Farinacci, 1992; Schwarz, Sanna, Skurnik, & Yoon, 2007). The"
78,436,0.985,The Onlife Manifesto : Being Human in a Hyperconnected Era,"From the above I shall assume that Stefan Arkadievitch participates in a real public, as opposed to Anders Behring Breivik who is a member of a fictitious public, mainly unfolding in virtual environments. The main distinction between the two ârealitiesâ is not, however, whether they are virtual as opposed to real, but rather whether thay are fictitious as opposed to public. The use of reason is still to some extent public in Arkadievitchâ case, while fictitious as far as Breivik is concerned. However, although the virtual character of the communication is not the main source of the problem, there still seems to be a non-trivial connection between virtual environments and a fictitious public, as virtual realities appear to be a necessary, though not a sufficient condition for fictitiousness. It could easily be argued that the distinction between the real and the fictitious equals the difference that could be drawn between the real and the virtual.7 This is, however, misleading, as a virtual reality may very well be communicative in e.g. a Habermasian sense, where communication is based on public use of reason. As an example there have been several occasions of political activist actions starting with mobilizing people in a virtual world online before spreading offline (Thorseth 2006). By contrast, the public in Breivikâs case is based in fictitious use of reason, while pretending to make appeals to a real and universal audience. As mentioned earlier, Cass Sunstein and many others have written extensively on the problem of filtering and group polarisation. A distinction between real and virtual worlds is anticipated to be the relevant distinction, and the virtual tends to be associated with a radical threat to public reason. From the arguments above I shall claim that this is partly misconceived, as the real threat rather has to do with fictitious publics. Against this one could of course object that the fictitious character needs not necessarily be associated with a threat to public reason since there need not necessarily be an internal link between extreme ideologies and their fictitiousness. This is the reason for my claim that the fictitious character should be procedurally defined. The fictitious character of a claimed public should be defined by its communicative methods. As a consequence it is an open issue whether fictitious publics need to prevail. This procedural criterion is based on an argument developed in Thorseth, arguing in favour of a distinction between legitimate and illegitimate paternalism in polyethnic conflicts (Thorseth 1999). Briefly, the salient point is that a claim on publicizability is basic to recognition. Unwillingness to discuss publicly what has already become contested in the public domain is in some cases based on procedural fundamentalism, as e.g. depriving others of autonomous âyesâ of ânoâ.8 Here I want to establish that real and virtual publics are not moral opposites, while the contrast"
31,21,0.985,Remembering and Disremembering the Dead,"Traditional definitions of medical death are unambiguous, describing a final event that leads to the absolute state of being deadâin which case the biological death of a human being (idem identity) is co-terminus with the social death of the person (ipse identity). The biological death of a person has narrative consequences in how we may configure personal identity. In the most formal terms this involves correct signification. Being dead signifies a corpse, a state of non-being, for which the personal pronoun in the phrase âI am (a corpse)â is no longer correct. A corpse refers to a husk, and a husk is no longer a person that actively possesses a body. Furthermore, physical death has relational consequences for others. My death, for example, would mean that my wife would undergo a relational/narrative change: that is, my wife would become a widow. Social death concerns our ipse identityâthe narrative identity of who we are. While social death is dependent on having existed, it is not necessarily co-terminus with existing as a biological entity. Real changes in our biology certainly prompt relational changes in how we may configure the narratives of our lives. After a heart attack, for example, there may be a subtle shift in who we understand ourselves to be through what we believe we are realistically capable of doing. This may signal a subtle shift in our personality. Less subtly, brain injury, as argued earlier, can lead to narrative inversions in our clinical status: from a living person that is self-conscious and aware of others, to a living human organism which in PVS is not conscious in this way. A relational change in the meaning of who a person is has both an existential as well as a biological dimension. The narrative of who we are is existentially anticipated in the face of our physical mortality. We existentially configure the meaning of our lives in anticipation of our physical death. This has a secular and religious dimension. In secular terms the meaning of our life matters beyond its physical annihilation. For example: we might suffer a serious brain injury that marks our autobiographical death destroying our dignity; we resonably anticipate being respectfully treated and honestly remembered after our physical life has ended. In spiritual/religious terms we may anticipate who we are and how we might continue in a life hereafter. If one believes that how we treat mortal remains matters for a disembodied life hereafter, then the burial rituals associated with keeping the corpse intact take on a special ânarrativeâ significance."
82,214,0.985,Fading Foundations : Probability and The Regress Problem,"4.1 Fading Foundations In the previous chapter we have introduced the idea of a probabilistic regress, and we have seen that such regresses are in general unproblematic: they mostly have a calculable limit, thus providing the target proposition, q, with a unique probability value. In all but a few exceptional cases there is no conceptual problem in saying that q is probabilistically supported by an epistemic chain of infinite length. An important part of our argument concerned the roÌle of the foundational or grounding proposition, p. In calculating the unconditional probability of the target, q, we managed to eliminate all the unconditional probabilities â except that of p. The factor P(p) remained the only term in the chain of which the value was unknown. Consider the finite chain"
360,356,0.985,Compositionality and Concepts in Linguistics and Psychology,"The first assumption is only for the sake of illustration: actually 16% of the participants in Poortman et al.âs experiments accepted I2 for sentences like (8a). It is likely that situations like I1 and I0 (one and zero relations) would get even lower acceptability, hence the actual x0 point is probably I1 or I0. The second assumption on typicality is based on the clear preference in Poortman et al.âs experiments for situations in which each agent only pinches one patient, rather than more than one."
80,764,0.985,Innovations in Quantitative Risk Management (Volume 99.0),"when the true copula is the t4 copula, (Tdâ1 , Ndâ1 , AD) performs well. Given the comparably numerically simple form of (Tdâ1 , Ndâ1 , AD), this method can be quite useful. Interestingly, by comparing Table 1 with Table 3, we see that if the transformation T with all d components is applied, there is actually a loss in power for the majority of families tested (the cause of this behavior remains an open question). Note that in Table 2 for the case where the Ali-Mikhail-Haq copula is tested, the power decreases in comparison to the five-dimensional case. This might be due to numerical difficulties occurring when K C is evaluated in this case, since the same behavior is visible for the method (K C , Ï 2 ). Table 4 shows the empirical power of the method (R, Nd , AD). In comparison to our proposed goodness-of-fit approach (Tdâ1 , Ndâ1 , AD), the approach (R, Nd , AD) overall performs worse. For d = 5, there are only two cases where"
257,120,0.985,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","In this program, every individual thread is secure, in the sense that it does not leak information about high variables to a low observer. Additionally, pairwise parallel composition of any of the threads is secure, too, including a benign race fork(l := 1); fork(l := 2). Even if we assume that the attacker fully controls the scheduler, the final value of l will be determined only by the scheduler of his choice. However, for the parallel execution of all the three threads, if the attacker can influence the scheduler, it can leak the secret value of h through public l. In this paper, we present a compositional and flexible type-and-effect system that supports compositional reasoning about information flow in concurrent programs, with minimal assumptions on the scheduler. Our type system is based on ideas from separation logic; in particular, we track ownership of variables. An assignment to an exclusively-owned low variable is allowed as long as it does not create a thread-local information flow violation, regardless of the parallel context. Additionally, we introduce a notion of a labeled scheduler resource, which allows us to distinguish and accept benign races as secure.2 A racy low assignment is allowed as long as the threadâs scheduler resource is low; the latter, in its turn, prevents parallel composition of the assignment with high threads, avoiding potential scheduler leaks. This flexibility allows our type system to accept pairwise parallel compositions of threads from Example 2, while rightfully rejecting the composition of all three threads. Following the idea of ownership transfer from separation logic, our type system allows static transfer of resource ownership along synchronization primitives. This enables typing of programs that use synchronization primitives to avoid races, as illustrated in the following example."
228,434,0.985,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"14.6 Conclusions-Method Comparision The presented method could be compared with the method proposed in the literature as provided in Table 14.3. As shown in Table 14.3, the proposed method does not require an expert to define the rules of possible attack as in a system with fuzzy logic proposed in some papers [16, 17]. Such an expert would have to possess extended knowledge about security and IP networks. This is something that allows us to use the proposed method in a very quick way. The second thing is the manner of gathering results of the observance of a DDoS attack. In the proposed method, the decision about the attack is made using simple calculations provided by the OFN description. This allows achieving the results very quickly and easily. The methods found in the literature make decisions by comparing a list of rules [16, 17]. Of course there are some solutions that use mathematical models [28], but they are much more complicated than the method using OFNs. The last thing that could be compared is the possibility of being implemented in a real environment, which means in real networks. The method proposed in the literature requires a lot of processing power. The method proposed in this chapter requires only solving a simple mathematical equation. This is very important, because it lets us use the proposed method in a real network, on real routers."
289,762,0.985,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","There are a number of techniques for reasoning under relaxed memory models, but besides the DRF theorems and some simple invariant logics [10,13], no other techniques have been proved sound for a model allowing the weak behaviour of LB+data+fakedep from the introduction. The âinvariant-based program logicsâ are by design unable to reason about programs like the random number generator, where having a bound on the set of values written to a location is not enough, let alone reasoning about functional correctness of a program. Relaxed Separation Logic (RSL). Among program logics for relaxed memory, the most closely related is RSL [27]. There are two versions of RSL: a weak one that is sound with respect to the C/C++11 memory model, which features out-of-thin-air reads, and a stronger one that is sound with respect to a variant of the C/C++11 memory that forbids load buffering. The weak version of RSL forbids relaxed writes completely, and does not constrain the value returned by a relaxed read. The stronger version provides singlelocation invariants for relaxed accesses, but its soundness proof relies strongly on a strengthened version of C/C++11 without po âª rf cycles (where po is program order, and rf is the reads-from relation), which forbids load buffering. When it comes to reasoning about coherence properties, even the strong version of RSL is surprisingly weak: it cannot be used to verify any of the coherence examples in this paper. In fact, RSL can be shown sound with respect to much weaker coherence axioms than what C/C++11 relaxed accesses provide. One notable feature of RSL which we do not support is read-modify-write (RMW) instructions (such as compare-and-swap and fetch-and-add). However,"
141,133,0.985,Cyber-Physical Systems of Systems: Foundations â A Conceptual Model and Some Derivations: The AMADEOS Legacy,"Description Hierarchy. A multi-level hierarchy that describes a set of related entities at different levels of abstraction is called a multi-level description hierarchy. A description hierarchy can be much simpler than the related structure hierarchy provided the structure hierarchy is highly redundant. If a complex structure is completely un-redundant, then it is its own simplest description [32] (p. 221). We distinguish two types of descriptions, state descriptions and process descriptions. State descriptions describe the state of the world at the instant of observation. Process descriptions explain how a new state of the world unfolds as time progresses that is how the state transitions happen. A description of behavior is a process description. The classiï¬cation of entities in a description hierarchy is usually based on cognitive models of the observer and thus may be dependent on the subjective view of the observer. Moreover, depending on the purpose, different levels of description of the same physical structure can be introduced by the observer. For example, the thermodynamic description of the behavior of a gas is at a higher level of description than the statistical description of the same physical material and the choice among them may depend on the purpose of the description. If the redundancy of a structure is removed from its description hierarchy, then a signiï¬cant simpliï¬cation of the description can be realized (e.g., [32] p. 220). In case the elements of a hierarchy are constructs, i.e. non-material entities that are the product of the human mind, the assignment of the constructs to hierarchical levels always results in a description hierarchy, the organization of which is determined by the purpose of the observer. In many, but not in all cases, the description hierarchy of a structure follows the structure hierarchy. Control Hierarchy. In a control hierarchy the macro-level provides some constraints on the structure or behavior of the parts at the micro-level thus establishing a causal link from the macro level to the micro-level. Constraints restrict the behavior of things beyond the natural laws, which the things must always obey. In many, but not all cases, the control hierarchy follows the structure hierarchy. Ahl [1] (p. 107) provides the following example: The concept army denotes a structure hierarchy that consists of the soldiers of all ranks and contains them all. In contrast, a general at the top of an army (a military hierarchy) controls the soldiers, but does not contain them. In some cases, as the example of the military hierarchy above shows, the control constraints originate from outside, i.e. above the macro-level. In other cases, the control constraints have their origin in the whole, i.e. the collective behavior of the parts of the micro-level. It is this latter case that is relevant for the analysis of emergence. Many equivalent examples can be found in Distributed Computing when we have centralized or decentralized control and management. Since behavior (function plus time) is a concept that depends on the progression of time, there is a temporal dimension in control hierarchies that deal with behavior."
264,739,0.985,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"same number of matches, some of them in matchboxes, with the same amount of matches in each box. The task was to ï¬nd a way to determine the number of matches in each box. JanÃen showed that the development of algebraic structure sense can be understood as happening in moments of tuningâwhere tuning is a form of social interaction characterized by a common interest and a common understanding of the situation and the goals of the activity, and further, by a common understanding of what actions are necessary to achieve the goals. In her talk, Heidi StrÃ¸mskag presented a semiotic analysis of three students teachersâ engagement with a generalization task in geometry. She explained how an evolution of the milieu (in Brousseauâs sense) enabled the student teachers to create manipulatives (plane geometrical ï¬gures) that were instrumental in the generalization process aiming at a relationship between percentage growth of length and area when looking at the enlargement of a square. It was shown how use of different notation systems constrained the interaction among the participants, and how transformation of percentage and fractional notation into geometrical ï¬guresâthat belong to a different semiotic registerâenabled the target mathematical knowledge to be expressed in algebraic notation. StrÃ¸mskag made a general point about design of milieus for algebraic generalization: the adidactical potential of a situation depends upon a coordination between the particular values that students are asked to work on and the semiotic register(s) expected to the used. Erik Tillema presented an interview study of eight Grade 10â12 studentsâ generalizations made in the context of solving combinatorics problems about cubic relationships. Studentsâ generalizing actions were characterized by schemes, where a scheme has three parts: an assimilatory mechanism; an activity; and, a result. Tillema showed how two schemes were pre-requisites to establishing the formula that Ã°x Ã¾ 1Ã3 Â¼ x3 Ã¾ 3  Ã°x2  1Ã Ã¾ 3  Ã°x  12 Ã Ã¾ 13 . The ï¬rst was a scheme to quantify the total number of three card hands using multiplication that was coordinated with a systematic way to list all possible outcomes, and the second was a scheme that enabled students to spatially structure 3-D arrays. Further, he showed that images based on quantitative relationships supported student generalizations. Tillema explained that the formula (above) that one student created was a formal statement of generalization (a reflection generalization) that was based on an abstraction in which she connected the activity of her scheme with the results of her schemes (reflective abstraction)."
372,526,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where we have included both positive and negative frequencies1 in the integral and assumed that V does not vary significantly over the observing bandwidth. Equation (6.4) represents the real part of the complex cross-correlation, and the way to obtain both the real and imaginary parts is explained later in this section."
77,335,0.985,A Life Course Perspective on Health Trajectories and Transitions (Volume 4.0),"We do not need to assume that there is no variation between periods in this model â indeed the period residual term u2j2 remains in the model meaning periods (and cohorts) can still have contextual effects. However, we do assume that there is no linear trend over time in the true period residuals, because these will be absorbed by the age and cohort effects in this model.5 If this assumption is justified, such a model will produce correct inference both about the linear age and cohort trends, and about the period and cohort random deviations from those trends (Bell and Jones 2014a). We would argue that often constraining the period trend to zero is a reasonable course of action. For us, the mechanism for long-run change is more easily conceptualised through cohorts than periods â change occurring by influencing people in their formative years rather than âsomething in the airâ that influences all age groups equally and simultaneously. However, this is of course dependent on the research question and subject area, and the researchers own understanding of the process at hand. Having made the above assumption, and thus (assuming the assumption is valid) dealt with the identification problem, the model can now be extended in a number of ways. First, using the multilevel framework, additional levels can be added to fit the structure of the data being used. The HAPC model was originally designed for repeated cross-sectional data (such as the ONS Longitudinal Study (Office of National Statistics 2008)), where a cross-sectional sample of individuals is measured on multiple occasions, but individuals are not followed through time across these occasions. Where panel data (such as the BHPS) is used, that is data that does follow individuals over time, an individual level should be included to account for dependency within individuals between occasions. For other data"
335,231,0.985,"Open Source Systems : Towards Robust Practices 13Th Ifip Wg 2.13 international Conference, Oss 2017, Buenos Aires, Argentina, May 22-23, 2017, Proceedings","Following from the same data set, we started a further statistical analysis; this section presents the preliminary results we gathered from applying survival analysis techniques. The general focus of survival analysis is on the modeling the time it takes until a specific event occurs, in social sciences one often speaks of event history [18]. We have found interesting findings from studying how many people keeps participating in the Debian project throughout the time, that is, to model the time until departure from the Debian project. The main motivation comes from the need to understand keyring population along time. Our sampled data is defined by the PGP keys that make up the curated WoT from the Debian Developers keyring [20]. The analyzed data is treated as a longitudinal study. We point out that intervals are not of the same length in time: each data point is a tag in the"
84,443,0.985,Eye Tracking Methodology,"17.2.6 Example Designs A general discussion of experimental designs is complicated without a specific context and specification of IVs/DVs being tested. It is often easier to settle on the IVs/DVs first and then develop the design. Doing so will stipulate the requirements for the number of groups or experimental trials required, depending generally on whether a within- or between-subjects design is adopted. In this section some basic designs are offered. Not all are applicable to eye tracking studies, but they are given for a better sense of completeness. Single Individual, Time Series This design is exemplified by the traditional drug effectiveness experiment. In this design, there is just one participant, and measurements are taken over time. Prior to administration of a treatment, a baseline measurement is taken. Subsequent measurements are then compared to the baseline to test for the drugâs effect. Figure 17.1 is an example of a simple ABAB type design where A indicates no treatment (baseline) and B indicates treatment. In Fig. 17.1 only ABA is shown. The dashed lines indicate administration and subsequent cessation of treatment. The graph itself would suggest some level of the dependent variable being measured (perhaps some form of subjective well-being or alternatively some physically measurable indicator such as blood pressure). Analysis of this type of experiment typically requires comparison of the DV mean during the specific time sequences during which it was known the drug was taken, e.g., time period over which treatment B was active. If this mean is statistically different from the mean during the period when the drug is not present, i.e., period of treatment A, then the drug is said to have an effect. Note that this design can be thought of as either one with two independent variables, A and B, or just one but with two levels. Relabeling the diagram in Fig. 17.1 with Aâ¦ and A1 would indicate one treatment, or factor, administered at two levels: Aâ¦ meaning absent, A1 meaning present. This would now be considered a single factor design. General PreâPost Design The single individual design can be adapted to a single group design, wherein the same group of individuals is given the treatment in the AB treatment administration sequence, and the data are now analyzed over the entire sample of participants. This design is known as preâpost because measurements are taken prior to and following treatment administration. This design is shown in block form, along with other designs, in Fig. 17.2. Once again, this too can be considered a single factor design."
249,54,0.985,Advances in Proof-Theoretic Semantics (Volume 43.0),"8 Concluding Remarks 8.1. We have thus shown that the notion of a weak valid argument taken constructively is extensionally equivalent with the notion of a BHK-proof. When weak validity is taken non-constructively, I have not been able to construct a BHK-proof of A from a weakly valid argument for A, but only in the other direction a weakly valid argument for A from a BHK-proof of A, given the induction assumption. In contrast, from a strongly valid argument for A, I have constructed a BHK-proof of A, given the induction assumption and the assumption that the reductions can be generated effectively, but have not been able to construct in the other direction a strongly valid argument for A from a BHK-proof of A. Since the mentioned constructions depend on the assumption that there are mappings in both directions for sub-sentences, nothing has been established about the relations between on the one hand BHK-proofs and on the other hand arguments that are weakly valid understood in a non-constructive sense or are strongly valid. 8.2. As has been seen above, when the notion of valid deduction is generalized to the notion of valid argument, the justifications come to play the major role and the inferences of the argument structures a correspondingly minor role. Some of the intuitions behind the notion of valid deduction are lost in this way. It would therefore be interesting to investigate a more restricted notion of reductions than the one used here in connection with arguments."
253,150,0.985,"Autonomous Driving : Technical, Legal and Social Aspects","As agents moving through an environment that includes a range of other road usersâfrom pedestrians and bicyclists to other human or automated driversâautomated vehicles continuously interact with the humans around them. The nature of these interactions is a result of the programming in the vehicle and the priorities placed there by the programmers. Just as human drivers display a range of driving styles and preferences, automated vehicles represent a broad canvas on which the designers can craft the response to different driving scenarios. These scenarios can be dramatic, such as plotting a trajectory in a dilemma situation when an accident is unavoidable, or more routine, such as determining a proper following distance from the vehicle ahead or deciding how much space to give a pedestrian standing at the corner. In all cases, however, the behavior of the vehicle and its control algorithms will ultimately be judged not by statistics or test track performance but by the standards and ethics of the society in which they operate. In the literature on robot ethics, it remains arguable whether artiï¬cial agents without free will can truly exhibit moral behavior [1]. However, it seems certain that other road users and society will interpret the actions of automated vehicles and the priorities placed by their programmers through an ethical lens. Whether in a court of law or the court of public opinion, the control algorithms that determine the actions of automated vehicles will be subject to close scrutiny after the fact if they result in injury or damage. In a less dramatic, if no less important, manner, the way these vehicles move through the social interactions that deï¬ne trafï¬c on a daily basis will strongly influence their societal acceptance. This places a considerable responsibility on the programmers of automated"
383,55,0.985,Elements of Risk Analysis with Applications in R,"Through this book, you will learn how to bring your deciding under risk and thinking to life. The structure of this exploration is outlined here. The chapters are divided into three broader units, each having a unique theme. However, there is a common thread throughout the book: building and analysing statistical models for the behaviour of some variable Y so that we can quantify the risk, and we can recommend the best actions under risk."
359,169,0.985,"Micro-, Meso- and Macro-Dynamics of the Brain","magnetic stimulationâa short magnetic pulse applied to the scalp to activate the underlying neuronsâand high-density EEG to record the strength of the responses of the rest of the cerebral cortex. The results were clear: the longer the subject was awake, the larger the responses, and it took a night of sleep for the responses to return to baseline (Huber et al. 2013). One should emphasize that exactly how this down-selection process would take place remains unclear, and the account above remains speculative. Indeed, the precise mechanisms are likely to vary in different species, in different brain structures, and in different developmental periods. For example, it is not known whether in invertebrates sleep is accompanied by intense neuronal activation or notâperhaps there the weakening of synapses can be accomplished without having to go through a large repertoire of old memories. Similarly, it may be that NREM sleep is the ideal time for weakening synapses in an activity-dependent manner in the cerebral cortex, due to the occurrence of slow waves; but that in the hippocampus, which does not generate slow waves, down-selection may happen preferentially during the faster, theta waves of REM sleep (Grosmark et al. 2012). Irrespective of the specific mechanisms, the evidence is strong, in several species, that overall synaptic strength goes up during wake and down during sleep. And if this is so, it has implications concerning the role of sleep that go beyond its benefits to memory consolidation and integration, as we will now briefly discuss."
23,89,0.985,Anti-Vivisection and The Profession of Medicine in Britain : a Social History,"At the turn of the century it remained a matter of great controversy whether evolution could have been responsible for the emergence of the most complex cognitive faculties, including the human capacity for love, imagination, and feeling, or whether there were some transcendent aspects of thought and consciousness that could never be explained in biological terms.60 The two great founders of evolutionary theory disagreed: Darwin thought that evolution could account for these mental phenomena, Alfred Russel Wallace (1823â1913) that it could not. For Christians who felt their humanity threatened by talk of the evolution of rationality, one solution was to emphasise the spiritual"
275,204,0.985,Foundations of Trusted Autonomy,"environment makes them somewhat noisy. However, the potential noisiness in the state transitions may be greatly reduced if the agents also learn an implicit model of the other agentsâ behaviors. If they can predict how those other agents will act, they can learn to predict what effect their choice of actions will have on their next snapshot view of their world with high accuracy. Thus it is possible, in principle, for the pairing of a sufficiently powerful computational device with a sufficiently powerful learning mechanism to learn to use an egocentric view of the external state as if it were an internal state, for systems such as the Legion II game. Future work must pursue this concept to determine what the limits of such a mechanism are. Does a feed-forward network embedded in an environment that it can manipulate become as powerful as a Turing machine? The ad hoc use of plots of an ad hoc metric for detecting the legionsâ division of labor in Sect. 5.6 also reveals a need for developing methods of behavior analysis. When studying agents in visible environments such as games and simulators, behavior is paramount [3]. Meaningful behavioral metrics are essential, and it would be useful to have methods that are abstract enough to be portable across application domains, and sensitive enough to detect similarities or differences in behavior when a domain involves more subtlety than a switch between two discrete behaviors. Work in this area is already underway, and will be a major component of the study of visibly intelligent behavior in the future. The Legion II ATA experiments also revealed a special challenge for the application of computational intelligence methods to agent behavior problems. The goal of a simulation as understood by a machine learning algorithm â e.g. minimizing pillage in the Legion II game â may be satisfied by some abstract optimization, with little or no regard for the appearance of details of the learned behavior. For example, the legions in the Legion II ATA experiment learned to switch between appropriate roles on the basis of context, but some of the details of their behavior are not satisfactory to an observer. The garrisonsâ learned behavior often produced âmindlessâ oscillations in and out of their cities when there were no barbarians nearby to threaten pillage, and such behavior would likely be the subject of ridicule if seen in the behavior of the agents in a commercial game. In principle such details of behavior can be addressed by careful specification of the goals of the training regimen, such as an evolutionary reward function that penalizes undesirable behavior, but for applications as complex as a commercial game it may be as difficult to specify an appropriate reward function as it has proven to be to write a script that covers all situations adequately. Therefore work is underway on suppressing such oddities of behavior and inducing other desirable traits that will make agents look intelligent to observers, rather than merely acting out some abstractly optimal solution to the problem they have been trained for. (See [3] for an extensive preliminary treatment.)"
253,1148,0.985,"Autonomous Driving : Technical, Legal and Social Aspects","30.4.2 Conclusions for Autonomous Driving Autonomous driving will be an everyday technology that is as closely connected to peopleâs lives as driving is today. That distinguishes it greatly from nuclear energy, while it shares this everyday-life quality with green biotechnology (through its use in food production) and mobile communication technology. From both risk debates we may extract the insight of how central the dimension of individual beneï¬t is. While any such beneï¬t from eating genetically modiï¬ed food has scarcely even been advanced by its proponents, the individual beneï¬ts of mobile telephones and mobile internet access are readily evident. And as soon as this beneï¬t is demonstrably large, people are prepared to assume possible risks. And this is absolutely rational from an action theory standpoint. What is irrational is to assume risks when the beneï¬ts are not evident or would only accrue to other actors (e.g. Monsanto in the green biotechnology debate). In such cases a risk debate can have dramatic consequences and obliterate any chance of acceptance. Another such knock-out scenario is the possibility of a massive catastrophe such as the âresidual riskâ of the meltdown of a nuclear power facility. This was decided on the political level, albeit with little chance of external influence. The situation was thus widely regarded as a passive risk situation in which people were exposed to potential harm by the decisions of others. Nothing can be gleaned directly from this scenario for autonomous driving as it would presumably be introduced in the familiar market context of conventional transportation and thus de facto depend on the acceptance of users from the outset. While it is possible to imagine other introduction scenarios (Chap. 10), state-mandated use of autonomous driving is all but unthinkable. The only indirect lesson from the history of nuclear energy is that expertocratic arrogance generates mistrust. An open discussion âbetween equalsââa lesson from the nanotechnology debate as wellâis a key precondition for a constructive debate on technology in an open society. These examples also show that talk of a German aversion to technology is a myth born of the experiences with nuclear energy and genetic engineering. All empirical studies"
223,409,0.985,Knowledge and Action (Volume 9.0),"Small Places, Big Issues Looking at the relationship between rationality and action in the domain of space, anthropologists first think of actions such as walking and the related decision to move or to stay. Walking may be considered the prototypical human action in a spatial setting. Correspondingly, the decision to move is the prototypical challenge to human practical reasoning in the context of moving through space. I wish to contribute to the topic of rationality and action by reviewing cases of human mobility and human orientation in space in some detail. This chapter is based on ethnographic work I have carried out with various groups of mobile hunters and gatherers over the years, particularly in southern Africa and Australia. Do these remote foragers have anything to offer to understanding decisions that matter most in the current world (regarding the current refugee and migration crisis, for instance)? I propose the following considerations with regard to this question. First, bringing in examples from far away is a key element in combating the common bias that âthere is no alternativeâ (see Widlok, 2009a). A case study exemplifying a very different mode of engaging rationality with action underlines that alternatives always exist and that it is worthwhile to spell them out clearly and develop them creatively. Second, the forager decision to move occupies the opposite end of the spectrum of human possibilities in that it focuses on rationality and action in a basic face-to-face setting without being confounded by effects of larger institutional frameworks. Third, the major global crises always come down to numerous smaller dilemmas and questions that social agents need to solve and that preoccupy them. For most agents the"
317,9,0.985,Transitions in Mathematics Education,"The history of mathematics is also a story of changes, of long-term genesis of concepts with moments of continuity and sudden ruptures. How history can inform research in mathematics education is a complex matter (Schubring 2011); here we consider how research in mathematics education has used the history of mathematics in studying transitions issues. For example, Dorier (2000) studies the genesis of linear algebra and shows that this theory has emerged in particular from a need of uniï¬cation of different problems concerning functions, sequences, and so forth. Thus linear algebra is what Robert (1998) calls a formalizing, unifying, and generalizing theory: it cannot be taught as a ânaturalâ extension of previous contents or a solution to a given problem. Studying the history of mathematics draws a landscape with gaps and long paths that do not admit shortcuts. An âepistemological obstacleâ is a form of unavoidable discontinuity which has been studied by many researchers (e.g., SierpiÅska 1987), most often by identifying ï¬rst such obstacles with a historical perspective then studying the associated cognitive transitions. Cognitive transitions are also observed outside of speciï¬c mathematical contents. Concerning the learning of proof in particular, several kinds of transitions have been investigated. In the transition from phases of conjectures to phases of proof continuities and discontinuities appear, and this leads to the introduction of the concept of cognitive unity of theorems (continuity existing between the production of a conjecture and the possible construction of its proof; see Garuti et al. 1996). Other researchers have identiï¬ed structural discontinuities in the transition between argumentation and proof (Arzarello and Sabena 2011). General models of transitions taking place in mathematical thinking also exist; in particular, different theories propose models for the âtransition from process to objectâ (Tall et al. 1999). This transition is sometimes called âencapsulationâ and sometimes âreiï¬cationâ; several authors claim that it is composed of a sequence of steps. Sfard (1991) considers the transition from computational operations to abstract objects to be accomplished in three steps: interiorization, condensation, and reiï¬cation. Dubinsky (1991) in the APOS theory also considers conceptualization processes as transitions composed of three successive steps: from action to process, from process to object (encapsulation), and from object to schema. Each of the steps, or sub-processes, is seen as continuous, while the change from one step to the next can be interpreted as a discontinuity. Nevertheless, the process-object transition is not discussed by these authors in terms of continuity/discontinuity. In contrast, Tall (2002), referring to Skemp (1962) for a study of âlong-term learning schemasâ discusses the process-object transition in terms of discontinuity. The author claims that long-term cognitive development in mathematics always faces discontinuities. He gives various examples concerning negative numbers, algebra, limits, and so forth. These discontinuities, according to him, cannot be avoided in the curriculum, and they require cognitive reconstruction on the part of the student. It is thus important for teachers to be aware of these discontinuities. Should we then maintain that cognitive development is generally a discontinuous process or merely a concatenation of continuous sub-processes? This issue is discussed in Sect. 2."
385,141,0.985,Advanced R,This seems a little magical (how does R know what the value of y is after the function has been called). It works because k preserves the environment in which it was deï¬ned and because the environment includes the value of y. Chapter 8 gives some pointers on how you can dive in and ï¬gure out what values are stored in the environment associated with each function.
130,232,0.985,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 1,"Although it could sound a little bit funny, the question ""why to formalize"" must be placed. The why must be answered before we get to any research. The formalization is believed to be a way of abstract modeling of a given problem domain. Anyway, browsing articles upon formalization, there is a lot of lines dealing with concrete formalisms but a few of them tries to answer why they formalize anything. For software architecture, (Allen, 1997) stated: Evidently, what is needed is a more rigorous basis for describing software architectures. At the very least, we should be able to say precisely what is intended meaning of a box-and-line description of some system. More ambitiously, we should be able to check that the overall description is consistent in the sense that the parts fit together appropriately. More ambitiously still, we would like a complete theory of architectural description that allows us to reason about the behavior of a system as a whole. The given quotation provides three points of formalization. Although it is not a definition of formalization, it clearly states three reasons for formalization in software architectures."
113,183,0.985,Pentecostalism and Witchcraft : Spiritual Warfare in Africa and Melanesia,"âappropriateâ and âinappropriateâ ways of interacting with and through This emphasis on responsibility contrasts with the more familiar Pentecostal approach to taming electronic commodities or goods coming from the West. Usually, prayer over an object breaks its occult ties (see Meyer 1998). Fabrice did not suggest prayer as a strategy to make the smartphone safer. Rather, the user was supposed to possess sufficient Christian knowledge to handle the technology in a moral way. Therefore, along with âmediationâ, a currently hot topic in the anthropology of technology, âconnectivityâ deserves greater theoretical investigation. Both âmediationâ and âconnectivityâ emphasize interaction (social or spiritual) and allow room for agency; yet, while âmediationâ emphasizes the in-between, the transfer of communication, power, and value, and at times even suggests the resolution of conflict, âconnectivityâ attends to the relationships generated, to âaccessibilityâ or âavailabilityâ, and to the possibility of entering into a relationship with an Other (socially and/or spiritually), whether technologically mediated or not. In Kinois terms, it is âyou who open the doorâ (ofungoli porte)âa familiar idiom to describe oneâs receptiveness to the Holy Spirit or to bad spirits. Witches are not just mediators of evil powers; they also connect the souls of their victims to demonic powers. Through kindoki, bewitched people have access to new, invisible, realms of identity, success, and power. Witches open the doors between the visible and invisible worlds. Very much like ICT, therefore, witches connect people with other worlds, even as people are not always aware of the consequences of entering therein. Telling in this regard is the name attributed to the ingredients of the medicine bugota used in magic by Sukuma farmers in Tanzania (Stroeken, this volume): shingila, entrances. As Stroeken describes, the power of this medicine depends on its relatedness to others parts of culture. âWherever âmagicâ works in the world, the reason is a sense of differential relatednessâ. Via the medicine, the patient is connected to multiple worlds. This complex state of belonging defies homogenization and total control and is characterized by ambiguity and negotiations. According to Stroeken, who draws on Gluckmanâs distinction between simplex and complex societies, Pentecostal Christianity cannot accept the multiplexity of peopleâs relationships and tries to reduce the ties between people. Pentecostal discourses emphasizes oneâs responsibility in âopening the doorâ via technology. The ways in which âconnectivityâ or"
363,19,0.985,History and Cultural Memory in Neo-Victorian Fiction,"upon boundary which grants to each its own identity and integrity; or, just as often, imperialistically, in that each tries to extend its own boundary and to invade, engulf, or encompass the other. In the first case history and fiction exist side by side as uncommunicating opposites; in the second, one dominates the other â as when history makes fiction into its subject and treats it as just another historical document, or when fiction makes history into one form of fictional narrative among many possible forms. (ibid.: 4) The contest turns upon the issue of truth. While Minkâs commonsense knows that history purports to be âtrueâ, it is unclear what, in this formulation, fiction purports to be. Fiction is constructed negatively, as the opposite of historyâs claim to truth. The distinction, presumably, is between an âactualâ past and an imagined one. And it is to the actual past that authority accrues. History is valorised as true, or real, while âmade-upâ fiction is reserved for entertainment. While the opposition between history and fiction is, paradoxically, commonsensical and fiercely contested, Hayden White has famously observed that history did not always demand the excision of fiction. In the eighteenth century, prior to the disciplinisation of knowledge, he argues, fictional techniques and literary devices were considered necessary for historical representation (see White, 1978b). Historians such as FranÃ§ois-Marie Arouet de Voltaire, Charles-Louis de Secondat, Baron de La BrÃ¨de et de Montesquieu and Edward Gibbon saw their task as discovering the meaning of past events, and this meaning might best surface through a combination of what actually happened â what is generally considered âfactâ â and what could have happened â details lost to the historical record but which do not obviously contradict it. Historyâs meaning was deeply embedded in rhetoric. It had a firmly philosophical purpose, functioning to enlighten and instruct the present. Thus, James Chandler argues that, far from striving for objectivity, much of the history written during the romantic period had a political motive, seeking to âstate the case of the nation â and to do so in such a ways as to alter its case â¦ [these writings] take on the national causeâ (Chandler, 1998: 6). This mingling of historical narrative and fictional techniques was accepted, in part, because the disciplines had yet to separate as distinct forms of knowledge. History formed part of a broad category of literature in the eighteenth century, which, in addition to history, included philosophy and political philosophy, as well as the poetry and novels that make up the category today (Gearhart, 1984: 10). Historians, artists"
333,13,0.985,Uses of Technology in Upper Secondary Mathematics Education,"The distinction that Feynman makes here shows how he sees his work as intrinsically interconnected with the symbolic system that he is working with. His ideas do not occur separately from their realization in written symbols; rather, they emerge through interaction with that symbol system. It is the same as with Du PrÃ© and her cello, where there is no music without both the artist and the instrument being present. Indeed, the process of coming to be able to operate fluently and effectively with tools and symbols is common to all learners as they appropriate the practices and âhabits of mindâ of a discipline. The human mind (and indeed the human brain) re-forms itself to accommodate these new discipline-speciï¬c ways of operating. For instance, Donald (2001, p. 302) has explained that literacy skills transform the functional architecture of the brain and have a profound impact on how literate people perform their cognitive work. The complex neural components of a literate vocabulary, Donald explains, have to be built into the brain through years of schooling to rewire the functional organization of our thinking. Similar processes take place when we appropriate numbers at school. It is easy to multiply 7 by 8 without representational supports, but if we want to multiply 12,345 by 78,654 then we write the numbers down and follow the speciï¬c rules of the multiplication algorithm. It is because we have been able to internalize reading and writing and the decimal system, that we are able to perform the corresponding operations with an understanding of their meaning."
82,210,0.985,Fading Foundations : Probability and The Regress Problem,"For example, if a = 31 and b = 53 , we find from (3.29) that P(q) = 34 . One might object that our argument so far is still not very realistic, to put it mildly. For a start, the assumption that conditional probabilities are known as precise numbers is a travesty of what is attainable in scientific practice. In real experiments the conditional probabilities are imprecise, merely being known to lie within some specified interval, and as a result, the unconditional probability of the target, too, is subject to measurement error. Fortunately, when the conditional probabilities are uniform, as for example in the case of Barbara, then it is relatively easy to determine the interval within which the target probability must lie. For suppose that P(An |An+1 ) is in the interval [Î±m , Î±M ], and P(An |Â¬An+1 ) is in the interval [Î²m , Î²M ]. It can be shown that expression (3.17) for P(q) is an increasing function of both Î± and of Î² ;32 and this means that the uncertainty in P(q) is given by"
270,141,0.985,The Huawei and Snowden Questions : Can Electronic Equipment From Untrusted Vendors Be Verified? Can An Untrusted Vendor Build Trust into Electronic Equipment? (Volume 4.0),"installation in Syria. During the attack, the Syrian radar system apparently failed to warn the Syrian army of the incoming attack. Intense speculation and an alleged leak from a US defence contractor point to a European chipmaker that built a kill switch into its chips. The speculation is therefore that the radars were remotely disabled just before the strike [1]. There are two classes of dynamic approaches for finding such Trojans in integrated circuits. One is the activation of the Trojan by executing test vectors and comparing the responses with expected responses. If the Trojan is designed to reveal itself only in exceptionally rare cases, such as after a specifically defined sequence of a thousand instructions, such an approach is unfortunately practically impossible, due to the combinatoric explosion in the number of possible stimuli [16]. The other method is the analysis of side-channels through measuring irregularities in power consumption, electromagnetic emission, and timing analysis. This is doable when there are chips without Trojans with which the measurements can be compared [14]. When these Trojans are deliberately introduced by the manufacturer, there may not be any Trojan-free chips for comparison. In our case, side-channel analysis can therefore be easily duped by the perpetrator. With hardware, unlike software, a great deal of effort goes into handling the problem of untrusted producers of code that goes into the product. It is therefore disconcerting that wrongdoers still seem to operate without substantial risk of being caught. There is yet no âsilver bulletâ available that can be applied to detect all classes of hardware Trojans with high confidence [15]. This observation should, however, not lead to criticism of the researchers in the area; rather, it should make us realize the complexity of the problem at hand. Continued research is the only real hope we have to be able to address it."
303,113,0.985,Multiculturalism and Conflict Reconciliation in the Asia-Pacific,"nation-states, regardless of their geographical location, as standardized and homogenized in the way that the Westphalian norms prescribe, hybrid forms of nation and state are far more likely in reality. In fact, many writers from the outer circle, such as Salman Rushdie, Rohinton Mistry, Shashi Tharoor, Amitav Ghosh, and Arundhati Roy, employ hybrid forms of English and question the monolithic image of nationhood. Dissanayake contends: These writers are seeking to gain entrance to their multifaceted subjectivities by âdecolonizingâ the English language and the sedimented consciousness that goes with it. Many of them regard the English language as the repressive instrument of the hegemonic colonial discourse. They wish to emancipate themselves from its clutches by probing deeper and deeper into their historical pasts, cultural heritages, and the intricacies of the present moment. Through these means, they seek to confront their protean selfhoods. What is interesting is that these writers are striving to accomplish this liberation through the very language that has in the past shackled them to what can be characterized as an ambiguous colonial legacy. (2006, p. 557) In the stories of these writers, we can locate the counter-narratives of nation and the passionate endeavor to destabilize the political maneuvers through which imagined communities with essentialist identities become possible. This pluralized English here becomes the strategic means by which the given identity of the nation-state is questioned. We can say here that English is no longer the exclusive property of those residing in the core, but is owned by the entire population, who use it every day as a device for communication. Second, if the theory of World Englishes not only transfers our focus onto a new awareness of the subjectivity of the periphery, it also questions the subjectivity of contemporary world affairs in general. As the above quotation reveals, the theory of World Englishes, in the age of postcoloniality, dismantles a fundamental notion. Identity is seen as neither rigid nor robust; rather, it is often protean and amalgam-like. This protean self often strategically takes an identity in one place and substitutes it with another identity in a different context. These writers are constantly crossing and recrossing boundaries both topographical and linguistic so as to capture the complex dynamics of the present historical conjuncture and cultural moment. Some of them move back and forth between home and exile, at times"
107,213,0.985,"Symbiotic interaction : 5Th international Workshop, Symbiotic 2016, Padua, Italy, September 29â30, 2016, Revised Selected Papers","However, the recognition of terrorists using false identities to move from a country to another is not the only practical context in which identity veriï¬cation is crucial. The identity veriï¬cation is a key issue for a large number of application domains, such as the security issue for online authentication (e.g., online banking, ecommerce websites) and the use of fake proï¬les in social networks. Biometric measures currently available for identity veriï¬cation exploit physiological or behavioural characteristics such as ï¬ngerprints, hand geometry, and retinas to check identity [4]. More recent approaches developed biometric identiï¬cation systems based on user-pc interaction characteristics, such as keystroke dynamics and mouse dynamics [5, 6]. Nevertheless in the context of terrorism and in other practical domains, these identity check tools are not useful because many of the suspects are unknown and their biometâ rical characteristics are not included in databases and, therefore, unidentiï¬able [7]. For this reason, one actual open challenge is to implement a reliable instrument for identity veriï¬cation that does not require any prior information about the suspect. In other words, an instrument that recognizes the speciï¬c user is not helpful to identify terrorists, thus a tool that detects the deception about identity in a more generic way is necessary. The deception production is a complex psychological process in which cognition plays an important role [8]. During the generation of a false response, the cognitive system does not simply elaborate a statement, but it carries out several executive tasks: it inhibits the true statement and, subsequently, it produces a false statement [9]. Moreâ over, the generation of a lie requires to monitor the reaction of the interlocutor and to adjust the behavior congruently to the lie [10]. All these mental operations cause an increase in cognitive load and, generally, a greater cognitive load produces a bad performance in the task the participant is carrying out, in terms of timing and errors [11]. In particular, participants manifest a lengthening of reaction times (RTs) and an increasing in error rate. This phenomenon has been observed by studying the RTs in double choice tasks: the choice between two alternatives becomes slower in the decepâ tive response than the truthful one [12]. According to the functioning of our cognitive system, behavior-based lie detection tools have been proposed. The most cited are RT-based Concealed Information Test (RT-CIT) [13] and the autobiographical Implicit Association Test (aIAT) [14] that are two memory detection techniques. Based on RT recording, these instruments can detect between two alternative memories presented to the participant in form of words or sentences which is true and which one is false. These techniques have been used also for identity veriï¬cation, to reveal which of two identities is the real identity of the examinee [15]. However, both RT-CIT and aIAT require that the true identity informaâ tion is available, while in the real application only the information provided by suspected is obtainable. As well as RTs are considered reliable behavioral indices of deception, kinematic analysis of hand movements may provide a clue for recognizing deceits [16]. In fact, recently researchers described as a simple hand movement can be used to study the continuous evolution of the mind processes underlying a behavioural response during a computer task [17]."
360,197,0.985,Compositionality and Concepts in Linguistics and Psychology,"It has been shown (e.g. Barsalou 1985) that when people are asked to say how typical an item is as a member of a category, then they are influenced by several different dimensions. First and foremost, Typicality ratings are assumed to be a pure measure of the underlying similarity in meaning, or degree of match of semantic features, between a member and its superordinate category. This dimension of"
93,273,0.985,Nordic Mediation Research,"8 Striving for Christieâs Norm Clariï¬cation In cases like the above, played out by the resentful offender and/or the uncompromising victim, the parties were quarrelling about who should take the blame and the responsibility, or they gave up on the dialogue entirely because it was hard to ï¬nd a basic consensus. Christie (1977) pinpoints that mediation is a possibility of discussions as communities, i.e. âopportunities for norm-clariï¬cationâ. He expands this point through a description of the losses potentially involved in upholding the conventional legal process: It is a loss of pedagogical possibilities. It is a loss of opportunities for a continuous discussion of what represents the law of the land. How wrong was the thief, how right was the victim? Lawyers are [. . .] trained into agreement on what is relevant in a case. But that means that it is difï¬cult to stage what we might call a political debate in court. When the victim is small and the offender bigâin size or powerâhow blameworthy then is the crime? [. . .] If the offender is well educated, ought he then suffer more or maybe less, for his sins?."
261,177,0.985,The Poetics and Politics of Alzheimerâs Disease Life-Writing,"as secondarily victimised. It is in this atmosphere, I so argue in my analysis of the accounts by Robert Davis, Diana Friel McGowin and Larry Rose, that patients follow traditional narrative concepts. In agreement with early criticism of these narratives, I identify them as deploying linear plots and established myths.6 But reading them from the vantage point of greater temporal distance, and in the context of Hawkinsâs illness narrative criticism as well as Couserâs ethical considerations, I believe these authorial choices to be purposeful for the patientsâ wider political cause. At the time of publication, these narratives appealed to a readership primed for the caregiverâs accomplished account. Complying with their readersâ expectations, patients could begin to gather an audience that learned to appreciate the patientâs continued personhood. It is against this changing background that slightly later published narratives could attempt a presentation that reï¬ects the patientâs narrative capabilities more credibly. My comparison of Cary Smith Hendersonâs narrative fragments to Thomas DeBaggioâs highly articulate and literary composition illustrates how patients struggled to ï¬nd narrative forms that would embrace the plot of their disease, while leaving space and scope for continued identity and personhood within this trajectory. I close this chapter with an analysis of the account of a Harvard cardiologist about his Parkinsonâs disease with rapidly progressing Lewy body dementia. Building on life-long experiences from the vantage point of a practicing clinician, Thomas Graboys now takes the viewpoint of the patient. His change in perspective enables him to bring the patientâs agenda to an audience that authoritatively includes the medical profession. Several years into patient life-writing, this agenda now can move on from the recovery of identity and personhood directly to address matters such as end-of-life treatment and choices."
375,152,0.984,Musical Haptics,"5.4 Conclusions The perceptual evaluation of musical instrument quality has traditionally been considered a unisensory experience in the scientific and industrial world alike, based exclusively on how the produced tone sounds in terms of pitch, dynamics, articulation, and timbre. To a certain extent, this is naturally expected. After all, the objective of playing a musical instrument is to make (musical) sounds. But while this holds true for the non-musician listener, it only tells part of the story from the perspective of the musician, where aural impression is accompanied by haptic feedback due to"
113,279,0.984,Pentecostalism and Witchcraft : Spiritual Warfare in Africa and Melanesia,"of the healers we worked with has the gift of X-ray sight, and she can see right through persons or materials. She is also a popular healer for businessmen who are afraid of competitors who might target them with nakaimas to drive them out of business. A healer is therefore often asked not only to bless new businesses, and thus protect them, but also, regularly, to âscanâ the places for sorcery. The healers adjust their treatment to the specific kind of evil that is in question. If a person is possessed by a demon, the healers need to identify the medium though which the demon has gained access to the patientâs body. If the symptoms are different, for instance just trouble at work, in marriage or politics, the cause might not be a demon but nakaimas. However, the healers often articulated that the differences or nuances between instruments or causes didnât matter to them. Whether an affliction was caused by ancestral spirits, urban demons, overseas magic or local sorcery items it still had the one and same origin and cure. It was the result of an opening or a crack in the moral constitution of the personâa crack that had allowed the evil forces inside the self â and the crack had to be closed by the Holy Spirit through discernment and prayer. In Port Vila evil is becoming an absolute phenomenon; there are no âgreyâ areas. There are no forms of sorcery or witchcraft or demons that are only slightly evil. And the question of evil must be located to the integrity of the person affected by it. As an extension of the argument forwarded by Robbins in his article on Pentecostal ritual (2004) we should add that probably the most important factor for explaining the popularity of Pentecostalism must be the role that healing rituals and rituals of discernment play for redefining an entirely new field of âspiritual powersâ around the individual person. The cleansing of neighborhoods, cities or nationsâwhere the ritual is taken out of the church building and into the streets, brings about the change that is the Pentecostal revolution. This is what Robbins calls âthe Pentecostal promotion of ritual as a mode of socialityâ (Robbins 2009: 63). People leave behind the church building and its ritual services and instead cast the everyday as a platform for generalized ritual activity. By ritual, we here imply a form of routinely engagement with forces that lie beyond the observable and tangible, that pertains to the sorting out of invisible forces that have penetrated into persons, things, or relations. The object of the rituals is the discernment of these invisible influences, their cleansing or casting out and the reestablishing of the normality of the situation. In Port Vila everywhere you go you are subject to attacks from these spiritual influences, and so the city is also becoming obsessed"
73,794,0.984,Balanced Urban Development : Options and Strategies For Liveable Cities,"refers to a weakening of ties between culture and place. In a simplistic way it can be said that gentriï¬cation and globalisation are two faces of the same process that is shaping the cityscape, in a State such as Lebanon where the debate between identities and cultures is nearly continuous and where construction is practically always given priority."
388,1008,0.984,The European Blood and Marrow Transplantation Textbook For Nurses : Under The Auspices of Ebmt,"15.5.2 Designs for Nursing Research Once the question has been clearly defined, identifying the appropriate methodology to answer the question most effectively is the next step. The research design has to be pertinent to the question itself, with the nature of the question guiding the choice of approach. Research questions may be exploratory (i.e., with no a priori theory of outcome), wanting to investigate a phenomenon that we know little about and how it is perceived by a group of individuals. This type of question lends itself to in-depth interviews and a qualitative research approach. If a research question is, for example, interested in the efficacy of a novel intervention, perhaps in comparison with to standard care, then a randomized controlled trial may be more appropriate. Two overarching categories of research are basic research, used to obtain empirical data (e.g., laboratory-based studies), which is unlikely to be immediately translatable to clinical practice, and applied research, which is usually directly relevant to the clinical setting. Nursing research tends to be applied research. Research can also be categorized into experimental or non-experimental. Although the suggested gold standard of evidence is the experimental approach of an RCT, it is not always possible or appropriate to use this approach, for example, where randomization may be unethical. This does not mean research other than that of an"
269,195,0.984,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"of those in the room, diagnosed the often disavowed affective underpinnings of a particular kind of critique that imagines itself as conducted on affectively neutral, reasoned, and deliberative grounds. The sentence, FC thinks, also expresses some bafflement that the neuroscientists in the room had been subject to such affectively intense critique, given that they were precisely those who had chosen to engage, in sustained ways, with those from other disciplines in order to interrogate neuroscientific models and evidence. But the affective dynamics of the workshop did not, FC believes, hold open the possibility of recognizing these kinds of âentangledâ cross-disciplinary labours: instead, they lurched towards a bifurcated field of âhatersâ and âplayasâ. This overheard sentence, then, which might well usually have been sloughed off as a casual interjection and one unimportant to the âworkâ of interdisciplinarity, made clear to FC how any attempt to understand that work needs to attend to the fraught and deep affective channels that shape how individual researchers â and the arguments they make â retreat, advance, exchange, and disengage with fields different from their own. The sentence achieved this through a bold and unexpected importation of popular culture â specifically via the use of a term that emerged in African-American hip hop to anatomize the changing economic and social hierarchies of the street and the music industry. We want to stress that, in our experience, it has not been uncommon for this kind of ironic, multi-modal word-play â an informal register that can carry irritation, acuity, grandiosity, and hurt more openly than that of frequently strangulated academic exchange â to be the means through which we ourselves, as well as others we have heard commenting on interdisciplinary scenes, bear witness to some of the most difficult elements of interdisciplinary working. A quick postscript: The term âplaya haterâ began to travel, sometimes invoked by us and by some of the people with whom we collaborate; it become a way to track interdisciplinary power dynamics. One of us (FC) has herself, on a number of occasions, been described, good-naturedly, as a âneuroscience playa haterâ. She certainly didnât easily identify with being one (not least given the unhappy analysis we have just provided above), and she certainly didnât see herself consciously wanting to endorse a bifurcated vision of the world in which there are simply the âplayasâ and the âplaya hatersâ. She did, however, reluctantly have to acknowledge that she did recognize certain aspects of herself, as well as of the affectively fraught field of interdisciplinary research on the mind and brain, that"
297,1151,0.984,The R Book,"First of all, notice that we have lost one degree of freedom, because there are now eight values of y rather than nine. The estimate of the slope has changed from â1.2167 to â1.1171 (a difference of about 9%) and the standard error of the slope has changed from 0.2186 to 0.1956 (a difference of about 12%). What you do in response to this information depends on the circumstances. Here, we would simply note that point (6, 2) was inï¬uential and stick with our ï¬rst model, using all the data. In other circumstances, a data point might be so inï¬uential that the structure of the model is changed completely by leaving it out. In that case, we might gather more data or, if the study was already ï¬nished, we might publish both results (with and without the inï¬uential point) so that the reader could make up their own mind about the interpretation. The important point is that we always do model checking; the summary.lm(model) table is not the end of the process of regression analysis. You might also want to check for lack of serial correlation in the residuals (e.g. time series effects) using the durbin.watson function from the car package (see p. 484), but there are too few data to use it with this example."
264,104,0.984,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"Abstract âWhat is Mathematics?â [with a question mark!] is the title of a famous book by Courant and Robbins, ï¬rst published in 1941, which does not answer the question. The question is, however, essential: The public image of the subject (of the science, and of the profession) is not only relevant for the support and funding it can get, but it is also crucial for the talent it manages to attractâand thus ultimately determines what mathematics can achieve, as a science, as a part of human culture, but also as a substantial component of economy and technology. In this lecture we â¢ discuss the image of mathematics (where âimageâ might be taken literally!), â¢ sketch a multi-facetted answer to the question âWhat is Mathematics?,â â¢ stress the importance of learning âWhat is Mathematicsâ in view of Kleinâs âdouble discontinuityâ in mathematics teacher education, â¢ present the âPanorama projectâ as our response to this challenge, â¢ stress the importance of telling stories in addition to teaching mathematics, and ï¬nally, â¢ suggest that the mathematics curricula at schools and at universities should correspondingly have space and time for at least three different subjects called Mathematics."
116,120,0.984,Moral Reasoning At Work : Rethinking Ethics in organizations,"The main conclusion I draw from the study of the moral reasoning of business leaders when confronted with the reference dilemma is that the concept of moral neutralization is very relevant for understanding how people in organizations can overcome moral dissonance and end up acting against their initial moral convictions. Ariely (2012) has a name for what happens when the original moral misgivings concerning a particular option disappears: The what-the-hell-effect. Once the moral resistance has gone, the road lies open for new routines and practices. The following quote from Tyler Hamiltonâs book about being a cyclist in Lance Armstrongâs team illustrates the mentality we can find on the other side of the fence: You could have hooked us up to the best lie detectors on the planet and asked us if we were cheating, and weâd have passed. Not because we were delusional, but because we didnât think of it as cheating. It felt fair to break the rules. (Hamilton and Coyle, 2012)"
186,99,0.984,Dignity in The 21St Century : Middle East and West,"To contain the end of an action in oneself is Kantâs way of saying that one is giving consent. If one agrees to an action, one carries the actionâs end in oneself. But liars do not reveal their true motives, they do not reveal their ends. Hence, it is not possible to consent to the ârealâ action, as one is being deceived, used for anotherâs purposes. And hence, dignity is an intrinsic and inviolable property of all rational beings, which gives the possessor the right never to be treated simply as a means, but always at the same time as an end. That dignity is an intrinsic or inviolable property can be derived partly from Kantâs claim that one cannot deny dignity even to a vicious man. At the same time, dignity is founded on the human ability to be self-legislative, and is therefore bound to rationality and restricted to rational beings. Kantâs Formula of Humanity can then be used to provide content to the meaning or at least the implications of dignity. Can one be satisï¬ed with these deï¬nitions? No, for two reasons. First, what does it mean to treat a person never simply as a means, but always at the same time as an end? Explanations of abstract concepts should be clear and not require philosophical training to be understood, assuming one can understand them at all. In this context, W.D. Ross has noted that the Formula of Humanity only has"
217,57,0.984,Finite Difference Computing With Pdes : a Modern Software Approach,"The moving plot window is invoked, and we can follow the numerical and exact solutions as time progresses. From this demo we see that the angular frequency error is small in the beginning, and that it becomes more prominent with time. A new run with Ât D 0:1 (i.e., only 10 time steps per period) clearly shows that the phase errors become significant even earlier in the time series, deteriorating the solution further."
346,417,0.984,"History Education and Conflict Transformation : Social Psychological Theories, History Teaching and Reconciliation","Middleton and Edwards 1990). Efforts to change these representationsâe.g. by promoting âprejudice reductionâ interventions through intergroup contactâmay not always be successful or may actually work to strengthen dualisms of good/bad and perpetrator/victim (Dixon et al. 2012). Yet, if such dualisms are so rigid and the cycle of nationalism is simply renewed every time through different means, then one wonders about the prospects of collective action for reconciliation. This tension is particularly evident in historical narratives taught in schools in many conflict and post-conflict societies, whereas such narratives provide a framework through which children make sense of and lay claim to a national collective memory (Davies 2004; Siegel 2002). History curricula implore students to remember the nationâs glories, leaders and warriors through practices which aim at establishing a historical consciousness that âaligns forgetting with evil forcesâ (Eppert 2003, p. 186) that threaten to destroy the nationâs identity and its very existence. In fact, one of the functions of collective memory is to highlight the victimhood of the in-group and silence the traumatic experiences of the out-group members, what has become known as one-sided victimization narrative (Bekerman and Zembylas 2012). However, students and teachers are not dopes answering the mandates of âpolitics of memoryâ (Todorov 2003; Simon 2005). Instead, a sense of rupture with official historical narratives and essentialized identities may be grounded in the notion of dangerous memories, for this idea challenges assumptions that âtransmitted memoriesâ are endlessly powerful and thus can facilitate conflict transformation processes. Dangerous memories are not a particular kind or function of memory that can be isolated and defined, points out Ostovich (2002, 2005); rather, they are âa disruptive practice of and from memoryâ (2002, p. 239, added emphasis). Any memory can become dangerous when it resists the prevailing historical narratives. what makes though a memory to be disruptive and therefore valuable to facilitate conflict transformation? Dangerous memories are disruptive, for example, when they call for solidarity with the âenemyâ on the basis of common human suffering. These kinds of disruptions come as dangerous memories when we remember events of the past that question our consciences and assumed horizons; âdangerousâ, then, takes the meaning of challenging, critical and hopeful while propelling individual and collective consciousness into a new process of narrativization. Re-claiming forgotten connections with"
211,71,0.984,Entrepreneurial Cognition : Exploring The Mindset of Entrepreneurs,"entrepreneurial passion Researchers established long ago that passion is a strong motivator of action (see David Hume 1711â1778; Jean Jacques Rosseau 1712â1778) as well as of entrepreneurial decisions (Smilor 1997). We turn to selfdetermination theory (Deci and Ryan 2001; Gagne and Deci 2005; Ryan and Deci 2000) and its extension to passion (Vallerand et al. 2003) to gain a deeper understanding of entrepreneurial motivation. Self-determination theory proposes that individuals attempt to satisfy three basic psychological needsâneed for competence, need for relatedness, and need for autonomyâand thus carefully bear these needs in mind when making decisions. When individuals are put in a decision-making situation, the intentionality of their efforts to meet these needs is either controlled or autonomous (Gagne and Deci 2005). Controlled motivation concerns a pressure to act, whereas autonomous motivation refers to individualsâ voluntary participation in an activity because they find it enjoyable and interesting. This difference between autonomous and controlled intentionality is reflected in the different types of passion. As a whole, passion is a âstrong inclination toward an activity that one loves and finds important, that is, self-defining and in which significant time and energy are investedâ (Houlfort et al. 2015: 85). Then, depending on whether passion stems from a controlled or autonomous source, it is labeled as obsessive passion or harmonious passion (Vallerand and Houlfort 2003; Vallerand et al. 2003), respectively."
78,295,0.984,The Onlife Manifesto : Being Human in a Hyperconnected Era,"of separation between nodes: for instance, if 3 nodes are randomly rewired, the degrees of separation decrease from 5 to 3. This means that, in a circle of 6 billion (people) nodes as our world could be represented today, if random links in the network would be about 2 out of 10,000, the degree of separation turns out to be 8. But if they are 3 out of 10,000, then 5! Since the pioneering work of Stanley Milgram (1967) and, later, of Mark Granovetter (1973), the idea of small world-networks became in few years one of the key words of contemporary scientific research by fostering a large set of empirical studies on the topology of complex systems. Significant effort has been made in order to structure analytical models able to capture the nature of small worldnetworks. Here, it suffices to mention only two of these. The first small worldmodel was proposed by Duncan Watts and Steven Strogatz (1998): they suggested to randomly rewire a small fraction of the edges belonging to a low-dimensional regular lattice so as to prove that the degrees of separation in the network would exponentially decrease. Yet, contrary to random networks, the shortening of the diameter proceeded along with high clustering coefficients as in regular networks. These small world-features explain the results of Milgramâs and Granovetterâs research because short diameters of the network and high clustering coefficients quantify both the low degrees of separation between two citizens picked up randomly in such a complex network like the American society studied by Milgram in the mid 1960s, and the âstrength of weak tiesâ stressed by Granovetter in the early 1970s. The second analytical model we need to examine was defined by Albert-LÃ¡szlo BarabÃ¡si (2002): he noted that most real world networks, such as the internet, grow by continuous addition of new nodes whereas the likelihood of connecting to a node would depend upon its degree of connectivity. This sort of special attachment in a growing system explains what Watts and Strogatz apparently missed, namely, the power-law distribution of the network in a topological scale-free perspective: small world-networks in the real world are indeed characterized by few nodes with very high values and by most nodes with low connectivity. The presence of hubs or of a small fraction of nodes with a much higher degree than the average offers the key to comprehend why small world-networks can be both highly clustered and scalefree. This occurs when small, tightly interlinked clusters of nodes are connected into larger, less cohesive groups. Drawing on this research, we can deepen the notion of complexity mentioned in the introduction. Todayâs onlife kosmos can indeed be comprehended in accordance with the nature of the hubs and the degree of their connectivity in a small world network, because the emergence of spontaneous orders, e.g. peer-to-peer (P2P) file-sharing systems on the internet, often goes hand in hand with the hierarchical structure of these networks (Pagallo and Durante 2009; Glorioso et al. 2010). Significantly, in The Sciences of the Artificial (new ed. 1996), Herbert Simon insisted on this point, i.e., the notion of âhierarchyâ as the clue for grasping the architecture of complexity and, moreover, the idea of ânearly decomposable systemsâ that reconciles rigid top-down and bottom-up approaches. In the wording of Simon, âthe clusters of dense interaction in the chartâ of social interaction âwill identify a rather well-defined hierarchic structureâ ( op. cit., p. 186). Furthermore, according"
269,50,0.984,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"Hubbub itself. Focusing on Hubbubâs different projects, the social interactions that make it up, its public statements, the strategies of its management team, and so on, this project resituates Hubbub within a broader field of early twenty-first century interdisciplinarity â and the projects within it as projects suspended within Hubbubâs own collaborative logic. We are also using a qualitative and quantitative questionnaire (designed by Angela Woods, and drawing on her work in âHearing the Voiceâ (see Robson, Woods, and Fernyhough 2015 )), which is taking the pulse of the 50 or so collaborators in Hubbub at two different time-points. This questionnaire includes a social network analysis, which will be used to track the density and dynamics of the projectsâ collaborations over time â and which, incidentally, draws on our neuroscientific collaboratorsâ expertise in topological methods that they employ in functional connectivity analyses of the brain (e.g. Margulies et al. 2013). We are also designing and trialling a new method called âIn the Diary Roomâ â which will be an automated collective reflection and self-tracking process, loosely inspired by âThe Diary Roomâ of Big Brother fame. Collaborators who volunteer will be randomly called into a small room just off our main project space, and will therein be asked a series of recorded, randomized questions about their day, how they feel the project is going, different feelings they have around collaboration in that project space, and so on. This is one of the ways in which we are gathering data about relations of power (see Chapter 6), and about the dynamics of affect (see Chapter 7). (For further details of all of these techniques, see Callard, Fitzgerald, and Woods 2015.) The important point is that all the different elements of building an interdisciplinary career that we have discussed â finding collaborators, getting training, meeting people, applying for grants, having interdisciplinary papers published â are worth tracking and reflecting on in their own right. And this is not simply out of a narcissistic sense that oneâs own and oneâs collaboratorsâ research trajectory is intrinsically fascinating, but rather because these mundane actions, spaces, and efforts make up what we might call, following Bruno Latour, the âplasmaâ of interdisciplinarity, viz. the âcirculations of totalizations and participationsâ of interdisciplinary collaboration that are still âwaiting for explication and compositionâ (2012, 93). To put it more prosaically: if a range of interdisciplinary scholars begin tracking their own progress through these fields, and begin following the trajectories of the different people, things, ideas, bureaucracies, machines, and so on, that make them up â then not"
238,242,0.984,Nanoinformatics,"structure and to more precisely identify features (for example, precipitates and interfaces) from the 3D data [23â30]. Because data is in the format of discrete points in some metric space, i.e., a point cloud, many data mining algorithms, which have been developed, are applicable to extract the geometric information embedded in the data [31â34]. Nevertheless, those geometric-based methods have certain limitations when being applied to solve the problems in atom probe data. We summarize below the limitations of geometric-based methods. In the category of supervised learning [35], many methods require prior knowledge about the data. In the case when the prior knowledge is not available, assumptions need to be made and a bias could be introduced. For example, regression usually assumes a mathematical function between the variables, which means the conclusion we draw from the regression would bias the function that is chosen. On the other hand, for unsupervised learning methods [34], there is usually some parameter(s) that needs to be determined for the algorithm. For example, clustering methods usually require the number of clusters (or some equivalent parameter) to be manually determined; in the case of dimensionality reduction, a common assumption is that the data resides on a lower dimensional manifold, which will sufï¬ciently represent the data, although the dimension of the manifold may not be something that can be determined by the algorithm. Due to the wide range of applications, there is hardly a universal rule to determine the values of the parameters required by the geometric-based methods. For a particular task, the parameters can be determined either empirically based on the constraints of the situation at hand, or by some algorithm [36]. In these cases, the hidden assumption is that the number of the parameter is ï¬xed once chosen. In some scenario, it would be worthwhile to make those ï¬xed parameters variables. This is not equivalent to giving a set of values to the parameters and collecting all of the results, since the results are independent from each other. What is needed is a scheme that can summarize the results as the parameter changes value. The lack of variability also exists on another level, that is, geometric-based approaches have the property of being exact, i.e., two points in a space are geometrically distinguishable as long as they do not share the same coordinates. As a result of this, for example, classiï¬cation algorithms determine the classes by using a set of hyper boundaries which are ï¬xed once obtained by training the algorithm. Topological-based methods have certain properties that are not available for the geometrical-based methods [37]: (i) topology focuses on the qualitative geometric features of the object, which themselves are not sensitive to coordinates, which means that the data can be studied without having to use some algebraic function, and thus no prior assumption or parameter needs to be dealt with; (ii) instead of using a metric for distance, topology uses a less clear metric, i.e., âproximityâ or âneighborhoodâ, since âproximityâ is less absolute than the actual metric, topology is capable of dealing with the scenarios where information is less exact;"
395,34,0.984,Beyond Safety Training : Embedding Safety in Professional Skills,"Workplace safety is a particular form of âorganizational competenceâ. In other words, it is a form of emerging competence sustained in working practices by interactions among various collective actors (Gherardi and Nicolini 2000),2 and various discourses on what constitutes safety. What we call âsafetyâ is the result of a set of working practices shaped by a system of symbols and meanings which orient action but which consist of something more. Safety can therefore be viewed as an emerging property of a sociotechnical system, the ï¬nal result of a collective process of construction, a âdoingâ which involves people, technologies and textual and symbolic forms assembled within a system of material relations. This system of relations is made up of heterogeneous components, and it does not display the traditional distinctions between human and non-human elements, cultural or natural aspects, action and constraints. Rather, all these elements are involved in a constant process of generation called the âengineering of heterogeneityâ (Law 1992). A âsafeâ workplace or a âsafeâ organization are the outcome of the quotidian engineering of heterogeneous elementsâcompetences, materials, relations, communications, peopleâintegral to the work practices. When we consider safety as a social and collective accomplishment, as something that is done with the collaboration of all the practitioners involved in a working practice, then we can say that it has the following characteristics: â¢ It is situated in the system of ongoing practices. It means that âsafetyâ cannot be separated from its practice and therefore we have to consider safe and safer working practices instead of studying, researching and intervening on safety in abstraction from its work context. â¢ It is relational and mediated by artifacts. Safety knowledge always manifests itself in social activities sustained by symbols, technologies and relations; i.e. action is always âmediatedâ. The essential instrument of mediation is language,"
223,298,0.984,Knowledge and Action (Volume 9.0),"How to Define and How to Obscure Knowledge One may question whether sociological or economic accounts of the knowledge society need to define knowledge. As soon as they do, however, quite different views surface. The best examples (also in the sense of solid, not simply deficient considerations) are found in the classical theories on the topic. Drucker (1969), probably the first writer to offer a conception of the knowledge society, was brief in definitional matters. Using an approach that has since become widespread (outside philosophy), he also made a specific point: âKnowledge, that is, the systematic organization of information and concepts,â¦ makes apprenticeship obsolete. Knowledge substitutes systematic learning for exposure to experienceâ (p. 268). In context, Drucker focused even more on issues of application: Knowledge is analyzed as crucial in increasing the productivity of labor. Four years later, Bell (1973) highlighted the opposite side when he noted a ânew centrality of theoretical knowledge, the primacy of theory over empiricismâ (p. 343, his italics). Accordingly, basic science is the main reference when he defines knowledge as âa set of organized statements of facts or ideas, presenting a reasoned judgment or an experimental result, which is transmitted to others through some communication medium in some systematic formâ (p. 175). This definition has remained popular in descriptions of the recent, computer-based take-off of the knowledge society (see Castells, 1996, p. 17, for example). But the focus on explicit statements cannot account for a central novelty that characterizes the work of contemporary knowledge workers or symbol analysts (Reich, 1991)âthe importance of situated problem-solving, which demands capacities of embodied or organizational knowledge. Such a capacity is probably at stake when Willke (1998) tries to define knowledge in structures where it matters ânot as truth but as a resourceâ (p. 161): âWhereas information designates systemically relevant differences, knowledge originates when such information is embedded in contexts of experienceâ (pp. 161â162, my translation).1 Unlike the standardized situations Drucker had in mind, this experience presumably affords more than textbooks can convey. Historical differences set aside, the given examples seem to offer three systematically distinguishable accounts of knowledge: 1. Knowledge as a systematic set of applicable recipes 2. Knowledge as an organized body of theoretical statements 3. Knowledge as a developed capacity of situated problem-solving These accounts do not necessarily constitute or presuppose different concepts of knowledge. Maybe they are really only about different contexts in which knowledge matters and thus give different perspectives on the same thing. But if there should"
48,125,0.984,Time Predictions : Understanding and Avoiding Unrealism in Project Planning and Everyday Life (Volume 5.0),"6.2.1 The Use of Alternative Interval Prediction Formats Instead of asking for a minimumâmaximum time interval corresponding to 90% confidence, we could turn the question around. We can ask for the level of confidence for a given minimumâmaximum time usage interval. If you think 100 work hours is a reasonable time prediction for a certain job, then you could use the time usage corresponding to, say, 50â200% of this prediction (in this case the interval between 50 and 200 work hours) and ask for the probability of the actual time usage falling between 50 and 200% of the predicted time. Studies report a remarkable reduction in overconfidence when using this alternative request format for wide time usage intervals [6]. When, on the other hand, the time prediction intervals were narrow, this request format did not increase realism [8]. For example, software developers believed that it was at least 60% probable that the actual use of time would be within Â±10% of the predicted use of time. In reality, only 15%- and not 60%âof the projects fell within the Â±10% prediction interval. A potential advantage of the alternative format is that it eases the use of historical data. In situations with tasks of varied size and complexity, it is difficult to use historical data on time usage to calculate a 90% prediction interval for a new task. It is easier to use the historical data from tasks of various sizes and degrees of complexity to develop a distribution for the time prediction error. The latter can, together with the alternative interval prediction method, be used to find prediction intervals and pX predictions. This method is illustrated below. Think about one type of work you often do. How often have you ended up spending more than twice (200%) the predicted time on this type of work? Say that this happens about 5% of the time. This means that it does not happen about 95% of the time. In other words, given that history is a good predictor of future prediction errors, you can"
311,515,0.984,The Physics of the B Factories,"r(Ît; Î¼, Ï) =fC Â· Gauss(Ît; Î¼C , ÏC )+ (1 â fC â fO ) Â· Gauss(Ît; Î¼T , ÏT )+ fO Â· Gauss(Ît; 0, ÏO ), (11.3.2) where Î¼C,T and ÏC,T,O represent the means and widths of the corresponding Gaussian distributions, respectively, and fC and fO represent the fraction of events in the core and outlier component, respectively. While very few events are expected that are not described by the convolution of the physics model with a core and tail Gaussian resolution term, it is important to include a wide outlier term in the resolution model, as otherwise a single event that is âfarâ from both core and tail models has the potential to contribute disproportionally to the likelihood and can strongly and unduly inï¬uence the ï¬t result, even when outliers only contribute at the permille level to the event sample. A common pragmatic choice for the outlier term is a very broad Gaussian distribution, as shown in the example of Eq. (11.3.2), but other shapes have also been used. The resolution model of the previous example describes the average performance of the decay-time reconstruction. Since the decay-time diï¬erence is calculated from the distance between two decay vertices, the resolution in the time diï¬erence will depend on the number of tracks used in the vertex ï¬ts as well as their conï¬guration, and the vertex ï¬t procedure returns an estimate of the uncertainty on the decay-time diï¬erence for each event. A more precise inference on the physics parameter Ï of the model f can be made by taking into account this perevent uncertainty on the decay time diï¬erence â weighting events with a precise measurement of ÏÎt more strongly than those with a poorer measurement by modifying the resolution model as follows"
45,93,0.984,Measurement and Control of Charged Particle Beams,"which is commonly applied. Figure 2.11 illustrates the error involved in approximating (2.28) by (2.29). The difference between the two expressions becomes important if Qx,y is close to an integer or half integer resonance, and for large changes ÎK [24]. A recent beta function measurement at the Fermilab Recycler is depicted in Fig. 2.12. The nonlinear dependence is well described by the complete (2.28). Care has to be taken that the applied change in quadrupole strength does not alter the beam orbit, which happens if the beam is off-center in the quadrupole whose strength is varied. If the orbit changes, part of the measured tune shift could then be caused by the closed-orbit variation at the sextupole magnets elsewhere in the accelerator. If a strong effect on the orbit is observed, the orbit should first be corrected with the help of steering correctors before the new (shifted) tune value is measured. Sometimes, several magnets are connected to the same power supply, and then the strengths Ki (i = 1, ..., m) of m quadrupoles must be changed simultaneously, all by the same amount ÎK. The above result is easily generalized to this case: the induced tune change is related to the average beta function at the m quadrupoles via Î²x,y m â Â±4Ï ÎQx,y /(mÎK). However, the disadvantage of averaging over several quadrupoles is that beta beating may be less evident."
297,323,0.984,The R Book,"The median (or 50th percentile) is the middle value of the sorted values of a vector of numbers: sort(y)[ceiling(length(y)/2)] There is slight hitch here, of course, because if the vector contains an even number of numbers, then there is no middle value. The logic here is that we need to work out the arithmetic average of the two values of y on either side of the middle. The question now arises as to how we know, in general, whether the vector y contains an odd or an even number of numbers, so that we can decide which of the two methods to use. The trick here is to use modulo 2 (p. 18). Now we have all the tools we need to write a general function to calculate medians. Let us call the function med and deï¬ne it like this:"
235,270,0.984,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","Since theological nomenclature hardly belongs to the standard repertoire of physicists but will be used later, as some termini technici will be mentioned upfront. Thereby we will mainly follow Philipp Frankâs (informal) definitions of gaps and miracles [219, 220]. In the theological context, creatio ex nihilo often refers to the âinitial boot up of the universe;â whereas creatio continua stands for the permanent intervention of the divine throughout past, present, and future. Alas, as we will be mainly interested with physical events, we shall refer to creatio ex nihilo, or just ex nihilo, as something coming from nothing; in particular, from no intrinsic [500] causation (and thus rather consider the theological creatio continua; apologies for this potential confusion). Ex nihilo denies, and is in contradiction, to the principle of sufficient reason (cf Sect. 17.1, p. 135), stating that nothing is without intrinsic causation, and vice versa. According to Frank [219, 220, Sect. II, 12], a gap stands for the incompleteness of the laws of nature, which allow for the occurrence of events without any unique natural (immanent, intrinsic) cause, and for the possible intervention of higher powers [219, 220, Sect. II, 9]: âUnder certain circumstances they do not say what definitely has to happen but allow for several possibilities; which of these possibilities comes about depends on that higher power which therefore can intervene without violating laws of nature.â Many scientists, among them Poisson, Duhamel, Bertrand, and Boussinesq [162, 494], have considered such gaps as a possibility of free will even before the advent of quantum mechanics. Maxwell may have anticipated a scenario related to deterministic chaos (cf. Chap. 18, p. 141) by considering singular points and instability of motion with respect to very small variations of initial states, whereas Boussinesq seemed to have stressed rather the nonuniqueness of solutions of certain ordinary differential equations [162, 343, 494]. This is different from a direct breach or âraptureâ of the laws of nature [219, 220, Sect. II, 10]; also referred to as ontological gap by a forced intervention in the otherwise uniformly causal connection of events [438, Sect. 3.C.3, Type II]. An Â© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_20"
223,281,0.984,Knowledge and Action (Volume 9.0),"The Role of Knowledge Accessibility in Planning and in the Control of Action To assist the individualâs pursuit of a goal effectively, implementation intentions need to specify relevant critical situations in the if-component and instrumental responses in the then-component (see also Gollwitzer, Wieber, Myers, & McCrea, 2009). Prior studies have generally observed that people can indeed identify and self-select suitable situations and responses (e.g., Adriaanse, de Ridder, & de Wit, 2009; Gollwitzer & BrandstÃ¤tter, 1997). In fact, both experimenter-provided and self-generated implementation intentions have been shown to foster goal attainment effectively (Armitage, 2009). But how do people generate effective plans? Individuals have to access goal-relevant knowledge before they can further process this information. Generally, psychological research shows wide agreement that knowledge accessibility is important for individualsâ cognition and behavior (overview by Wyer, 2008). As for the accessibility of goal-related knowledge, goal systems theory (Kruglanski et al., 2002) affords a helpful conceptual framework for understanding how pursuing a goal affects the accessibility and application of knowledge that is relevant to planning. This theory rests on a cognitive approach to motivation. Its proponents apply a network conceptualization that allows for dynamic and malleable modeling of the activation and permits application of cognitive content to motivation content. Within this âmotivation-as-cognitionâ approach, goal systems are defined as âthe mental representations of motivational networks composed of interconnected goals and meansâ (Kruglanski et al., 2002, p. 333). Given this connectedness of goals and means, the activation of a mental representation of a goal should also activate the mental representation of suitable means to pursue this goal. When this idea is applied to planning, it follows that when one is pursuing a goal (e.g., to prepare a healthy dinner), knowledge of possible means that is relevant to planning the when, where, and how of goal-striving becomes easily accessible (e.g., thinking of the salad in oneâs fridge and of the tomatoes that one has to purchase on the way home). Two properties of the interconnections are thus especially interesting for the activation of goal-relevant knowledge: structure and strength. As far as the structure of the interconnections are concerned, the number of means that are attached to a goal can vary. For one person, activating the physical fitness goal might activate only the means of going to the gym, but for another person it might activate a multitude of means (e.g., going to the gym, riding a bike to work, and taking the stairs). In addition to such interindividual differences, the number of means connected to a goal might also vary from one goal to the next. For instance, there might be numerous ways to pursue the goal of eating healthily (e.g., eating at least five portions of fruit and vegetables a day, drinking water rather than soft drinks) but only a few ways to pursue the goal of acquiring a driverâs license (i.e., taking the official test). Concerning the strength of the interconnections, one may expect the strength of the cognitive association between the goal and the means for achieving it to be stronger when the number of those means is relatively low than when it is relatively high."
280,37,0.984,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"the midline and/or the veins must be either more rapid or slower than movement elsewhere. One way to accomplish this is by having a required metabolite or precursor to the reaction distributed in a pattern that is symmetrical to the midline. A clear candidate for this is the gradient left behind by the midline pattern that preceded the formation of the focal spot (Fig. 1.8). This midline concentration gradient decays only gradually, and its profile depends on the parameter values and initial fuel distribution. The hypothesis then is that the shape of the parafocal elements is determined by a gradient left behind by the process that formed the focus. This idea can be tested computationally. Figures 1.10a and 1.11 show a sample of the diversity of parafocal element shapes that can be produced by this model. Although these shapes closely mimic those of real parafocal elements (e.g., Fig. 1.9), the shape of the ocellus is not circular, as would typically be the case. To produce both perfectly circular eyespots and the right diversity of parafocal element shapes, it is necessary to assume that the focus could be the source of two different signals (one perhaps initiated by notch and the other by Distal-less) that use different substrates. If one signal uses a homogeneously distributed substrate, it will produce a circular eyespot (Fig. 1.10b), and if the other uses the gradient left behind by the focus-forming process, it produces the parafocal element. Interestingly, this second source also produces an arc-shaped pattern on the proximal side of the eyespot (Fig. 1.10câg). This finding is consistent with SÃ¼ffertâs idea that the parafocal element is the distal band of the border symmetry system: the parafocal element and the proximal arc produced by the second source make up paired bands of the border symmetry system. These model results also support the ideas about the nature of parafocal elements and border symmetry systems proposed by Otaki (Dhungel and Otaki 2009; Otaki 2009, 2011)."
49,445,0.984,Artificial Intelligence and Cognitive Science IV,"Even if PS interacts with dynamic environment and reacts to its changes but there is not performed any knowledge acquisition. Therefore, the other two modules provide means for learning the RB. The first module the CAS uses a method of the so-called bid competition being inspired by auction trade. Matched rules offer a part of their strengths. After such an auction the winner(s) pays (pay) this part of its (their) strength(s) for the possibility to be fired. If the fired action is rewarded then the reward will enhance strengths of all fired rules otherwise they will lose. In such a manner a hierarchy of rules with different strengths is created in RB. Some rules are gradually removed or at least weakened and some others are strengthened until the environment is changed. There is a number of methods, which realize CAS, e.g. the bucket brigade algorithm or profit sharing plan [6]. As CAS contributes to the discovering of rule conflicts considerably it is often connected together with CRS as a CA/CR system. CAS acts as a rule filter. After a certain time RB is reduced to a small set of rules with high strengths or fitness. Such an approach converges to solutions in a subspace, where probability of optimal solutions is low. To stimulate the system to search also in other parts of the space of solutions it is necessary to add new rules when the CAS reaches a steady state. This is the task of RDS, which utilizes GAs for this purpose. Mainly the rules with high strengths are selected for parents and GA will generate a population of offsprings, which will be after that included into RB and using CAS the strengths will be calculated for them. The only difference from a GA is that the selection of rules is performed in CAS and not in RDS. This whole learning process of creating new rules is performed cyclically, where some additional inner cycles exist, too. It will be stopped if no new rules are created, which can happen only if there are no new significant changes of the environment. The pseudocode of the basic algorithm is shown in fig. 6. The mentioned process of Michigan approach was described very roughly, where only basic processes were noticed. There is a number of various modifications, e.g. [3, 27] but all of them have a common characteristic feature, which differs from Pittsburgh approach described in sect. 3.2. Systems based on Michigan approach represent their individuals as rules. Thus a population is equivalent to a RB, which is the result of competitions among individual rules (individuals). 3.1.1"
213,205,0.984,Collider Physics Within The Standard Model : a Primer,"2.10 Measurements of Ës Very precise and reliable measurements of Ës .mZ / are obtained from eC e colliders (in particular LEP), from deep inelastic scattering, and from the hadron colliders (Tevatron and LHC). The âofficialâ compilation due to Bethke [99, 311], included in the 2012 edition of the PDG [307], is reproduced here in Fig. 2.32. The agreement among so many different ways of measuring Ës is a strong quantitative test of QCD. However, for some entries the stated error is taken directly from the original works and is not transparent enough when viewed from the outside (e.g., the lattice determination). In my opinion one should select a few of the theoretically cleanest processes for measuring Ës and consider all other ways as tests of the theory. Note that, in QED, Ë is measured from a single very precise and theoretically clean observable (one possible calibration process is at present the electron g 2 [242]). The cleanest processes for measuring Ës are the totally inclusive ones (no hadronic corrections) with light cone dominance, like Z decay, scaling violations in DIS, and perhaps Â£ decay (but for Â£ the energy scale is dangerously low). We will review these cleanest methods for measuring Ës in the following."
249,500,0.984,Advances in Proof-Theoretic Semantics (Volume 43.0),"there is no proof-theoretic semantics of implication, at least not along the common line that an implication expresses that we have or can generate a hypothetical proof. We would need instead a semantics from outside. A proof-theoretic semantics for the placeholder view of assumptions (Sect. 2.1), even though it is assertion-centred, is not necessarily verificationist in the sense that it considers introduction rules for logical operators to be constitutive of meaning. Nothing prevents us from considering elimination rules as primitive meaning-giving rules and justifying introduction rules from them (see Prawitz [49], Schroeder-Heister [67]). However, the placeholder-view forces one particular feature that might be seen as problematic from certain points of view, namely the global character of the semantics. According to the placeholder-view of assumptions, an open derivation D of B from A would be considered valid if for every closed derivation of A the derivation"
84,535,0.984,Eye Tracking Methodology,"When eye movements are recorded during extended search, fixations tend to be longer than in reading. However, there is considerable variability in fixation time and saccade length as a function of the particular search task (Rayner 1998). Specifically, visual search tasks vary widely, and tasks in which eye movements have been monitored consist of at least the following: search (a) through text or textlike material, (b) with pictorial stimuli, (c) with complex arrays such as X-rays, and (d) with randomly arranged arrays of alphanumeric characters or objects. Because the nature of the search task influences eye movement behavior, any statement about visual search and eye movements needs to be qualified by the characteristics of the search task. Henderson and Hollingworth (1998) list factors that may vary from study to study: image size (usually measured in visual angle), viewing task (e.g., later recognition memory task, image preference, counting nonobjects, visual search, âfree viewingâ (it is well known that viewers place their fixations in a scene differently depending on viewing task), viewing time per scene (ranging from very short, tachistoscopic to longer durations, e.g., on the order of 50 msâ10 s or even 30 min (Yarbus 1967)), image content (e.g., artwork, ânatural scenesâ, human faces), and image type (e.g., highly artificial and regular such as sine wave gratings, color, monochrome, grayscale, or full-color computer graphics imagery). These factors could each produce main effects and could also interact with each other in complex ways to influence dependent measures of eye movement behavior such as saccadic amplitudes, fixation positions, and fixation durations. As demonstrated by Yarbus (1967) and then by Noton and Stark (1971a, b), eye tracked scanpaths strongly suggest the existence of a serial component to the picture viewing process. However, serial scanpaths do not adequately explain the brainâs uncanny ability to integrate holistic representations of the visual scene from piecemeal (foveal) observations. That is, certain perceptual phenomena are left unaccounted for by scanpaths, including perception of illusory images such as the Kanizsa (1976) figures or the Necker (1832) cube. Although scanpaths cast doubt on a purely Gestalt view of visual perception, it would seem that some sort of holistic mechanism is at work which is not revealed by eye movements alone. Models of visual search attempt to answer this dilemma by proposing a parallel component, which works in concert with the serial counterpart exhibited by eye movements. In visual search work, the consensus view is that a parallel preattentive stage acknowledges the presence of four basic features: color, size, orientation, and presence and/or direction of motion (Doll 1993; Wolfe 1993). Todd and Kramer (1993) suggest that attention (presumably in the periphery) is captured by sudden onset stimuli, uniquely colored stimuli (to a lesser degree than sudden onset), and bright and unique stimuli. There is doubt in the literature whether human visual search can be described as an integration of independently processed features (Van Orden and DiVita 1993). Visual search, even over natural stimuli, is at least partially deterministic, rather than completely random (Doll et al. 1993). Determinism stems from either or both of two kinds: the observerâs strategy determines the search pattern (as in reading), and/or"
289,258,0.984,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","While t-closedness is a semantic condition (cf. Definition 10), there are simple syntactic conditions to guarantee it. For instance, assertions that carry a non-strict comparison â²â³ â{â¤, â¥, =} between two bounded probabilistic expressions are t-closed; the assertion stating probabilistic independence of a set of expressions is t-closed. Precondition Calculus. With a concrete syntax for assertions, we are also able to incorporate syntactic reasoning principles. One classic tool is Morgan and McIverâs greatest pre-expectation, which we take as inspiration for a pre-condition calculus for the loop-free fragment of Ellora. Given an assertion Î· and a loopfree statement s, we mechanically construct an assertion Î· â that is the precondition of s that implies Î· as a post-condition. The basic idea is to replace each expectation expression p inside Î· by an expression pâ that has the same denotation before running s as p after running s. This process yields an assertion Î· â that, interpreted before running s, is logically equivalent to Î· interpreted after running s. The computation rules for pre-conditions are defined in Fig. 8. For a probability assertion Î·, its pre-condition pc(s, Î·) corresponds to Î· where the expectation expressions of the form E[eÌ] are replaced by their corresponding preterm, pe(s, E[eÌ]). Pre-terms correspond loosely to Morgan and McIverâs preexpectationsâwe will make this correspondence more precise in the next section. The main interesting cases for computing pre-terms are for random sampling and conditionals. For random sampling the result is Pxg (E[eÌ]), which corresponds to the [Sample] rule. For conditionals, the expectation expression is split into a part where e is true and a part where e is not true. We restrict the expectation to a part satisfying e with the operator E[eÌ]|e = E[eÌ Â· 1e ]. This corresponds to the expected value of eÌ on the portion of the distribution where e is true. Then, we can build the pre-condition calculus into Ellora. Theorem 1. Let s be a non-looping command. Then, the following rule is derivable in the concrete version of Ellora: {pc(s, Î·)} s {Î·}"
192,174,0.984,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"preventing a U.S. monopoly on atomic weapons. In other words, researchers fall victim to the matheme of desire ($ â a). Classified information becomes the object a precisely because it is classified as top secret, withdrawn from the knowledge commons. This has grave repercussions for the individuals involved. As soon as it becomes clear that atomic knowledge may give rise to an atomic bomb, the biographies of the scientists involved (previously uninteresting more or less) become ârecordsâ or âfilesâ, while problematic entries (such as: involvement in certain political activities) become items of concern. Thus, from now on, scientists involved in classified research are scrutinised and monitored. A researcher with communist leanings is from now on a scientist with a divided loyalty (p. 312), who âprofesses loyalty first and foremost to the Soviet Unionâ (p. 229). The communist is the Mister Hyde ($) concealed beneath the allegedly impassive expert persona (S2), who is therefore likely to fall victim to the matheme of desire and to give in to the inclination to perpetrate. When Sebastian is about to be appointed as scientific director of the Valhalla project, it is made clear to him that âfrom here on â¦ you will have to resign yourself to the fact that youâll probably be under more or less constant observation. I would simply assume that everything you say, and everything said to you, whether directly or over the telephone, is being monitored, that your mail is being opened and your movements followedâ¦ Youâll be watched closelyâ (p. 217), all this because of his involvement with the Thing. Due to the Thing, Sebastian becomes a file, scrutinised by secret service experts, trawling it for symptoms of divided loyalty. Thus, Sebastian becomes the target of investigations by the secret service, but the object of these investigations is not Bloch as a living human persona, but something inside Bloch, something enigmatic: the âMr. Hyde in this Dr. Jekyllâ (p. 40) as Gregg phrases it, his communist leanings, in combination with his uncanny gift for influencing people, his almost âtelepathicâ talents of persuasion, bordering on the occult. Bloch is an enigma (p. 68) who does not limit himself to one particular speciality, like normal scientists, but seems completely at home in the whole realm of science; and in the arts as well (p. 40).10 There is something strange about his eyes and voice, moreover, and his face seems suffering and haunted: a âstrange, arrestingâ face with a âbrooding, uncanny lookâ (p. 67), a âdemoniac lookâ (p. 258), with a compassionate yet cruel set of eyes (p. 67). Due to his involvement with the Thing, his face becomes even more unsettling: an âabstract maskâ (p. 261), while his âblack eyes become enormousâ (p. 343). For Gospodin Gregg, the archetypal secret agent, this enigmatic âsomethingâ, the Mr. Hyde in Dr. Bloch, the $ inside S2, becomes the object a, to which he devotes many years of research and around which he designs a completely new type of âexperimentâ. Gregg is a psychologist studying a physicist, driven by his antiIn art a similar dynamics is discernible as in science, Sebastian argues. Beginning with CÃ©zanne, modern painters had âdestroyed painting â¦ had begun the process leading to disintegration that made the exhibits of modern painters look like the cemeteries and ossuaries of artâ (p. 147/148). The monster exemplifies this disintegration of âsubstance and senseâ (p. 201)."
223,170,0.984,Knowledge and Action (Volume 9.0),"The Option-Generation Framework A third theory that deals with the relation between knowledge and action is the option-generation framework by Kalis et al. (2008). Studying the weakness of will (a phenomenon known as acrasia), these researchers concentrated on option generation, a little-understood process that precedes option selection and action initiation. Figure 6.3 illustrates the idealized process of option generation, option selection, and action initiation and gives the background of the ideas that Kalis et al. (2008) have about degenerative processes in this area. Table 6.1 affords an overview of the ways in which dysfunctions in option generation can result in irrational behavior. The table presents two dimensionsâdysfunction in the quantity of options (hypogeneration and hypergeneration) and dysfunction in the quality of options. The two rows separate instrumental irrationality from noninstrumental irrationality, meaning that options can be seen either as a means to realize certain goals (i.e., the instrumental understanding) or as irrationality in the goals themselves (i.e., noninstrumental irrationality). This concept links knowledge and action in a special way: It makes a connection between options and actions."
8,615,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","exists for all Ë. Hence, even in this limit, still no singularity exists in spite of the exponential spectrum and in spite of the fact that now integrations and sums do go to infinity. This proof does not, however, apply to the situation under which the singularity was found. In [4], the limit was not taken in the usual way, viz., as in Eq. (24.1): first calculate ln Z for fixed V, then let V ! 1. Instead, the âavailable volumeâ  D V  Vi , where Vi is the proper volume of P the i th particle, was used as a volume parameter and kept constant. Thus V D  C Vi , so that, when sums over particle numbers and integrals over masses were done, V was pushed to infinity. Then expectation values hV.Ë; ; /i, hE.Ë; ; /i, etc., could be calculated and densities could be defined by hE.Ë; ; /i=hV.Ë; ; /i, etc., which did indeed show a singularity. Since therefore the existence of a singularity depends on the limiting procedure, it seems important to clear this up. A simple example shows that there is nothing like a universally âcorrectâ limiting procedure, but that different limiting procedures correspond to different physical situations. Imagine a high pressure container of volume V filled completely with water at room temperature and atmospheric pressure and then hermetically closed. One may heat it up to any temperature and the water will not boil; putting infinitely many such boxes together and removing interior walls (V ! 1) will change nothing. If, on the other hand, one closes the vessel by a movable piston, one sees the water boil if pressure and temperature fall in a certain interval. In this last case, the water pushes the volume to ever larger values similarly to the situation considered in [4]. We believe that at temperatures and densities where hadron matter changes into quark-gluon matter, no fixed volumes should be used in theoretical considerations, since boxes do not exist in this regime. Forces keeping a system together (the tendency to cluster is just such an internal force, while gravity might be considered as an external one) control pressure and densities rather than volume."
280,169,0.984,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"Eyespots, concentric bands of pigment patterning, constitute one of the most studied pattern elements on the wings of butterflies (c.f., Fig. 6.3 for an example). Each eyespot develops around a focus, a small group of cells that sends out a morphogenetic signal that determines the synthesis of circular patterns of pigments in their surroundings. In this work, we consider a model that provides a possible mechanism underlying the determination of the number and locations of eyespots on the wing surface. The model we consider, first described by Sekimura et al. (2015), provides a mechanism that places the foci around which eyespots form in various locations on the entire wing surface. We do not address here subsequent stages of eyespot formation that occurs after the development of the foci. The model we consider is based on that of Nijhout (1990). The main novelty of the work in Sekimura et al. (2015) was to illustrate that simply changing the conditions assumed to hold at the proximal veins was sufficient to determine whether or not an eyespot formed in a given wing cell. In the present work, we extend the investigations of the models proposed in Sekimura et al. (2015). We show that it is possible to determine the location of eyespots within a wing cell simply by changing the conditions that are assumed to hold at the lateral wing veins that bound the wing cell. Furthermore, we illustrate that it is possible, using a two-stage model, to recapitulate the results of artificial selection experiments in terms of selection and location of eyespots in butterfly wings."
297,622,0.984,The R Book,"You need to experiment with the amount argument to get the degree of scatter you require (this speciï¬es the limit on the x or y axis of the amount of jitter on either side of the actual value). An alternative function is called sunflowerplot, so called because it produces one âpetalâ of a ï¬ower for each value of y (if there is more than one) that is located at that particular point. Here it is in action:"
297,601,0.984,The R Book,"Statisticians do not like pie charts because they think that people should know what 50% looks like. Pie charts, however, can sometimes be useful to illustrate the proportional make-up of a sample in presentations. The function pie takes a vector of numbers, turns them into proportions, and divides up the circle on the basis of those proportions. It is essential to use a label to indicate which pie segment is which. The label is provided as a vector of character strings, here called data$names. Because there are blank spaces in some of the names (âoil shalesâ and âmethyl clathratesâ) we cannot use read.table with a tab-delimited text ï¬le to enter the data. Instead, we save the ï¬le called piedata as a comma-delimited ï¬le, with a â.csvâ extention,"
82,35,0.984,Fading Foundations : Probability and The Regress Problem,"To say that infinitism has consistently had a bad press would be claiming too much. Infinitism had no press at all, since until very recently nobody took it seriously. The reason for this is not difficult to discern. In an epistemological tradition dominated by Aristotelian and Cartesian foundationalism, a position like infinitism is highly counterintuitive to say the least; for how could anybody, in Aristotleâs words, âgo through infinitely many thingsâ? It is therefore not surprising that infinitism is hardly, if ever, mentioned in treatises or textbooks; and if it is mentioned, then it usually serves as an example of a blatantly ridiculous way to go. Yet it cannot be denied that infinitism sits well with some modern ideas about the nature of knowledge, such as that knowledge is essentially fallible, and that the human search for it is, indeed, without end. Despite many attempts to show the contrary, it is not at all clear how these ideas, which so many of us endorse, can be smoothly combined with foundationalism or even coherentism.15 In this book we will investigate the consequences of an infinitist response to the regress problem. We do not propose to defend infinitism as such. Rather our aim is twofold. On the one hand, we intend to show that some standard objections to the position are not as strong as they might seem at first sight. On the other hand, we explain how our analysis of these objections brings about insights that cast new light on the traditional positions, foundationalism and coherentism; as we will see, a careful analysis of infinite justificatory chains will teach us interesting novel facts about finite ones. In the end we somehow try to get it all, sketching the contours of an infinitist version of coherentism, which also acknowledges the foundationalist lesson that we should somehow make contact with the world. We will return to this in the final chapter. All-important for the development of infinitism was the work by Peter Klein. Around 2000 Klein wrote a number of papers in which he took the bull by the horns and presented infinitism as a genuine competitor to coherentism and foundationalism. Here is how Klein introduces his view in a relatively early paper: The purpose of this paper is to ask you to consider an account of justification that has largely been ignored in epistemology. When it has been considered, it has usually been dismissed as so obviously wrong that arguments against it are not necessary. The view that I ask you to consider can be called âInfinitismâ. Its central thesis is that the structure of justificatory reasons is infinite and nonrepeating. My primary reason for recommending infinitism is that it can 15 For a prominent attempt at reconciling foundationalism and fallibilism, see Audi"
297,1407,0.984,The R Book,"Up to this point, the response variables have all been continuous measurements such as weights, heights, lengths, temperatures and growth rates. A great deal of the data collected by scientists, medical statisticians and economists, however, is in the form of counts (whole numbers or integers). The number of individuals who died, the number of ï¬rms going bankrupt, the number of days of frost, the number of red blood cells on a microscope slide, and the number of craters in a sector of lunar landscape are all potentially interesting variables for study. With count data, the number 0 often appears as a value of the response variable (consider, for example, what a 0 would mean in the context of the examples just listed). In this chapter we deal with data on frequencies, where we count how many times something happened, but we have no way of knowing how often it did not happen (e.g. lightning strikes, bankruptcies, deaths, births). This is in contrast to count data on proportions, where we know the number doing a particular thing, but also the number not doing that thing (e.g. the proportion dying, sex ratios at birth, proportions of different groups responding to a questionnaire). Straightforward linear regression methods (assuming constant variance, normal errors) are not appropriate for count data for four main reasons:"
214,471,0.984,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"12.3.1 Ensembles There are three typical types of climate-model ensemble projections. One is an ensemble of different models with the same conï¬guration, each running the same scenario. This is designed to focus on structural uncertainty in the models. Initial condition uncertainty is present, but it goes away for long experiments (over 50 years). It explicitly removes scenario uncertainty. The second type is a set of ensemble simulations with the same model and the same scenario that start with slightly different initial conditions to sample the initial condition or internal variability uncertainty. This explores the possible states in a single model conï¬guration, eliminating structural and scenario uncertainty. The third type of ensemble focuses on scenario uncertainty, for example, by running the same model for more than 50 years to remove model and initial condition uncertainty. All three types are used in climate analysis. Which type is used depends speciï¬cally on the application. For example, scenario uncertainty need not be treated on 20- to 50-year time horizons but dominates in the longer term. Using these different techniques leads to the conclusion that on the century scale scenario uncertainty is the largest uncertainty, not model uncertainty."
271,396,0.984,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"communication (Riegelsberger et al. 2007). Second, the importance of face-to-face interactions is also supported by observations from other areas of decision-making, for example from the private business sector: Especially when it comes to complex decisions that require a significant amount of trust and/or involve decisions on resource allocation and distribution, actors rely on face-to-face meetings.1 Finally, various shortcomings of technically mediated forms of decision-making interactions have been identified, for example in the context of studies on online deliberation or protest movements.2 But although these studies support the idea that face-to-face interactions are of particular importance to political decision-making, they do not present answers to the core question: Why exactly do face-to-face interactions structure figurations of political decision-making in this way? And, furthermore, why might a changing media environment lead to a point at which they become more important? Following the concept of communicative figurations,3 a promising approach of empirical research that tries to find an adequate answer to these questions is a focus on the construction of relations within actor constellations. If one analyzes the micro-level of political interactions, and especially the micro-level of decision-making interactions, it is possible to identify typical sequences of interaction that constitute important relational structures within figurations of political decision-making. Furthermore, the micro-analysis of these typical sequences has implications for the meso-analysis of the political domain as a whole. This link becomes apparent if one takes a closer look at the way Norbert Elias introduces the concept of âfigurationâ: The network of interdependencies among human beings is what binds them together. Such interdependencies are the nexus of what is here called the figuration, a structure of mutually oriented and dependent people. Since people are more or less dependent on each other first by nature and then through social learning, through education, socialization, and socially generated reciprocal needs, they exist, one might venture to say, only as pluralities, only in figurations. (Elias 2012: 525)"
257,36,0.984,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","that an input state produces an output state is already captured (soundly) by pT so we do not change that. This is useful because the approximation of pT does not degrade with the use of the interval domain in the way the approximation of the size degrades (as illustrated in Fig. 3(b)). Using sampling is an attempt to regain the precision lost on the size component (only). Finally, the confidence we have that sampling has accurately assessed which input states are in the support is orthogonal to the probability of any given state. In particular, PT is an abstraction of a distribution Î´T , which is a mathematical object. Confidence Ï is a measure of how likely it is that our abstraction (or, at least, the size part of it) is accurate. We prove (in our extended report [43]) that our sampling procedure is sound: Theorem 2 (Sampling is Sound). If Î´0 â Î³P (P0 ), S P0 = P , and [[S ]]Î´0 = Î´ then Î´T â Î³P (PT + ) with confidence Ï where"
275,638,0.984,Foundations of Trusted Autonomy,"ciency, so we should not expect to have an economically efficient allocation of resources. This is a central point in this chapter: conventional notions of utility maximisation break down rapidly under complexity. Moreover, the quality of decisionmaking of agents under capital rationing is especially sensitive to unforeseen changes in the future cost of capital, which means for autonomous agents potentially exquisite sensitivity to failure under uncertainty. Given hard and soft constraints under ontological uncertainty, decisions regarding how much resource to put into each side of the strategy amounts to judgements about own tolerance to loss, more than it is about the potential for the environment to dish up favourable or unfavourable outcomes. So at the core of the strategy is the requirement for autonomous machines to be equipped with a theory of self, specifically for the purpose of evaluating exposure to unacceptable failure and deciding hedging plans, and for recognising feasible opportunity for reward and determining investment plans given a range of such opportunities. Theory of mind in the usual sense is then really an extension of theory of own mind as the more fundamental concept for autonomous systems research. The strategy amounts, therefore, to a mechanism for substituting the unapproachable problems of prediction and knowledge acquisition in uncontrolled unstructured environments in general with the much more manageable problem of self-knowledge. Note that soft capital rationing is self-imposed, and amounts to judgements and this condition appears to very directly imply a requirement for an agent theory of self. It also seems that a theory of self would imply that, in a sense, such agents would effectively talk to themselves, much as humans do [28, 38], constructing a kind of narrative of self as they debate with themselves about different investment options, and moderate and alter their own beliefs and expectations. There is a deep issue lurking here that motivates and underpins this proposition about agency. Incompleteness means that self-knowledge, and thus knowledge of oneâs own sensitivity to failure, is inherently limited; after all, we all observe ourselves from inside ourselves. A theory of mind supporting the development and application barbell-type strategies accommodates this in two ways simultaneously. Firstly, the judgement caveat on plasticity and thus on autonomy concerning limits we consider to be operationally acceptable is about making visible to the agent itself the consequences of the limits of its own self-knowledge in terms of managing the effects of the limited ability of anything - or anyone - to determine its own failure modes. Secondly, with respect to determining potential exposure to unacceptable failure, I am advocating a defensive kind of posture: exposure to decisive failure is a matter of choosing boundaries beyond which unacceptable failure is a potential rather than a certainty. We cannot determine failure sensitivity completely or with exactitude, but we can choose those boundaries conservatively or optimistically and in priority order depending on our faith in the broader social enterprise to absorb the possible consequent failures. It seems that this problem of self-knowledge is much more manageable, however, both by virtue the fact that the system we then have to deal with is much smaller than that of the entire environment, which, after all, includes the agent itself, and by virtue of the fact that we humans are a testament to how successful in uncertain worlds agents armed with self-knowledge can be."
139,434,0.984,Programming for Computations - MATLAB/Octave (Volume 14.0),"which is known as a two-point boundary value problem. This is nothing but the stationary limit of the diffusion problem in Sect. 5.1.4. How can we solve such a stationary problem (5.38)? The simplest strategy, when we already have a solver for the corresponding time-dependent problem, is to use that solver and simulate until t ! 1, which in practice means that u.x; t/ no longer changes in time (within some tolerance). A nice feature of implicit methods like the Backward Euler scheme is that one can take one very long time step to âinfinityâ and produce the solution of (5.38). a) Let (5.38) be valid at mesh points xi in space, discretize u00 by a finite difference, and set up a system of equations for the point values ui ,i D 0; : : : ; N , where ui is the approximation at mesh point xi . b) Show that if t ! 1 in (5.16)â(5.18), it leads to the same equations as in a). c) Demonstrate, by running a program, that you can take one large time step with the Backward Euler scheme and compute the solution of (5.38). The solution is very boring since it is constant: u.x/ D C . Filename: rod_stationary.m. Remarks If the interest is in the stationary limit of a diffusion equation, one can either solve the associated Laplace or Poisson equation directly, or use a Backward Euler scheme for the time-dependent diffusion equation with a very long time step. Using a Forward Euler scheme with small time steps is typically inappropriate in"
82,354,0.984,Fading Foundations : Probability and The Regress Problem,"and so on. Is it possible to calculate P(q) if the format goes on to infinity? Rescher thinks not. If the hierarchy is endless one cannot know anything about the probability of q, for âwe are propelled into a vitiating regress of presuppositionsâ14 . The situation looks like a probabilistic analogue of the Tortoiseâs interminable query to Achilles, where the latter successively satisfies the former pro tem in higher and higher-order querulousness without end.15 However, this similarity is only apparent. Between the probabilistic and the nonprobabilistic version of the Tortoiseâs challenge to Achilles there is an essential difference: the latter might be hopeless, the former is not. It is true that the Tortoise can always ask about an unknown P(An ) after the weary warrior has taken n steps in his argument. It is also true that the unknown P(An ) could have any value between zero and one. However, the influence that P(An ) has on the value of P(q) will be smaller as the distance between An and q gets bigger â even if P(An ) were to take on the largest allowed value of 1, see Section 4.3. As we know now, in the limit that n tends to infinity, the influence of P(An ) on P(q) will peter out completely, leaving the value of P(q) as a function of the conditional probabilities alone. Note again that this is not because P(An ) itself becomes smaller as n becomes larger: indeed, 14 Rescher 2010, 37. 15 Carroll 1895."
366,23,0.984,"The Future of the Law of the Sea : Bridging Gaps Between National, Individual and Common Interests","low-water line, and it is submitted to any evaluation.12 The majority of States in the world have established normal baselines in a sense that they are considered as the âdefaultâ baselines.13 However, a straight baseline has a particular regime."
84,548,0.984,Eye Tracking Methodology,"and place task, the steps required to perform the task of picking up a certain object from a group of objects can employ deictic references by fixating the object to be picked up next, without having to internalize a geometric reference frame for the entire set of objects. Ballard et al. tested the above deictic reference assumption to see whether humans in fact use their eye movements in a deictic fashion in the context of natural behavior. A head-mounted eye tracker was used to measure eye movements over a threedimensional physical workplace block display, divided into three areas, the model, source, and workspace. The task assigned to subjects was to move and assemble blocks from the source region to the workspace, arranging the blocks to match the arrangement in the model area. An example of the setup is shown in Fig. 21.8. By recording eye movements during the block pick-and-place task, the authors were able to show that subjects frequently directed gaze to the model pattern before arranging blocks in the workspace area. This suggests that information is acquired incrementally during the task and is not acquired in toto at the beginning of the tasks. That is, subjects appeared to use short-term memory frugally, acquiring information just prior to its use, and did not appear to memorize the entire model block configuration before making a copy of the block arrangement. In a similar block-moving experiment, Smeets et al. (1996) were able to show that horizontal movements of gaze, head, and hand followed a coordinated pattern. A shift of gaze was generally followed by a movement of the head, which preceded the movement of the hand. This relationship is to a large extent task-dependent. In goal-directed tasks in which future points of interest are highly predictable, the"
82,212,0.984,Fading Foundations : Probability and The Regress Problem,"which had grown out of earlier life forms, which sprang from inanimate matter, which originated in a supernova explosion, and so on. This is of course true, and it makes short shrift of any remaining thought about a beginning in the form of a first bacterium.33 For our approach, however, the issue is moot. The reason is that the further away a node in the chain is from the target, the smaller its influence on the target becomes. Applied to Barbara: long before we reach the stage where her ancestor bacteria evolve from more primeval life forms, they have become totally irrelevant to the question whether Barbara has T . This phenomenon we call âfading foundationsâ, and it is explained in the next chapter."
297,1168,0.984,The R Book,"The popular notion is that predicting the future is impossible, and that attempts at prediction are nothing more that crystal-gazing. However, all branches of applied science rely upon prediction. These predictions may be based on extensive experimentation (as in engineering or agriculture) or they may be based on detailed, long-term observations (as in astronomy or meteorology). In all cases, however, the main issue to be confronted in prediction is how to deal with uncertainty: uncertainty about the suitability of the ï¬tted model, uncertainty about the representativeness of the data used to parameterize the model, and uncertainty about future conditions (in particular, uncertainty about the future values of the explanatory variables). There are two kinds of prediction, and these are subject to very different levels of uncertainty. Interpolation, which is prediction within the measured range of the data, can often be very accurate and is not greatly affected by model choice. Extrapolation, which is prediction beyond the measured range of the data, is far more problematical, and model choice is a major issue. Choice of the wrong model can lead to wildly different predictions (see p. 471). Here are two kinds of plots involved in prediction following regression: the ï¬rst illustrates uncertainty in the parameter estimates; the second indicates uncertainty about predicted values of the response. We continue with the tannin example: reg.data <- read.table(""c:\\temp\\regression.txt"",header=T) attach(reg.data) names(reg.data)"
235,36,0.984,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","1.7 Nesting Nesting [30, 31] essentially amounts to wrapping up, or putting everything (the object-cut-observer) into, a bigger (relative to the original object) box and consider that box as the new object. It was put forward by von Neumann and Everett in the context of the measurement problem of quantum mechanics [206] but later became widely known as Wignerâs friend [571]: Every extrinsic observation mode can be"
192,320,0.984,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"All of a sudden, the tumours seem to be shrinking. There seems to be a result. The experimentâs object a is decidedly not the mouse, and it is no coincidence that the mice are furless, that their skin is nearly âtransparentâ, for they actually are a âliving libraryâ (p. 25) of proteins and genes. The object a is a spectral something inside these animals, something which may have invaded these mice, something toxic or at least exceptional which temporarily cures them (from cancer), but eventually kills them, because the mice are merely a kind of living stage, allowing the viral drama to unfold, and bound to be sacrificed in order to study the impact. The mice are merely vehicles or ecosystems: the object a is a particular type of virus (labelled R-7), the frustrating, enigmatic target which now suddenly seems to live up to its promises and expectations, for there is something missing in the mice, a disconcerting but at the same time promising abnormality or gap: the tumour has decreased. Marion and Feng discover that three mice have tumours significantly smaller than before. After repeated failure, one of Cliffâs viral variants actually seems to have some effect. Is it significant? Or is the atypical tumour decline âcontaminatedâ by some other (unknown) condition? The responses to this event differ. Whereas Marion (the scientific supervisor, the labâs epistemological super-ego) remains sceptical, Sandy is exuberant, because he immediately sees new possibilities for writing grant proposals for NIH.3 Sandy takes a U-turn by considering Cliff suddenly as the labâs trump card (Miedema 2012, p. 76). Cliff throws himself into work and experiences a second lease on life. He works even longer hours than before and his appetite for science revives. These are his experiments, his mice. This is his crucial moment. His moods swing âsickeningly between delight and despairâ (p. 48), fuelled by âthe propulsive energy of scientific questions, the relentless force of an investigation that might succeedâ, but also tormented by the possibility that âhis good fortune might evaporateâ, that the âremission of the mice is nothing more than a freak occurrenceâ (p. 48). In other words, he becomes trapped in the matheme of desire ($ â a). He forgets about the outside world, loses track of time, becoming âparanoidâ even ($ in the lower-right position; p. 48). All his previous work had given him nothing, but this was his chance. More carefully than ever before, he keeps and copies his records. Do not move, do not touch! These are his mice, his proprietary tumours, his results. Indeed, Cliff develops âa proprietary interest in his virus and his miceâ (p. 51). The âweâ of normal laboratory research has decidedly shifted to the first person singular. As the novel phrases it: All his thoughts and actions served R-7. Cliff saw now that you could not become possessive of this kind of research. Instead, he, the researcher, had become possesses by his creation (p. 178, my italics)."
49,266,0.984,Artificial Intelligence and Cognitive Science IV,"logical and fuzzy Petri nets in the literature [38, 39, 40, 41]. For our purposes the approach presented in [41] and further developed in [19] is the most suitable. For knowledge representation we use a set of propositions which can have the values true or false in the case of a logical knowledge base. For knowledge propagation we consider a set of production rules. Production rule describes the relation between two sets of propositions. Set A of propositions represents the antecedent of the production rule and a set B of propositions represent the consequent of the production rule. The knowledge is propagated by firing of a production rule. The interpretation of firing a production rule is following: IF all propositions in the antecedent A have value true THEN the propositions in the consequent B are true. We consider a simple knowledge base given by a set of propositions and a set of production rules of the following form: logical product of the propositions in the antecedent A implies the logical product of the propositions in the consequent B. Generally, the knowledge is propagated by firing a sequence of these rules, where the consequent of one rule is used as the antecedent of the next rule. In many cases the validity of a proposition is not always certain. For such cases, it is suitable to use fuzzy values for the propositions. In the case of a fuzzy knowledge base fuzzy values from the closed interval of real values <0,1> can be used, where value 0 represents the case in which the proposition is not true and the value 1 represents the case in which the proposition is true. Values between 0 and 1 represent the measure of validity for the proposition. For example consider the following proposition âThe temperature of a patient is highâ. Obviously the validity of this proposition is uncertain. We know that this proposition is more valid if a patient has temperature 40Â°C than if he has temperature 38Â°C. In the cases when the validity of propositions is expressed by fuzzy values also the relation between propositions of the antecedent and propositions of the consequent is fuzzy. A production rule with fuzzy relation is called fuzzy production rule. The mechanism of firing the fuzzy production rule and the knowledge propagation in a fuzzy knowledge base will be explained in the section 5.2 devoted to fuzzy Petri nets. 5.1 Logical Petri Nets A logical Petri net (LPN) is defined as 4-tuple:"
8,296,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","field of high energy physics. Those who prefer precise wording might criticize the use of the expression âlast stepâ because it could be easily misunderstood: namely as the last possible instead of the latest accomplished, as I meant to say here. But there is no mistake in expression. I meant both and especially the last possible step. Instead of an error of style it has to do with a hypothesis, which is being described by this lecture. It appears that we have reached, in elementary particle physics a completely new paradigm, a kind of a terminal situation, in which the question about the composition of matter receives an unforeseen and satisfying answer. This is actually surprising, because we still canât overcome the old difficulties. Whenever someone says to me, that he has now found the true atom, the building block of all matter, I always ask him, then what is this thing made from? One can just read Kant, to see in what sort of cul-de-sac that leads. And now I claim that high energy physicsâperhaps!âoffers a final solution to this dilemma? I do not want to be misunderstood: first, I am making a claim, which is not accepted by all of my colleagues, and second, I do not claim that we are about to understand everything about elementary particles. But this new approach seemsâat least from a particular perspectiveâto offer us the view, which could be used to take the picture."
8,761,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","The Way to Equilibrium It is necessary to understand why there can be local equilibrium at least approximately. The problem has been considered in the literature [43â45]. Clearly, the approach to equilibrium takes time. After a sufficient time has elapsed, any system will come to equilibrium. What is less obvious is that the equilibrium state reached will, everything else being equal, depend on the volume available [12, 45]. Here I shall outline the ideas without going into detail. There are two kinds of equilibrium: kinetic and chemical: â¢ Kinetic equilibrium means equipartition of the total kinetic energy among the particles then present. This is a fast process which needs only very few collisions per particle. We assume this kind of equilibrium to be established instantaneously and locally, which means that a local temperature can be defined meaningfully. This temperature can still vary in space and time. â¢ Chemical equilibrium is equilibrium between the numbers of different species of particles. Being in equilibrium means for a given species that its rate of creation balances its death rate. To arrive at that state may take a short or a long time depending on cross-sections, lifetimes, densities, and so on."
259,53,0.984,The Little Book of Semaphores,Consider again the Rendezvous problem from Section 3.3. A limitation of the solution we presented is that it does not work with more than two threads. Puzzle: Generalize the rendezvous solution. Every thread should run the following code: Barrier code
272,600,0.984,Reconsidering Constitutional Formation Ii Decisive Constitutional Normativity : From Old Liberties To New Precedence (Volume 12.0),"1 What Is a Constitution? Delegation, Octroi or Contract? What kind of legal instrument is a constitution? In the nineteenth century, the answer to this basic ontological question would produce conflicting replies depending on where, when and who you asked. Particularly so in Norway, where"
84,85,0.984,Eye Tracking Methodology,"of light and hence information entering the brain. That is, the discussion is presented âfront-to-backâ starting with a description of the eye and ending with a summary of the visual cortex located at the back of the brain. Emphasis is placed on differentiating the processing capability of foveal and peripheral vision, i.e., the simplified âwhatâ and âwhereâ of visual attention, respectively. However, the reader must be cautioned against underestimating the complexity of the visual system as presented in this text. The apparent âwhatâ and âwhereâ dual pathways are most probably not independent functional channels. There is a good deal of interconnection and âcrosstalkâ between these and other related visual centers which deems the dichotomous analysis overly simplistic. Nevertheless, there is a great deal of valuable information to be found in the neurological literature as human vision is undoubtedly the most studied human sense."
223,261,0.984,Knowledge and Action (Volume 9.0),"Motivation and Opportunity Fazio (1990) describes two modes of thinkingâa spontaneous processing mode based on attitude accessibility, and a deliberative processing mode based on attitude behavior. These modes of thinking are remarkably similar in structure to the RIMâs proposed systems, certain differences in mechanisms notwithstanding. Therefore, the MODE model (Fazio, 1990), which predicts when the deliberative mode will be engaged in processing the possible consequences of behavior, may be applicable to the RIM as well. In this conceptualization, engagement in deliberative processing depends on motivation and opportunity. Motivation in the MODE model is generated by the fear of invalidity (Kruglanski & Freund, 1983), a function of the perceived costliness of a judgmental mistake to the self, whereas opportunity is a function of the available time and resources for processing. Applying this framework to the RIM, one finds that the defined properties of the reflective system are in accord with these predictions. Reflective processing is accompanied by a feeling of subjective effort and so requires motivation, whereas both the reliance on the resources of working memory and the relatively slow speed of the reflective system make it clear that the reflective system can influence behavior only if the opportunity is given. Evidence for this dependence on opportunity exists in many domains. Cognitive load, a manipulation often used to impair deliberative processing, has been applied in various different studies whose results can be explained with the RIM. Selfcontrol (e.g., Lattimore & Maxwell, 2004; Wegner, Erber, & Zanakos, 1993), processing of negated stimuli (Deutsch et al. 2009), social judgments and attributions (Gilbert et al., 1988; Krull & Erickson, 1995; Trope & Alfieri, 1997), moral judgments (Greene, Morelli, Lowenberg, Nystrom, & Cohen, 2008), and general reasoning (De Neys, 2006) have all proven to be impaired by cognitive load in ways"
360,286,0.984,Compositionality and Concepts in Linguistics and Psychology,"5 Discussion This paper reports on an experimental investigation into the interpretation of plural sentences with predicate conjunction, and its connection to typicality. I proposed that the extent to which non-intersective interpretations are available directly correlates with the atypicality of an event in which the two predicates apply simultaneously. Experiment 1 revealed a continuum of acceptability values of 36 sentences in a non-intersective, âsplitâ situation, ranging from 24% to 100% acceptable. Such a continuum is unexpected under the extended SMH by Winter (2001), which assumes that any given sentence is either true or false in a particular situation, depending on what the context allows. Next, Experiment 2 showed that differences in compatibility exist between different predicate pairs. The compatibility ratings for 36 pairs ranged over the entire 6-point scale. I assumed that the compatiblity measurement is an indirect measurement of typicality, namely of the typicality of event E (in which two predicates apply simultaneously) for each predicate concept in isolation, and hence that this effect is similar to the effects that were found repeatedly for one-place predicates (e.g. Rosch 1973). I proposed to extend the Maximal Typicality Hypothesis (Kerem et al. 2009; Poortman et al. 2017) by formulating it for predicate conjunction in such a way that typicality relates to acceptability so that the less compatible the two predicates in Experiment 2 are judged to be (i.e. the less typical event E is), the more a non-intersective interpretation is available. Based on a correlation analysis, I conclude that this prediction was borne out. Note that this correlation does not hinge on my assumption that compatibility is an indirect way of measuring typicality. I merely"
139,129,0.984,Programming for Computations - MATLAB/Octave (Volume 14.0),"We now turn our attention to solving mathematical problems through computer programming. There are many reasons to choose integration as our first application. Integration is well known already from high school mathematics. Most integrals are not tractable by pen and paper, and a computerized solution approach is both very much simpler and much more powerful â you can essentially treat all integrals a f .x/dx in 10 lines of computer code (!). Integration also demonstrates the difference between exact mathematics by pen and paper and numerical mathematics on a computer. The latter approaches the result of the former without any worries about rounding errors due to finite precision arithmetics in computers (in contrast to differentiation, where such errors prevent us from getting a result as accurate as we desire on the computer). Finally, integration is thought of as a somewhat difficult mathematical concept to grasp, and programming integration should greatly help with the understanding of what integration is and how it works. Not only shall we understand how to use the computer to integrate, but we shall also learn a series of good habits to ensure your computer work is of the highest scientific quality. In particular, we have a strong focus on how to write Matlab code that is free of programming mistakes. Calculating an integral is traditionally done by"
37,111,0.984,The Making of Islamic Heritage : Muslim Pasts and Heritage Presents,"this chapterââframing the primordial.â They indicate a number of potential drawbacks for any attempt to contextualize terms such as âIslamic archaeology,â âIslamic heritage,â or âIslamic past.â Taking a side in debates about faith is one of these major drawbacks. Islamic archaeology and emerging critical studies in Islamic heritage usually approach the material culture of different cultural and religious movements among Muslim societies and individuals from a secular tradition. This approach might be effective at avoiding a situation where Islamic archaeology is wielded as tool for questioning the validity of these faiths. However, this secular stance also takes a side in these debates by gathering different expressions of faith under the âIslamicâ adjective. The description of the pre-Saudi period as al-jahiliyyah in the takï¬rist Saudi historiography, as well as ever-changing ofï¬cial and local perceptions concerning the Hejaz Railway, express the ï¬uidity of the term âIslamic heritage.â This ï¬uidity pinpoints the second potential bias: the overlooking of any sociopolitical or personal connotations behind examples of tangible heritage. The formation processes of the Hejaz Railway structures, and the nationalist, global, and dynastical elements in the Saudi museumsâ narratives, can generate different adjectives depending on the outlook. This point leads us to the ï¬nal drawback: the vulgarization and fetishization of the âIslamicâ adjective by attesting a certain material culture and timeframe to it. Generally speaking, the relationship between Muslims and a pre-Islamic past is presented as a case of apathy (see Blau 1995, 122; Liverani 2005, 225; Potts 1998, 195â196). The ambiguity of Muslims to a pre-Islamic past is expressed through few and overly repeated examples: Ibn al-Kalbi, al-Maqrizi, and Abd alLatifâs studies on the pre-Islamic past; revivalist demonstrations, like the 2,500 year celebration of the Persian Empire in 1971; and the admiration for al-Jahiliyyah poetry (see Hawting 1999, 2; Insoll 1999, 230; Milwright 2009, 5; Petersen 2014, 6269). The Saudi museums examined here suggest that a Muslim ambiguity to the pre-seventhcentury AD period is beyond the scope of these fragmentary examples and references, at least on an ofï¬cial level. These museums manifest a primordial timeline within the contexts of Islam, ofï¬cial historiography, and archaeological studies. Above all, these case studies indicate that there is a much broader understanding of the Islamic past in regard to the Muslim belief that Islam is the primordial faith (Hawting 1999, 21; Kuzgun 1997; Leaman 2006, 242)."
49,414,0.984,Artificial Intelligence and Cognitive Science IV,"reasoning to the truth of consequences. A fundamental relevance of logic for cognitive science is based on that observation. We have to reflect the development of logic from Aristotelian times to the state, where a rich variety of logical systems and ways how to do logic (Makinson [18]) is available. Schemes of reasoning uncovered by a logical system preserve truth, if logical constants (each, some, if â then, possibly, etc.) are understood in the way specified by the logical system. Reasoning to an interpretation, as understood by (Stenning and van Lambalgen [21]) is a procedure leading to a selection of an appropriate understanding of logical constants for a given reasoning task. Moreover, a need to specify and to model in an abstract way new logical constants or some new meaning of a logical constant leads often to a new logical system and to a new option for a reasoning to an interpretation. However, the characterization of the variety of reasoning procedures and styles is not exhausted by simple truth preservation. Different types of schemes of truth-preserving reasoning provide a characterization of different forms of deduction. We have to mention also hypothetical reasoning also. Fortunately, (at least some of) people reason even if they do not have only true premises at their disposal. This kind (more precisely, a class of kinds) of reasoning is studied intensively in artificial intelligence. Non-monotonic reasoning and defeasible reasoning are the terms used in artificial intelligence. We will use the term âhypothetical reasoningâ and note that it can be (and is) described in many different ways. We sketch only a simple characterization here. When people reason hypothetically, they consider a set of defeasible assumptions. In general there are some incompatible assumptions in the set. Some assumptions attack other assumptions (via their consequences). Usually, it is not possible to speak about the correctness of an isolated assumption. It is more productive to consider sets of assumptions and to check whether they are defended against the attacks of some (counter)assumptions. A conflict-free set of assumptions S is admissible if it counterattacks each attack against each member of S. This is the basic idea of Dung [10], where this notion was introduced precisely. In Bondarenko et al. [7] it was adapted for assumptionbased frameworks and applied to a characterization of default reasoning in the frame of various non-monotonic formalisms. Different kinds of non-monotonic logic, defeasible logic, argumentation frameworks, logic programming etc. study hypothetical reasoning and various types of sets of admissible assumptions. Some argumentation semantics, based on notions of conflict-freeness and admissibility are discussed later in this section."
173,9,0.984,"intelligent Human Computer interaction : 9Th international Conference, Ihci 2017, Evry, France, December 11-13, 2017, Proceedings","Abstract. The study of living bodies reveals that in order to solve complex problems in an efï¬cient, fast and elegant way, evolution has developed processes that are based on principles that are neither trivial nor simple. I called them âsimplexesâ. They concern for example detours, modularity, anticipation, redundancy, inhibition, reduction of dimensionality etc. They often use detours that seem to add an apparent complexity but which in reality simpliï¬es problem solving, decision and action. Among these general principles, âvicarianceâ is fundamental. It is the ability to solve some problem by different processes according to the capacity of each one, the context, etc. It is also the ability to replace a process by another in the case of deï¬cits. It is also the possibility to create new solutions. Indeed, it is the basis of creative flexibility. I will give examples borrowed from perception, motor action, memory, spatial navigation, decision-making, relationship with others and virtual worlds. I will show its importance for the compensation of neurological deï¬cits and the design of humanoid robots for example. Finally, I will mention their importance in the ï¬elds of learning and education."
310,11,0.984,Online Hate Speech in The European Union : a Discourse-Analytic Perspective,"legal and policy-making circles, the relatively little attention it has received by researchers of linguistic pragmatics and discourse analysis is arguably disproportionate to its social relevance and importance. In this respect, the main aim of this volume is to showcase that an implementation of certain research methodologies that linguists, and more speciï¬cally discourse analysts, have at their disposal can fruitfully contribute to the better understanding of a phenomenon that, as we saw, is becoming increasingly widespread these days. In light of this, the contents of the present volume should be approached as more of a âproof of conceptâ demonstration, rather than an exhaustive analysis of hate speech in the EU. The reason for this is simple: as McGonagle (2013: 3) points out even though the term âhate speechâ is often incorporated, at least as a notion, into legal and policy documents, there is still no universally accepted deï¬nition for it, which on its own warrants further investigation into the ways in which hate, in the relevant sense, is both expressed and perceived. Generally speaking, hate speech could be described as the expression of hatred towards an individual or group of individuals on the basis of protected characteristics, where the term âprotected characteristicsâ denotes membership to some speciï¬c social group that could, on its own, trigger discrimination (cf. OSCE/ ODIHR3 2009: 37â46). What these protected characteristics are, however, remains open to interpretation, with different states including different categories under this rubric, as will be discussed in more detail in the following section of this introductory chapter. Just to give an example, the EU deï¬nition of hate speech that is put forth in the Council Framework Decision 2008/913/JHA of 2008 conï¬nes hate speech to âall conduct publicly inciting to violence or hatred directed against a group of persons or a member of such a group deï¬ned by reference to race, colour, religion, descent or national or ethnic originâ (Council of the European Union 2008), essentially leaving out of the equation such characteristics as sex, gender identity and sexual orientation. As Baider (2017) notes, however, in an attempt to deï¬ne âhate speechâ more broadly, one could follow the International Covenant on Civil and Political Rights which does not single out any particular protected characteristics and instead proposes that hate speech essentially amounts to an âadvocacy of discriminatory hatred which constitutes incitement to hostility, discrimination or violenceâ (UN General Assembly 1966, our italics; see also OHCHR 2013). While the question of how to exactly interpret the words âhatredâ, âdiscriminationâ, âviolenceâ and âhostilityâ in this deï¬nition still remains open, it manages to express more concretely the forms that the expression of hatred, in the relevant sense, may take. What is more important here, however, is the word âincitementâ, which takes centre stage and renders the intention to trigger potential actions against members of protected groups a precondition for considering a speech act hate speech, assuming, thus, a link between hate speech and hate crime, with the former presumably leading to the latter."
261,222,0.984,The Poetics and Politics of Alzheimerâs Disease Life-Writing,"Ramanathanâs remark that patients elaborate their telling in negotiation with their audience âlike those of us who are normalâ.4 Ramanathan made this observation in 1997, two years after Lennard J. Davis had asserted that âour construction of the normal world is based on a radical repression of disabilityâ. Davis speciï¬cally concentrates on the physical body, but his brief excursion on madness as a condition that similarly âshows up as a disruption in the visual ï¬eldâ makes his work directly applicable to cognitive decline: [T]he fear is that the mind is fragmenting, breaking up, falling apart, losing itself â all terms we associate with becoming mad. With the considerable information we have about the biological roots of mental illness, we begin to see the disease again as a breaking up of ânormalâ body chemistry: amino acid production gone awry, depleted levels of certain polypeptide chains or hormones.5"
95,498,0.984,Elements of Robotics,"15.3 Swarm Robotics Based on Physical Interactions In Sect. 7.2 we looked at a typical example of efficient swarm behavior: a colony of ants finding a path from their nest to a source of food. That example used indirect communications in the form of pheromones deposited on the ground. In this section we look at another form of swarm behavior that is mediated by physical interactions. We start with ants collaborating on the task of pulling a stick from the ground and a robotic version of this task. This is followed by a discussion of how forces exerted by several robots can be combined, demonstrated by a simple but clever algorithm called occlusion-based collective pushing."
113,320,0.984,Pentecostalism and Witchcraft : Spiritual Warfare in Africa and Melanesia,"Ahamb people understand a good person to be someone who has love, concern, and sympathy for others. These are the same values that also characterize a good Christian. The antithesis of the good person, however, is one who is selfish and proud. Such a person is typically talked about as âdifferentâ (difren in Bislama) and doubts are continuously raised about such a personâs Christian moral as he or she seems to acknowledge neither God nor kin in his or her life. Christianity is in this sense taking the role of an all-encompassing system on Ahamb, incorporating the main moral framework for social action as well as the cosmological powers of God that are regarded as stronger than any traditional spirit used in sorcery. Christianity thus acts as an umbrella for the good and what is worth aspiring toward, while sorcery, which brings destruction, is the most potent manifestation of the bad and evil. As with other Pentecostal movements, the revival was particularly clear in marking out this distinction, and brought about a more aggressive confrontation against the evil than had been normal. As I have argued, the opposing role of good and evil seem to some extent to be mutually reinforced and reproduced through the existence and cultivation of the other. For Ahamb people, however, this mutual reinforcement has not merely been discursive but is experienced in social practice: the more of the good (especially success), the greater the chance of an envious person (typically a sorcerer) coming to destroy it. The more evil, the more important it is that the church is strong to encompass it. This mutual reinforcement resonates with examples from Africa where Pentecostal Christianity has been argued to take on the logic of witchcraft and make the church into part of the witchcraft problem itself, as well as a solution to the problem (De Boeck 2006: 173; Newell 2007). In the Pentecostal churches of the Ivory Coast, for example, Newell reports on the elaborate belief in witchcraft and a simultaneous assertion of the ability of the church to transcend it. But some Ivoirians claim that the church is also a haven for the witches, as it asks people to detach themselves from the âcorruptingâ influence and obligations of kin and rather focus their attention (and money) on the church (and their leaders). The church is thus promoting activities that resemble the self-centered actions of the witch (Newell 2007: 484; Meyer 1999: 170). Pentecostalism is therefore sometimes equated with witchcraft as it encourages exactly the kind of antisocial behavior that Ivoirians understand as witchcraft. As churches recognize the efficacy and reality"
264,433,0.984,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"â¢ It remains crucial to come to grips with what it means and takes to master mathematics. â¢ Focusing on the enactment of mathematics in a broad sense is seen as essential in more and more places. â¢ Conceptualisation of this enactment needs further theoretical clariï¬cation and empirical investigations. â¢ Understanding the relationships and balances between the enactment of mathematics and other components of mathematical insight and knowledge remains a challenge. â¢ There is a need to clarify the role of attitudinal, volitional and dispositional factors in the conceptualisations and the reality of mathematical competencies. â¢ Terminological issues continue to cause confusion. To what extent are things called by the same nameâe.g. competenciesâactually equivalent? And to what extent do things called by different names actually cover different notions? And to the extent they do, what exactly are the relationships between them? â¢ The lack of a uniï¬ed conceptual and theoretical framework for competencies, proï¬ciency, processes, practices etc. tends to impede the possibilities of overcoming the challenges identiï¬ed. â¢ In summary, a plethora of research and development work will be facing us in the years to come. There is every reason to expect, therefore, that there will be substantial progress to report on in future ICMEs."
191,413,0.984,Collaborating Against Child Abuse : Exploring The Nordic Barnahus Model,"[1974]) concept of power illustrates how power is contextually dependent and takes on different forms. He divides power into three dimensions: one-, two- and three-dimensional power. One-dimensional power relates to concrete action in decision-making on issues where there is an observable conflict of articulated interests, which could be labelled âformal decision-making powerâ. According to Lukes, two-dimensional power also enables analysis of how decisions are prevented on issues where there is a conflict of interests. Two-dimensional power thus refers to the extent to which an actor or professional group (consciously or unconsciously) creates or reinforces barriers to conflicts being articulated in public, and the same actor or group possesses power (Lukes 2005, 20), which could be labelled ânon-decision-making powerâ. According to Lukes, the two-dimensional power view is still too narrow since it does not capture the ways in which latent conflicts are being oppressed (Lukes 2005, 58â59). Lukes therefore includes a three-dimensional power concept to acknowledge elements of power that are not directly visible. According to Lukes, power is at its most effective when least observable, in other words when it operates in disguise or is made invisible (Lukes 2005). In my understanding, invisible power has to do with conflicting institutional interests being neutralised, avoided or completely concealed. In relation to collaboration, as in Barnahus, such a perspective is important. The idea of collaboration exerts a kind of âcognitive powerâ that builds on the idea of consensus, which may conceal underlying conflicts of interests between the organisations involved. Subsequently, the collaborative actors might be unaware of these conflicts, both those exercising power and those upon whom power is exercised. In this way, the third dimension of power addresses âthe power over thoughtâ, that is, the capacity to influence actors ways of thinking in a certain direction, or to dominate or redirect their interests.2 It is about âsetting the agendaâ in a wider cognitive sense by making certain ways of thinking dominant. Three-dimensional power is thus about affecting the preferences of others in a way that makes them accept their role and position in the prevailing institutional order and, in that sense, ensure actorsâ voluntary compliance to dominance (Lukes 2005). As I interpret Lukes, however, the three-dimensional form of power does not replace the other two,"
269,148,0.984,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"In this chapter, we have worked with and through some of the spatiotemporal imaginaries through which researchers might plot as well as practise interdisciplinary research. We have used four terms (which are also, and variously, phenomena, constructs, abstractions, and metaphorical resources) to open up ways of conceptualizing the spatial organization of interdisciplinarity that push beyond those indebted to particular ways of carving territory and terrain, and that centre on producing connection, entanglement, and ingestion in the collaborative sphere. Yet one important problem remains. In the current impetus towards collaboration, there is, we suggest, an implicit normative assumption of connection itself, and indeed of relationality as such. What happens to collaboration when connection is impossible to refuse? What becomes of those ontological and epistemological voids â which is to say, those spaces and temporalities that cannot be produced or even glimpsed within the current dispensation? Paul Harrison has argued, in response to the interest across a number of disciplines in ârelationâ and ârelationalityâ (and here our earlier invocations of matrices and topologies are two such instances), that the question of the non-relational is at threat of occlusion:"
315,156,0.984,Situating Children of Migrants Across Borders and origins : a Methodological Overview,"With only three distinct incorporation paths, the segmented assimilation approach is able to account for the âdiversityâ of trajectories to only a limited degree. Also, each path of incorporation is generally associated with a particular immigrant group.6 This means that the theory does not help to explain why individuals within the same group of origin have divergent trajectories (on this point, see also Santelli 2007, as well as her contribution in this volume, and Tucci et al. 2013). Interestingly, in one of his papers, Rumbaut (2005) employs the central concept in the life course approach of turning point (Elder 1974; Hareven and Masaoka 1988; Abbott 1997). Following Elder, Rumbaut defines turning points as ânew situations that âknife offâ the personâs past from the present and serve as catalysts for long-term behavioral change by restructuring routine activities and life course pathways, enabling identity transformations and setting in motion processes of âcumulating advantages and disadvantagesââ (Rumbaut 2005: 1043). Describing the downward assimilation of one segment of the second generation, Portes and Rumbaut (2006) point later to possible negative effects of segregated neighborhoods and peers engaged in informal activities. Immigrantsâ descendants change in orientations and in their logic of action and shift their life course in other directions: âFor second-generation youths, the activities of gangs, sale of drugs, and other elements of âstreetâ culture amount to an alternative path of adaptation, away from school and homework and in direct opposition to their parentsâ expectationsâ (Portes and Rumbaut 2006: 262). One drawback of segmented assimilation theory is that it does not explicitly conceptualize the possible switch of individuals from one pathway to the other. As pointed out by Bidart, analyzing the life courses of young adults âimplies (â¦) a focus on turning points rather than on continuity, the identification of driving forces, the consideration of subjective as well as objective changesâ (Bidart 2012: 3). In order to do so, methodological approaches that go beyond the analysis of individual outcomes and the use of specific data are needed. In this chapter we advocate for the aggregation of different types of data."
372,1316,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"parts or the conjugate values (but not both) are counted independently. In images made using the fast Fourier transform (FFT) algorithm, there are equal numbers of grid points in the .u; v/ and .l; m/ planes, but not all .u; v/ grid points contain visibility measurements. To maintain the condition for convergence, it is a common procedure to apply CLEAN only within a limited area, or âwindow,â of the original image. In order to clean an image of a given dimension, it is necessary to have a dirty beam pattern of twice the image dimensions so that a point source can be subtracted from any location in the image. However, it is often convenient for the image and beam to be the same size. In that case, only the central quarter of the image can be properly processed. Thus, it is commonly recommended that the image obtained from the initial Fourier transform should have twice the dimensions required for the final image. As mentioned above, the use of such a window also helps to ensure that the number of components removed does not exceed the number of visibility data and, in the absence of noise, allows the residuals within the window area to approach zero. Several arbitrary choices influence the result of the CLEAN process. These include the parameter ! , the window area, and the criterion for termination. Note that a point-source component in the image can be removed in one step of CLEAN only if it is centered on an image cell. This is an important reason for choosing ! ! 1. A value between 0.1 and 0.5 is usually assigned to ! , and it is a matter of general experience that CLEAN responds better to extended structure if the loop gain is in the lower part of this range. The computation time for CLEAN increases rapidly as ! is decreased, because of the increasing number of subtraction cycles required. If the signal-to-noise ratio is Rsn , then the number of cycles required for one point source is "" log Rsn = log.1 "" ! /. Thus, for example, with Rsn D 100 and ! D 0:2, a point source requires 21 cycles. A well-known problem of CLEAN is the generation of spurious structure in the form of spots or ridges as modulation on broad features. A heuristic explanation of this effect is given by Clark (1982). The algorithm locates the maximum in the broad feature and removes a point-source component, as shown in Fig. 11.2. The negative sidelobes of the beam add new maxima, which are selected in subsequent cycles, and thus, there is a tendency for the component subtraction points to be located at intervals equal to the spacing of the first sidelobe of the synthesized (dirty) beam. The resulting image contains a lumpy artifact introduced by CLEAN, but the image is consistent with the measured visibility data. Cornwell (1983) introduced a modification of the CLEAN algorithm that is intended to reduce this unwanted modulation. The original CLEAN algorithm minimizes"
305,116,0.984,Quantum Computing for Everyone,"We do not know the probabilities that should be assigned to the configurations. There are eight possible configurations, so it might seem plausible that they each occur with probability 1/8, but they perhaps are not all equal. Our mathematical analysis will make no assumption about these probabilitiesâ values. We can, however, assign definite probabilities to the measurement directions. Both Bob and Alice are choosing each of their three bases with equal probability, so each of the nine possible pairs of bases occurs with probability 1/9. Notice that each row contains at least five As, telling us that given a pair of qubits with any configuration the probability of getting an A is at least 5/9. Since the probability of getting an A is at least 5/9 for each of the spin configurations, we can deduce that the probability overall must be at least 5/9, no matter what proportion of time we get any one configuration. We have now derived Bellâs result. The quantum theory model tells us that Aliceâs and Bobâs sequences will agree exactly half the time. The classical model tells us that Aliceâs and Bobâs sequences will agree at least 5/9ths of the time. It gives us a test to distinguish between the two theories. Bell published his inequality in 1964. Sadly, this was after the death of both Einstein and Bohr, so neither ever realized that there would be an experimental way of deciding their debate. Actually carrying out the experiment is tricky. John Clauser and Stuart Freedman first performed it in 1972. It showed that the quantum mechanical predictions were correct. The experimenters, however, had to make some assumptions that could not be checked, leaving some chance that the classical view could still be correct. The experiment has since been repeated"
264,196,0.984,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"Conclusion While the presentations during the ï¬rst part of the panel showed that the different perspectives tended to address different questions, the answers presented here to identical questions show consequences of the choice of perspectives in terms of differences in the answers. Most of the time, the answers of the panelists complemented each other. Whatever approach was chosen, there was a consensus acknowledging the complexity of transition phenomena. The transition is not composed of an initial state, a ï¬nal state, and a gap in between that can be spanned by an appropriate bridge. Instead, it is a complex, cumulative path to be managed."
289,143,0.984,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","constructor type arguments turns into access to local function types; conversely, access to existential destructor type arguments in the codata world turns into access to local function type arguments. GADT = GAcoDTT . We can see that the relation between GADTs and GAcoDTs is as promised when looking at Figs. 2 and 3. These two figures show a slightly different representation of the List (co)datatype and associated functions from Fig. 1. In this presentation, we have dropped all keywords from the language, such as function, data and codata. The reason for dropping these keywords is that now function signatures in the data fragment look the same as destructor signatures in the codata fragment, and constructor signatures in the data fragment look the same as function signatures in the codata fragment. Figure 2 organizes the datatype in the form of a matrix: the first row lists the datatype and its constructor signatures, the first column lists the signatures of the functions that pattern-match on the datatype, the inner cells represent the equations for each combination of constructor and function. Figure 3 does the same for the List codatatype: The first row lists the codatatype and its destructor signatures, the first column lists the signatures of functions that copattern-match on the codatatype, the inner cells represent the equations for each combination of function and destructor. We can now see that the relation between GADTs and GAcoDTs is now indeed rather simple: It is just matrix transposition. An essential property of this transformation is that other (co)datatypes and functions are completely unaffected by the transformation. For instance, the Tree datatype (or codatatype, regardless of which version we use) looks the same, regardless of whether we encode List in data or in codata style. Defunctionalization and refunctionalization are still global transformations in that we need to find all functions that pattern-match on a datatype (for refunctionalization) or find all functions that copattern-match on a codatatype (for defunctionalization), but the rest of the program, including all clients of those (co)datatypes and functions, remain the same. Infinite Codata, Termination, Productivity. The semantics of codata is usually defined via greatest fixed point constructions that include the possibility to represent âinfiniteâ structures, such as streams. This is not the focus of this work, but since our examples so far did not feature such âinfiniteâ structures but we do not want to give the impression that our codata types do somehow lack the expressiveness to express streams and the like, hence we show here an example of how to encode a stream of zeros, both in the codata representation (left) and, defunctionalized, in the data representation (right)."
78,415,0.984,The Onlife Manifesto : Being Human in a Hyperconnected Era,"One particular incidence makes up the main background for the argument to be presented below. On July 22 in 2011 Norway faced a tragedy of enormous dimensions. The right winged terrorist Anders Behring Breivik bombed the governmental building in Oslo, the capitol of Norway, and later the same day he cold-bloodedly killed 77 youth of the Norwegian Labour party who attended a summer camp on a small island outside Oslo. In the aftermath it was much debated in the medias how he could possibly be able to carry out this misdeed. We shall leave out all practical aspects here and rather concentrate on one particular moral issue, having to do with toleration. More specifically I shall frame this as a question whether we could possibly tolerate the political opinion upon which this action was based. Breivikâs opinions had been presented in a Manifesto online long before July 22. The author here laid out a conspiracy theory about the threat from inferior races against Arian and European people, and seriously discusses how to solve this problem. Much of his speech is right wing propaganda, presented in a quasi-dialogic form where Breivik interviews himself. Part of the story is his claim that he represents an Heraldic Order lead by himself. Without getting into further details I shall describe this Manifesto as employing fictitious use of reason. One of the issues in the trial was whether there had ever been others but the author himself being a member of this Order. Few believe that there are. For the sake of argument we shall assume that the whole story was fictionalâthere had never been any real public. The main issue is then whether it makes a difference if the public is fictitious, as in Breivikâs case, or rather a real public in the sense of consisting of a certain amount of people. I believe it is not. The most important criterion of the fictitious character is not the amount of participants, but rather the use of reason involved. Thus, the difference between the real and the fictitious is procedurally defined."
65,104,0.984,Handbook of Ocean Wave Energy,"Traditionally, sea-states have been characterised using a representative wave height, which before any method for recording waves existed was based solely on observation. That is, the representative wave height was deï¬ned as the wave height as reported by an âexperienced observerâ, whom we must assume had spent many years listening to the estimates of other experienced observers so that a relatively consistent estimate of the wave height could be made. This was called the Signiï¬cant Wave Height, symbolized by Hs. However, it is clear that the accuracy of this method is highly dependent on the experience of the observer and as such subject to signiï¬cant error. When it became possible to record the variation in the water surface elevation, an alternative method of deï¬ning wave height was developed. With a record of the variation in the water surface elevation, it is possible to measure the height of individual waves and thus produce a more reliable estimate of the wave height. In order to be consistent with historical reports, it was decided that the new records of surface elevation should be analysed so as to produce an estimate equivalent to the Hs. It was found that a good estimate of Hs was given using the average height of the third highest waves. In modern times, the variation in wave surface elevation is typically recorded digitally, which provides the potential for signiï¬cant analysis of the wave record. The most signiï¬cant development in the representation of the sea is the deï¬nition of the sea using a spectrum. To understand the concept of the wave spectrum, it is ï¬rst necessary to accept that the variation in water surface can be represented as the linear super-position of sinusoidal waves of different frequencies, amplitudes, directions and"
297,1529,0.984,The R Book,"Model checking involves the use of plot(model2). As you will see, there is no pattern in the residuals against the ï¬tted values, and the normal plot is reasonably linear. Point no. 4 is highly inï¬uential (it has a large Cookâs distance), but the model is still signiï¬cant with this point omitted. We conclude that the proportion of animals that are males increases signiï¬cantly with increasing density, and that the logistic model is linearized by logarithmic transformation of the explanatory variable (population density). We ï¬nish by drawing the ï¬tted line though the scatterplot: windows(7,7) xv <- seq(0,6,0.01) yv <- predict(model2,list(density=exp(xv)),type=""response"")"
294,299,0.984,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"Application We can run the same simulation as in Figs. 4.16, 4.18, and 4.21, for 40 periods. The 10 last periods are shown in Fig. 4.24. The results look as impressive as those of the Euler-Cromer method. Implementation The stages in the 4th-order Runge-Kutta method can easily be implemented as a modification of the osc_Heun.py code. Alternatively, one can use the osc_odespy.py code by just providing the argument odespy_methods= [odespy.RK4] to the compare function. Derivation The derivation of the 4th-order Runge-Kutta method can be presented in a pedagogical way that brings many fundamental elements of numerical discretization techniques together and that illustrates many aspects of ânumerical thinkingâ when constructing approximate solution methods."
244,192,0.984,"Advancing Human Assessment : The Methodological, Psychological and Policy Contributions of Ets","We have chosen to use the term linking instead of equating when it comes to describing the IRT true-score approach that is in wide use. This linking procedure defines the true-score equating that exists between true scores on Test X and true scores on Test Y, which are perfectly related to each other, as both are monotonic transformations of the same IRT proficiency estimate. Typically, this true-score equating is applied to observed scores as if they were true scores. This application produces an observed-score linking that is not likely to yield equated scores, however, as defined by Lord (1980) or Holland and Dorans (2006); hence our deliberate use of linking instead of equating."
8,291,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Part II addresses properties of hot hadronic gas (HG) matter and the proposal and characterization of the phase transformation between HG and quark-gluon plasma (QGP). The opening Chap. 16 is a long-lost review, appearing for the first time in English. It describes the meaning of limiting (Hagedorn) temperature TH , the Statistical Bootstrap Model (SBM), and its role in the Big Bang and Universe evolution. Chapter 16 can be read by a general science-versed reader. Hagedornâs comprehensive technical 1995 retrospective of the experimental and theoretical developments that compelled introduction of TH and SBM follows in Chap. 17. Chapter 18 is a commentary on Chap. 19, Hagedornâs first unpublished 1964 paper introducing TH and the exponentially growing mass spectrum .m/. Chapter 20 presents the experimental 1968 data for .m/, and Chap. 21 offers a contemporary discussion of this central result. Chapter 22 is Hagedornâs unpublished 1972 guide to SBM literature. Chapter 23 is a 1979 unpublished conference paper which presents SBM in its covariant form, introducing finite sized hadrons, and allowing for finite baryon density characterized by a chemical potential. This work shows the transformation from hadron gas to a collapsed single fireball drop that we call QGP today. This phase transformation is made mathematically more precise in the following Chap. 24. This is Hagedornâs 1981 unpublished resolution of a critisism of Chap. 23 as extended with the concept of the available volume, discussed further in the following Chap. 27. Chapter 25 is Hagedornâs 1984 retrospective about development of the SBM leading on to our work on the phase transition to quarkgluon plasma. Hagedorn explains in plain language and resolves many questions that arise in the study of the material of this book. Noteworthy for Part II are the two paragraphs below Eq. (25.16) which discuss the relation of the phase limit temperature with a limiting temperature. A short quote from Chap. 16 explains this further: Hagedorn draws the parallel between boiling hadronic matter and boiling water: â. . . with increasing temperature, it becomes ever easier for a molecule to free itself from the liquid, and when the temperature approaches the boiling point, it is so easy for them to leave, they all want out and actually escape in a rapid manner. They absorb all the heat made available and leave the molecules still remaining behind no energy to increase their temperature.â Hagedorn places emphasis on the fact that water cannot get hotter but vapor in principle, could. However the 1968 view was: â. . . boiling HG matter can never overcook, because it is the supplied energy itself which materializes and so ensures that more new particles are always being born. Therefore there can never arise the process corresponding to the continued heating the water vapor. . . . TH D 1:8  1012 K is the highest ever possible temperature in a stationary thermodynamic equilibrium.â This position evolved with the development of the nuclear bootstrap model for the gas phase, incorporating a finite hadron volume, see Chap. 23. With the rise of QGP as the new phase of matter, the meaning of TH expands to be the phase transformation condition. The new phase, QGP, can be heatedâquark and gluon tempeature rises without limit, T > TH ."
191,294,0.984,Collaborating Against Child Abuse : Exploring The Nordic Barnahus Model,"Final Reflections The most significant difference between âpractice as usualâ in Norway (DCM) and sequential interviews (SI) is first and foremost a new form of interdisciplinary collaboration that lasts throughout the whole interview process. In one of the group interviews, a Barnahus counsellor says: I define us as a team, and we will do this together when there is sequential interviewing. (â¦) Now we [the Barnahus staff] can contribute with our knowledge, working as a team throughout the whole procedure. Start-up isnât when the actual interview begins, but when it is scheduled."
311,3064,0.984,The Physics of the B Factories,"pectation. There is also no clear prediction for the flavor diagonal observable (g â 2)Î¼ . There are also other extensions of the MFV hypothesis, beside GMFV. At the practical level the GMFV is equivalent to the Next-to-Minimal Flavor Violation (NMFV) hypothesis, even though the original motivations were diï¬erent. NMFV was put forward in (Agashe, Papucci, Perez, and Pirjol, 2005) by demanding that NP contributions only roughly obey the CKM hierarchy, and in particular can have O(1) new weak phases. The consequences of spurions that transform diï¬erently under GF than the SM Yukawa coupling matrices have been worked out by Feldmann and Mannel (2007). The MFV hypothesis has also been extended to the leptonic sector (MLFV) in (Cirigliano and Grinstein, 2006; Cirigliano, Grinstein, Isidori, and Wise, 2005). In MLFV the most sensitive FCNC probe in the leptonic sector is Î¼ â eÎ³, while Ï â Î¼Î³ could be suppressed below the sensitivity of future super flavor factories. 25.2.2.5 MFV SUSY Low energy supersymmetry (SUSY), where the superpartners have â¼TeV scale masses is one of the most popular solutions to the hierarchy problem. Since this model is perturbative one can make reliable predictions. This aids the popularity of SUSY among theorists. Already its minimal incarnation â the Minimal Supersymmetric Standard Model â has the salient features of gauge coupling unification and contains a viable dark matter candidate. The âMinimalâ in the MSSM refers to the field content. Each SM particle obtains only one superpartner, and also the extension of the Higgs sector is minimal. However, the flavor structure need not be minimal. The parameters that describe the supersymmetry breaking, e.g., the squark masses and trilinear couplings can in principle carry very diï¬erent flavor structures from the one seen in the quark sector of the SM. In total there are 124 parameters in the MSSM, much more than the 19 parameters of the SM (Berger and Grossman, 2009; Dimopoulos and Sutter, 1995; Haber, 2001). Of these parameters, 110 are in the flavor sector: 30 masses, 39 real mixing angles and 41 phases. If all of the mixing angles and phases were O(1) this would lead to FCNCs that are orders of magnitude larger than the experimental bounds. The SUSY breaking does have to be non-generic and further assumptions about its structure are required in order to have an acceptable phenomenology. An attractive hypothesis is MFV, which we discussed in general terms in the previous subsections. The flavor breaking is assumed to arise only from the Yukawa interactions (in this case from the superpotential), while the SUSY breaking is flavor blind. This means that the squark masses can be written as mÌ2qL =mÌ2 (a1 1 + b1 YU YUâ  + b2 YD YDâ  b3 YD YDâ  YU YUâ  + b4 YU YUâ  YD YDâ  + Â· Â· Â· ),"
118,357,0.984,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"than that of science, or of technology, or of society is worthy of attention to understand the development of my argument. For example, if nuclear physics is completely successful in understanding a chain reaction, technology such as nuclear engineering could fail in controlling the reaction as in the case of Chernobyl.1 Or if nuclear engineering is almost completely successful in containing radioactive materials within reactors, social decision-making could fail as in the case of Three Mile Island (TMI).2 Or if society is completely successful in setting goals for the development of renewable energy technologies, science and/or technology could fail as in the case of Ocean Thermal Energy Conversion (OTEC).3 In a word, the success or failure of science, of technology, and of society cannot be overlapped automatically [9, 10]. In particular, there seems to be something missing in-between, which has unique characteristics of its own. The concept of âstructural disasterâ is intended to explore this state. What is in-between could be institutional arrangements, organizational routines, tacit interpretations of a formal code of ethics, invisible customs, or the networks of interests of different organizations. The âstructural disasterâ consists of one or more of the following elements [11]: 1. Adherence to erroneous precedents causes problems to be carried over and reproduced. 2. The complexity of a system under consideration and the interdependence of its units aggravate problems. 3. The invisible norms of informal groups essentially hollow out formal norms. 4. Quick fixes for problems at hand lead to further such fixes for temporary counter measures. 5. Secrecy develops across different sectors and blurs the locus of agents responsible for the problems to be addressed. This chapter focuses on, among other things, the interdependence of heterogeneous agents, which come into play in the science-technology-society interface and give rise to secrecy in a specific social condition. This chapter will make clear the interdependence by tracing it back to the hidden prewar accident, which will give us an important clue to the understanding of the Fukushima Daiichi accident from the perspective of âstructural disasterâ as defined above. To understand the social context of this hidden prewar accident, it is necessary to move away from the current social condition of the post-Fukushima situation to the prewar wartime mobilization of science and technology, within which the clarification of this hidden accident can be properly pursued. After the clarification, we will move back to the current situation surrounding the Fukushima Daiichi accident, to present the sociological implications of the hidden accident for the Fukushima Daiichi accident and for potential future extreme events. 1 For a sociological investigation into the relationships between the Chernobyl and Wind scale"
269,56,0.984,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"Felicity and a scientific collaborator are giving a joint presentation at a conference on social consequences that arise from the neurosciences. They have a carefully choreographed set of slides, which demand them passing the ball â as it were â a number of times, one to the other, so as to ensure that each presents different kinds of empirical and conceptual data. Each ventriloquizes some of the scientific figures who feature in their analysis of the emergence of the field of resting state fMRI; at times, one completes the slide that the other has started out presenting. Itâs perhaps difficult, if you didnât know either of them, to discern who is the social scientist, and who the neuroscientist. Then itâs time for questions. Someone in the front raises her hand, looks from one to the other with a somewhat baffled expression, and says, âWhich way does it go between you two?â She looks at Felicity. âI mean, do you study him?â She looks at Felicityâs collaborator. âOr ... ?â"
28,182,0.984,A History of Self-Harm in Britain,"Poisoning, overdosing and drugs: local and national concerns Kessel does not totally close off other behaviours possibly covered by attempted suicide (self-cutters or throat- or wrist-slashers, for example) but his terminology is exclusionary, even if those so identified are still treated at the ward.24 Awareness of the phenomenon of self-poisoning with drugs increases during the 1940s and 1950s. According to one Edinburgh toxicologist: âThe first resuscitation centre dedicated to poisoned patientsâ opened in Copenhagen in 1949. In England, the North-East Metropolitan Regional Barbiturate Unit was set up in Romford in the 1950s.25 Comments made in the late 1950s by the head of the Romford unit indicate that certain forms of poisoning have affinity (in the eyes of some clinicians) with suicidal gestures: âbarbiturate poisoning is notorious in that it is not a particularly lethal variety of poisoning[;] [it] is important because of its frequency and not because it is highly lethalâ.26 He does not comment further on the consequences of toxicological assessments of lethality for psychological assessments of intent. However, Stengel and Cook make a connection with poisoning in general, arguing that â[c]learly, the degree of danger to life is not a reliable measure of seriousness of intent, especially with poisoning, i.e. in the majority of suicidal actsâ.27 Thus, ambiguity of method is transposed onto ambiguity of intent, giving this method increased visibility. However, there is nothing inherently ambiguous about this method; such a claim falls into technological determinism. The explicit, conscious nature of the appeal in Kesselâs self-poisoning overrides any ambiguity"
311,994,0.984,The Physics of the B Factories,"In the case of time-dependent Dalitz plot analyses, the resonance parameterizations above are combined with the equation describing the time-dependent decay properties of the B and B meson as given in Equation 13.2.17. In this case, a great deal of attention has to be given to the tagging and resolution functions. Charmless B decays, especially those without access to tree decay diagrams, may have a large non-resonant contribution. This can be as high as 90% for B â KKK. The contribution is not uniform across the Dalitz diagram and so a parameterization must be adopted that depends on position in the Dalitz Plot. In some analyses, BABAR and Belle have adopted the same non-resonant parameterization but in most cases they diï¬er, which can complicate comparisons. The statistical errors on the measured fit fractions and CP parameters are often derived from fits to a large number of MC experiments generated with the fitted parameters obtained from the data. These MC experiments are also vital for understanding the minimization process. With a large number of floating parameters, the fit can sometimes have more than one local minimum. There can be systematic shifts in the fit caused by the starting values of the floating parameters. A number of techniques for investigating this eï¬ect have been applied, including using diï¬erent minimizers, scanning through a set of starting values, randomly initializing the starting values, and the use of genetic algorithms. Each has its benefits and drawbacks but there is no one method that works better than the others in all circumstances. The systematic uncertainties that aï¬ect the final result are very similar to those seen in other charmless B decays. However their eï¬ects can be modified since there are more opportunities for correlations between parameters and the fitted results are often reported as ratios rather than absolute numbers. Although the magnitude and phase of the complex coeï¬cients of the amplitude are sometimes transformed to a more orthogonal set of parameters, this"
281,227,0.984,Stochastics of Environmental and Financial Economics (Volume 138.0),"Let us explain some properties of the proposed numerical scheme. First of all, there are two approximation errors, where the first one (N scale) is coming from the Fourier transformation at (6) and the second one (M scale) is coming from the discretization error obtained at (10). It is important to understand the meaning of knowing the information about the involved processes up to time t0 . When the stochastic model for the process (Ït )tâR is uncoupled with (X t )tâR , then we may use u = t â Ï at (8). Indeed, in typical applications such as turbulence and finance this is the case: (Ït )tâR is usually modeled via a jump diffusion process driven by a LÃ©vy process, which might be correlated with the Brownian motion W . However, when the process (X t )tâR is itself of a diffusion type, i.e. Xt = Î¼ +"
257,287,0.984,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","the follower. In general, knowledge of the leaderâs action may help the follower choose a more advantageous strategy. Phase 2: The attacker observes the output of the selected channel Cda and performs his attack on the secret. In case he knows the defenderâs action, he is able to determine the exact channel Cda being used (since, of course, the attacker knows his own action), and his payoff will be the posterior vulnerability V [Ï, Cda ]. However, if the attacker does not know exactly which channel has been used, then his payoff will be smaller. Note that the issues raised in Phase 2 are typical of leakage games; they do not have a correspondence (to the best of our knowledge) in traditional game theory. On the other hand, these issues are central to security, as they reflect the principle of preventing the attacker from inferring the secret by obfuscating the link between secret and observables. Following the above discussion, we consider various possible scenarios for games, along two lines of classification. First, there are three possible orders for the two playersâ actions. Simultaneous: The players choose (draw) their actions in parallel, each without knowing the choice of the other. Sequential, defender-first: The defender draws an action, and commits to it, before the attacker does. Sequential, attacker-first: The attacker draws an action, and commits to it, before the defender does. Note that these sequential games may present imperfect information (i.e., the follower may not know the leaderâs action). Second, the visibility of the defenderâs action during the attack may vary: Visible choice: The attacker knows the defenderâs action when he observes the output of the channel, and therefore he knows which channel is being used. Visible choice is modeled by the operator . Hidden choice: The attacker does not know the defenderâs action when he observes the output of the channel, and therefore in general he does not exactly know which channel is used (although in some special cases he may infer it from the output). Hidden choice is modeled by the operator â¨. Note that the distinction between sequential and simultaneous games is orthogonal to that between visible and hidden choice. Sequential and simultaneous games model whether or not, respectively, the followerâs choice can be affected by knowledge of the leaderâs action. This dichotomy captures how knowledge about the other playerâs actions can help a player choose his own action. On the other hand, visible and hidden choice capture whether or not, respectively, the attacker is able to fully determine the channel representing the system, once defender and attackerâs actions have already been fixed. This dichotomy reflects the different amounts of information leaked by the system as viewed by the adversary. For instance, in a simultaneous game neither player can choose his action based on the choice of the"
124,197,0.984,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"relation in terms of this interpretation. Unless we were engaging in a purely technical investigation, we should have some story to tell about what makes one Belnapian world relevant to anotherâsome account of what one world would have to be like in order to be relevant to another.12 However, given that (as we are now assuming) each point of evaluation will occur in only one world, we could consider a relevance relation directly between points of evaluation. In the closest analogy to standard models, instead of having world w relevant to world wâ² , we could have all wâs points of evaluation relevant to each of wâ² âs. Then â¦p would be true at a given point of evaluation m/ h iff p is true at each point m â² / h â² relevant to m/ h. Once we contemplate such point-to-point relevance, however, we should at least consider the possibility that relevance could be more selectively defined, so that perhaps only some of wâs points of evaluation would be relevant to ones in wâ² , and perhaps only to selected points of evaluation in wâ² . This would call for rethinking the relevance relation, to provide an interpretation which could reasonably be understood to be so selective. Depending on what that interpretation might be, we might also contemplate the possibility that the relevance relation could hold between selected pairs of points of evaluation within the same Belnapian world. In principle, these relaxations of the relevance relation open up a whole new dimension of potential sensitivity for systems based on such supermodelsâa dimension surely worthy of at least preliminary technical exploration. We shall not explore it further here, however. Looking in a different direction: instead of seeking to pursue the analogy with standard models, we could consider pursuing an analogy with neighborhood models based on possible worlds. In classical models the relevance relation relates a world to relevant neighborhoods, i.e. sets, of worlds. One common rationale for doing so is to take advantage of the fact that in possible worlds semantics, any given proposition will naturally be associated with a uniquely determined set of worlds: the worlds at which the proposition is true. The neighborhood is then used to represent the comprehensive proposition which captures all that is true throughout the neighborhood but which is false at all other worlds. Other interpretations similarly associate sets of worlds with events, or with actions. In each such case, worlds are gathered into neighborhoods in their capacity as points of evaluation, and so the apt analog for our supermodels would be neighborhoods made up of moment/history pairs, rather than of worlds. The default view would be that the neighborhoods could, and typically would, include points of evaluation from different worlds: the proposition that p, for example, would be represented in the supermodel by the set of all points m/ h at which p was true.13 12 One classic illustration is the specification, in standard deontic logic, that world wâ² is to be"
147,124,0.984,Trailblazing in Entrepreneurship : Creating New Paths For Understanding The Field,"tion in the form of a new product, service, or process presents a potential opportunity. That is, by engaging in activities, the individual comes across new information; an anomaly reflects the acquisition of new information (of a problem), which then triggers efforts to analyze, interpret, and understand the nature of the anomaly (as it does for scholars) (Shepherd & Sutcliffe, 2011). After beginning to understand the anomaly, the individual is able to generate conjectures of a potential opportunity (i.e., potential solutions to the anomaly) (Shepherd, McMullen, & Ocasio, 2016). The individual forms these conjectures of potential opportunities in his or her mind by combining elements of knowledge as a potential solution; undertaking bisociation, with different perspectives offering new insights; and/or otherwise âtapping intoâ and challenging existing knowledge. The ability to do this is consistent with âassimilationâ and âtransformationâ and is in line with an entrepreneurial mindset (GrÃ©goire, Barr, & Shepherd, 2010; McGrath & MacMillan, 2000) that informs entrepreneurial action (Baker & Nelson, 2005; Smith & DeGregorio, 2002). The individual can then test the opportunity conjecture in the worldâthat is, communicate the potential opportunity to a community of inquiry. Although such action is likely to represent probes into an uncertain future (Brown & Eisenhardt, 1997; McGrath, 1999), it can be considered an application of the new knowledge (in the form of a potential opportunity) that has been created and internalized (Lyles & Schwenk, 1992) and therefore represents a form of exploitation (Zahra & George, 2002). Of course, the process does not stop there. When the entrepreneur releases the potential opportunity from his or her mind into the world, there is an interaction between the community of inquiry and the potential opportunity. This interaction provides new information that reflects how the potential opportunity can be refined, and/or the community of inquiry acts in a way that changes the nature of the potential opportunity. Either way, the community of inquiry, through its interaction with the potential opportunity, generates new information. To the extent that the entrepreneur (i.e., the mind) is able to absorb this new informationâ namely, acquire, assimilate, transform, and exploit itâhe or she can make additional refinements to the potential opportunity. Therefore, there is a mutual adjustment between the mind and the world through a potential opportunity (Shepherd, 2015; Chap. 2). The extent of this mutual adjustment is likely to be higher for those who have high ACAP. However, given the iterative process of generating and refining potential opportunities"
147,126,0.984,Trailblazing in Entrepreneurship : Creating New Paths For Understanding The Field,"ACAP is a dynamic capabilityâit is a capability that facilitates change in an organizationâs routines, systems, and processes. However, at least in the mutual adjustment process of developing and refining a potential opportunity, ACAP itself will likely change, making the dynamic capability itself dynamic. It is important to think about the mechanisms of this dynamism. This requires greater theorizing about the form of each capability âhousedâ in the firm. Specifically, what are the routines of a new information-acquisition capability, assimilation capability, transformation capability, and exploitation capability? Given such an understanding, we are in a position to explore how these capabilities change as part of the mutual adjustment process of potential opportunity refinement. These changes may be reflected in one or many of the capabilities and/or in the routines that connect these capabilities. The entrepreneurial process is a particularly appropriate context to explore these ideas because the potential opportunity is not the sole property of the mind as it changes through social interaction (Shepherd, 2015; see also Chap. 2). While the opportunity, as well as the mind, changes as a result of social interaction, so too does the community of inquiryâit is transformed. This transformation of the community of inquiry (over and above change to the potential opportunity) is also information that, if absorbed, can be useful to the entrepreneurial actor. For example, perhaps two communities arise from one as the potential opportunity changes. Recognizing this transformation of the community of inquiry into two could lead to two different versions of the potential opportunity (different forms of refinement) now representing two potential opportunitiesâone for each community (e.g., customer target segments). Those with superior ACAP are likely to be in a better position to notice and act upon the new knowledge stemming from the bifurcation of the community and of the opportunity. From the individual to the firm level of analysis. Maintaining a finer-grained treatment of ACAPâconsidering each of its four dimensions independentlyâwill likely provide greater scope for understanding how a firm generates and refines an opportunity through its interaction with a community (or communities) of inquiry. When investigating the role of ACAP in the generation and refinement of a potential opportunity in the context of an established firm, we need to ask several questions. How does a firm acquire information that generates an anomaly? How does the firm assimilate new knowledge with existing knowledge to understand the problem and then transform prior knowledge of the problem to generate a potential opportunity that is then exploited through interactions with a"
214,215,0.984,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"We have discussed the atmosphere, the ocean, and the sea ice that floats on top of the ocean. The remaining major component of the earth system is probably the closest to you right now: the surface of the earth (unless you are reading this on a boat or a plane). The earthâs surface is certainly closest to home. We can think of all the components of the climate on the (permanent) solid surface of the earth as the terrestrial system. Although this is commonly thought of as just modeling the land surface, it also includes two other important components: the cryosphere (ice and snow) that sits on land and the anthroposphere (the role of humans) in the climate system. We also discuss how human systems are simulated in general, and in climate models. Since all these interactions occur on the surface of the earth, the most useful way to discuss them is by looking at the land, cryosphere, and humans as parts of the terrestrial system.1 Here we review the role of terrestrial systems in climate and discuss how they are simulated."
249,68,0.984,Advances in Proof-Theoretic Semantics (Volume 43.0),"Although we will see below that the decidability of the proof relation has occasionally been disputed, these passages make clear why it has traditionally been thought to play a crucial role in ensuring that the BHK clauses are compatible with the general goal of explaining how truth can be understood in terms of constructive provability. To see how this is related to GÃ¶delâs second and third points about how the class of constructive proofs may be characterized, note that if we assume that the proof relation itself is decidable, then the clauses (Pâ ), (PÂ¬ ), and (Pâ ) are all analogous in form to Î 10 statements in the language of arithmeticâi.e. they begin with an unrestricted"
372,1208,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"10.2.6 Wide-Field Imaging To take full advantage of large new instruments with wide bandwidths, high sensitivity, and full polarization responses, it is necessary to measure the radio sky down to the level of the background radiation from the Epoch of Reionization (EoR) and to be able to separate out components from individual radio sources that overlie the background. The width of the synthesized field may be much greater than a few degrees, so the image is no longer the Fourier transform of the visibility function. The basic requirement for such an analysis is an equation for the visibility values that would be measured for a given brightness distribution, taking account of all details of the locations and characteristics of the individual antennas, the path of the incoming radiation through the Earthâs atmosphere including the ionosphere, the atmospheric transmission, etc. This is the interferometer measurement equation introduced in Sect. 4.8. In its basic form, it describes the response of a single pair of antennas and is thus applicable to any specified system of antennas and any brightness distribution, to provide values of the visibility for each antenna pair. It includes direction-dependent effects such as the primary beam patterns of the antennas, polarization effects that vary with the alignment of the polarization of the source relative to that of the antennas, and the baselines of the antenna pairs. These must be accounted for without small-field or other approximations. Direction-independent effects such as large-scale propagation in the atmosphere and the ionosphere, and the response of the receiving system, can also be included. The reverse operation, i.e., the calculation of the optimum estimate of the image from the measured visibility values, is less simple. Taking the Fourier transform of the observed visibility function usually produces a brightness function with physically distorted features such as negative brightness values in some places. However, starting with a simple but physically realistic model for the brightness, the measurement equation can accurately provide the corresponding visibility values that would be observed. By comparing these with the observed values, it is possible to adjust the brightness model toward the observed distribution and, by iterative repetition of this process, to arrive at an image that agrees with the visibility"
269,205,0.984,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"This book disturbs the tendency, in writings about interdisciplinarity, to cleave to the terrain of ideas, disagreements, and knowledge claims â and, in so doing, to disavow the complex ripples, wrinkles, and patternings of affect that course through what is all-too-often assumed to be an unruffled plane of interdisciplinary interaction. We pay attention, here, to how those wrinkles get rubbed out, how they are sometimes smoothed over, and how, sometimes, they endure within and between the unsuspecting bodies of interdisciplinary researchers themselves. We have tried in this chapter to bring to visibility some of the political, methodological, ontological, and epistemological work that those emotions perform. We hope to have shown: (1) how emotion can be influential in carving out the perimeters of an interdisciplinary space (as well as determining who is inside and outside of it); (2) how researchers can engage in various tactics to ensure that collaboration does not entirely fall apart; (3) how political and ontological differences can be experienced affectively (and vice versa); (4) how the eruption of unexpected â and superficially unimportant â moments of affect can be diagnostic of important lines of conjunction and contestation within interdisciplinary spaces; and (5) how acknowledgement of affective bewilderment while in interdisciplinary spaces is easily misconstrued as either a deliberate or unwitting removal from the terrain of the political. We worry that our inclusion of instances of mostly negatively valenced emotion has been done, at least in part, through some desire to seek sympathy, to settle scores, or to parade, in the interests of exhibitionism, shameful episodes of our own making. But we also want to insist that to decide not to attend to the movements and textures of, and manifold responses (whether regulatory or dysregulatory) to such emotion in interdisciplinary spaces, is to miss much of what is happening in them. If the would-be collaborator hears a strong note of caution â even"
188,143,0.984,Responsive Open Learning Environments : Outcomes of Research From The Role Project,"General Discussion: Qualitative Versus Quantitative In the foregoing sections we present an array of quantitative and qualitative methods for data collection and analysis. The selection of a particular type of method depends on individual researchersâ assumptions, values, and expertise. Some researchers defy the value of quantitative data with the argument that numbers cannot tell us anything, insisting on capturing solely qualitative data. Any method fundamentalism is wrong, not least in the light of a postulate for a wide repertoire of research skills among researchers. Still such standpoint is often found in practice, particularly by those critics instigating methodological discussions with the aim to dismantle or even discredit a particular piece of quantitative work they do not agree with. It is in our opinion, however, not that simple: Methods cannot be differentiated into good and bad, and if a particular method fails to provide results (or even more often: results beyond tautologies), then this probably says more about their competent handling, rather than their validity or reliability. Exceptions prove the rule, of course. In our view, there are two aspects to consider that influence methodological choices. First, it all depends on why the evaluation is needed, what the goal of the evaluation is, and who the recipient of the evaluation data is. For example, if the target is to feed back into psycho-pedagogical or technological development, qualitative means can provide deeper insights on what has gone wrong, what works, and what leaves room for improvement. Moreover, qualitative methods bear the potential to discover, why this is the case."
287,27,0.984,Theories in and of Mathematics Education,"The central concept of activity has been described as âthe speciï¬cally human form of activity, of interaction with the world in which man changes it and himself at the same timeâ (Giest and Lompscher 2006, p. 27, translated). Activity takes place through the conscious influence of a subject on an object in order to shape the latter in accordance with the motive of the activity. To this end, such actions (material or spiritual) are performed within one activity line that each time realises certain sub-goals through to the ultimate product of the activity. At the same time, the concept of operation serves to further distinguish another form of subordinate activity that differs from actions by the fact that operations result from concrete conditions for action and pass in an automated manner without conscious control or goal formation. These represent shortened actions. In the course of their lives, humans, in their confrontations with the world, develop various forms of activity, such as play, work, or learning activities that feature different characteristics in each case. For schools and for didactic research, the concept of learning activity has been of key importance. There, learning activity has been understood âas the activity aimed speciï¬cally at acquiring social knowledge and competence (learning topics) for which purpose speciï¬c means (learning resources) under specially arranged conditions have to be adopted.â (Giest and Lompscher 2006, p. 67, translated). According to Lompscher (1985), three essential subjective requirements must be met on the part of the learners to achieve a learning activity:"
281,61,0.984,Stochastics of Environmental and Financial Economics (Volume 138.0),"precisely some details. For example, in [8] a slightly more restrictive definition of strong-viscosity solution was adopted, see Remark 12. Recently, a new branch of stochastic calculus has appeared, known as functional ItÃ´ calculus, which results to be an extension of classical ItÃ´ calculus to functionals depending on the entire path of a stochastic process and not only on its current value, see Dupire [17], Cont and FourniÃ© [5â7]. Independently, Di Girolami and Russo, and more recently Fabbri, Di Girolami, and Russo, have introduced a stochastic calculus via regularizations for processes taking values in a separable Banach space B (see [12â16]), including the case B = C([âT, 0]), which concerns the applications to the path-dependent calculus. In the first part of the present paper, we follow [9] and revisit functional ItÃ´ calculus by means of stochastic calculus via regularization. We recall that Cont and FourniÃ© [5â7] developed functional ItÃ´ calculus and derived a functional ItÃ´âs formula using discretization techniques of FÃ¶llmer [23] type, instead of regularization techniques, which in our opinion, better fit to the notion of derivative. Let us illustrate another difference with respect to [5]. One of the main issues of functional ItÃ´ calculus is the definition of the functional (or pathwise) derivatives, i.e., the horizontal derivative (calling in only the past values of the trajectory) and the vertical derivative (calling in only the present value of the trajectory). In [5], it is essential to consider functionals defined on the space of cÃ dlÃ g trajectories, since the definition of functional derivatives necessitates of discontinuous paths. Therefore, if a functional is defined only on the space of continuous trajectories (because, e.g., it depends on the paths of a continuous process as Brownian motion), we have to extend it anyway to the space of cÃ dlÃ g trajectories, even though, in general, there is no unique way to extend it. In contrast to this approach, we introduce a new space larger than the space of continuous trajectories C([âT, 0]), denoted by C ([âT, 0]), which allows us to define functional derivatives. C ([âT, 0]) is the space of bounded trajectories on [âT, 0], continuous on [âT, 0[ and with possibly a jump at 0. We endow C ([âT, 0]) with a topology such that C([âT, 0]) is dense in C ([âT, 0]) with respect to this topology. Therefore, any functional U : [0, T ] Ã C([âT, 0]) â R, continuous with respect to the topology of C ([âT, 0]), admits a unique extension to C ([âT, 0]), denoted u : [0, T ] Ã C ([âT, 0]) â R. We present some significant functionals for which a continuous extension exists. Then, we develop the functional ItÃ´ calculus for u : [0, T ] Ã C ([âT, 0]) â R. Notice that we use a slightly different notation compared with [5]. In particular, in place of a map U : [0, T ] Ã C([âT, 0]) â R, in [5] a family of maps F = (Ft )tâ[0,T ] , with Ft : C([0, t]) â R, is considered. However, we can always move from one formulation to the other. Indeed, given F = (Ft )tâ[0,T ] , where each Ft : C([0, t]) â R, we can define U : [0, T ] Ã C([âT, 0]) â R as follows: U (t, Î·) := Ft (Î·(Â· + T )|[0,t] ),"
21,132,0.984,intertwingled : The Work and influence of Ted Nelson,"In real lifeâthat is to say on paperâPriestleyâs Chart of Biography is large, about three feet long and two feet tall. The bottom edge is a timeline running from 1200 BC to 1800 AD, measured in regular intervals. The chart contains six big horizontal bands, each devoted to a general category of achievement [6]. The categories themselves are a fascinating artifact of their time, and a good reminder that, as many affinities as we may find between our world and the world of the eighteenth century, these are different times. In the top band of the chart, we find the Historians, Antiquaries, and Lawyers; below them are the Orators and Critics; then come the Artists and Poets; the Mathematicians and Physicians; the Divines and Metaphysicians; and finally at the very bottom, the Statesmen and Warriors. The interior area of Priestleyâs chart is filled to varying densities with about 2,000 solid black horizontal lines that begin and end at the dates for the birth and death of the figures depicted in the diagram (Fig. 13.7). Priestleyâs system is another sort of hypertext. And his discussion of its hypertextual features is explicit. Each of the life lines on Priestleyâs chart refers to a particular person, as indicated by a name above it. But, given his druthers, Priestley would have hidden the names. A rollover feature might have worked very nicely. But with the technology of print, Priestley saw no other practical solution than to put the names on the chart in a very tiny font. As Priestley recognized, the distribution of names into categories was based on subjective judgment. Priestleyâs own biography was a case in point. A great figure in several fields, he could easily have been placed among the scientists or the theologians of his time. Still, Priestley ventured that the patterns visible on the chart revealed real historical phenomena, among which he highlighted two. And these will bring us back to our main argument and to Ted Nelson. First, Priestley notes a difference between patterns in fields for the history of art and science compared with those for the history of politics and war. We see this for example in the contrast between the range devoted to the Mathematicians and Physicians (in other words, the scientists) and that devoted to the Statesmen and Warriors. From the changing densities of achievement discovered in the former, Priestley is able to spin out a story of the Classical, Medieval, and Renaissance periods. From the latter, nothing. In the realm of politics and war, from the beginning to the end of the historical record, Priestley finds abundance everywhere and no meaningful, patterned change at all. Here is how Priestley puts it, in a passage that I think it still resonates today: By the several void spaces between . . . groups of great men, we have a clear idea of the great revolutions of all kinds of science, from the very origin of it; so that the thin and void places in the chart are, in fact, no less instructive than the most crowded, in giving us an idea of the great interruptions of science, and the intervals at which it hath flourished."
297,1165,0.984,The R Book,"This is highly signiï¬cant (p < 0.0001), so we conclude that there is a non-linear relationship between response and area. Let us get a visual comparison of the two models: windows(7,7) plot(area,response,pch=21,col=""green"",bg=""orange"")"
139,273,0.984,Programming for Computations - MATLAB/Octave (Volume 14.0),"The reader should notice our careful use of words in the previous paragraphs. We started out with modeling a very specific case, namely the spreading of a flu among pupils and staff at a boarding school. With purpose we exchanged words like pupils and flu with more neutral and general words like individuals and disease, respectively. Phrased equivalently, we raised the abstraction level by moving from a specific case (flu at a boarding school) to a more general case (disease in a closed society). Very often, when developing mathematical models, we start with a specific example and see, through the modeling, that what is going on of essence in this example also will take place in many similar problem settings. We try to incorporate this generalization in the model so that the model has a much wider application area than what we aimed at in the beginning. This is the very power of mathematical modeling: by solving one specific case we have often developed more generic tools that can readily be applied to solve seemingly different problems. The next sections will give substance to this assertion."
78,153,0.984,The Onlife Manifesto : Being Human in a Hyperconnected Era,"Natanson points out that these efforts at description are difficult to undertake, difficult to articulate, and difficult to take up in part because of three centuries of Cartesian philosophy that, contrary to phenomenological approaches, insists that âman can be understood in qualitatively the same terms as all other objects and events in the natural order. (1970, p. 4). This is to say: phenomenology resolutely resists the subordination of human beings, our experiences, and our self-understandings to the early modernist polarities of âsubjectiveâ vs. âobjectiveâ knowledge. Rather, phenomenology shares with existentialism the insistence on the epistemological legitimacy of first-person experience, contra its denigration as âmere subjectivityâ in early modernity. This phenomenological refutation of Cartesian mind-body dualism is further elaborated in the work of Merleau-Ponty, which inspired, for example, the neologism developed by the German philosopher Barbara Becker, LeibsubjektââBodysubjectâ (2001). More recently, Susan Stuart has likewise built on the work of Merleau-Ponty (and others) in her conjunction of enactivism with phenomenology. Enactivism foregrounds how ââ¦ through a sensori-affective, felt dynamics, we build up non-conscious intentional expectations about how our world will continue to beâ (2008, p. 256). Stuart sees this view of embodied cognition as directly meshing with phenomenological accounts of our experiencing the world as embodied knowersand-agents. Specifically, the embodied agent portrayed in enactivism is âessentially"
8,295,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","with these tricks we can only go so far, because in order to express how small an elementary particle is and how large the currently observable part of the Universe is, we must use numbers that are again beyond our direct comprehension. There are as many protons in a centimeter as there are for example centimeters in the diameter of Earthâs solar orbit, and as another example consider the many Earth orbitâs diameters needed to reach from here to the furthest visible spiral nebulaâ that is to say, somewhere between 1013 and 1014 . Who can comprehend the number 1013 ? With an effort I can have a feeling for one million, 106 : a million teaspoons of water is about one cubic meter. But even 109 âa billionâis difficult. Do you want to be a billionaire? Put aside a Swiss Franc every second for 32 yearsâthen youâll be one. One million years yields 3  1013 s. String protons together, one each secondâin a million years youâll have a chain barely 3 cm long; string together centimeter-sized pearls, one each second, and in a million years the chain will reach from here to the sun. Lay together an Earth orbit every second, and after a couple million years you will reach the furthest visible spiral nebula (or to be precise, where that spiral nebula was a couple billion years ago, when its light started in our direction). And a last example, which we all know: on a distant island is a diamond mountain, and every hundred years a bird sharpens its beak on the mountain. When the mountain has been whetted away, the first moment of eternity will be finished. Mont Blanc would be whetted away after 1040 s (the Milky Way is only 1017 s old!) and for just as long must one lie proton next to protonâone each secondâto reach the furthest spiral nebula. After this attempt, to make the incomprehensible more comprehensible, I propose my assertion: In order to explore the enormously gigantic (1014 diameters of the earthâs orbit), we must apply our knowledge of the extremely small (10 13 cm). In large things the Universe follows the laws of macrophysics: mechanics, electrodynamics, thermodynamics, relativity and hydrodynamics. For most part we encounter conditions that differ vastly from those surrounding us. They are more akin to those present in a nuclear experiment carried out at a cosmic scale. How can the inner structure of matterâthe extremely smallâbe the building principle of the Universe, determining for the large part the emergence of galaxies and stars and the course of their lives? All this originates and depends nevertheless on these so unusual circumstances to which matter is subjectedâor perhaps one should say, conventional conditions, a statement allowing for the fact that the conditions under which we live are extraordinary. Under these circumstances one can anticipate that each new step in understanding the extremely small develops new relationships in the extremely large and leads us further on the way, which we hope, succeeds in bringing us to a new theory capable to explain simultaneously the functioning of the Universe in both the very large and the very small. The most recent step into the very small began a few years ago, and it leads today to few if any consequences for our conceptual understanding of the Universe; I believe, however, that these will come soon. With the last step I am referring to the"
118,360,0.984,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"we might be able to properly understand what âstructural disasterâ implies (see Table 10.1). There are two reasons for paying attention to the technological trajectory to understand the Fukushima Daiichi accident as âstructural disaster.â First, every reactor there had a long history of successful operation extending over 30 years since its start in the 1970s, which forces our attention to turn to the possibility of a more âstructuralâ cause of the accident beyond picking up individual ad hoc troubles and errors. Second, as the ratios of domestic production indicate, the reactors at the Fukushima Daiichi power station embody the turning point leading from licensed production to self-reliant production. For these reasons, there could exist common characteristics throughout the reactors in question at the Fukushima Daiichi power station and it is possible that such characteristics are somehow related to the âstructural disasterâ of the science-technology-society interface as manifested in the accident. In a word, the causes of âstructural disasterâ can be divided into two different categories, organizational errors and technological trajectory, as the first step to explaining the Fukushima Daiichi accident.4 If we can substantiate these two elements in understanding other independent cases as âstructural disaster,â then we will be able to have a stronger position to learn lessons from the Fukushima Daiichi accident as a âstructural disasterâ and to extend their implications for potential future extreme events. What follows is an independent substantiation of these two elements by examining the hidden accident happened long before the Fukushima Daiichi accident with a focus on a complex relationship between success and failure in the science-technology-society interface and secrecy in the interface. The hidden accident long before the Fukushima Daiichi one is a very perplexing accident of the naval turbine developed by the Imperial Japanese Navy, which occurred immediately before the outbreak of WWII. This accident enables us to redefine the complex relationship between success and failure in the science-technology-society interface both in peacetime and wartime. The accident was treated as top secret because of its timing. The suppression of information about the accident means that it has not been seriously considered as an event in the sociology of science and technology up to now. However, the description and 4 On organizational errors in the context of technological failures, see [14â17] regarding the"
46,29,0.984,A Philosophical Examination of Social Justice and Child Poverty,"valuable achievements that must be considered for evaluating a personâs situation comprehensively. If not, it might get overlooked that a good choice with respect to one domain was â all things considered â a tough or even tragic one. Since capabilities are a kind of freedom, it also becomes clear that the approach gives a high value to peopleâs agency, which is, according to Sen, understood as the faculty to act and bring about change according to oneâs values and objectives (Sen 1999b, 19). In the end, people should be able to identify with their choices and actively shape their own lives; it is therefore decisive for a just society to provide the conditions to make this, in fact, possible. It is crucial to understand that the notion of well-being as it is used in the capability approach must not be identified with what is typically termed âwelfareâ in political philosophy or economics, where the term is understood exclusively in relation to individual preferences or happiness. As shown, this position was powerfully rejected by Sen. Or to put it differently, the notions of well-being, on the one hand, and functionings and capabilities, on the other, are closely related, and there is by now a vast literature confirming this diagnosis (Comim, Qizilbash and Alkire 2008; Deneulin and Shahani 2009; Biggeri, Ballet and Comim 2011). Welfare, on the other hand, in Senâs terminology, is only one aspect of the overall well-being of a person and must not be reduced to it. A personâs capabilities (but also achieved functionings) depend on many different factors. They are a product of a personâs abilities and skills, as well as the political, social and economic context she finds herself in. They obviously usually depend on resources; without the necessary goods, it is simply not possible to live a self-determined life according to oneâs own conception of the good. However, what matters is the ârelationship between persons and goodsâ (Sen 1980, 216) and what the relationship allows us to do and be. In this context, the term âconversion factorsâ is helpful. It was introduced by Sen to conceptualize the relation between resources and the realization of certain functionings, and it calls attention to the degree a person in fact can use the goods at her disposal for her purposes. At least three different kinds of such factors can be identified, all of which have to be taken into account when evaluating the real freedoms somebody has access to (Sen 1992, 19â21; Sen 1999b, 70â72; Robeyns 2005, 98â100). First, there are personal conversion factors. Our physical, psychological and emotional characteristics, as well as our achieved levels of skills, influence what we can âgetâ out of the resources we command. If we are in good health, for example, we do not need a lot to achieve basic mobility. However, due to illnesses or impairments, moving around can"
297,1889,0.984,The R Book,"The idea behind hierarchical cluster analysis is to show which of a (potentially large) set of samples are most similar to one another, and to group these similar samples in the same limb of a tree. Groups of samples that are distinctly different are placed in other limbs. The trick is in deï¬ning what we mean by âmost similarâ. Each of the samples can be thought of a sitting in an m-dimensional space, deï¬ned by the m variables (columns) in the dataframe. We deï¬ne similarity on the basis of the distance between two samples in this m-dimensional space. Several different distance measures could be used, but the default is Euclidean distance (for the other options, see ?dist), and this is used to work out the distance from every sample to every other sample. This quantitative dissimilarity structure of the data is stored in a matrix produced by the dist function. Initially, each sample is assigned to its own cluster, and then the hclust algorithm proceeds iteratively, at each stage joining the two most similar clusters, continuing until there is just a single cluster (see ?hclust for details)."
147,208,0.984,Trailblazing in Entrepreneurship : Creating New Paths For Understanding The Field,"when does the refinement of a potential opportunity lead to divergence of the family and the business (or divergence within either sub-system)? (6) Finally, how does the sequence of engaging different communities of inquiry impact the evolution of the potential opportunity and the mind of the originator of the idea?"
249,250,0.984,Advances in Proof-Theoretic Semantics (Volume 43.0),"2.3 Internal and Intuitive Truth The next question to consider is whether the validity of the (T) Schema picks out a unique notion of truth. Tarski seems to hold that it does. In [21] he expresses the conviction that the material adequacy condition imposed onto the definition of truth, is capable to select the classical Aristotelian notion of truth as correspondence. The conviction is not explicitly stated, but it can be inferred from the following facts: (i) In section I.3 Tarski expresses an intention: We should like our definition to do justice to the intuitions which adhere to the classical Aristotelian conception of truth [â¦] we could perhaps express this conception by means of the familiar formula: The truth of a sentence is its agreement with (or correspondence to) reality.17"
311,2051,0.984,The Physics of the B Factories,"Having measured the first coeï¬cients of this expansion, Eq. (19.1.58) can constrain the others. This constraint is used in B meson semileptonic decays and found to be quite eï¬ective. For charm, this approach is not really justified because the charm quark mass is rather light rendering the perturbative QCD determination of the function Î¦ questionable and because the z physical range is quite limited. Numerically it appears that the first measured coeï¬cients are quite small and no useful constraint can be placed on higher order coeï¬cients (ak with k â¥ 3). It seems preferable to consider phenomenological modP 2 els to describe the q 2 variation of f+ (q ) because they have a simple physical interpretation. In the simple pole model, f+ (q 2 )simple pole ="
266,19,0.984,Societal Implications of Community-Oriented Policing and Technology,"We use the x-axis of Fig. 1.1 to illustrate the range of analytic rigour that may be applied to the analytic reasoning process. At the start of an inquiry there is usually very little known about a case. It is therefore of little use to treat information and inferences rigorously as the analyst is still trying to understand what the data means and whether it is sensible to create an argument. The type of thinking and reasoning employed by the analyst at this stage may be characterised as being creative, having to deal with high uncertainty as there are many unknowns and missing data. The need at this stage is to gain traction and to get the investigation started. Analysts engage in the tentative and playful generation of plausible stories and hypotheses that may account for their observations. They tend not to commit to a single explanation and are likely to explore alternatives. At the high rigour end of the spectrum, the type of thinking and reasoning required may be characterised as âcritical thinkingâ, evaluative, deliberate, and final. As an investigation approaches the closing stages, most of the data required will be known. It is then possible to rigorously structure, organise, or analyse the data, and to make sure that every conceivable logical discussion can be evaluated. By this stage, analysts would have employed a variety of structured analytic techniques (see for example, Heuer and Pherson 2014) to establish strong and rigorous arguments. Then usually having done all the analyses and checks â would be committed to an explanation."
360,326,0.984,Compositionality and Concepts in Linguistics and Psychology,"Here we concentrate on the effects of the noun concept on the interpretation of contrastive pairs like red hair/car. Such contrasts show a simple enough illustration of the problem for the conjunctive rule. Therefore, from a theoretical perspective, they are useful for analyzing the relations between membership and typicality with complex expressions. Like other gradable concepts, the concept RED imposes a natural ordering on entities. Here the ordering is naturally based on hue, and can be expressed by the comparative statement x is redder than y. Let us now consider the concept CAR in the phrase red car. The relation between the concepts RED and CAR is what S&O classify as a ânondiagnosticâ relation: the typicality of CAR instances is likely to remain by and large unaffected by changes in hue. More formally, suppose that we are given two situations S1 and S2 with a car, where the only difference between S1 and S2 is in the carâs hue. We may reasonably assume that TYPCAR (S1 ) is close to TYPCAR (S2 ). Specifically, suppose that S1 has a car painted focal red, and S2 has the same car painted some other hue, quite distant from focal red. Both instances are expected to be equally typical for the concept CAR. Unlike the concept CAR, the concept HAIR clearly has more typical and less typical colors. For instance, various shades of black and brown are more typical for human hair than, say, shocking pink. Among the hues between orange and focal red, some hues, at the margins of the concept RED, may be categorized as quite typical for HAIR. These are the hues that are most common for hair that is classified as RED HAIR. Let us informally refer to these hues as âgingerâ. When we consider the complex expression red hair, we see a typicality conflict (S&Oâs ânegatively diagnosticâ relation), which is due to the effect of hue on the typicality for HAIR. Starting from those hues that we called âgingerâ, the redder the hue gets, the lower the typicality is for the concept HAIR. We classify this effect as downward-monotonicity of the typicality function for the concept HAIR, and say the function TYPHAIR is downward-monotone relative to the order imposed by the gradable concept RED. More formally: For any two instances x1 and x2 of HAIR, where x1 and x2 âs hues are between ginger and focal red: if x1 is redder than x2 , then TYPHAIR (x1 ) â¤ TYPHAIR (x2 ) Note that this downward-monotonicity is only local: if we look at the hues that lie between the ginger hues and, say, the green hues, the HAIR typicality function is upward-monotone, since ginger hues are more typical as hair colors than green hues. Therefore, the ginger hues give a local maximum of the typicality for the concept HAIR among the hues that may reasonably be categorized as red.13"
278,14,0.984,Taking Stock of Industrial Ecology,"Here âliving wellâ is to be interpreted in two senses. First, it means living prosperously â with a decent level of material comfort, security and dignity. Second, it has a moral sense â not living at the expense of the well-being of others â and thus feeling your life is good in ethical terms. Recognition of ecological limits is fundamental: if it were possible to expand economic activity without limits, sustainability of development would not be a concern. A widespread interpretation of sustainability, which is implicit in many of the chapters in this book, recognises three dimensions or types of constraints. This is shown schematically in Fig. 1 in the form of a Venn diagram in which each lobe represents a possible operating space bounded by constraints, which may be âhardâ or âsoftâ. âTechno-economic efficiencyâ represents the ranges of activities available to us, limited by our technical skills and ingenuity, by the laws of thermodynamics and by the need to be efficient as defined by the prevailing economic system. An important point, picked up in several chapters in this book, is that that the laws of thermodynamics are âhard-wiredâ into the universe, whereas the âlawsâ of economics are human constructs and therefore mutable, for example by changes to the fiscal system. âEnvironmental compatibilityâ represents the range of activities which can"
356,73,0.984,Re-engineering the Uptake of ICT in Schools,"Innovation with Respect to the Toolkit Process As discussed in section âThe Challenge to Innovateâ, innovation within a scenario is not merely dependent on the technology employed but is a combination of technology and pedagogy. For example, the result of implementing a scenario might be students doing a presentation to illustrate their understanding of biodiversity. A presentation is not particularly innovative, but if the students were responsible for identifying the research questions, designing interview schedules, collaborating to devise and run experiments, etc. the process might be highly innovative. In contrast, placing QR codes around a historical part of town describing the importance of the buildings might have an innovative outcome, but if in previous years the same information appeared on a paper map, the process is not innovative. However, there is more to iTEC than the production of innovative scenarios, importantly there is also the process of creating scenarios. The act of measuring technological innovation can be found in the âOslo Manualâ (OECD 1997). This makes a helpful distinction between technological product and technological process innovations that can be transferred to the context of education. The product is the desired learning outcome as expressed as a teaching objective,"
192,381,0.984,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"replication attempts (unless they have something to hide of course). But the objective of this chapter is not to solve the (unsolvable?) replication problem (apparently more challenging even in social psychology than in science in general) about which so many books and papers have already been written. The question rather is what a dialectical-psychoanalytical view can contribute to our understanding of Stapelâs misconduct in response to the challenges emerging in his âimpossible professionâ. Was his misconduct perhaps a (misguided) response to the replication crisis and other embarrassing problems occurring in this type of university discourse? There may be more continuity between âfabricationâ (in the non-normative sense) of social psychological facts âbackstageâ, and the âfabricationâ (now in a pejorative sense) of fraudulent data published in Stapelâs papers than the committees (in their persistent use of metaphors such as âcleansing operationâ) seem able or willing to acknowledge. What I find remarkable is that the three committees, while claiming to have âscrutinizedâ a whole body of work, fail to address the methodological discussions that are part of this work (discussions which increasingly reflect a sense of despair on the part of the author). As if âscrutinizingâ an oeuvre means something else than reading and addressing its content, but I will return to this issue below. Interestingly, Stapelâs fraud has been regarded as an experiment in itself. Question: how often and to what extent can an established scientist commit fraud before it is detected? Abma cites a colleague who argued that we should seriously consider the possibility that Stapelâs fraud is actually a research trial, involving his peers and colleagues as research subjects, and that we should not be surprised if he (Stapel) at a certain point decides to publish an analysis of his results (2013, p. 157). What is pointed out here is that the technique of deception of research subjects (which is accepted practice in social psychology) is applied by Stapel to the research community as such. What is the crucial normative difference between these two instances of deception?"
187,363,0.984,Managing The Complexity of Critical infrastructures : a Modelling and Simulation Approach (Volume 90.0),"â¢ Qk is the availability level of Service k (if Qk = 0 the Service is fully unavailable). Qk depends explicitly on time and describes the pattern followed by the outage of the k-th Service during the time course of the Crisis. The function Qk(t) is the outcome of the Impact Analysis. The elements rk(tij) are the measure of the relevance of the Service k for the Wealth achievement in a given Sector element. For this reason, they will be identiï¬ed as Service Access Wealth (SAW) indices. They may be different from each other: a Sector element can be more vulnerable to the absence of a given PS and, thus, its Wealth most affected if that speciï¬c PS would fail. We then consider a closure relation, such as"
342,13,0.984,Semiotics in Mathematics Education,"The basic ideas of this semiotic theory are as follows. Ferdinand de Saussureâs (1959) semiology was developed in the context of his structural theory of general linguistics. In this theory, a linguistic sign is the result of coupling two elements, a concept and an acoustic image. To anticipate ambiguities de Saussure proposed to understand the sign as the relation of a signiï¬ed and a signiï¬er, in a close, inseparable relationship (metaphorically, like the two sides of a single piece of paper, as he suggests). He uses two now classical diagrams to exemplify the sign. In the ï¬rst, the Latin word arbor [tree] (on the bottom) and the French Â«arbreÂ» [tree] (on top) form a sign, where the former is the signiï¬er and the latter the signiï¬ed. In the second diagram, arbor is retained as the signiï¬er but the drawing of a tree takes the place of the signiï¬ed. It is noteworthy that both components in this dyad are psychological1: the acoustic image is a psychological pattern of a sound, which could be a word, a phrase, or even an intonation. These signiï¬ers are arbitrary, in"
289,1391,0.984,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","ÎX [[s1 s2 ]](S) = ÎX [[s1 ]] â¦ ÎX [[s2 ]](S) where vars(e) is the set of variables in the expression e. For input data usage, the initial set of strongly live variables contains the output variables of the program. Example 9. Let us consider again the program P shown in Example 7. The strongly live variable analysis begins from the set {passing} containing the output variable passing. At line 3, the set of strongly live variables is {math, bonus} since bonus is used in an assignment to the strongly live variable passing, and math is used in the condition of the if statement. Finally, at line 1, the set of strongly live variables is {english, math, bonus} because english is used in the condition of the if statement at line 2. Thus, strongly live variable analysis is able to conclude that the input variable science is unused. However, it is not precise enough to determine that the variable english is also unused. The imprecision of the analysis derives from the fact that it does not capture implicit flows of information precisely (cf. Sect. 8) but only over-approximates their presence. Thus, the analysis is unable to detect when a conditional statement, for instance, modifies only variables that have no impact on the outcome of a program; a situation likely to arise due to a programming error, as shown in the previous example. However, in virtue of this imprecise treatment of implicit flows, we can show that strongly live variable analysis is sound for input data usage, even for non-terminating programs. We define the concretization function Î³X : P (X) â P (P (Î£ Ã Î£â¥ )) as:"
264,1166,0.984,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"Horoks, there was a speciï¬c aim to draw on teacher knowledge and expertise and put that experience to use in interpreting eventsâthe video clip is carefully selected in relation to speciï¬c curriculum items and research ï¬ndings. The role of the facilitator is therefore markedly different in each case, either attending to the kind of thing being said (e.g., is it an evaluation or is it a description of detail?), with less attention on the content (Coles); or attending to the content of what is said (e.g., does it display awareness of the complexity of teaching and learning?), with less attention to issues around whether it is offered judgmentally or not (Chesnais and Horoks). It was clear from the Workshop that both ways of working have affordances and constraints. What has been powerful is sharing the detail of what we do as this has emerged for us as the only way of beginning to understand how each of us interprets the words we use to describe what we do."
372,1266,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"related effect discussed by Bos (1984) is the introduction of âghostâ images into the image derived from the observations. The ghost structure appears at a position that, relative to the true structure, is diametrically opposite with respect to the field center. For each spectral channel, the amplitude of the ghost structure is proportional to the amplitude of the ripple component. Thus, it is most serious for the channels at the edges of the receiver passband, as can be seen from Fig. 10.14b. The ghost phenomenon is most easily explained by considering a simple example. Suppose there is a point source of unit amplitude at position .l; m/ D .l1 ; 0/, where .0; 0/ is the field center, and it is observed over a range of baselines u. The fringe visibility of a point source is the Fourier transform4 with respect to l of a delta function at l1 , which is V1 .u/ D e!j2""ul1 D cos.2""ul1 / ! j sin.2""ul1 / :"
235,80,0.984,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","Indeed, already Sommerfeld had warned his students not to get into these issues, and Feynman [211, p. 129] predicted the âperpetual torment that results from [[the question]], âBut how can it be like that?â which is a reflection of uncontrolled but utterly vain desire to see [[quantum mechanics]] in terms of an analogy with something familiar.â Therefore he advised his audience, âDo not keep saying to yourself, if you can possibly avoid it, âBut how can it be like that?â because you will get âdown the drainâ, into a blind alley from which nobody has yet escaped.â But heresy has continued. Clauser [of the ClauserâHorneâShimonyâHolt (CHSH) inequalities [145]], in a noteworthy paper [144], pointed out the dogmatism of âevangelical theoreticians . . . their ecumenical leadership, and especially given Bohrâs strong leadership, the net legacy of their arguments is that the overwhelming majority of the physics community accepted Bohrâs âCopenhagenâ interpretation as gospel, and totally rejected Einsteinâs viewpoint.â At some point Clauser got thrown out of the office by the impatient Feynman (who often liked to market himself as âcoolâ). âA very powerful . . . stigma began to develop within the physics community towards anyone who sacrilegiously was critical of quantum theoryâs fundamentals. . . . The net impact of this stigma was that any physicist who openly criticized or even seriously questioned these foundations (or predictions) was immediately branded as a âquack.â â Clauser continues by noticing, âTo be sure, there remained alive a minority of the theoryâs founders (notably Einstein, SchrÃ¶dinger, and de Broglie) who were still critical of the theoryâs foundations. These men were obviously not quacks. Indeed, they all had Nobel Prizes! Instead, gossip among physicists branded these men âsenile.â â As time passed by, another, more optimistic phase of the perception of quantum foundations followed, which, however, might not have sufficiently and critically reflected the previous evangelical theoreticiansâ orthodoxy. On the contrary, quantum mechanics has been marketed to the public and to policy makers alike as a hocuspocus type capacity [522]. This author believes [504] that interpretation is to the formalism what a scaffolding in architecture and building construction is to the completed building. Very often the scaffolding has to be erected because it is an indispensable part of the building process. Once the completed building is in place, the scaffolding is torn down and the opus stands in its own full glory. No need for auxiliary scaffold any more. But beware of those technicians who claim to be able to erect skyscrapers without any of those poles and planks! In addition, when it comes to claims of applicability of the formalism, and its ontological commitments, the suppression of semantic content in favour of mere syntax makes us vulnerable: in many ways the formalism could be extended to domains in which it cannot be applied safely and properly. Thereby, the resulting certifications, alleged capacities and predictions could be wrong. Hence, if it comes to utilize the formalism, interpretation serves not only as scaffolding, but also provides guiding principles and precautionary methods of evaluation and application."
3,74,0.984,Instructional Scaffolding in STEM Education : Strategies and Efficacy Evidence,"2.4.1.4.3 Operationalization of Scaffolding As the goals of scaffolding differ depending on the theoretical framework that undergirds their design and use, so does the operationalization of scaffolding. From an activity theory perspective, stretching studentsâ abilities to the maximum potential is desired (Jonassen & Rohrer-Murphy, 1999; Roth & Lee, 2007). As such, one designs scaffolding so as to maximize productive struggle (Belland, 2014; Reiser, 2004; Simons & Ertmer, 2006). Productive struggle refers to struggle within the areas of the task that are most likely to lead to target learning outcomes and which is not likely to lead to disengagement (Belland, Kim, et al., 2013). Thus, within reason, struggling is not cause for concern, but rather represents an opportunity for learning. In this way, adding scaffolding is not desirable, but rather removing (fading) scaffolding is (Pea, 2004). From an ACT-R perspective, struggle is counterproductive, and thus intelligent tutoring systems allow students to request hints when they struggle (Anderson et al., 1997; Koedinger & Aleven, 2007). The first hint is more subtle, but as the student requests more, the hints become more direct, eventually ending in a bottom-out hint that provides the answer (Koedinger & Aleven, 2007; Koedinger & Corbett, 2006)."
289,1082,0.984,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","To this end, we use specialized variants of the reasoning rules, whose premises and conclusions take the form {$ n â H} (e) {Q}. Furthermore, to simplify the nonnegativeness side conditions that must be proved while reasoning, we make all cost expressions obviously nonnegative by wrapping them in max(0, â). Recall that c+ stands for max(0, c), where c â Z. Our reasoning rules work with triples of the form {$ c+ â H} (e) {Q}. They are shown in Fig. 6. Because we wish to synthesize a cost expression, our Coq tactics maintain the following invariant: whenever the goal is {$ c+ â H} (e) {Q}, the cost c is uninstantiated, that is, it is represented in Coq by a metavariable, a placeholder. This metavariable is instantiated when the goal is proved by applying one of the reasoning rules. Such an application produces new subgoals, whose preconditions contain new metavariables. As this process is repeated, a cost expression is incrementally constructed. The rule WeakenCost is a special case of the consequence rule of Separation Logic. It is typically used once at the root of the proof: even though the initial goal {$ c1 â H} (e) {Q} may not satisfy our invariant, because it lacks a â+ wrapper and because c1 is not necessarily a metavariable, WeakenCost gives rise to a subgoal {$ c+ 2 â H} (e) {Q} that satisfies it. Indeed, when this rule is applied, a fresh metavariable c2 is generated. WeakenCost can also be explicitly applied by the user when desired. It is typically used just before leaving the scope"
297,1632,0.984,The R Book,"r temporal pseudoreplication, involving repeated measurements from the same individual; r spatial pseudoreplication, involving several measurements taken from the same vicinity. Pseudoreplication is a problem because one of the most important assumptions of standard statistical analysis is independence of errors. Repeated measures through time on the same individual will have non-independent errors because peculiarities of the individual will be reï¬ected in all of the measurements made on it (the repeated measures will be temporally correlated with one another). Samples taken from the same vicinity will have non-independent errors because peculiarities of the location will be common to all the samples (e.g. yields will all be high in a good patch and all be low in a bad patch). Pseudoreplication is generally quite easy to spot. The question to ask is this. How many degrees of freedom for error does the experiment really have? If a ï¬eld experiment appears to have lots of degrees of freedom, it is probably pseudoreplicated. Take an example from pest control of insects on plants. There are 20 plots, 10 sprayed and 10 unsprayed. Within each plot there are 50 plants. Each plant is measured ï¬ve times during"
84,505,0.984,Eye Tracking Methodology,"A wide assortment of eye tracking studies can be found in the increasingly related fields of neuroscience and psychology. Topics range from basic research in vision science to the investigation of visual exploration in aesthetics (e.g., perception of art). A useful approach to navigating through vast collections of early and contemporary literature is to (for the outset) dissociate high-level cognitive studies from those concerned with a low-level functional view of the brain. In this sense, to use a computational analogy, one can distinguish between the âhardwareâ (low-level brain circuitry) on which the âsoftwareâ (high-level cognition) functions. In a complementary view of the apparently disparate disciplines, neuroscience identifies the physiological components that are ultimately responsible for perception. In the context of vision and eye movements, knowledge of the physiological organization of the optic tract as well as of cognitive and behavioral aspects of vision is indispensable in obtaining a complete understanding of human vision. To illustrate the interdependence of neuroscience and psychology, consider again the scene integration problem (see Chap. 1). Neurophysiological studies clearly identify the visual components involved in dynamic (or active) vision. That is, due to the limited informational capacity of the fovea, the eyes shift from point to point while scanning the visual field. The neuronal organization of retinal cells, which in a sense is the reason for eye movements, is well known. Furthermore, the general organization of foveoâperipheral vision has also been mapped along the magno- and parvocellular pathways leading to deeper regions of the brain and farther still into regions implicated in higher cognitive functions. From psychological observations, we know that humans are aware of a large field of view, even though physiology does not permit a holistic cameralike capture of the entire scene in one exposure. This is the crux of the scene integration problem. Psychologists show us that we are quite adept at maintaining a fairly accurate mental image of the visual scene in front of us. Indeed, various illusory pictures such as the Kanizsa (1976) square show us that we âseeâ more than what is physically there. The main question of how the brain is able to âpiece togetherâ small high-resolution snapshots of the scene remains a mystery. Â© Springer International Publishing AG 2017 A.T. Duchowski, Eye Tracking Methodology, DOI 10.1007/978-3-319-57883-5_21"
13,48,0.984,Feeling Gender : a Generational and Psychosocial Approach,"creative interchange between the conscious and the unconscious. In their perspective the unconscious is not entirely equated with dynamically repressed emotions and fantasies, since the unconscious is also seen as a creative and generative part of our psychological organisation. This makes us co-creators of meaning and reality, not passive victims of the world. Neither Loewald nor Chodorowâs vision is to replace unconscious life with conscious, but rather to infuse and integrate unconscious life in the conscious. It is this infusion and integration that gives conscious life its depth, texture and richness. Fantasy and reality come to resonate in a way where fantasy deepens and enriches the experience of reality, and reality keeps us rooted and connected in the world (Chodorow 1999: 248). The constant intertwining of the conscious, the preconscious and unconscious, of past and present, self and reality, subject and object are captured by the concept of transference. In Chodorowâs words, transference is the phenomena âthat we personally endow, animate, and tint, emotionally and through fantasy, the cultural, linguistic, interpersonal, cognitive, and embodied world we experienceâ and, by this process, âany single thought or feeling simultaneously creates and embeds itself in both realitiesâ (Chodorow 1999: 244, 14). Thus, feelings are always part of experienced meanings, of the ways in which the subject makes senseâor cannot make senseâof things. Nancy Chodorow has argued that Hans Loewald together with Erik H. Erikson and others can be seen as representatives of a specific American âintersubjective school of ego psychologyâ, which combines an understanding of the ego functions as integrating and synthesising experience in a creative way, with a relational perspective that sees the self as developed through interpersonal relations and by processes of transference between subject and object (Chodorow 2004). This ego-psychological and object-relational perspective where development is not only seen in terms of instinctual drives or universal conditions for subject formation, but also in terms of relational attachment and culture, and where the unconscious is not only understood as the dynamically repressed, but also as a part of the self that organises and may enrich experience and inform creativity and agency, has been criticised for severing the critical potential in Freudâs theory of desire. Psychoanalytic perspectives indebted to Lacan reject the notions of ego and reality, and talk instead about a radical"
107,43,0.984,"Symbiotic interaction : 5Th international Workshop, Symbiotic 2016, Padua, Italy, September 29â30, 2016, Revised Selected Papers","RISK: To me this represents a prototypical symbiotic relation since parts of the decisional powers are externalized by the physician to the machine based on data it receives. It is a whole system including patientsâ data, physicianâs decision and systemsâ elaboration/recommendations. The most important part of this scenario is the fact that the surgeon is responsible for deciding whether or not he/she will take the advice of the machine. The moment this changes and the surgeon must do what the machine tells it (whether this is an explicit formal policy at the hospital or an implicit one) the scenario changes and the relation is no longer ethically acceptable or desirable. The second scenario is one in which the surgeonâs freedom to choose has been limited. This limitation threatens the professionalization of medicine as one would have to be concerned about who is taking decisions and who is liable in case of problems, i.e. if the surgeon does what the machine tells it to do then will the machine be liable if someone goes wrong? Further, will we sue or ï¬re the machine for damages? But responsibility from an ethics point of view is more than liability; it requires a moral"
363,74,0.984,History and Cultural Memory in Neo-Victorian Fiction,"Belsey I examine romance as the excess that conventional history must exclude in its production of itself as the real, as that which exceeds the knowable. Romance and desire are thus positioned as the uncertainty that undermines the assertion of historical narrativeâs privileged relationship to the real, as those elements that defy the authority of conventional history. I argue that integral to the novelâs recasting of history as desire is its embodiment of this relationship of romance to realism, whereby romance appears as the excess elements to be excluded from Tomâs wandering and far-reaching narrative, but which nonetheless reappear, disrupting and subverting any attempt at a linear, coherent narrative. These very elements of excess ensure that the model for historical inquiry posed by the novel, an endless questioning prompted by a never-ceasing desire or curiosity, is never finished or complete. The very notion of excess ensures that there are always more elements to consider, more questions to ask. The novel achieves its characterisation of history and historiography by establishing an opposition between âartificialâ history, including written histories, stories and âthings made to happenâ, and ânaturalâ history; that which, supposedly, lies outside of representation, including nature and empty reality itself. This opposition rests heavily upon the novelâs evocation of the Victorian era, particularly its association of the period with ideals of historical design and progress and with a view of nature as both sublime and inviting of human effort to control, contain and use it for its own purposes. Moreover, the novel draws upon a concept of nature, conventionalised during the period of Britainâs industrialisation, as more authentic, more real than human constructs."
372,1828,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"15.1.3 Assumptions in the Derivation and Application of the van CittertâZernike Theorem At this point, it is convenient to collect and review the assumptions and limitations that are involved in the theory of the interferometer response. 1. Polarization of the electric field. Although the electric fields are vector quantities with directions that depend on the polarization of the radiation, the components received by antennas from different elements of the source can be combined in the manner of scalar quantities. The fields are measured by antennas at P1 and P2 , and each antenna responds to the component of the radiation for which the polarization matches that of the antenna. If the fields are randomly polarized and the antennas are identically polarized, then the signal product in Eq. (15.4) represents half the total power at each antenna. However, the antenna polarizations do not have to be identical since, in general, the interferometer system will respond to some combination of components of the source intensity determined by the antenna polarizations. The ways in which the antenna polarizations can be chosen to examine all polarizations of the incident radiation are described in Sect. 4.7.2. Thus, the scalar treatment of the field involves no loss of generality. 2. Spatial incoherence of the source. The radiation from any point on the source is statistically independent from that from any other point. This applies almost universally to astronomical sources and permits the integration in Eq. (15.6) by allowing cross products representing different elements of the source to be omitted. The Fourier transform relationship provided by the van CittertâZernike theorem requires the source to be spatially incoherent. Spatial coherence and incoherence are discussed in Sect. 15.2. Note that an incoherent source gives rise to a coherent or partially coherent wavefront as its radiation propagates through space. If this were not the case, the mutual coherence (or visibility) of an incoherent source, measured by spaced antennas, would always be zero. 3. Bandwidth pattern. The assumption required in going from Eqs. (15.4) to (15.5), that .R2 ! R1 /=c is less than the reciprocal bandwidth .'$/!1 , can be written ld u"
234,225,0.984,Mobile Professional Voluntarism and international Development : Killing Me Softly?,"These reports underline the fact that we know so little about the effectiveness of AID and that the solution to this problem is seen to lie in the production of ever-more quantitative metrics. No attempts are made to question the underlying epistemological biases of this logic or, put more simply, the fact that metrics have never worked and never will capture the âproblems and solutionsâ facing health systems in LMICs (or indeed the UK). Fundamentally, we are suffering from a form of myopia generated by the domination of medical science perspectives or knowledge paradigms which determine the diagnoses, the interventions and epistemological approaches to evaluation. The quest for statistical outcomes (ideally gained through the gold standard of randomised controlled trials) underpinned by the narrowing blinkers of systematic review restricts our ability to understand social processes. There is also a tendency within this paradigm to pathologise or patronise individuals whilst failing to understand the impact of structural constraints. Despite growing recognition of the importance of context, this is often in the form of lip service at best acknowledging it as a cluster of variables that we donât understand (as external ânoiseâ) or, at worst, rather than understanding and capturing its iterative quality, trying to insulate our interventions from it through vain or inappropriate attempts to control it. Trying to cleanse data through what feminist researchers have called a âsanitisation processâ (Harding 1987, 1991) will not generate cleaner facts. It will take us further away from the truth as the cleansing process strips data of its real value: of understanding the social processes that shape phenomenon. It may be that what we are swilling away in the efï¬uent is that which is of greatest value. This narrow conceptualisation of knowledge not only affects the approach to evaluation, but it also reproduces a partial understanding of knowledge mobilisation as an activity. The emphasis on explicit clinical skills and neglect"
8,737,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Here we claim that the asymptotic part em=T0 of the mass spectrum is the same for pions and baryons. Qualitatively, this can be understood by considering all hadrons with a given baryon number b and a very large mass such that m bmN . For such large masses, the presence of a few baryons is irrelevant as most of the mass is due to excitation of non-baryonic degrees of freedom. Hence, for any fixed baryon number b, the asymptotic part of the mass spectrum must be the same and equal to the pionic one. This conclusion can be proved rigorously [37]. Consider now the baryonic partition function ln ZN D V"
293,26,0.984,"integration Processes and Policies in Europe : Contexts, Levels and Actors","Time and Generations The heuristic model developed and explained above may be used as a tool to describe and analyse the position of individual immigrants and groups of immigrants at a certain point in time. But an important element in the logic of integration processes is the time factor. Integration of newcomers is a long-term process by its very nature. This immediately becomes apparent if we look through the lens of newcomers. At the individual level, adult immigrants may adapt cognitively and adjust their behaviour when they learn how things are done, by whom, and so on. This part is relatively easy and pays off quickly. However, their adaptation in the aesthetic (relating to the five senses) and normative realms takes more time. Feelings, likes, dislikes, and perceptions of good and evil remain rather persistent"
372,578,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"The factor of two arises because of the contributions of the real and imaginary parts of "". If the measurement is made using only a single-multiplier correlator, one can periodically introduce a quadrature phase shift at one input, thus obtaining real and"
229,48,0.984,Constructions of Cancer in Early Modern England,"Read added that âwhatsoever it claspeth with the clawes, it holdeth it firmly ... [so] that it seemeth to be nailed to the partâ.32 The grip of the crab was understood not only as painful but as immensely strong and tenacious, matching precisely the intractability and resistance to cure which was one of cancerâs most distinctive features. A renowned French practitioner Pierre Dionis made the connection explicit in 1701 when he explained that ââTis no more possible to extirpate [cancer], than force a Crab to quit what he has grasped betwixt his griping Clawsâ, while in the sixteenth century, ParÃ© deemed the link between the âtenacityâ of cancer and the âtoothed clawsâ of the crab so instructive that he inserted a picture of the creature into his writing on the subject, to drive home the âperspicuousâ nature of the comparison.33 In the figure of the crab, early modern medical practitioners effectively united the diverse visible and invisible symptoms of cancer. Moreover, this practice appears not to have problematized, or been problematized by, understandings of cancer as humoral in origin. This phenomenon is seen amplified in Chapter 3 of this book, where I discuss the casting of cancer as a type of worm or wolf. Although medical practitioners had a good sense of cancerâs symptomatology, however, there remained an element of doubt in any diagnosis. As Witherâs verse suggested, in order to really be sure that a patient was suffering from cancer, one had to see whether the suspect tumour followed the most distinctive cancerous âbehaviourâ, that of expanding and spreading throughout the body. Malignancy was, as I shall discuss, fundamental to the very meaning of this disease, setting âtrueâ cancers apart from the myriad of less dangerous ulcers and neoplasms. Furthermore, it presented a counterpoint to all medical writersâ diagnostic criteria. The way to âknowâ a cancer was to see it growing; however, that hardly required medical expertise, and once a cancer had grown large, it was much more difficult to treat. Diagnosis therefore presented the first of this diseaseâs many challenges to medical wisdom. Encounters with suspect tumours were not only matters of clinical determination, but of defining human relationships to cancer."
305,241,0.984,Quantum Computing for Everyone,"can factor a product of two large primes in polynomial time. But, on the other hand, nobody has a proof that such an algorithm doesnât exist. This is where Shor enters the picture. He constructed a quantum algorithm that does factor a product of large prime numbers. The algorithm belongs to class BQP, which means that it works with bounded error in polynomial time. One thing that needs to be emphasized is that we are no longer talking about query complexity. We are not assuming that we can ask questions of an oracle. We are counting the total number of steps or, equivalently, the time needed to get from the beginning to the end of the computation. Shor is giving a concrete algorithm for each step. The fact that the algorithm belongs to BQP means that if it is implemented it becomes feasible to factor large numbers, and, more important, it means that if the quantum circuit can be actually constructed, then RSA encryption is no longer secure. Shorâs Algorithm Shorâs algorithm involves a significant amount of mathematics. We will just give a short and somewhat vague description of the quantum part. An important part of the algorithm is a gate that is called the quantum Fourier transform gate. This can be thought of as a generalization of the Hadamard gate. In fact, for one qubit the quantum Fourier transform gate is exactly H. Recall that we used a recursive formula that told us how to get from the matrix for H â nâ1 to the matrix for H â n . Similarly, we can give a recursive formula for the quantum Fourier transform matrix. The major difference between H â n and the quantum Fourier matrix is that the entries in the latter case are generally complex numbersâmore specifically, they are complex roots of unity. Recall that the entries for H â n are either 1 or â1. These are the two possible square roots of 1. When we look for fourth roots of 1 we again just get Â±1 if we are using real numbers, but we get two other roots if we use the complex numbers. In general, 1 has n complex nth roots. The quantum Fourier transform matrix on n qubits involves all the 2 n th complex roots of unity. Simonâs algorithm was based on the properties of H â n . It used interference, the amplitudes were either 1 or â1, which meant that when we added terms, the kets either canceled or reinforced one another. Shor realized that a similar idea applied to the quantum Fourier matrix, only now the amplitudes are given not just by 1 and â1, but also by all the 2 n th complex roots"
77,329,0.984,A Life Course Perspective on Health Trajectories and Transitions (Volume 4.0),"intercept in the micro model by a non-varying constant Ë0 , and a residual term for each period and cohort. The period, cohort and level-1 residuals are all assumed to follow Normal distributions, each with variances that are estimated. This is an appealing conceptual design: âtreating periods and cohort as contexts, and age as an individual characteristic, is intuitive to some degree because we move from one period to another as time passes, and we belong to cohort groups that have common characteristics, whereas aging is a process that occurs within an individualâ (Bell and Jones 2014a, p. 340). However, Yang and Land go beyond this, arguing that this model does not incur the identification problem, because (a) the age effect is specified as a quadratic equation, and (b) because the multilevel model treats age differently from periods and cohorts: the underidentification problem of the classical APC accounting model has been resolved by the specification of the quadratic function for the age effects (Yang and Land 2006, p. 84) An HAPC framework does not incur the identification problem because the three effects are not assumed to be linear and additive at the same level of analysis (Yang and Land 2013, p. 191) This contextual approach : : : helps to deal with (actually completely avoids) the identification problem (Yang and Land 2013, p. 71)"
264,419,0.984,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"Roughly at the same time, but independently of the KOM Project, projects in the USA worked along similar lines. The National Research Councilâs (NRC) Adding It Up: Helping Children Learn Mathematics (2001) and the RAND Mathematics Study Panel (2003) adopted the term mathematical proï¬ciency, specifying ï¬ve interwoven strands: âconceptual understandingâ; âprocedural fluencyâ; âstrategic competenceâ; âadaptive reasoningâ; and âproductive dispositionâ (p. 116). To the NRC team this notion captures what is believed âto be necessary to learn mathematics successfullyâ (our italics), whilst to the RAND panel it captures âwhat it means to be competent in mathematicsâ (our italics). The RAND Panel also introduced the notion of mathematical practices:"
8,621,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Next we observe that, for  > 0 , the integrals in Eq. (24.13) converge in the limit so that, unless g.Ë; V; / D 0, the second term is a non-vanishing function C.Ë; ; /. Indeed,  > 0 implies exponential suppression of large volumes. Therefore this function C.Ë; ; / represents the corrections to the energy density coming from finite volume effects. If finite volume effects are neglected already in defining Z.Ë; V; /, then g  0. In that case Ë D 1=.  0 / has a simple pole at 0 which cancels out in E .Ë; ; / and the second term of Eq. (24.13) is absent. Furthermore, both E .Ë; ; / and q.Ë; ; / are trivial analytic functions of , namely, constants in the whole  plane. This particularly simple case is a good illustration of what happens. While hEi and hVi both have a pole at 0 and become negative at  < 0 , the energy density does not care: the pole cancels and with it the whole  dependence. The E calculated from Eq. (24.13) is just the usual one obtained from @Åln Z=VÂ=@Ë. In the more general case where finite volume effects are not neglected, i.e., g.Ë; V; / Â¤ Q, the correction term in Eq. (24.13) is present for  > 0 . It vanishes, however, identically for   0 due to our assumption that limV!1 @g=@Ë D 0 [see Eq. (24.6)]. The simple proof is by de lâHÃ´pitalâs rule. Thus, if finite volume effects are included in the definition of Z.Ë; V; /, we recover the usual thermodynamic limit results for E .Ë; ; / for all   0 [there E .Ë; ; / becomes independent of ], while for  > 0 , finite volume corrections appear explicitly. All this is physically obvious: for  < 0 , large volumes have an exponentially increasing weight in the integration, whence the main contributions come from âinfiniteâ volumes where finite volume effects are absent by definition. Once this happens, it does not matter how fast the exponential weight increases. Therefore, E .Ë; / is independent of  for   0 . Again, E .Ë; ; / defined by Eq. (24.13) is a meaningful physical quantity which may be evaluated at any , while the individual integrals in Eq. (24.13) go to infinity in the limit W ! 1. This introduction thus results in two useful conclusions: â¢ Whatever the singularity of Ë.Ë; ; / at 0 may be, it has no significance for quantities like E .Ë; ; / and q.Ë; ; /. While hE .Ë; ; /i and hV.Ë; ; /i do have a singularity at 0 and may become meaningless for   0 , the singularity (pole, branch point, cut) cancels in calculating the above densities, which may be evaluated at any . â¢ If one wishes to obtain explicit finite volume corrections, one must evaluate densities at  > 0 . If, on the other hand, one evaluates at   0 , it is irrelevant whether or not finite volume terms, or more precisely, surface terms, have been included in the definition of Z.Ë; V; /: they are suppressed by the exponentially increasing weight of large volumes. The real power of the pressure partition function formalism is this: it may happen that Ë.Ë; ; / can be calculated explicitly as an analytic function of , while the direct analytic calculation of Z.Ë; V; / is impossible. In that case, we can obtain exact results from Ë.Ë; ; / which we could not obtain from Z.Ë; V; /. This is precisely what happens in our problem of the van der Waals statistical bootstrap model."
294,362,0.984,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"which is known as a two-point boundary value problem. This is nothing but the stationary limit of the diffusion problem in Sect. 5.1.4. How can we solve such a stationary problem (5.38)? The simplest strategy, when we already have a solver for the corresponding time-dependent problem, is to use that solver and simulate until t ! 1, which in practice means that u.x; t/ no longer changes in time (within some tolerance). A nice feature of implicit methods like the Backward Euler scheme is that one can take one very long time step to âinfinityâ and produce the solution of (5.38). a) Let (5.38) be valid at mesh points xi in space, discretize u00 by a finite difference, and set up a system of equations for the point values ui ,i D 0; : : : ; N , where ui is the approximation at mesh point xi . b) Show that if t ! 1 in (5.16) - (5.18), it leads to the same equations as in a). c) Demonstrate, by running a program, that you can take one large time step with the Backward Euler scheme and compute the solution of (5.38). The solution is very boring since it is constant: u.x/ D C . Filename: rod_stationary.py. Remarks If the interest is in the stationary limit of a diffusion equation, one can either solve the associated Laplace or Poisson equation directly, or use a Backward Euler scheme for the time-dependent diffusion equation with a very long time step. Using a Forward Euler scheme with small time steps is typically inappropriate in"
223,306,0.984,Knowledge and Action (Volume 9.0),"Knowledge as Practice: Keeping Information Available A new approach is thus not only desirable but apparent as soon as one specifies how a pragmatic philosophy of knowledge should proceed. It should not simply identify as true what proves useful (although a notion of use will indeed be important). It should ask how people act in contexts in which the concept of knowledge makes sense. The strategy just outlined has already been employed by Craig (1990), who introduced an additional reflection to make his point: What is called knowledge can be best constructed in contrast to a sociocognitive state of nature without or before If what I shall say is along the right lines, the core concept of knowledge is an outcome of certain very general facts about the human situation; so general, indeed, that one cannot imagine their changing whilst anything we can still recognise as social life persists. Given those facts, and a modicum of self-awareness, the concept will appear; and for the same reasons as caused it to appear, it will then stay (p. 10)."
118,347,0.984,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"In their seminal paper, Kaplan and Garrick [35] lay the foundation for our current risk assessment model by asking three questions: (a) What can go wrong? (b) How likely is it to happen? (c) What are the consequences? The first question is answered by defining an accident sequence beginning with an initiating event, followed by multiple failures (or events) and an âend state.â This is commonly called an accident sequence. The answer to the second question is the frequency or annual probability of that sequence occurring. Lastly, the end state defines the consequences and answers the third question. Posing risk in this fashion has led a number of practitioners to quantify risk as a convolution or summation over accident sequences of consequences multiplied by annual probability. This approach results in an operational or instrumental definition of risk as an expected value. Hence one of the common operational definitions of risk is the âexpected value of an undesirable consequenceâ as:"
78,13,0.984,The Onlife Manifesto : Being Human in a Hyperconnected Era,"sense. Our perception and understanding of the realities surrounding us are necessarily mediated by concepts. These work like interfaces through which we experience, interact with, and semanticise (in the sense of making sense of, and giving meaning to), the world. In short, we grasp reality through concepts, so, when reality changes too quickly and dramatically, as it is happening nowadays because of ICTs, we are conceptually wrong-footed. It is a widespread impression that our current conceptual toolbox is no longer fitted to address new ICT-related challenges. This is not only a problem in itself. It is also a risk, because the lack of a clear conceptual grasp of our present time may easily lead to negative projections about the future: we fear and reject what we fail to semanticise. The goal of The Manifesto, and of the rest of the book that contextualises, is therefore that of contributing to the update of our conceptual framework. It is a constructive goal. We do not intend to encourage a philosophy of mistrust. On the contrary, this book is meant to be a positive contribution to rethinking the philosophy on which policies are built in a hyperconnected world, so that we may have a better chance of understanding our ICT-related problems and solving them satisfactorily. Redesigning or reengineering our hermeneutics, to put it more dramatically, seems essential, in order to have a good chance of understanding and dealing with the transformations in (a)â(d) and hence shape in the best way the novelties in (1)â(4). It is clearly an enormous and ambitious task, to which this book can only aspire to contribute. Disclaimer All the information and views set out in this book are those of the authors and do not necessarily reflect the official opinion of the European Union. Neither the European Union institutions and bodies nor any person acting on their behalf may be held responsible for the use that may be made of the information contained therein. Acknowledgements Too many people helped us since the shaping of the project for The Onlife Initiative in 2011 to be able to mention them explicitly here. However, a few individuals have been pivotal in the realization of this book, and to them go all our gratitude. We, as a group, would like to thank, within DG Connect, Robert Madelin, Director-General; Franco Accordino, Head of the Task Force âDigital Futuresâ; and Roua Abbas, Igor Caldeira, Orestis Kouloulas, Julia MoleroMaldonado, and Nicole Zwaaneveld, of the Secretariat of the Advisors to the Director-General; and, within Springer, Ties Nijssen, Publishing Editor for History and Philosophy of Science & Logic, and Lue Christi, Editorial Assistant for History and Philosophy of Science & Logic. My personal thanks go to all the onlifers, as we came to be known, for their wonderful contributions and for all that I have learnt from them, and to Penny Driscoll, my PA, for her indispensable help in editing the volume."
232,577,0.984,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"metaphysical parameters that influence the conditionsâand qualityâof a particular response to a speciï¬c catastrophe. In this chapter, I would like to discuss the potential and limits of resilience as an operative concept. Building on the idea that knowledge about disaster to come is necessarily incomplete, I will ï¬rst give an overview of the emergence of the notion of resilience in the different ï¬elds of social science. Then, building on the ï¬eld research conducted in the Bay Area of San Francisco between 2009 and 2013, I will show how experts and scientists of the Bay Area have learnt to use their empirical experience the earthquake risk and combine it with state of the art science production, creating de facto a corpus of knowledge that I will describe as âhybridized.â"
117,145,0.984,Care in Healthcare : Reflections On Theory and Practice,"The corporeal experience is much more powerful than a conscious examination of the self and the world can be. In this sense, alienation from the Other, the baby, is secondary to the immediate experience of the Other and only imaginable as a reflective and conscious act. This conscious alienation is exactly what agencies demand from the surrogates:"
223,62,0.984,Knowledge and Action (Volume 9.0),"In the instrumentally rational model both orientation and classification are closely related to what Max Weber called âdisenchantment of the worldâ (Weber, 1922/1980, p. 308). Giddens (1990) characterized this pithily as âemptying of spaceâ (p. 18) and âemptying of timeâ (p. 18). Such disenchantment and emptying of formerly stable and invariable meanings convey the formalization of the interpretation of reality. This formalization builds upon the metrization of spatial expanse and thus facilitates classification and calculation. Formalization and metrization (e.g., longitude and latitude) are the basis of modern cartographic representations of the earthâs surface and their use as an orientation for action. If the spatial dimension is included in the course of action in the instrumentally rational model, it is only as purely formal aspects of action; substantively, however, the spatial dimension is no longer tied to specific actions in a general, invariable way. With regard to norm-oriented day-to-day activities, spatially bound prescriptionsâthe relation between norm orientation and spatial expanseâare key. When relating to the physical world, actors apply, hypothetically, a classificatory criterion and a relational criterion to orient their actions. Using the classificatory criterion, they apply specific criteria (e.g., park) to categorize (e.g., public/private) the circumstances that are relevant to their actions. Using the relational criterion, actors attribute a relation to these categories (e.g., accessible/inaccessible) according to certain social or legal norms and cultural values. Of particular societal relevance are relations with normative-prescriptive spatial connotations, such as permitted/prohibited or, âYou are allowed to do activity X here but not there.â Such attributions result from processes of territorialization based on clearly measurable delineations. Control over people and the means of violence are organized via action-related territorialization, with the human body being the pivotal element. The combination of norm, body, and spatial context is exemplified by the modern nation-state with its territorially bound law and jurisdiction. The spatial connotation of understanding rests on a distinctive focus on the body as the central element of interaction and communication. The significance of the body (KÃ¶rper) for the spatial connotations becomes obvious as soon as the body is understood as the âparticularly suitable linkâ (SchÃ¼tz, 1981, p. 41, my translation) between the subjective and the extended, spatial physical world. From this perspective one can understand the body as a kind of a âfunctional linkâ (Werlen, 1993b, p. 75), switching element, or mediator for subjective biographical knowledge and symbolic appropriation of physical elements of contexts of action. Assuming that the meaning of the circumstances deemed relevant to someoneâs actions depends on the personâs available knowledge, then the way meaning is attributed arguably depends on that hitherto acquired knowledge. A decisive factor bearing on the formation of the knowledge stock is the bodily relation in the sense of presence/absence, in other words, the relation between direct and mediated experiences of the world. The significance of copresenceâthe sharing of corporeality in the here and nowâis based on the direct experience of the world through oneâs senses. The significance lies in having seen something with oneâs own eyes and having heard something with oneâs own ears and having gained"
147,104,0.984,Trailblazing in Entrepreneurship : Creating New Paths For Understanding The Field,"dIscussIon and conclusIon With the recognition that âopportunityâ is central to entrepreneurship and that opportunities are shrouded in uncertainty, a logical next step is to embrace the importance of understanding the implications of entrepreneurial failure. Entrepreneurial action and failure go hand in hand (given the high uncertainty), and we have only scratched the surface of this relationship. Whether exploring the actions of independent entrepreneurs in their newly founded (or emerging) organizations or those of corporate entrepreneurs in their innovative projects within established organizations, the nature of failure and its financial, social, emotional, and psychological implications likely have a critical impact on subsequent entrepreneurial action. However, despite its âcriticality,â we are just starting to gain an understanding of the complexity of the many interwoven relationships at play for these failure events. Although it is important to continue to explore main-effect unidirectional causal relationships, it will also be important to progress to multiple contingent/configurational mutually causal relationships of the antecedents and consequences of failure as the basis for a more dynamic micro-foundation theory of entrepreneurial action. That is, scholars need to overcome their anti-failure bias (McGrath, 1999) to better understand the decision making, cognitions, emotions, relationships, and behaviors of those involved in the entrepreneurial process. We believe that such research will help reconcile a number of paradoxes. A paradox involves âcontradictory yet inter-related elements that exist simultaneously and persist over timeâ (Smith & Lewis, 2011, p. 382). Being able to identify the underlying tension between two sets of relationships that seem to make sense when they are viewed individually but appear to be contradictory when viewed at the same time can lead to theorizing as an approach to resolving the paradox (Shepherd & Suddaby, 2017). Specifically, we can work toward the following: 1. Reconciling the perspective of âfail often and quicklyâ (e.g., real options reasoning [McGrath, 1999; McGrath & Nerkar, 2004] and design thinking [Brown, 2008; Brown & Wyatt, 2015]) with the perspective that failure âhurtsâ for those who directly experience it (Byrne & Shepherd, 2015; Shepherd, 2003; Shepherd et al., 2011). 2. Reconciling the perspective that failure is a badge of honor (e.g., Landier, 2004) with the perspective that failure can stigmatizeâput a denigrating stain or mark onâthose involved with the failure"
297,1493,0.984,The R Book,"That simpliï¬cation was justiï¬ed, so we keep time in the model but as a two-level factor. That was hard, I think you will agree. You need to be extremely well organized to do this sort of analysis without making any mistakes. A high degree of serenity is required throughout. What makes it difï¬cult is keeping track of the interactions that are in the model and those that have been excluded, and making absolutely sure that no nuisance variables have been omitted unintentionally. It turns out that life can be made much more straightforward if the analysis can be reformulated as an exercise in proportions rather than counts, because if it can, then all of the problems with nuisance variables disappear. On p. 643 the example is reanalysed with the response variable as a proportion in a GLM with binomial errors. This is possible because we have just two species, so we can reformulate the response as the proportion of all lizards that are A. opalinus. This is a big advantage because it does away with the need to retain any of the nuisance variables."
245,89,0.984,The European Higher Education Area : Between Critical Reflections and Future Policies,"Interestingly, the objective of balanced mobility was set in the EHEA context without any prior explanation of what is actually understood through balanced mobility. Or to express this differently, under which conditions mobility flows would be considered as balanced. Would only situations of perfect equilibrium between inflows and outflows be regarded as balanced or would small differences also be acceptable? These issues were not explored in the Bologna Process policy documents, balanced mobility lacking a proper deï¬nition therein. There are different potential explanations as to why this happened (or has not happened)âfor example the ministers might have thought that the concept of balance was self-explanatory, or they believed that clarifying the concept would not be a task for themselves, but of the operational arm of the processâthe BFUG. Nevertheless, irrespective of the motives behind this lack of clarity, there have been earlier attempts to deï¬ne what âbalanced mobilityâ could mean. Applying this concept of balance to total student inflows and outflows, Teichler et al. proposed in 2011 to deï¬ne as balanced a situation where the difference between inflows and outflows is smaller than 10 percentage points. Therefore, balanced would be not only cases where there is full equilibrium between the number of incoming and outgoing students (which is almost impossible to achieve in practice), but also cases where the differences are considered negligible or non-detrimental. This is the deï¬nition that we will be working with in the following sections in order to analyse how balanced or imbalanced EHEA mobility flows are. Apart from the lack of a proper deï¬nition, another peculiarity of this objective in the Bologna Process context is that, while the concept of balanced mobility is pursued here primarily in degree mobility, the idea of reciprocity, of balance, is actually the cornerstone of another type of mobility, i.e. credit mobility (student exchanges). Therefore, balance in degree mobility is a borrowed concept. Reciprocity as such was one of the original aims of the ERASMUS Programme, in the sense that the programme wanted to break away from up to then traditional mobility patterns (i.e. East to West and South to North) and to foster also reverse flows (West to East and North to South). Therefore, even in the context of credit mobility balance was not meant as full reciprocity, but rather as having flows in both directions. Knowing that the concept of balance is speciï¬c to credit mobility, we cannot help but wonder if it is at all applicable to or pursuable for degree mobility. Or to express this doubt differentlyâwould governments have the same tools at their disposal to influence balance in degree mobility as they have in credit mobility? The short answer to this question is no, they do not. Earlier studies (Kelo et al. 2006; Teichler et al. 2011) have highlighted the intrinsic differences between credit and degree mobility, labelling the ï¬rst as a horizontal and the second as a vertical type of mobility. Credit mobility is horizontal in the sense that students move for study purposes between higher education systems that are more or less on an equal par. The main aim of credit mobility is"
390,711,0.984,The Hidden Language of Computer Hardware and SW,"Chapter Twenty-Eight seriously flawed. This fundamental problem often makes it maddeningly difficult to find something specific in Google Books. Almost in complete contrast to Google Books is JSTOR (www.jstor.org), short for Journal Storage, a collection of academic journals whose articles have been organized and catalogued in a gloriously meticulous manner. JSTOR started out as a restricted site, but after a shameful incident involving the prosecution and tragic suicide of a programmer determined to make the contents of JSTOR freely available, it has been made much more accessible to the general public. For those who can read traditional Western music notation, the International Music Score Library Project (imslp.org) is to music scores what Google Books is to books. IMSLP is an enormous repository of digitized scores that are no longer in copyright, and they are fortunately catalogued and indexed in a highly usable manner. If you think about the ideas of Vannevar Bush and Ted Nelson in regard to the ability to create our own web of documents, something seems to be missing. Websites such as Google Books, JSTOR, and IMSLP are resistant to the type of arbitrary linking that they envisioned. Modern word processing and spreadsheet applications accept storing links to sources of information, but not in a very flexible manner. The website that comes closest to Wellsâs concept of a World Encyclopedia is obviously Wikipedia (wikipedia.org). The basic concept of Wikipediaâan encyclopedia that can be edited by its usersâcould easily have resulted in something that degenerated into chaos and consequent uselessness. But under the studious and conscientious direction of Jimmy Wales (born 1966), it has instead become the single most essential website on the internet. In World Brain, H. G. Wells wrote An Encyclopedia appealing to all mankind can admit no narrowing dogmas without at the same time admitting corrective criticism. It will have to be guarded editorially and with the utmost jealousy against the incessant invasion of narrowing propaganda. It will have a general flavour of what many people will call scepticism. Myth, however venerated, it must treat as myth and not as a symbolical rendering of some higher truth or any such evasion. Visions and projects and theories it must distinguish from bed-rock fact. It will necessarily press strongly against national delusions of grandeur, and against all sectarian assumptions. It will necessarily be for and not indifferent to that world community of which it must become at last an essential part. If that is what you call bias, bias the World Encyclopedia will certainly have. It will have, and it cannot help but have, a bias for organization, comparison, construction and creation. It is an essentially creative project. It has to be the dominant factor in directing the growth of a new world."
192,198,0.984,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"To bring this âdeeperâ impulse to the fore, I will read the novel from a Lacanian angle, to come to terms with this disconcerting normative âflawâ, this death drive fuelling what is purported to be the âscience of lifeâ. But before explaining the design of this chapter more fully, let me first provide an outline of the plot."
8,181,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","which satisfies the Beckenstein bound Eq. (8.9) only if the size of the string is larger than s , a conclusion that can be reached also by independent considerations. I have told you that s is of the same order as P , but the more precise relation is actually"
228,545,0.984,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"where a is the learning rate, the number from the [0; 1] interval, R is the reward function, and ei j is the eligibility trace between n i and n j neurons from two adjacent layers (see Fig. 18.3). Both the reward function and eligibility trace are complex problems. Expanding them with further calculations requires a lot of caution, because it is easy to overload the process with time-consuming computations. Therefore to improve a solution of the problem with determining weights of neurons the learning rate coefficient seems the right choice, for a start. It is worth noting that the learning rate at the 0 level is in fact no change of weight, thus no adaptation will proceed in such a step. Therefore the interval (0; 1] could be used instead. However, the zero level can represent the perfect optimum where the adaptation of this weight definitely ends, thus formally we keep zero as the low-bound in the interval of values for the learning rate."
275,130,0.984,Foundations of Trusted Autonomy,"In a first-order theory of mind, the reasoner considers that other people have beliefs, desires, etc. that influence their behaviour; e.g. they believe we are attacking from the opposite direction. In a second-order theory of mind, the reasoner allows that others are doing the same about us and other people; e.g. my co-pilot believes that the enemy believe that we are attacking from the opposite direction. In higher-order theories of mind, this nesting continues; I believe that my co-pilot believes the enemy believe that we are attacking from the opposite direction, and I believe my co-pilot believes I believe this. Such reasoning has received much attention in empirical studies of childrenâs and adultsâ reasoning, e.g. [6, 23â25] and there is considerable evidence that many adults have ToM abilities of levels 3 and 4, with some subjects succeeding in tasks requiring level 5 reasoning, yet even level 2 reasoning is beyond the reach of almost all state-of-the-art planning tools. Multi-agent systems research has contributed a deep understanding of concepts such as group knowledge, group belief, and collective intention, often informed by philosophical and psychological perspectives, e.g. [26â30]. Studies have also examined computational models of ToM, e.g. [8, 31, 32], and also the impact of different levels of awareness that an agent has about the others acting in a team task context, e.g. [33]. Although the tools used in such investigations are highly expressive â typically description logics and rich multi-modal logics, and some bespoke algebraic belief update mechanisms â they are not accompanied by efficient reasoning engines, so fall short of providing practical means for systematically operationalising complex analyses. Existing multi-agent planning tools that do take into account the beliefs, goals, intentions and capabilities of others, e.g. [34], consider a third-person view, in which a plan is constructed for a team, and each member is given their part to execute. When planning must be distributed amongst a team (including, when humans are to be in the loop), a semi-autonomous system must plan for its own actions while considering others explicitly - i.e. such reasoning demands a first-person view."
249,104,0.984,Advances in Proof-Theoretic Semantics (Volume 43.0),"Although Girard does not comment further on the claim that the second clause is âwithout mathematical contentâ, several subsequent commentators appear to expand on his point that it leads to a substantial complication in how we should understand the meaning of implication. For instance Prawitz writes One may ask whether [what is known in understanding an implication] should not consist of a description of the procedure together with a proof that this procedure has the property required, as suggested originally by Kreisel [25]. But this would lead to an infinite regress and would defeat the whole project of a theory of meaning as discussed here [35, p. 27]"
259,138,0.984,The Little Book of Semaphores,"tobaccoSem . wait () makeCigarette () agentSem . signal () smoke () Parnas presents a similar solution that assembles the boolean variables, bitwise, into an integer, and then uses the integer as an index into an array of semaphores. That way he can avoid using conditionals (one of the artificial constraints). The resulting code is a bit more concise, but its function is not as obvious."
82,16,0.984,Fading Foundations : Probability and The Regress Problem,"Abstract The attempt to justify our beliefs leads to the regress problem. We briefly recount the problemâs history and recall the two traditional solutions, foundationalism and coherentism, before turning to infinitism. According to infinitists, the regress problem is not a genuine difficulty, since infinite chains of reasons are not as troublesome as they may seem. A comparison with causal chains suggests that a proper assessment of infinitistic ideas requires that the concept of justification be made clear."
8,818,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","which assures us that interacting cold quark matter is an excited state of nuclear matter. We have assumed that, except for T, there is no relevant dimensional parameter, e.g., quark mass mq or the quantity  which enters into the running coupling constant Ës .q2 /. Therefore the relativistic relation between the energy density and pressure, viz., ""  B D 3.P C B/, is preserved, which leads to"
289,1937,0.984,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","While this is very natural from the viewpoint of a crdt, there is no linearization of the events that includes both +0a -0b +0e and +0e -0f +0a , since +0a and +0e must appear in different orders. Indeed, this lpo is not valid under the definition of the previous subsection. First note that all events are mutually dependent. To prove validity we must find a linearization that satisfies the given requirements. Any linearization of the mutators must end in either -0b or -0f . Suppose we choose +0a -0b +0e -0f and look for a mutator prefix to satisfy â0g . (All other choices lead to similar problems.) Since -0f precedes â0g and is the last mutator in our chosen linearization, every possible witness for â0g must end with mutator -0f . Indeed the only possible witness is +0a +0e -0f â0g . However, this is not a valid specification string. The problem is that we are linearizing events, rather than labels. If we shift to linearizing labels, then execution (4) is allowed. Fix the final order for the mutators to be +0 -0 +0 -0. The execution is allowed if we can find a subsequence that linearizes the labels visible at each event. It suffices to choose the witnesses as follows. In the table, we group events with a common linearization together. +0a , +0e : +0 -0b , -0f : +0-0"
289,1707,0.984,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","Following Lodaya and Weil [22], if U is a pomset language such that U = eBKA for some e â T , we say that the language U is series-rational. Note that if U is such that U = eCKA for some term e â T , then U is closed by definition. To axiomatise semantic equivalence between terms, we build the following relations, which match the axioms proposed in [20]. The axioms of CKA as defined in [8] come from a double quantale structure mediated by the exchange law; these imply the ones given here. The converse implication does not hold; in particular, our syntax does not include an infinitary greatest lower bound operator. However, BKA (as defined in this paper) does have a finitary greatest lower bound [20], and by the existence of closure, so does CKA."
297,582,0.984,The R Book,"There is a superï¬cial similarity between the two plots in that both have numerous green vertical bars. But there the similarity ends. The histogram on the left has Growth.rate on the x axis, but the bar plot on the right has Growth.rate on the y axis. The y axis on the histogram shows the count (frequency) of the number of times that values from a given interval of growth rates were observed in the whole experiment. The y axis on the bar plot shows the arithmetic mean growth rate for that particular experimental treatment. There is no need to labour the point, but you must be absolutely sure that you understand the difference between a histogram and a bar plot, and try not to refer to a bar chart as a histogram or vice versa. 5.7.2"
269,94,0.984,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"The urban brain One of the â many â uncomfortable facts about collaboration is that some kind of strategic reduction (of oneâs own concepts and immediate intellectual ambitions) is often the price of entry. One of our core messages is that learning to become an interdisciplinarian means coming to terms with that price. We would suggest, entirely seriously, and without prejudice, that any social scientist who is committed to writing 12,000-word papers on different genealogies of âthe socialâ, or who doesnât think she could bear the epistemic violence of using a construct like âSESâ (socioeconomic status) as a proxy for dense inter-lacings of class, social position, and education, is probably as well off not moving much beyond her"
32,624,0.984,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","Enjyo were identified. Moreover, one of the cases of appropriate Enjyo extinction was observed. From my viewpoint as a journalist, information making the rounds on the web or SNS seems to transform itself and reach its conclusion much faster than the speed of mass media coverage. Our challenge for the future is to develop a method of predicting an outbreak of Enjyo seen in the culture of âvertically-structuredâ and âread-the-atmosphereâ society, Japan, through analyzing some extracted web data (e.g. tweets), which will hopefully be like weather forecast services. Open Access This book is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
103,371,0.984,Solar Particle Radiation Storms Forecasting and Analysis : The Hesperia Horizon 2020 Project and Beyond (Volume 444.0),"thermal population, only, would limit the injection efficiency of the shock especially at obliquity angles greater than 20Ä± quite substantially: according to Battarbee et al. (2013), the injection efficiency in an oblique coronal shock would go down by an order of magnitude if we would use a steeper seed population with  D 15. Given all the possible ways to limit the number of precipitating protons in our model, we would regard the result of getting more than enough high-energy protons precipitating at the Sun to be a supporting rather than a countering result for the shock acceleration scenario. The final shortcoming in the shock models we have employed is their assumption of the open topology of the upstream magnetic fields. Especially if the -ray event occurs during a period of closely spaced CMEs, the second one may drive a shock through a set of large closed loop-like or flux-rope structures, which would alter both the shock acceleration properties and, more importantly, the ability of the particles to escape upstream from the shock. In fact, developing codes capable of modelling particle acceleration and transport in more complicated upstream fields than the radial/Archimedean-spiral fields could be listed as one of the most urgent things needing improvement on the way towards physical space-weather modelling capabilities. One of the most difficult things to estimate is the size of the source of nearrelativistic protons in the event. While the 3-D modelling of the shock front can be performed in a relatively accurate and detailed manner, high-resolution density and magnetic-field structure of the corona are crucial for the correct determination of the shock properties and, thus, the total number of interacting protons in the event. Therefore, even a fully global 3-D model of coronal shock evolution and particle acceleration might not capture every detail affecting the total number of relativistic protons in the CME system. In this work, we resorted to estimating numbers based on the filling factor of field lines being capable of facilitating proton acceleration to relativistic energies based on an evaluation of shock properties on a large set of field lines. We believe that such statistical method to estimate the total number of interacting protons is the most efficient way to address the problem. In conclusion, while a number of simplifications have been introduced in the modelling performed, we have still demonstrated that coronal shock acceleration and subsequent diffusive downstream particle transport is a viable option to explain pion-decay -ray events from the Sun observed by Fermi/LAT. More elaborated simulation models are needed (especially for the particle transport back to the Sun) but our results serve as a motivation by indicating that the end result of this vast modelling effort can be positive."
192,80,0.984,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"upper-left position as agent: the power of the Master is subverted (S2/S1), and the scheme takes a quarter turn to the left. Hegelâs dialectics of Master and Servant, developed in his Phenomenology of the Spirit (Hegel 1807/1973) can elucidate this inevitable dialectical turn. Initially, the Servant acknowledges the supremacy of the Master. Instead of challenging the latterâs authority, the Servant willingly relinquishes his autonomy, opting for an attitude of devotion and servitude. Such servants are put to work, in the interest of the Master. Rather than aspiring to become Masters themselves, which would lead to competition and warfare, they accept a subordinate position of dependency. In fact, this type of servitude produces a particular form of jouissance. Inevitably, however, a dialectal dynamics unfolds, which eventually subverts the situation in the sense that the discourse of the Master becomes increasingly dependent on the work of these servants. They become increasingly dexterous and skilful. Not only as custodians and interpreters of the Masterâs founding words, for the emancipation will not stop there. Rather, instead of relying on the signifiers coined by the Master to understand nature (S2), the servants will explore and interact with nature with their hands, but even more so with their instruments, allowing them to manipulate nature, on the basis of laboratory manuals. And this should be taken literally, for etymologically speaking, to manipulate (derived from manus) means to handle. Increasingly, the Masterâs apodictic views are suppressed (pushed beneath the bar), as the servants come to rely on hands-on, practical interactions with nature, developing powerful tools to manipulate and manage natural objects more effectively: the birth of the experimental method. Exegesis (hermeneutics, the initial craft of the scholarly adept) increasingly gives way to experimental work (manipulating and quantifying nature). Increasingly, via skills and know-how, the servants assume mastery over the situation. Servants become scientists (S2), scientific agents, while the meta-physical pontifications of the Master become a superfluous burden, so that the power relationship becomes subverted, and a new type of discourse emerges, to which Lacan refers as the university discourse:"
359,263,0.984,"Micro-, Meso- and Macro-Dynamics of the Brain","Computers A decade or two ago, the idea that an inestimable number of simultaneous non-linear equations could represent a theory of brain organization, if dreamt of by a few, seemed such an unlikely proposition that it merited no more than a passing frisson. There were two fundamental problems: how to make the calculations, and how to amass the data on which to make them. The first problem is largely solved, at least in principle and partly in practice. The most powerful super-computers currently available are at the peta-flop level (http://www.top500.org/list/2013/06/). The IBM roadmap predicts the production of an exascale computer around 2018 (1  1018 flops/s). Extrapolating todayâs Blue Brain Project numbers, exascale is probably the minimum required to simulate the entire brain. This level of performance is just sufficient for the simultaneous computation of the present estimate of the number of equations needed to provide a first holistic version of a brain model, one that instantiates the nonlinear interactions that give rise to the emergent properties of living brains. As to data storage, this is a practical problem that has effectively been solved by cloud computing and distributed storage with appropriate addressing; it is data analysis and aggregation with efficient database queries that are challenges at this scale."
271,45,0.984,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"Beyond detailed research results, existing studies on mediatization agree that any process of mediatization is very specific in relation to the social domain under consideration. This term social domain is used by various representatives of mediatization research (amongst others Hjarvard 2013; EkstrÃ¶m et al. 2016; Lunt and Livingstone 2016), while coming close to the everyday understanding of âspheresâ of society. In its widest sense, the term âsocial domainâ refers to those âspheresâ as being meaningful in everyday practice. The scaling of the different âmeaningful domainsâ can be very different, reaching from certain social groups or organizations to whole social fields or systems. We can understand this scaling to be a problem of terminological blurriness. However, the main argument being pushed forward in mediatization research is different. By hinting at the domain specificity of mediatization, scholars want to emphasize the variety of mediatization across different spheres of society. Mediatization is not a homogeneous process but very much differs from one area to another. It is a âdomain-specificâ phenomenon. We can understand this as taking up a long tradition in social sciences relating to the idea of âsocialâ as well as âculturalâ differentiation (Winter and Eckert 1990: 142â151; Hahn 2000: 14â24; Schimank 2013: 37â50; 131â149). Max Weber, for example, used the term WertsphÃ¤ren (Weber 1988 [1919]: 611) to reflect this. Pierre Bourdieu (1993) described processes of differentiation by analyzing differences within and across âsocial fieldsâ. Roger Friedland and Robert Alford (1991) preferred the idea of âinstitutional fieldsâ. In system theory, we find the concept of âsubsystemâ (Luhmann 2012, Vol. 2: 4â27), a term which was also used by JÃ¼rgen Habermas (1992) to describe social differentiation. In a similar vein, phenomenology puts emphasis on different (small) âlife-worldsâ (SchÃ¼tz 1967: 139â144; Luckmann 1970: 587), with a certain relationship to the âsocial worldsâ of symbolic interactionism (Shibutani 1955: 566; Strauss 1978; Clarke 2011: 384â385). More recently, Luc Boltanski and Laurent ThÃ©venot (2006) proposed the idea of different âorders of justificationâ."
297,966,0.984,The R Book,"The data in this book fall into two distinct categories. In the case of planned experiments, all of the treatment combinations are equally represented and, barring accidents, there are no missing values. Such experiments are said to be orthogonal. In the case of observational studies, however, we have no control over the number of individuals for which we have data, or over the combinations of circumstances that are observed. Many of the explanatory variables are likely to be correlated with one another, as well as with the response variable. Missing treatment combinations are commonplace, and the data are said to be non-orthogonal. This makes an important difference to our statistical modelling because, in orthogonal designs, the variation that is attributed to a given factor is constant, and does not depend upon the order in which factors are removed from the model. In contrast, with non-orthogonal data, we ï¬nd that the variation attributable to a given factor does depend upon the order in which factors are removed from the model. We must be careful, therefore, to judge the signiï¬cance of factors in non-orthogonal studies, when they are removed from the maximal model (i.e. from the model including all the other factors and interactions with which they might be confounded). Remember that, for non-orthogonal data, order matters. Also, if your explanatory variables are correlated with each other, then the signiï¬cance you attach to a given explanatory variable will depend upon whether you delete it from a maximal model or add it to the null model. If you always test by model simpliï¬cation then you will not fall into this trap. The fact that you have laboured long and hard to include a particular experimental treatment does not justify the retention of that factor in the model if the analysis shows it to have no explanatory power. ANOVA tables are often published containing a mixture of signiï¬cant and non-signiï¬cant effects. This is not a problem in orthogonal designs, because sums of squares can be unequivocally attributed to each factor and interaction term. But as soon as there are missing values or unequal weights, then it is impossible to tell how the parameter estimates and standard errors of the signiï¬cant terms would have been altered if the non-signiï¬cant terms had been deleted. The best practice is as follows:"
257,349,0.984,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","â ({Q}, â, â) ââââ (Q, Ïâ² , Ï â² ), with s =Ï sâ² â Î â² â¢ ÏÏ â¼ Ïâ² Ï â² : LL â c, and for all x â dom(Ï) â© dom(Ï â² ), there exists cx such that Î â² â¢ Ï(x) â¼ Ï(x) : Î â² (x) â cx and cx â c. Note that this finer invariant guarantees that we can restrict our attention to the instantiations considered for defining consistency. As a by-product, we obtain a finer type system for equivalence, even for processes with long term keys (as in [28]). For example, we can now prove equivalence of processes where some agent signs a low message that comes from the adversary. In such a case, we collect sign(x, k) â¼ sign(x, k) in the constraint, where x has type LL, which we can now prove to be consistent (depending on how x is used in the rest of the constraint)."
213,208,0.984,Collider Physics Within The Standard Model : a Primer,"where REW is the electroweak-corrected Born approximation, and Ä±QCD , Ä±NP are the perturbative (logarithmic) and non-perturbative (power suppressed) QCD corrections. For a measurement of Ës (in the following we always refer to the MS definition of Ës ) at the Z resonance peak, one can use all the information from Rl , Z D 3l C h C inv , and F D 12l F =.m2Z Z2 /, where F stands for h or l. In the past, the measurement from Rl was preferred (taken by itself it leads to Ës .mZ / D 0:1226 Ë 0:0038, a bit on the large side), but after LEP there is no reason for this preference. In all these quantities Ës enters through h , but the measurements of, say, Z , Rl , and l are really independent, as they are affected by entirely different systematics: Z is extracted from the line shape, and Rl and l are measured at the peak, but Rl does not depend on the absolute luminosity, while l does. The most sensitive single quantity is l . It gives Ës .mZ / D 0:1183 Ë 0:0030. The combined value from the measurements at the Z (assuming the validity of the SM and the"
297,957,0.984,The R Book,"r the null model; r the minimal adequate model; r the current model; r the maximal model; and r the saturated model. The stepwise progression from the saturated model (or the maximal model, whichever is appropriate) through a series of simpliï¬cations to the minimal adequate model is made on the basis of deletion tests. These are F tests or chi-squared tests that assess the signiï¬cance of the increase in deviance that results when a given term is removed from the current model. Models are representations of reality that should be both accurate and convenient. However, it is impossible to maximize a modelâs realism, generality and holism simultaneously, and the principle of parsimony is a vital tool in helping to choose one model over another. Thus, we would only include an explanatory variable in a model if it signiï¬cantly improved the ï¬t of the model. The fact that we went to the trouble of measuring something does not mean we have to have it in our model. Parsimony says that, other things being equal, we prefer:"
48,108,0.984,Time Predictions : Understanding and Avoiding Unrealism in Project Planning and Everyday Life (Volume 5.0),"The above two situations illustrate that we should expect greater time overrun for larger tasks when the task size is measured as the actual time usage (or cost) and greater time underrun for larger tasks when the task size is measured as the predicted time usage (or budgeted cost). This was also the case in a comparison of the results from 13 different studies on the magnitude effect [27]; all seven studies that had used actual time or actual cost usage as the measure of task size found greater underestimation of larger tasks. This finding is in accordance with the common claim that overrun increases with increasing task size. In contrast, the studies that used predicted time usage or budgeted cost as their measure of task size found little or no underestimation of larger tasks. So what is the true story about the relation between task size and time overruns? One way of reducing the methodological challenges of studying this relation is through controlled experiments. In controlled experiments, the task size may be set by the experimenter and there is no need to use predicted or actual time as a size measure.4 One controlled experiment on this topic used the number of sheets of paper in a counting task as the measure of task size [28]. Participants received stacks of paper and predicted how long it would take to count the sheets of paper. An analysis of the time predictions showed that people were more optimistic for larger stacks of paper (larger tasks) than for smaller stacks (smaller tasks).5 Other controlled experiments with the task size set by the experimenter have shown similar results: larger tasks were more likely to be underestimated than smaller tasks were [26]. In fact, an entire literature shows that people typically underestimate large quantities of any kind (duration, size, luminance, etc.) more than smaller quantities [30]. Consequently, the true magnitude effect, supported by the findings of controlled experiments, is that we should expect greater overestimation, or at least less underestimation, with smaller tasks and greater underestimation, or at least less overestimation, with larger tasks. A natural question is then what constitutes small and large tasks? Not surprisingly, what is perceived as small and large and, consequently, the magnitude effect depends on the context. An experiment on time perception may serve as a good example of how the context defines whether a task is small or large [31]. In this experiment, people watched a circle on a computer screen for varying amounts of time (between 494 milliseconds and 847 milliseconds) and were asked to reproduce this interval by pressing the spacebar on a computer keyboard. The data showed that the longer intervals were underestimated, whereas the shorter intervals were overestimated. The intervals in the middle of the distribution were rather accurately estimated. The next week, the participants repeated the procedure but, now, with a change in the range of the intervals (between 847 and 1200 milliseconds). In the first session, the 847-milliseconds interval was the longest and most underestimated, but in the second session it was the shortest and was consequently overestimated. This rather elaborate experiment 4 The experimental manipulation means that we neutralize the effect of the random variation in the"
330,118,0.984,Dynamics of Long-Life Assets : From Technology Adaptation to Upgrading the Business Model,"discussing and clustering information of the story or insights to reveal latent motives and solution requirements. The following is a possible template: Who (stakeholder A) needs what (requirement) to do what (task) to fulï¬l what (motive). For instance, Peter needs a calm place with electricity supply and place for his laptop to work in the train to reduce his workload to gain time (motive) to feel that the time has used meaningfully (latent motive). This POV has once again to be validated by the chosen stakeholder group. Especially the reaction of the stakeholders by confronting them with the latent motive might lead once again to new insights. One example, what else besides work brings a feeling of use time meaningfully in train? Hint: Technique to reveal latent motives are also called and described as âjob to be doneâ (Silverstein et al. 2012). Another technique is âpersonaâ that describes ab archetype of a stakeholder group. In principle, the deï¬ne phase ceases when an accurate deï¬nition of a POV exists. However, it might be that new insights emerge during the remaining three phases which require the POV to be reformulated and re-explored. Since this is possible, the speed by which the remaining three phases are executed becomes crucial. It is feasible to have a full-fledged POV within three or four days, when the team has expertise about the design scope. By formulating the POV the working mode of design team changes from formulating customerâs needs to ï¬nding solution for those needs. For our example: How might we support Peter in working efï¬ciently while he is travelling home from workplace during rush hour? The POV forces the team to focus. Without this focus the team ï¬nds itself in an ongoing search without any result, therefore moderation within the design thinking process is recommended."
103,111,0.984,Solar Particle Radiation Storms Forecasting and Analysis : The Hesperia Horizon 2020 Project and Beyond (Volume 444.0),"3.2.2 Resonant Wave Acceleration As discussed above, generating electric fields with quasi-static components along the magnetic field is not easy. Instead of large scale fields, the fields can also be at the gyromotion scale, and such fields can be carried by various plasma waves. At first sight, if the electric field is fluctuating, it is quasi-periodically pointing in opposite directions and the time integral of the acceleration would seem to vanish in most cases. In this case, however, it is possible for the particle to be in resonance with the waveâs electric field: if the period of particle motion agrees with the period of the wave, the phase of the wave at the location of the particle can be constant, leading to a constant accelerating electric field felt by the particle. This process can be illustrated with a simple linearized model. Consider the electric field of a circularly polarized wave propagating along the mean magnetic field taken to be constant and along the z axis. Thus, E1 D E1 Åex cos.kz"
372,602,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"the fringes are stopped, but for the other, they are lost in the averaging at the correlator output because the fringe frequencies are high. Thus, for one playback, we have the signal of an SSB system and the noise of a DSB p system in each of the real and imaginary outputs, that is, an SNR of V=.2 2Ë,/ and a relative sensitivity of 1=.2Ë/ for each individual sideband. 8. Measurement of cross-correlation as a function of time offset. Digital spectral correlators that measure cross-correlation as a function of time delay are described in Sect. 8.8. In a lag-type correlator, the cross-correlation is measured as a function of time offset, implemented by introducing instrumental delays. The Fourier transform of the cross-correlation as a function of relative time delay between the signals is the cross-correlation as a function of frequency, as required in spectral line measurements. As mentioned in Sect. 6.1.7, it is necessary to use only simple correlators for this measurement. The range of time offsets of the two signals covers both positive and negative values, and the resulting measurements of cross-correlation contain both even and odd components. Fourier transformation then provides both the real and imaginary components of the cross-correlation as a function of frequency. The full sensitivity is obtained so long as the range of time offsets is comparable to the reciprocal signal bandwidth or greater; see Table 9.7. Note that in Table 6.1, we have not included the quantization loss discussed in Sect. 8.3.3. A demonstration of the sensitivity using a simple correlator when the measurements are made as a function of time delay is given by Mickelson and Swenson (1991). Of the cases included in Table 6.1, the SSB with complex correlator is the one generally used where possible, because of the sensitivity and avoidance of the complications of DSB operation. Cases 2 and 3 in the table are included mainly for completeness of the discussion. As mentioned earlier, for frequencies of several hundred gigahertz, the most sensitive type of receiver input stage may be an SIS mixer. This has an inherently double-sided response, and if necessary, a sideband can be removed by filtering or using a sideband-separating arrangement (Appendix 7.1). For DSB operation, the most important cases in Table 6.1 are 6a and 6b. The case in which the unwanted sideband is only partially rejected is discussed in Appendix 6.1."
82,407,0.984,Fading Foundations : Probability and The Regress Problem,"has been introduced as a graphic representation of the relation in such networks.8 Figure 8.1 is an example of a very simple justification tree. This tree has two branches, with A1 and Aâ²1 as nodes on the one level, and A2 and Aâ²2 as nodes on a lower level. It should be read as: proposition q is justified by A1 and Aâ²1 , A1 is justified by A2 , and Aâ²1 is justified by Aâ²2 . In this section we shall describe what happens when we replace the finite or infinite onedimensional probabilistic chain by a finite or infinite probabilistic network in two dimensions, along the lines of a justification tree."
321,81,0.984,Beyond the Limits to Growth : New Ideas for Sustainability from Japan,"to India as well, then nothing can be done to save the global environment.â But that kind of argument does not take into account the advance of technology. The reason that arguments about the energy consumption of vehicles time and again become vague is that they are not based on an orderly theory. And so they do not reach a clear conclusion regarding the extent to which the energy efficiency of automobiles can be improved. For instance, in the graphs of fuel economy used in automobile catalogs, because it is often the case that engine displacement is shown on the horizontal axis and fuel consumption on the vertical axis, it is not possible to comprehend how far energy efficiency can be improved through technological progress. If, as in Fig. 3.1, on the vertical axis we plot the number of liters of gasoline used to run 1 km and on the horizontal axis the weight of the vehicle, from the very beginning we may come to see the future. In short, for the energy consumption required to propel a vehicle, zero is the theoretical limiting value. In the example of the Swiss team mentioned previously, the fuel efficiency was close to this limit and falls near the origin of this graph. Through the development of energy conversion technology and lightweight material technology, how close to this origin we can come is in fact the goal. Thinking in this way, even if the number of cars cruising through the world quadruples from the present number, if the energy efficiency is five or even ten times the current level, then the energy consumed to power these vehicles will decrease. That this is possible has been shown by theory; the forecast for technology provides the view of how it is possible to approach this theoretical limiting value. To sum up, in the case of automobiles, realizing the technology to approach the theoretical limiting value is the answer to the problem of a âLimited Earth.â Not only for automobiles, but also for such devices as air conditioners and water heaters which use a great deal of energy, the gap between the theoretical limit and the present state is large, and there is ample room for improving energy efficiency. This fact is one of the grounds for the possibility that technology can contribute to the future."
124,121,0.984,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"4 The concept of case-relative truth Theorem 1 told us that necessarily there is an intensionally unique elementary rangeâour internal surrogate for âcaseââthat happens. This takes us part way to finding the idea of âtrue in a caseâ inside of CIFOL+. Let us step back and think for a minute about âtrue in a case.â That phrase is special, partly because there is very little intuitive support for the phrase âtrue in a world,â even though one can be led by well-worked-out formal machinery to prize the idea. A substitute phrase, âtruth according to a worldâ seems marginally more idiomatic, although hardly an expression of conversational English. In contrast, âtrue in a caseâ is somewhat more idiomatic. I donât mean the truth of sentences, which isnât idiomatic at all, but rather certain informal âtrue thatâ expressions relativized to cases: â¢ Itâs true that we shall get wet in case it rains, but otherwise not. â¢ I shall be sad in case you find fault with my examples."
45,163,0.984,Measurement and Control of Charged Particle Beams,"2.8.7 Emittance near Sum Resonance The driving term of the sum resonance Îº+ may also be inferred by measuring the emittance in the vicinity of this resonance [78]. However, we caution the reader that experimental experience with this scheme is scarce, and that there appear to be some uncertainties in the predictions by different theories and computer simulations. Using the formulae in [74] and [79], the vertical emittance should depend on the measured distance to the sum resonance, ÎQI,II,+ â¡ |QI + QII â p| (where p is the integer which minimizes the expression) as [78] Ç«y â Ç«x0"
217,318,0.984,Finite Difference Computing With Pdes : a Modern Software Approach,"The new scalar_solver takes the same arguments as wave1D_u0.scalar and calls wave1D_u0v.scalar, but always supplies the extra argument version= âscalarâ. When sending this solver_function to wave1D_u0.viz, the latter will call wave1D_u0v.solver with all the I, V, f, etc., arguments we supply, plus version=âscalarâ. Efficiency experiments We now have a viz function that can call our solver function both in scalar and vectorized mode. The function run_efficiency_ experiments in wave1D_u0v.py performs a set of experiments and reports the CPU time spent in the scalar and vectorized solver for the previous string vibration example with spatial mesh resolutions Nx D 50; 100; 200; 400; 800. Running this function reveals that the vectorized code runs substantially faster: the vectorized code runs approximately Nx =10 times as fast as the scalar code!"
245,517,0.984,The European Higher Education Area : Between Critical Reflections and Future Policies,"mission. This can, of course, mean taking a (partial) success. Probably for this reason, it would require recognition by policy making bodies, as well as more attention and more attention within the Bologna Process as well. The third mission is not a redeï¬nition of the university. At the end, the third mission is a vehicle of further diversiï¬cation and proï¬le building. However, getting there requires more consideration on the system, institutional, and individual level. It requires both a top-down and a bottom-up approach. Already what is happening at the universities is much of what the university does not know, because it is not recorded or documented. Often there are initiatives of university members who are active through an inner drive out. Here, it is necessary to further protect these forces and to motivate others, without it becoming a compulsion. Finally, there is an organic and cultural development that allows this area to be understood as an integral part of the duties of a university to design and Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
299,82,0.984,Happiness Is The Wrong Metric : a Liberal Communitarian Response To Populism,"Whatever measurement is used to assess peopleâs satisfaction, whether this is based on their own statements or divined from the factors researchers assume (ought to) make them happy, the communitarian approach here developed requires adding a major measurementâone that would assess the extent to which the members of a society live up to a set of values. Theoretically, one could measure the extent to which they live up to their particular societyâs set of values. Such a score, however, would be of limited merit because it would be high for societies that set low standards for themselves or values that are considered to be of little virtue in the eye of the observer. Religious extremist societies, such as the ISIS caliphate, are one such example of a society that would score high on such a measurement of the extent to which a society lives up to its own values. Instead, such a measurement should be based on adherence to a universal set of values, such as measured by rates of child abuse, corruption, and volunteerism in a given society (Etzioni 2011). Because there is only moderate consensus about what these universal values should be, different"
192,264,0.984,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"An important aspect of his position as Master is that, although from the perspective of his junior collaborators he seems wholly devoted to research, he actually leads a double life, as we have seen, a secret life as an affluent, high-brow gentleman. In his spare time, he engages in high culture, as an erotic art connoisseur for instance, being the owner of seven original erotic drawings by Egon Schiele. We learn that Cantor inherited a fortune from his father-in-law â a wealthy Jewish industrialist from Vienna, whose only daughter Cantor had married when she was thirty-six â and this heritage included a complete art nouveau interior, a fin-de-siÃ¨cle Viennese decor, transplanted to Chicago, whose most remarkable item is a seating machine (Sitzmaschine). But this sample from Viennese existence is now embedded in the American way of life and combined with a splendid view over Lake Michigan. In the course of the novel it becomes clear that Cantorâs actions are much more calculated, strategic and self-centred than is initially apparent. The race for priority (and indirectly for the Nobel Prize) is much more important to him than something like scientific âtruthâ. And Cantor is quite good at playing the publication game. At a certain point he deplores the abolishment of the pli cachetÃ©, the âsealed envelopeâ system (p. 62), a reference which requires some explanatory remarks concerning the history of the scientific journal which, originally, was not invented as a communication device, but as a device for solving priority conflicts (Zwart 2001). By establishing formal outlets in the form of journals, discoveries could now be attributed to the scholar who first published about it, or whose paper first reached the editor of an acknowledged journal. And the âsealed envelopeâ procedure meant that an article could be submitted so that a journal editor, who would date it upon receipt, but would refrain from opening it until the author was sufficiently certain that its content could be validated and replicated, or when a competitor was about to publish something similar. In that case, the original submission data of the pli cachetÃ© would demonstrate priority. If still in place, it would significantly reduce the risk of retraction, and it would certainly have solved Cantorâs dilemma. But it would also turn publishing into a kind of card game, with the sealed envelope functioning as a kind of trump card. âI wonder what made me think of the pli cachetÃ© systemâ, Cantor asks himself, âI hope itâs not some unconscious wish of mineâ (p. 63). From a Lacanian perspective, one could argue that the sealed envelope system demonstrates the extent to which the fate of the scientific subject may come to"
82,19,0.984,Fading Foundations : Probability and The Regress Problem,"cannot be said to know the proposition q, or the chain must come to a stop, but then it seems we are not justified in claiming that we really can know q, since there is no reason for stopping. Laurence Bonjour called considerations relating to the regress problem âperhaps the most crucial in the entire theory of knowledgeâ, and Robert Audi observes that no epistemologist quite knows how to handle the problem.1 The roots of the regress problem extend far back into epistemological history, and scholars often refer to the Greek philosopher Agrippa. Little is known about Agrippa, apart from the fact that he probably lived in the first century A.D. and might have been among the group of sceptics discussed by Sextus Empiricus, a philosopher and practising physician who allegedly flourished a century later. Sextusâ most famous work, Outlines of Pyrrhonism, contains an explanation and defence of what he takes to be the philosophy of another shadowy figure, namely Pyrrho of Elis (c. 365â270 B.C.), who himself wrote nothing, but became known for his sober life style and his aversion to academic or theoretical reasoning. So-called Pyrrhonian scepticism advocates the attainment of ataraxia, a state of serene calmness in which one is free from moods or other disturbances. An important technique for reaching this state is the practicing of argument strategies known as tropoi or modes, i.e. means to engender suspension of judgement by undermining any claim that conclusive knowledge or justification has been attained. For example, if it were claimed that a particular sound is known to be soft, a Pyrrhonian would point out that to a dog it is loud, and that we cannot judge the loudness or softness independently of the hearer. Typically, a Pyrrhonian will try to thoroughly acquaint himself with the modes, so that reacting in accordance with them becomes as it were a second nature. In this manner he will be able to routinely refrain from assenting to any weighty proposition q or Â¬q, and thus avoid getting caught up in one of those rigid intellectual positions that he loathes so much. In Book 1 of Outlines of Pyrrhonism, Sextus discusses five modes which he attributes to âthe more recent Scepticsâ (to be distinguished from what he calls âthe older Scepticsâ), and which Diogenes Laertius in the third century would identify with âAgrippa and his schoolâ.2 Of these five modes the 1 Bonjour 1985, p.18; Audi 1998, 183â184. The thought is echoed by Michael Hue-"
95,283,0.984,Elements of Robotics,"We continue to display the computations in graphs but you may find it easier to follow them in Table 8.1. Each row represents the belief array of the robot following the action written in the first column. Initially, after sensing dark gray at a position where there is a door, we only know with probability 0.125 Ã 0.9 = 0.1125 that a door has been correctly detected; however, there is still a 0.125 Ã 0.1 = 0.0125 probability that it has mistakenly detected a wall. After normalizing (Appendix B.2), the belief array is:"
305,249,0.984,Quantum Computing for Everyone,"Feynman thought that one of the main applications of quantum computers would be to simulate quantum systems. Using quantum computers to study chemistry that belongs to the quantum world is a natural idea that has great potential. There are a number of areas where it is hoped that quantum computing will make important contributions. One of these is to understand how an enzyme, nitrogenase, used to make fertilizers actually works. The current method of producing fertilizers releases a significant amount of greenhouse gases and consumes considerable energy. Quantum computers could play a major role in understanding this and other catalytic reactions. There is a group at the University of Chicago that is looking into photosynthesis. The transfer of sunlight to chemical energy is a process that happens quickly and very efficiently. It is a quantum mechanical process. The long-term goal is to understand this process and then use it in photovoltaic cells. Superconductivity and magnetism are quantum mechanical phenomena. Quantum computers may help us understand them better. One goal is to develop superconductors that donât need to be cooled to near absolute zero. The actual construction of quantum computers is in its infancy, but even with a few qubits it is possible to begin studying chemistry. IBM recently simulated the molecule beryllium hydride (BeH2) on a seven-qubit quantum processor. This is a relatively small molecule with just three atoms. The simulation does not use the approximations that are used in the classical computational approach. However, since IBMâs processor uses just a few qubits, it is possible to simulate the quantum processor using a classical computer. Consequently, everything that can be done on this quantum processor can be done classically. However, as processors incorporate more qubits we get to the point where it is no longer possible to simulate them classically. We will soon be entering a new era when quantum simulations are beyond the power of any classical computer. Now that we have seen some of the possible applications, we will briefly survey some of the ways that are being used to build quantum computers. Hardware To actually make practical quantum computers you need to solve a number of problems, the most serious being decoherenceâthe problem of your"
289,1728,0.984,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","The preclosure operator discussed above covers the non-sequential pomsets in the language e  f CKA ; it remains to find a term that covers the sequential pomsets contained in e  f CKA . To better give some intuition to the construction ahead, we first explore the observations that can be made when a sequential pomset W Â· X appears in the language e  f CKA ; without loss of generality, assume that W is nonsequential. In this setting, there must exist U â eBKA and V â f BKA such that W Â· X â U  V . By Lemma 3.6, we find pomsets U0 , U1 , V0 , V1 such that W â U0  V0"
307,478,0.984,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"15.1.5 Markov Models in Terms of Systems of Differential Equations The model of the action potential for a whole cell is a system of ordinary differential equations. For parts of the system this is clear from the equations, but for the Markov models, this may seem unclear. In Sect. 1.3 we explained how to formulate a system of ordinary differential equation associated with the reaction scheme defining a Markov model. Since the Markov models considered in the present chapter are considerably more complex, we will give one more example of this transition in order to clarify matters. To this end, consider the Markov model presented in Fig. 15.5. The associated system of ordinary differential equations governing the"
84,462,0.984,Eye Tracking Methodology,"18.1.6 Task Selection Task selection may be the most critical consideration of all, particularly for eye tracking studies. Because eye movements (and attention) are deployed as a combination of bottom-up (stimulus-driven) and top-down (goal-driven) cognitive processes, the nature of the task will influence eye tracking outcomes (the scanpaths). Eye movements are task-dependent and therefore the tasks must be chosen with care. Task selection depends on whatever human activity is being studied. Eye movements will generally be related to whatever action the participant is involved in, and therefore, the visual task may be rather specific. For example, visual search is a very specific visual task, and is distinct from other tasks such as âfree viewingâ a piece of art. Because the nature of the task will influence recorded eye movements, the task(s) issued to participants must be clearly defined and subsequently stipulated in the research report. During the actual running of the experiment, to maintain consistency, it is often a good idea to script the task instructions to participants. Related to the tasks given to participants, the type of stimulus presented to participants must also be decided ahead of time and reported in sufficient detail following the study. Stimulus types include static images (how many, how were they created, what are the differences between them, etc.), dynamic images (image sequences, videos), Web pages (browser), virtual environments, natural environments, etc."
303,118,0.984,Multiculturalism and Conflict Reconciliation in the Asia-Pacific,"What becomes the central theme in comprehending this complex system is the âperiphery,â as in the case of World Englishes. By focusing on the periphery of the system, it becomes clear that âinclusiveness,â âmediation,â and âdifferentiationâ are the essential characteristics of a tribute structure of multiple centers (Hamashita, 1994, p. 3). According to Hamashita, what characterized the indispensable functions that those peripheries performed in the tributary system was the spontaneous relationship among ports. The intricate network was composed of a web of maritime trading routes between the center and the peripheries, and between one periphery and another, in each case on a bilateral basis. Unlike the general perception that prevails in the contemporary geographical understanding of oceans, which sees them as obstacles and impediments to trading, the perception presented by Hamashita (2003) is one in which the oceans shaped a public sphere in Asia before the sudden arrival of Western modernity and civilization. The regionâs socio-political and socio-economic dynamism was mainly generated at the peripheral areas instead of at the center. This is because there were multiple centers in the system; the entire structure of tribute-cum-trading was constructed on the premise of multiple circles overlapping with the major system (of concentric circles) and with each other. The peripheries thus occurred at the intersection of the various circles, and were characterized by mixed cultures. The above analysis reveals the blurred core of the tribute system, and resembles the concept of diffuse centers of World Englishes. The World Englishes theory suggests that the dynamism of English is mainly generated in the âouter circle.â Similarly, the primary functioning part of the tribute-cum-trading system was the periphery. This in turn reveals the inflexible perception of the hegemony of contemporary IR, and raises serious doubts about the unquestioned superiority of the subjectivity residing at the core of the system of IR. How could we theorize the blurred subjectivity evident in both the theory of World Englishes and the tribute-cum-trading system? This vague image of the subject is completely opposed to that assumed by traditional Western philosophyâan autonomous and sovereign subject with strictly demarcated boundaries. In order to find a possible answer to this question, we have to go back to what Nishida Kitaro calls the âplace of nothingness.â Nishida argues that the subject is not autonomous or independent. The subject in the ordinary sense is always to open to society and depends on relationality for its construction. However, at the same time, this subject encompasses all the relationality that appears to the subject. Therefore, the subject is"
271,420,0.984,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"The examples seem to suggest that participants of text-based computer-mediated decision interactions are not able to create practices of relatednessâpractices that rest on triadic constellations of interactions, non-verbal aspects of communication and micro-scale acts of reciprocal reassurance. And although they are able to reach a joint decision, the dominance ofârather unstableâdyadic constellations even leads to a point where two participants engaged in a dialogue are not sure whether the other participants are still present. Obviously, the above-described practices of relatednessâpractices that exhibit a complex, triadic, synchronous structureâcannot be performed or substituted in a chat environment. And although there are more sophisticated platforms for online deliberation available, difficulties remain (see for example Klein 2011; Spada et al. 2014).16 Furthermore, empirical evidence from various areas of research in political science suggests that David Graeber might be right when he concludes that âdecision making is the one thing that is almost impossible to do on the Internetâ (Graeber 2009: 237)âor while using other forms of technically mediated communication."
58,235,0.984,Enabling Things to Talk,"In order to track and monitor Physical Entities, they have to be identified. There are basically two ways for how this can be done, as is very well described in (Furness 2009): Using either natural-feature identification (classified as âprimary identificationâ) or using some type of Tags or labels (classified as âsecondary identificationâ) that are attached to the Physical Entity. Both means of identification are covered in the IoT Domain Model. Tags are modelled as Devices that explicitly identify a Physical Entity. Natural-feature identification can be modelled, for example, by using a camera â a kind of Sensor â that monitors the Physical Entity and an additional Resource that does the natural feature extraction (i.e. a dedicated software component). The result of the naturalfeature extraction can be used as search term for looking up the corresponding Virtual Entity. RFID Tags are a prominent example in IoT. As they come with their own electronic circuitry it seems quite natural to classify RFID Tags as Devices in terms of the IoT Domain Model. The case is less clear-cut regarding the classification of a barcode label, however. As pointed out elsewhere (Haller 2010), classifying a barcode label as a Device seems a little far-fetched; regarding it as a ânatural featureâ of the Physical Entity it is attached to, seems to be more appropriate. However, as with many modelling questions, this is a matter of taste â the IoT Domain Model is not prescribing which variant to use."
82,189,0.984,Fading Foundations : Probability and The Regress Problem,"governing the process is regular, and the iteration is guaranteed by Markov theory to converge to the solution of the fixed point, pâ = Î² + (Î± â Î² )pâ . However, this quick route to (3.17) only works when the conditional probabilities are the same from step to step: in the general case that we consider in the next section Markov theory does not help, which is why we have not used it here. We shall discuss fixed points more fully in Sections 8.4 and Appendix D. 28 This example shows that James Van Cleveâs defence of Lewis, and thereby his attack on Reichenbach, is mistaken (Van Cleve 1977). Van Cleve argues that an infinite iteration of the rule of total probability must be vicious, because âwe must complete it before we can determine any probability at allâ (ibid., 328). But our counterexample to Lewis demonstrates that an infinite iteration may well be completable, in the sense that it is convergent and can be summed explicitly, yielding a definite value for P(q)."
234,190,0.984,Mobile Professional Voluntarism and international Development : Killing Me Softly?,"experiences. This level of analysis has supported our ability to design very practical evidence-based interventions. This concept of bounded rationality (in the economic model) is perhaps far more relevant than the architects of these papers anticipated. One of the limitations of the papers, perhaps reï¬ecting the level of abstraction, is the lack of attention to conï¬ict and power: the possibility of multiple realities and parallel organisational cultures. Corruption is systemic in Uganda especially within the public sector; it can be described as a culture. It is pervasive and starts from the very top of organisations and the systems within which they are based and operates through powerful, organised syndicates. Health workers (and patients) are acutely aware of its existence, the personal beneï¬ts that derive from it and the profound risks associated with challenging it. It is interesting to note the reference by MuÃ±oz et al. to the entrepreneur as a âdestabilising agentâ (2011: 199). âDestabilisingâ in this context could imply creative disruption triggering innovation (in the right direction). Alternatively, or simultaneously, it could refer to the impact of corruption. This detailed (tacit and explicit) knowledge of how corruption works at every level and in every decision necessarily shapes both imagined realities and action plans. In this sense, there may be two parallel systems operating in marked tension with each other within a health facility or authority. Chapter 3 has discussed the impact of corruption on professional volunteers and their relationships with Uganda health workers identifying the dynamics of power and hierarchy (positionalities). None of the theories reviewed pays explicit attention to these dimensions of context. Interestingly it is often the lack of knowledge on the part of foreign agencies and individual volunteers rather than their superior clinical knowledge that limits impact and generates unintended consequences.23 Of course, corruption pollutes not only organisations but also the wider system that nurtures it, fundamentally weakening a sense of identity with the state at national or local level and also with âleadersâ (at every level). Whilst we can identify closely with the concept of âaction planningâ as a vehicle for the exercise of individual agency based on the recombination of disparate knowledges, we have some concerns that the emphasis in the material reviewed, perhaps reï¬ecting the business/private sector context, fails to explain inaction or stasis. Or, situations when human action, qua rational, as MuÃ±oz and Encinar (2014a: 75) put it, could amount to non-decisions or inaction. On a practical level, it may prove impossible to imagine returns on an investment (in training, for example,"
262,98,0.984,"Reality Lost : Markets of Attention, Misinformation and Manipulation","The context is different, but âsignal legislationâ are bills: 1. Whose primary goal is to signal a specific point of view 2. Proposed with no real interest in the consequences 3. Often, with no interest in the real scope of the problem (Elholm 2011) Signal legislation has the potential for political speculation as it is immune to, and detached from, what other experts or science might have to say. The point is not about the facts, the particulars, or calculating the consequences of a legislation"
93,193,0.984,Nordic Mediation Research,"7 Linguistic Analysis As we conducted our analysis of substance and creativity, we found the agreements strikingly similar in structure and language. This motivated us to do a third analysis of our material: an examination of the âverbal wrappingâ of the mediated agreements. We focused our investigation on examining whether the agreements reï¬ect a standard linguistic practice and, if so, what the characteristics of this standard practice are. In the tradition of critical discourse analysis and among others Fairclough, we understand language as not just a neutral tool that depict a reality (Fairclough 1992). Rather, we understand language as a social practice that plays a role in shaping our perception of identities, roles, social relations etc. (JÃ¸rgensen and Phillips 1999; TÃ¸nnesson 2008). Hence, we can learn something about a practice by studying the discourse used in that practice. Nordic mediation literature has different approaches as to who should write the agreement. Some ï¬nd that agreements are naturally written up by the courtconnected mediator (e.g. Kjelland-MÃ¸dre et al. 2008), while others ï¬nd that the parties and their advisors should do the writing (e.g. JÃ¸rgensen and Lavesen 2016). In all of the mediations in our observational studies of court-connected mediations meetings mentioned above (Adrian 2012; Mykland 2011), either the mediator or the partiesâ lawyers authored the agreement. Based on these observations combined with the linguistic appearance of the agreements in our dataset laid out below, it seems safe to assume that either the mediator or the lawyer drafted the mediated agreements that we analyse."
297,1106,0.984,The R Book,"Regression analysis is the statistical method you use when both the response variable and the explanatory variable are continuous variables (i.e. real numbers with decimal places â things like heights, weights, volumes, or temperatures). Perhaps the easiest way of knowing when regression is the appropriate analysis is to see that a scatterplot is the appropriate graphic (in contrast to analysis of variance, say, where it would have been a box-and-whisker plot or a bar chart). We cover seven important kinds of regression analysis in this book:"
307,468,0.984,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"15.1.2 Definition of Calcium-Related Fluxes We need to define all the fluxes entering the system (15.2)â(15.6) and we start with the simple diffusion fluxes. Some of them have been used in earlier chapters, but we need a little more notation here, so we redefine all the terms. 15.1.2.1 Flux Jd;c from the Dyad to the Cytosol We assume that the pure diffusion flux from the dyad to the cytosol can be written Jd;c D kd;c .cd"
28,40,0.984,A History of Self-Harm in Britain,"This effort to homogenise and assimilate might well have a present utility, as well as broadly progressive political effects, as in the case of Canadian deserters and PTSD. However, in order for objects to have such a transhistorical and decontextualised existence, their conditions of production must be obscured; in other words, they only make sense outside of context â utterly unhistorical. Thus the meaning of an epidemic of communicative attempted suicide is more precisely stated: a specific understanding of behaviour, inseparable from its context. Having established the importance of context in general, the specifics can now be tackled. This object emerges from of a complication of behaviour previously thought to be suicidal. Behaviour that presents at hospital ostensibly as an attempt to inflict death upon oneself is recast as a communication, as an appeal to a social setting. To understand how this attempted suicide comes to be a clinical, statistical and an epidemiological object, we must compare the historical and institutional contexts through which self-harm and completed suicide are accessed and analysed in Britain in the nineteenth and twentieth centuries. Mid-twentieth-century suicide strongly resembles its nineteenthcentury counterpart as it is accessed through coronersâ court records. However, objects called attempted suicide in the nineteenth and twentieth centuries (including communicative overdoses in the latter period) are separated by a profound difference, with far-reaching consequences."
305,147,0.984,Quantum Computing for Everyone,"Looking at AND, if we get an output of 1, then we know that the input values must have been both 1, but if we get an output value of 0 there are three pairs of input values that have this output, and if we are not given any other information, we have no way of knowing which one of the three possibilities was actually input. Consequently, AND is not a reversible gate. The half-adder is also not reversible. There are two pairs of input values that give a digit of 1 and a carry of 0. In both of these cases we have two bits of input, but are not getting two bits of output. We have lost some information doing the computation. The study of reversible gates and reversible computation began by looking at the thermodynamics of computation. Shannon defined entropy for information. Entropy is also defined in thermodynamics. In fact, this is where Shannon got the idea. How closely are these two entropies related to one another? Can some of the theory of computation be expressed in terms of thermodynamics? In particular, can one talk about the minimum energy required performing a calculation? John von Neumann conjectured that when information was lost energy is expendedâit dissipates as heat. Rolf Landauer proved the result and gave the minimum possible amount of energy to erase one bit of information. This amount of energy is called the Landauer limit. If the computation is reversible, however, no information is lost and theoretically it can be performed with no energy loss. We will look at three reversible gates: the CNOT, Toffoli, and Fredkin gates. Controlled Not Gate The controlled not gate or CNOT gate takes two inputs and gives two outputs. The first input is called the control bit. If it is 0, then it has no effect on the second bit. If the control bit is 1, it acts like the NOT gate on the second bit. The control bit is the first input bit and denoted by x. This bit is not changed and becomes the first output. The second output equals the second input if the control bit is 0, but it is flipped when the control bit is 1. This function is given by f ( x, y ) = ( x, x â y ) or , equivalently , by the following table."
13,81,0.984,Feeling Gender : a Generational and Psychosocial Approach,"concept of cohort seems to be more straightforward. However, Alwin and McCammon (2004: 25, 27) also make the opposite argument: the concept of generation has more potential for understanding the origins and nature of social change as generations are more a matter of quality than degree. Even though cohort and generation are not the same, it may still be the case that cohort effects are âgiven lifeâ though interpretative and behavioural aspects of the generation. In this way the combination of cohort and generation may also contribute to a better understanding of the agency of a given cohort. This inclusive approach to cohort and generation makes sense in relation to our study. Some patterns emerge from informants in a generation clustering around certain birth cohorts, while others may emerge from informants sharing similar conditions or formative experiences as children and youth. Informants who are on the margins of these main clusters may show something important about the gradual character of the process of generational change: the oldest informants in the middle generations often combine patterns from the oldest and the middle generations, for instance, when it comes to obligations to their parents or the work ethic. This again leads to a perception in their children of having âold parentsâ with different ideas and values than other parents, and by this the child may also have a modernising effect on the parents and contribute to further modifying them as border figures between historical generations. In this way one can argue that a generational data set with âfuzzy edgesâ may actually be an advantage to understand the gradual emergence of generational patterns. Used with methodological precautions, the value of the study of intergenerational transmission in a historical context is that it may grasp some of the more elusive aspects of social reproduction and change. As Alwin and McCammon (2004: 42) conclude: âGenerations lack specific boundaries and are meaningful in their distinctiveness largely as subpopulations, but offer the potential of being used as powerful explanations in and of themselves for distinctive patterns of attitudes, beliefs, and behaviorsââand, once again, I would add feelings. Another consequence of not being able to control the characteristics of the generational samples derived from the anchor generation is that the class composition of each generation becomes unequal. In our case the social composition of the different generational groups reflects the"
124,288,0.984,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"2 Action, Indeterminism, and Facing the Future I will first give some reasons for thinking this is the case regarding the logical framework of FF before turning to further issues that have to be addressed in order to fully answer the Intelligibility Question. First, there are a number of issues and topics in the philosophy of action related to free will that are made more precise by the stit logic developed in FF, which philosophers who deal with action theory (usually only in informal ways) would do well to take note of. The distinction between the achievement stit and the deliberative stit (pp. 32â40) is particularly important in my view for discussing issues about free will. The achievement stit involves an earlier moment of choice or action that guarantees the later outcome A of an action. The deliberative stit, by contrast, is evaluated at the moment of choice itself, the very moment at which the agent sees to it that the outcome A will occur. The outcome A is guaranteed by the present choice at the moment of choice itself. Both achievement"
311,718,0.984,The Physics of the B Factories,"B physics The main objective of the B Factories was to perform measurements of the decays and CP asymmetries of B mesons. While the asymmetric set-up and high luminosities of the B Factories allowed us for the ï¬rst time to perform statistically signiï¬cant measurements of time-dependent CP asymmetries, the symmetric predecessors of the B Factories, DORIS and CESR, had already produced some data on B decays. Experiments at LEP and the Tevatron had provided a proof of principle of the time-dependent CP asymmetry measurement in the golden mode B 0 â J/ÏKS0 , and improved our knowledge of Bd0 mixing. Most of the time, the B Factories took data near the Î¥ (4S) resonance, which decays almost exclusively into B 0B 0 and B +B â pairs. As a consequence, the overwhelming majority of B Factory measurements relate to these states: these measurements are described in the following sections. However, some data has been taken at the (â)0 (â)0 Î¥ (5S) resonance, which also decays into Bs B s pairs. Measurements of Bs decays performed with this data are discussed in Chapter 23. There are many ways to arrange this vast amount of material. The scheme adopted for this book uses the Unitarity Triangle as an organizing principle. We start from a discussion of the ways the sides of the triangle are constrained, including theoretical methods as well as experimental results in the corresponding sections. Hence we start with the measurements determining the magnitude of the CKM matrix elements Vcb , Vub , Vts , and Vtd . This is followed by a discussion of the decay rates of charmed and charmless non-leptonic processes, including a comparison with theoretical expectations. The reason for this is that many charmed and charmless non-leptonic decay modes are used in the measurement of CP asymmetries, and therefore should be discussed before moving on to review work related to the angles of the Unitarity Triangle. Before treating the CP asymmetries related to the angles of the Unitarity Triangle, we discuss measurements of B lifetimes and B 0 â B 0 mixing, which are needed to understand the time-dependent analyses performed for the extraction of the angles. Searches for CP T and other symmetry violations which are based on the lifetime- and mixing-measurement techniques are then presented. Following on from this one will ï¬nd the description of measurements of CP violation, i.e. the extraction of the angles Ï1 , Ï2 , and Ï3 . The end of this chapter is devoted to special processes. These are either rare decays related to ï¬avor changing neutral current transitions of the b quark, processes involving Ï leptons or baryons in the ï¬nal state, or decays which are very rare or forbidden in the Standard Model."
13,46,0.984,Feeling Gender : a Generational and Psychosocial Approach,"society as a whole and by every single individual. The idea that changes can happen on any level of society is particularly relevant to my argument about the generative role of feelings of gender. No structure has priority and subtle changes may take place before they are properly understood. Williams offers the example of gradual changes in literary style. Such small elements of qualitative changes are not necessarily epiphenomena to institutional changes or just accidental variation. They are social in two ways: they are âchanges of the presentâ and they are effective before they are classified and understood (Williams 1977: 132â133). Williamsâ concept of structure of feeling, his take on incremental changes and how they can happen anywhere in a system are highly generative ideas for my analysis of how patterns of feelings in different generations, genders and classes represent an agency that may contribute in creative ways to the gradual change of gendered practices. However, the concept of feelings needs more psychological elaboration in order to become operative in a study of lived lives."
82,300,0.984,Fading Foundations : Probability and The Regress Problem,"where the arrow represents entailment, where A0 does duty for the target, q, and where Am+1 stands for the foundation or ground. Then of course the only way to know for sure if A0 is true is by knowing that Am+1 is true. In the words of Aikin: âConceptual arguments start from the deep, and I think right, intuition that epistemic justification should be pursuant of the truthâ.15 But if we are ignorant of the truth or falsity of the ground, Am+1 , we are groping in the dark about the truth value of A0 . When we make chain (6.1) infinite, so that it looks like: A0 ââ A1 ââ A2 ââ A3 ââ A4 ââ . . ."
359,152,0.984,"Micro-, Meso- and Macro-Dynamics of the Brain","Late Metastable Activity as a Signature of Consciousness Why does the global response to auditory novelty track conscious processing? We have hypothesized that conscious perception corresponds to the entry of information into a global neuronal workspace (GNW), based on distributed associative areas of the parietal, temporal and prefrontal cortices, that stabilizes information over time and broadcasts it to additional processing stages (Dehaene and Naccache 2001; Dehaene et al. 2003, 2006). Even if the incoming sensory information is very brief, the GNW transforms and stabilizes its representation for a period of a few hundreds of milliseconds, as long as is necessary to achieve the organismâs current goals Such a representation has been called âmetastableâ (Dehaene et al. 2003) by analogy with the physics of low-energy attractor states, where metastability is defined as âthe phenomenon when a system spends an extended time in a configuration other than the systemâs state of least energyâ (Wikipedia). Similarly, conscious representations are thought to rely on brain signals that persist for a long duration, yet without being fully stable because they can be suddenly replaced as soon as a new mental object becomes the focus of conscious thought. The brain activity evoked by global auditory violations in the local/global paradigm fits with this hypothesis. First, this signal is only present in conscious subjects who can explicitly report the presence of deviant sequences. Furthermore, this signal is late, distributed in many high-level association areas including prefrontal cortex, and stable for an extended period of time (Bekinschtein et al. 2009). The latter point is particularly evident in temporal generalization matrices, which show that the global effect, although triggered by a transient auditory signal (a single 150-ms tone), is reflected in a late and approximately square (Fig. 3) or thick-diagonal (Fig. 4) pattern of decoding Such a pattern indicates that the evoked neural pattern is stable over a long time period. Our results indicate that the neural activation pattern can be either quasi-stable for hundreds of milliseconds (as occurs in Fig. 3, where subjects simply had the instruction to attend to the stimuli), or slowly changing with considerable temporal overlap among successive neural codes (as occurs in Fig. 4, where subjects were instructed to perform a motor response to global deviants, thus enforcing a series of additional decision, response and monitoring stages). Many additional paradigms have revealed that conscious access is associated with an amplification of incoming information, its transformation into a metastable representation, and its efficient propagation to subsequent processing stages (Del"
289,2041,0.984,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","If B1 bl B2 holds, then the compiler can replace block B2 with block B1 irrespective of the whole program, i.e. B2  B1 is a valid transformation. Thus, deciding B1 bl B2 is the core problem in validating compositional transformations. The language semantics is highly significant in determining observational refinement. For example, the code blocks B1 : store(x,5) and B2 : store(x,2); store(x,5) are observationally equivalent in a sequential setting. However, in a concurrent setting the intermediate state, x = 2, can be observed in B2 but not B1 , meaning the code-blocks are no longer observationally equivalent. In a relaxed-memory setting there is no global state seen by all threads, which further complicates the notion of observation. Compositional Verification. To establish B1 bl B2 , it is difficult to examine all possible syntactic contexts. Our approach is to construct a denotation for each code-block â a simplified, ideally finite, summary of possible interactions between the block and its context. We then define a refinement relation on denotations and use it to establish observational refinement. We write B1 â B2 when the denotation of B1 refines B2 ."
360,248,0.984,Compositionality and Concepts in Linguistics and Psychology,"6 Conclusion Our results point to noun context effects on the interpretation of color adjectives, whose meanings show shifting boundaries for truth-judgments. Color adjectives may seem context-independent and intersective for many categories when the category-speciï¬c color spaces converge, but when we consider color judgments for categories with an intrinsic color bias in the real world, we observe context-dependent truth-judgments in uses of color terms. Similar effects of extensional feedback (Hampton 1988) in truth and categorization judgments may arise for many other adjective classes that have traditionally been analyzed as intersective.2 Compositional processes that go beyond classical logic and set theory (such as Boolean conjunction and set intersection) are so pervasive in natural language that they cannot simply be set aside as a peripheral issue in semantic theory and pose a serious challenge to accounts of meaning composition as set intersection (e.g., Chierchia and McConnell-Ginet 1996; Heim and Kratzer 1998). It would also be important in future research to pursue further the compositional processes at varying degrees of frequent and conventionalized adjective-noun combinations. Dynamic, context-dependent ârecalibrationsâ of a predicate meaning (Kamp and Partee 1995) or modiï¬cation of a comparison class to which a predicate applies (Klein 1980) seem to point to general processes of meaning composition in any domain where we have extensional feedback based on our world knowledge, not limited just to a small class of vague predicates. Experimental studies in categorization and reasoning have made strides in mapping our conceptual space (GÃ¤rdenfors 2000) and ï¬ne-tuning our ideas about the combination of different conceptual dimensions. Fine-grained quantitative comparisons in the degree or amount of overextension between our study and earlier ones, especially Hampton (1996), would be difï¬cult due to the subtlety of color space and color manipulation (or the lack of detailed descriptions of stimuli in Smith and Osherson 1984). A combination of tasks, such as truth-value judgments, color shift judgments in simultaneous presentation of multiple colors, forced-choice preferences between given colors, and phrase-picture matching tasks, should take us closer to better understanding of concept combination involving color adjectives. There are important additional insights and challenges from the theoretical literature on color-term interpretation. One is the source or ontological status of the colorsâi.e., whether they represent two distinct kinds (e.g., brown vs. black horses) or two stages of the same kind (green vs. red tomatoes that ripen over time). Kennedy and McNally (2010) argue that color adjectives are ambiguous between a gradable reading (denoting a degree scale for the color quality/quantity) and a non-gradable one (denoting a binary presence/absence of an underlying property"
80,635,0.984,Innovations in Quantitative Risk Management (Volume 99.0),"2 Assessing Structural Risk We now turn to the second topic of the paper, on how to assess structural changes in a model computationally. This is based on dynamical systems, in which the dynamical behavior depends on a certain model parameter. Critical threshold values of this parameter will be decisive. Below we shall understand âstructural riskâ as given by the distance to the next threshold value of the critical parameter. An early paper stressing the role threshold values (bifurcations) can play for a risk analysis is [11]. The approach has been applied successfully in electrical engineering for assessing voltage collapse, see [3]. We begin with recalling some basic facts from dynamical systems."
253,776,0.984,"Autonomous Driving : Technical, Legal and Social Aspects","understanding of this process and improve applied machine-learning methods. With reference to the currently understood difference between both forms of learning, machine learning is understood in this chapter as an algorithm generated by a human. The running of the software follows these algorithms, just as with all other software. Our objective is not to compare a learning human with a learning robot. Rather, it is to discuss why, whether, and with which challenges and approaches machine learning is possible in its current form in autonomous driving. This chapter portrays the view of vehicle technology in particular on this question, and is based on experience from the literature for the area of machine learning."
28,68,0.984,A History of Self-Harm in Britain,"particular context enables violence to be consistently invoked. Thus the potential for violence emerges between therapeutic techniques. The Manchester Guardianâs report emphasises the financial aspect over the therapeutic dispute.27 However, rather than reduce the significance of the case to any one primary cause, it is useful to sketch out the arguments pursued in these different registers. The arguments that reach the Home Office are more likely to involve the spending of public money and the police, whereas those issues recede in a coronerâs court where it is a question of establishing fault or not in a particular death. This becomes transposed onto the technical question of facilities (which is accepted by both parties) and the question of facilities best equipped to deal with violence. The point is to lay out a field of argument, structured by a specific mental/physical divide, where attempted suicide emerges."
113,224,0.984,Pentecostalism and Witchcraft : Spiritual Warfare in Africa and Melanesia,"Here, we can see echoes of the claim made in the introduction of this book that âenvy in a fundamental way articulates the crucial claim to equality that is fundamental to both consumerism and to the relation between persons and Godâ (p. 22), as well as fundamental to Trobriand sociality. Perhaps Pentecostalism is in a way a more productive (in the emic view) means of accessing what Englund calls the radical promise of equality inherent in such a practice of Christian fellowship than the destructive, consumptive, and threatening power of the witch (Englund 2003)."
223,329,0.984,Knowledge and Action (Volume 9.0),"a value in one dimension without giving it a value in one or more others. For example, an object cannot be given a hue without also giving it a brightness, and the pitch of a sound always goes along with its loudness. Dimensions that are not integral are said to be separable, as is the case with the size and hue dimensions. This distinction allows a domain to be defined as a set of integral dimensions separable from all other dimensions. The notion of a domain has been used to some extent in cognitive linguistics (e.g., Croft, 2002; Croft & Cruse, 2004; Langacker, 1986). Langacker (1986) presented his notion of a basic domain as follows: It is however necessary to posit a number of âbasic domains,â that is, cognitively irreducible representational spaces or fields of conceptual potential. Among these basic domains are the experience of time and our capacity for dealing with two- and three-dimensional spatial configurations. There are basic domains associated with various senses: color space (an array of possible color sensations), coordinated with the extension of the visual field; the pitch scale; a range of possible temperature sensations (coordinated with positions on the body); and so on. Emotive domains must also be assumed. It is possible that certain linguistic predications are characterized solely in relation to one or more basic domains, for example, time for [BEFORE], color space for [RED], or time and the pitch scale for [BEEP]. However most expressions pertain to higher levels of conceptual organization and presuppose non-basic domains for their semantic characterization. (p. 5)"
297,1740,0.984,The R Book,"r if I reject the null hypothesis at the 5% level, then there is a 5% chance that the null hypothesis is true; r if I establish a 95% conï¬dence interval for the value of a parameter, then the value of the parameter lies within these bounds with probability 0.95. Of course, both these assertions are wrong, but they are wrong in ways that are genuinely hard for beginners to understand. In the ï¬rst case, the p value (say 0.05) means that a test statistic as large, or larger, than the The R Book, Second Edition. Michael J. Crawley. Â© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd."
13,432,0.984,Feeling Gender : a Generational and Psychosocial Approach,"the modern family (Benjamin 1995).3 The three psychological models were formulated during or in the aftermath of the childhoods of the three generations, and even if the fits are not seamless, the relatively better match between the models and the childhoods from the same period is notable.4 This indicates that it is not only the theories that have changed as a result of critical work, but also the gendered psychologies they set out to describe. My claim is not that the Oedipal constellation was not present for the younger generations or that issues of separationâindividuation were not relevant for the oldest. Not everyone in these generations fits into the same psychological models. As we have seen, there are important differences depending on social class, and definitely also on the many particular ways in which mothers and fathers transmit gender to their children. Therefore, there are always a variety of outcomes. But as has been consistently argued in this book, individual variation does not prevent social patterning across these variations. What emerges is an affinity between the generational patterns of gendered psychologies and the different theories of gender psychology. This suggests that the different psychological constellations and tensions described in these theories have had different impacts in different historical contexts. It also suggests that the reason these theories developed as they did was that they caught the contours of a changed generational pattern of the times in which they were formulated: new structures of feelings related to gender. Seen through a theoretical lens, the generational feelings of gender could be described as follows."
264,246,0.984,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,I have used quotation marks because the idea of success is always relative to a set of values and aims; these can be influenced by research results but are situated out of range of scientiï¬c validation.
229,59,0.984,Constructions of Cancer in Early Modern England,"Conclusion This chapter set out to answer an apparently simple query. What, I asked, did early modern people talk about when they talked about cancer? The firmest conclusion of the chapter is that this is a question worth posing, for we have seen the degree to which the concept of cancer was at once a malleable construction, and a disease of which the fundamental âcharacterâ remained stable even as medical practitioners debated its specifics. Visible throughout early modern sources on the naming, diagnosis and causes of cancer is the urge to turn this disease from a disparate and confusing collection of incidences into a singular and understandable entity. Thus, the often confusing language of cancer consistently returned to a single image, that of a biting creature; the symptoms of"
80,380,0.984,Innovations in Quantitative Risk Management (Volume 99.0),"7 Conclusion In this paper, we have shown that the positive and negative multivariate tail risk exposures present in currency carry trade baskets are additional factors needing careful consideration when one constructs a carry portfolio. Ignoring these exposures leads to a perceived risk return profile that is not reflective of the true nature of such a strategy. In terms of marginal model selection, it was shown that one is indifferent between the log Generalised Gamma model and the frequently used GARCH(1,1) model. However, in combination with the three different Archimedean copula models considered in this paper, the log Generalised Gamma marginals provided a better overall model fit. Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
271,599,0.984,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"The research interest of Klein et al. lies in establishing how media play a part in five life spheres: work, intimate relations, parenthood, asset building, and civil society engagement: âwhich role does the intervieweesâ media repertoires play in their conduct of life with respect to disturbances and copingâ (364). In pursuing this aim they are concerned with exploring how the participantsâ own horizon of relevance can be given priority: what method can ensure a large measure of openness while retaining the researcherâs thematic focus on media? The obvious problem faced by many qualitative researchers is that if researchers state their media-focused research interest in their initial framing of the interview and frequently during the course of the interview, âthey might steer the intervieweesâ response behaviourâ (366) and impose a media-centric focus which may not accurately reflect how the interviewees perceive media in the different life spheres. In order to find a solution to this problem, Klein et al. devise an impressively systematic research design, which takes into account that media-centrism is a continuum. They set up a taxonomic methodological system of media-centrism, which enables researchers to choose the degree of media-centrism that best serves the knowledge interest of their project (Table 15.1: 372). Out of the taxonomyâs nine possible interview strategies they select four for experimental testing, with an increasing order of media-centrism: â¢ Strategy 1: The least mediacentric interview strategy, in which the researcher does not mention media at all, waiting for interviewees to spontaneously bring media into the talk. â¢ Strategy 3: Media are mentioned in a non-conspicuous way at the beginning of the interview, and are only brought in at the very end after having dealt with the life spheres. â¢ Strategy 4: Media are mentioned in a non-conspicuous way at the beginning of the interview, and are brought in explicitly after each of the life spheres has been dealt with. â¢ Strategy 2: Media are emphatically mentioned at the beginning of the interview, and are also brought in explicitly after each of the life spheres has been dealt with. The analysis of these four interviewing strategies then looked for the prominence that media repertoires displayed in the participantsâ accounts"
217,74,0.984,Finite Difference Computing With Pdes : a Modern Software Approach,"The code segments are found in the file vib_empirical_analysis.py. Since a[i] and p[i] correspond to the i-th amplitude estimate and the i-th period estimate, respectively, it is most convenient to visualize the a and p values with the index i on the horizontal axis. (There is no unique time point associated with either of these estimate since values at two different time points were used in the computations.) In the analysis of very long time series, it is advantageous to compute and plot p and a instead of u to get an impression of the development of the oscillations. Let us do this for the scaled problem and Ât D 0:1; 0:05; 0:01. A ready-made function plot_empirical_freq_and_amplitude(u, t, I, w)"
360,319,0.984,Compositionality and Concepts in Linguistics and Psychology,"The average oddness rate for this pair of verbs was 3.39 on a scale between 1 and 6, where 1 meant ânot odd at allâ and 6 meant âphysically impossibleâ. We conclude that such a situation is atypical for the concepts WALK and WRITE. Nevertheless, the situation is highly acceptable as a legitimate instance for each of those concepts: when Lucy happens to be walking and writing at the same time, we do not expect many speakers to reject the description âLucy is walkingâ just because the event is atypical for the concept WALK. Similarly we do not expect the sentence âLucy is writingâ to be rejected if she happens to be walking at the same time. To describe the dissociation between judgements about vagueness and typicality, it is common to associate concepts with typicality functions. Similarly to our treatment of acceptability functions, we will use typicality functions that map situations to a value between 0 and 1.9 The typicality value assigned to a situation may be different than the value assigned by the acceptability function. Summarizing the judgements in Examples 1â3 above, we make the following assumptions on the typicality and acceptability functions. For the concept HAIR in Example 1, let S1 be a situation with a focally red hair. Based on the observations above, we denote: ACCHAIR (S1 ) â 1 and TYPHAIR (S1 ) âª 1. For the concept SHAKE in Example 2, let S2 be the situation in Fig. 2b. We denote: ACCSHAKE (S2 ) â 1 and TYPSHAKE (S2 ) âª 1. For the concepts WALK and WRITE in Example 3, let S3 be a situation where Lucy is walking and writing simultaneously. We denote: ACCWALK (S3 ) â 1 and TYPWALK (S3 ) âª 1, and similarly for WRITE. With this background on concepts, their vagueness and typicality effects, we can move on to the question of concept composition."
95,542,0.984,Elements of Robotics,"16.5.4 Multiple Rotations There is a caveat to composing rotations: like matrix multiplication, three-dimensional rotations do not commute. Let us demonstrate this by a simple sequence of two rotations. Consider a rotation of 90â¦ around the z-axis, followed by a rotation of 90â¦ around the (new position of the) x-axis (Fig. 16.16). The result can be expressed as (up, out, right). Now consider the commuted operation: a rotation of 90â¦ around the x-axis, followed by a rotation of 90â¦ around the z-axis (Fig. 16.17). The result can be expressed as (out, left, down), which is not the same as the previous orientation."
293,18,0.984,"integration Processes and Policies in Europe : Contexts, Levels and Actors","The Study of Integration Processes1 A Definition of the Concept We define integration as âthe process of becoming an accepted part of societyâ. This elementary definition is intentionally open in two regards. First, it emphasizes the process character of integration rather than defining an end situation. Second, in contrast to the normative models developed by political theorists, it does not specify beforehand the degree of or even the particular requirements for acceptance by the receiving society. This makes the definition highly useful for empirical study of these processes. Measuring the degree of becoming an accepted part of society will allow us to capture the diversity of (stages of) the process. We do need to specify within this basic definition what should be measured; that is, what are the indicators of integration and where might we find them."
141,38,0.984,Cyber-Physical Systems of Systems: Foundations â A Conceptual Model and Some Derivations: The AMADEOS Legacy,"Many systems store information about their interactions with the environment (since the start of a system with a clean memory) and use this information to influence their future behaviour. State: The state of a system at a given instant is the totality of the information from the past that can have an influence on the future behaviour of a system. A state is thus a valued data structure that characterizes the condition of a system at an instant. The concept of state is meaningless without a concept of time, since the distinction between past and future is only possible if the system is time-aware. Stateless System: A system that does not contain state at a considered level of abstraction. Statefull System: A system that contains state at a considered level of abstraction."
297,536,0.984,The R Book,"The best way to identify multiple individuals in scatterplots is to use a combination of colours and symbols. A useful trick is to use as.numeric to convert a grouping factor (the variable acting as the subject identiï¬er) into a colour and/or a symbol. Here is an example where reaction time is plotted against duration of sleep deprivation for 18 subjects: data <- read.table(""c:\\temp\\sleep.txt"",header=T) attach(data) plot(Days,Reaction) I think you will agree that the raw scatterplot is uninformative; the individuals need to stand out more clearly from one another. The main purpose of the graphic is to show the relationship between sleep deprivation (measured in days) and reaction time. Another aim is to draw attention to the differences between the 18 subjects in their mean reaction times, and to differences in the rate of increase of reaction time with the duration of sleep deprivation. Because there are so many subjects, the graph is potentially very confusing. One improvement is to join together the time series for the individual subjects, using a non-intrusive line colour. Let us do that ï¬rst. We need to create a vector s to contain the numeric values (1 to 18) of the Subject identity numbers (which range, with gaps, between 308 and 372): s <- as.numeric(factor(Subject)) This vector will be used in subscripts to select the x and y coordinates of each subjectâs time series in turn. Next, the subjects, k, are taken one at a time in a loop, and lines with type=""b"" (both points and lines) are drawn in a non-intrusive colour (gray is useful for this):"
123,6,0.984,Fallibility At Work : Rethinking Excellence and Error in organizations,"is able to convince him that they are any better, since their claims to wisdom do not hold up under critical scrutiny. Socrates, at least, admits that his beliefs about the world and society may turn out to be false, and so he may be the wisest, in the sense that he realizes the limitations in his own convictions and beliefs about the world (Plato, 1966). There is a Socratic quality to the intervention from the pushback tractor driver at the airport, a willingness to challenge a person in power, in the name of doing things right together. He persists with his questioning even after he has experienced rejection and irritation from the higher ranked person on the receiving end of his messages. Socratic philosophy is practical at heart, both in the sense that it can address concrete questions about how one should act and live, here and now, and in the wider sense of being oriented toward the goal of leading a richer and better life. It rests on an assumption that an examination of personal beliefs, desires, and habits can lead to significant breakthroughs of knowledge regarding how to live a good life. You may come to realize that your current priorities and ways of living are not consistent with what you actually value and see as important, and so have reasons to make changes. We can interpret âKnow yourself â as a recommendation to look inwards, to examine oneâs own feelings, desires, commitments, preferences, and habits. Another interpretation of the Socratic motto is that the process of attaining self-knowledge requires you to look outwards, and take note of your own place and role in a community. Who are you among these people? How is your life and your aspirations connected to what other people are attempting to do in their lives? This relational dimension of being a person can be lost if Socratesâ motto is understood solely as an exercise in inward meditation on what matters in oneâs own life. Self-examination in the Socratic sense can consist in an inward and an outward orientation. The former may be the one that springs to mind when we read the motto in isolation, but the latter discloses the social dependencies of human endeavors, and is essential in attempts to understand fallibility at work. A Socratic examination of life at work can consist of asking questions that highlight relational and collaborative aspects: How is what you are trying to achieve at work dependent upon your colleaguesâ efforts? How"
270,154,0.984,The Huawei and Snowden Questions : Can Electronic Equipment From Untrusted Vendors Be Verified? Can An Untrusted Vendor Build Trust into Electronic Equipment? (Volume 4.0),"9.5 Semantic Translation When we have a program and a specification of what the program should do, the task of semantic translation is to extract from the two a set of proof obligations. These proof obligations are a set of mathematical expressions that, if proven to be true, imply that the program behaves according to the specification. Our entry point into this issue is Hoare logic [14], which is an axiomatic basis for reasoning about imperative programs. The core of Hoare logic is the Hoare triple, denoted {P}S{Q}. In this triple, S is a program and P and Q are logic expressions on the state of the program. Intuitively, the triple means that if P is true before the execution of S, then Q will be true after the execution. An example of a valid triple is {x + 1 < y} x := x + 1; {x < y} Hoare logic devises deduction rules for reasoning on such triplets over constructs that occur in imperative languages, such as assignments, if statements, loops, and statement composition. Using these deduction rules, it is possible to establish a triple {P}S â² {Q} for programs of any complexity. Finally, instantiation of the consequence rule below relates this to the specified precondition and postcondition (see Sect. 9.2): Pr econdition â P {P}S{Q} Q â Postcondition {Pr econdition}S{Postcondition} This rule states that, if a program is specified by a precondition and a postcondition and Hoare logic is used to prove the triple {P}S{Q}, then the resulting mathematical proof obligations are that the precondition implies that P holds and that Q implies that the postcondition holds. Unfortunately, proving a Hoare triple by the deduction rules given by Hoare was soon shown to be highly nontrivial. Later developments therefore attempted to reformulate the deduction rules to form the basis of algorithmic support for building this part of the proof. This approach led to the notion of weakest precondition, introduced by Dijkstra [9], which reformulated the ideas of Hoare logic into a system where the formation of a weakest possible precondition from a given postcondition could partly be determined automatically. Later, the work of Hoare and Dijkstra was extended to parallel programs [16] and object-oriented programs, to mention just two [22]. As stated in the previous section, process algebra was borne out of the observation that the distinction between specification and implementation seen in Hoare logic creates difficulties when it comes to parallel and distributed systems. Since"
124,114,0.984,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"The proof of the Fact is straightforward. We now turn to the proof in CIFOL+ of Theorem 1 stating that a strictly unique elementary case happens; the proof, which has five parts, occupies the rest of Sect. 3. We note that lines of the proof that contain any of P0 , Î², or Î¸, the latter two of which are defined in terms of P0 , do not have the status of theorems of CIFOL+. Neither Axiom 2 nor any such line begins with the sign of necessity.11 We emphasize the distinction when we mark proper theorems with the customary turnstile, â. The last line of the proof is 4.12, which can be seen by inspection to contain no notation that relies on P0 for its definition. For convenience, we break up the proof of Theorem 1 into five parts. Parts Ia and Ib prove that the individual constant Î¸ denotes a proper range, and is (extensionally) equal to tâwhich says that Î¸ happens. The conclusion of this part, since it contains Î¸, depends on Axiom 2 as a hypothesis. Only at the end of Part IV can we discharge Axiom 2 as a hypothesis. The annotation âC.P.â signifies âConditional proof,â and ââ©â advertises a quantifier rule. Watch for the role of Î¸."
311,1013,0.984,The Physics of the B Factories,"Being the first term of a systematic expansion, it is reassuring that these numbers are in the right ballpark. Note that the rates have to be proportional to m5Q to compensate the dimension of the Fermi coupling GF ; however the full dependence on the heavy quark mass is not as strong due to the phase space factors. The fact that the bottom and charm lifetimes are still comparable is due to the small magnitude of the CKM element |Vcb | relative to |Vcs |. The prediction that the heavy-hadron lifetimes are identical was considered a problem in the early days of the heavy quark expansion. In fact, we have for example Ï (D+ )/Ï (D0 ) = 2.52 Â± 0.09, indicating large corrections from higher-order terms in the expansion. Furthermore, the leading term depends on a high power of mQ , such that any uncertainty in mQ would be amplified so much that it was originally believed that no precise predictions could be made. However, including QCD corrections in"
328,62,0.984,a Philosophy of Israel Education : a Relational Approach,"THE EDUCATOR For many people, the educator is the key factor in education. Whether one accepts this generalization or not, it is clear that educators play a central role in the elaborate process of Israel education that we have described in the previous two chapters. Indeed, our past discussions point to certain core values, aptitudes, and tasks of an Israeli educator."
264,50,0.984,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"0 and 1, a common idea, or is it because he started at 0 and counted the tick marks at 0 and at 1? The work of teaching involves a fluency of mathematical reading and interpretation, to surmise what 30 (or more) different children might mean, and preparing to ask questions or to probe wisely or to comment strategically, all in real and rapidly moving time. At least three kinds of reasoning and interpretation are involved. First is to consider the mathematical issues embedded in the task or problem. For example, this task involves interpreting the number line, including what is considered the âwhole,â what to countâintervals or tick marksâand how to determine the name of a point on the number line. That the point is named by the distance from 0 in terms of the number of intervals of a particular length (e.g., 1/3) embeds all of these dimensions. Reading childrenâs writing is supported if the reader has a ï¬rm orientation to the things that a childâs representation contains. Second, and closely related, is to anticipate how the children might interpret the task or decide on their answer. Knowing, for example, that many children might count tick marks instead of intervals or start at 0 instead of at 1 can help a reader to see and interpret what a child has written. Finally, this work entails a fluency in reading childrenâs written representations, including spelling, spacing, handwriting (formation and orientation of letters, numbers, and symbols), ellipses and missing words or letters, and the composition on the page (for instance, that their writing is often not linear from top to bottom). There is also the complex work of leading a discussion, often misleadingly represented as âgetting out of the wayâ and âletting the children do the teaching.â First, the teacher chooses whose work launches the discussion. This is a key and consequential decision. In this case, while circulating as the children worked independently, the teacher saw many different answers by different children. Her decision about whose answer to start with involves considering the particular children and how they are positioned in the class and how who gets the floor and is given recognition for their thinking influences that positioning. The teacherâs decision also involves thinking about the mathematical issues on which the children are focused and those key to her instructional goal. She chooses Aniyah to present"
106,166,0.984,Community-Oriented Policing and Technological Innovations,"A situational measure of lifestyle must capture the true essence of a lifestyle as it is manifested in various situations; in the interactions between a person and the environment. When fear of crime is studied in terms of the units in which it is experienced (situations), it may be possible to unravel the mediating effect of lifestyles on the historically demonstrated connections between proxy measures and levels of fear. There is research that points in this direction (see Mesch 2000), although it has not employed a genuine situational approach."
28,260,0.984,A History of Self-Harm in Britain,"In looking for causes of DSH [deliberate self-harm] it is important to consider self-laceration separately in order to discern psychopathological mechanisms which may be peculiar to it and which are not shared by those who take drug overdosage.102 Again, the importance of this claim can be seen retrospectively from the point in the early twenty-first century, where self-cutting and self-poisoning are significantly different. Morgan mentions a number of âAmerican writersâ who tend towards a stereotype of a self-cutter being âan attractive young womanâ and suggest that self-cutting is âin the nature of a schizophrenic psychotic reactionâ. Like Myre Sim (see Introduction), he is unconvinced about the femininity of the stereotype, noting that â[o]ur Bristol survey demonstrated that, at lease in one provincial English city, men outnumber women amongst patients presenting at Hospital Accident and Emergency Departments following self-lacerationâ and thus the âbeautiful and femaleâ stereotype is simply âone amongst manyâ.103 Thus there is both influence and distance from the American studies from British-based clinicians. Self-cutting emerges from its inpatient context and takes on renewed significance as a psychological object in its own right, whether presenting in an inpatient institution or at A&E. Morgan sees self-laceration as concerned with an altered state of consciousness, a need to obtain relief from tension and a high incidence of obsessional, phobic and narcissistic tendencies. Immediately after this discussion, as if to restore a sense of balance, he states that âDSH cannot be understood entirely in terms of intrapsychic pathology. There is a massive body of evidence testifying to its close relationship with interpersonal social events, and not merely as a blind reaction to themâ.104 Remembering that, for Morgan, DSH refers to both self-cutting and self-poisoning, it is clear that the behaviours are still linked, even if self-cutting requires a level of differentiation and discrete concern."
21,141,0.984,intertwingled : The Work and influence of Ted Nelson,"From the endlessly quotable Ted Nelsonâwhose neologisms pepper the language we use to understand the present, from âhypertextâ to âvisualizationââperhaps no phrase is better known than, âYou Can and Must Understand Computers NOW.â It was emblazoned across the Computer Lib side of his 1974 Computer Lib/Dream Machines (CL/DM), the most influential book in the history of computational media.1 Nelsonâs call is not only memorable today, but still quite relevant. For example, consider the recent revelations of massive government surveillance, as disclosed by Edward Snowden and others. Without a deep understanding of computing, one might debate whether the vision of Total Information Awareness is morally right, or is instead sending us down a path to an âOrwellian,â 1984-style future. However, with a deep understanding of computing, one can not only raise the questions of morality in more depth, but one can also see that Total Information Awareness is a technically unworkable fantasy (like the Star Wars program pursued by the Reagan administration in the non-fictional 1980s) providing a false rationale for treating everyone as a suspect. In other words, one reason that we must understand computers now is so that we can understand what is happening, and make informed choices, as members of a computationally-steeped democracy. We need to understand computing so that we can see past deceptions about what computers can do, and how computers work. As"
139,105,0.984,Programming for Computations - MATLAB/Octave (Volume 14.0),"The new thing here is the while loop only. The loop will run as long as the boolean expression y(i) >= 0 evaluates to true. Note that the programmer introduced a variable (the loop index) by the name i, initialized it (i = 1) before the loop, and updated it (i = i + 1) in the loop. So for each iteration, i is explicitly increased by 1, allowing a check of successive elements in the array y. Compared to a for loop, the programmer does not have to specify the number of iterations when coding a while loop. It simply runs until the boolean expression becomes false. Thus, a loop index (as we have in a for loop) is not required. Furthermore, if a loop index is used in a while loop, it is not increased automatically; it must be done explicitly by the programmer. Of course, just as in for loops and if blocks, there might be (arbitrarily) many code lines in a while loop. Any for loop may also be implemented as a while loop, but while loops are more general so not all of them can be expressed as a for loop. A problem to be aware of, is what is usually referred to as an infinite loop. In those unintentional (erroneous) cases, the boolean expression of the while test never evaluates to false, and the program can not escape the loop. This is one of the most frequent errors you will experience as a beginning programmer. If you accidentally enter an infinite loop and the program just hangs forever, press Ctrl+c to stop the program."
118,342,0.984,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"The paradigm within which Industrial Age technologies are understood is based on an Enlightenment worldview. As said, this worldview is atomistic (reductionism), deterministic (cause and effect) and objectivistic (universal laws). In other words, the laws governing the behavior of these complicated systems can be: â¢ Understood by studying the behavior of their component parts, â¢ Deduced from cause and effect (a search for causal links or chains), and â¢ Determined independent of the observer, that is, only deduced from âobjectiveâ empirical observations. The context within which our Post-Industrial Age Technologies and their underlying science are understood is based on a nonlinear worldview. This worldview gives rise to complex systems that are characterized by at least one of the following [28]: â¢ Holistic/emergentâthe system has properties that are exhibited only by the whole and hence cannot be described in terms of its parts, â¢ Chaoticâsmall changes in input often lead to large changes in output and/or there may be many possible outputs for a given input, and â¢ Subjectiveâsome aspects of the system may only be described subjectively. It is often said that for complex systems, âthe whole is greater than the sum of its partsâ. What this means is that there is an emergent quality (sometimes called an emergent property) that is not exhibited by the parts alone. Examples include electric power transmission grids, the disposal of high-level radioactive waste, and the response of social systems to severe natural phenomena. I believe that the new issues regarding national and international security also fall into this category. In each case, the system is simultaneously a whole and a part of a larger whole, a characteristic of complex systems. It should be made crystal clear that the impacts of human activities on both society and the environment (from the development of the steam engine to the development of the jet engine) have always been complex. In the past, however, the only undesirable consequences of an Industrial Age technology, such as a nuclear power plant, that were considered in a PRA were geographically local (public health effects out to one mile or 25 miles) or they were observable in ârealâ time (a hydrogen explosion). This gave the impression that the current risk paradigm is accurate because locality and observability were two characteristics of the impact. This lens is changing, and yet our practices are still based on the same paradigm. That is, a nuclear power plant accident has âglobalâ impacts (an accident at one plant affects the operation of all plants) and manifests very quickly (e.g. loss of public confidence worldwide). In the case of disposal of radioactive waste, the undesirable consequences are almost imperceptible (e.g. the migration of high-level radioactive waste takes place over geological timescales or millennia). Moreover, these impacts may be temporally persistent and/or irreversible (e.g. the degradation of public welfare due to nuclear proliferation). Thus, as a result of the complexity inherent in Post-Industrial Age Technology, societal and environmental impacts are no longer geographically local, nor perceptible in real time, nor reversible. Rather, complexity can produce impacts that are"
311,297,0.984,The Physics of the B Factories,"at zero as expected, making ÎE especially helpful for discriminating from physics background events involving misidentiï¬cation. On the other hand, mES will not change if a particle is misidentiï¬ed, leading to peaking background from true B decays with incorrectly assigned particle identities. While this is true for symmetric-energy e+ eâ colliders operating at the Y(4S) (such as CLEO), where the laboratory system and the CM system are identical, it does not hold for the asymmetric B Factories. The B momentum vector can only be boosted to the CM frame after masses have been assigned, and the result depends on these mass assignments, although much weaker than for ÎE. To strictly keep mass independence, BABAR is using a modiï¬ed variable, which makes use of the three-momenta in the laboratory system and of the beam energy in the CM system: mES = (s/2 + pB p0 ) /E02 â p2B . (7.1.8)"
80,628,0.984,Innovations in Quantitative Risk Management (Volume 99.0),"1.1 Efficiency of Algorithms The performance of algorithms can be well compared in a diagram depicting the costs (computing time) over the achieved relative error. In case, the output of an algorithm consists of more than one real number, then we think of the largest of all these errors. Now, for a certain computational task, select and run a set of algorithms, and enter the points representing their performance into the diagram. Schematically, the dots look as in Fig. 1.1 For nontrivial computational tasks, there will be hardly a method that is simultaneously both highly accurate and extremely fast; there is always a trade-off. Hence, one will not find algorithms in the lower left corner, below the curve in Fig. 1. This (smoothed) curve is the efficient frontier. It can be defined in the Pareto sense as minimizing computing time and maximizing accuracy. Clearly, the aim of researchers is to push the frontier down; the curve is not immutable in time. The smoothed frontier in Fig. 1 may serve as idealized vehicle to define efficiency: Each method on the frontier is efficient. This notion of efficiency allows to define the âbestâ algorithm for a certain task almost uniquely. A reasonable computational accuracy must be put into relation to the underlying model error. So, indicate the size of the model error on the horizontal axis, and let a vertical line at that position cut the efficient frontier, which completes the choice of the proper algorithm. Of course, the efficient frontier is a snapshot that compares an artificial selection of algorithms."
58,510,0.984,Enabling Things to Talk,"In the course of its own project roadmap, our sister project â the Internet of Things Initiative (IoT-i) â has targeted three different (but connected) activities directly relating to the IoT-A architecture work as shown below: 1. To review and categorise existing reference models having a connection to the IoT field (or underlying disciplines, as IoT as such is more a technology umbrella). Example of the reference models reviewed by IoT-i are ETSI M2M, IETF Core, EPCglobal, Ucode and NFC to name just a few (IoT-A D1.2); 2. To put online a survey, the goal of which was to capture, people understanding and expectation, as far as reference models are concerned. This exercise was very important because people have generally different understanding about what are reference models, architectures and what they should consist of; 3. Finally, to come back on reference models introduced and summarised in previous versions of this deliverable and to do a reverse mapping exercise towards the IoT Reference Model. The goal of this exercise was to show that the reference model as defined by IoT-A is expressive enough in order to allow a modelling of those (pre- IoT-A) existing IoT reference models using the IoT-A one. In other words, if we would consider that IoT-A does not attempt to define what is an IoT system using sentences and words, but defining models where any IoT system (from the IoT understanding) shall fit, then all those existing reference models would be IoT systems reference models. In this Section we aim at giving some details about this reverse mapping exercise applied to ETSI M2M, EPCglobal and uID. Some of the material in this Section comes directly from the IoT-A D1.5 deliverable (Carrez et al. 2013) (especially the UML figures and concept tables). In order to improve readability, we do not use direct citations, although the work presented in the following Section was performed by the IoT-A project and reported in their deliverable D1.5. In addition to the standards that we have mentioned above, we also apply the IoT Architectural Reference Model to a concrete architecture, namely the architecture of the MUNICH (MUNICH 2010) project in order to validate the IoT ARM against a real system in contrast to an abstract standard. Furthermore we show a reverse mapping to the information model of the IoT-related research project BUTLER1."
8,83,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","meeting ground between particle and statistical physics, a dialog between theory and experimentâ. Rolf Hagedornâs work started long before I came to CERN. My understanding is that Bruno Ferretti, who was head of the Theory Division when Hagedorn joined it, asked him to try to predict particle yields in the accelerator high energy collisions of the time. This he started with Frans Cerulus. There were few clues to begin with, but they made the best of the fireball concept which was then supported by cosmic ray studies and used it to make predictions about particle yields and therefore the secondary beams to be expected from the machine beam directed at a target. Many key ingredients brought soon afterward by experiment helped refine the approach. Among them one should quote the limited transverse momentum with which the overwhelming majority of the secondary particles happen to be produced. They show an exponential drop with respect to the transverse mass. One should also quote the exponential drop of elastic scattering at wide angle as a function of incident energy. Such exponential behaviors strongly suggest a thermal distribution for whatever eventually comes out of the reaction and it is to Rolfâs great credit to have clung to this thermal interpretation and to have used it to build production models which turned out to be remarkably accurate at predicting yields for the many different types of secondaries which originate from high energy collisions."
307,124,0.984,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"see page 37. We also briefly sketched a numerical method for solving it; see (2.25). The numerical solution of systems of this form is used repeatedly in these notes, so solution methods deserve a little more attention. In this appendix we will present one way of solving the system; by consulting literature in numerical methods for solving PDEs, the reader will find a huge number of alternatives. The numerical solution of systems of this form is an active field of research and we will by no means argue that the method we present here is any better than other methods. Our focus is simplicity."
38,16,0.984,The GEO Handbook on Biodiversity Observation Networks,"Fig. 1.1 The contemporary deï¬nition of biodiversity embraces three aspects of variation (differences in composition, structure and function) and several levels of biological organisation (from the enzyme, to the biosphere). There is not a ârightâ level to observe biodiversity, nor a ârightâ aspect to observe: ideally you should be capturing elements of all aspects and all levels, and be able to move seamlessly between them. In practice, in any particular situation there will inevitably be stronger emphases on some levels or aspects. Historically, many people considered âbiodiversityâ to consist only of composition, at the species level. Be guided primarily by what the users of the information need, secondly by what is observable using the available technology, and only then by what happened to have been collected in the past. As you shift downward from the ecosystem towards the organism and ultimately the gene, the entities with which you are dealing become more focussed and precise, but the price you pay is a loss of information about interactions between them and the emergent properties which arise from those interactions (Source based on Noss 1990)"
375,394,0.984,Musical Haptics,"is less well suited to detecting which limb is striking a drum and when, and better suited to detecting which drum has been struck. This can have advantages in situations where the same limb plays more than one drum, but can have disadvantages where, for example, two limbs alternate in their playing of a single drum (Fig. 11.5). Yet another design dimension involves the choice of body location(s) when applying haptics. Different locations have different advantages for different applications. For example, as noted earlier, the tension-based system by Yamakazi et al. [40] allows clear communication through the chest of highly detailed bass vibrations, whereas Lewiston [43], Huang et al. [44, 45] and Siem et al. [46, 47] focus on individual fingers, and the Haptic Bracelets focus primarily on the limbs. MuSS-bits by contrast emphasises flexibility in choice of body locations for its wireless sensorâdisplay pairs. Choice of body location for haptics can have a variety of subtle effects on the perception of haptic signals beyond the scope of this chapterâa general discussion of this issue can be found in [49]. Finally, there is an important difference between the work by Grindlay [42], Tamaki et al. [48] and our own, related to the dimension of control. Although very different, their systems are both able to physically control human movements, while in our work (and most other related work) the haptics only communicate signals to guide the userâs movement, and the user remains in control of all physical actions."
46,14,0.984,A Philosophical Examination of Social Justice and Child Poverty,"letâs assume with Brighouse that in such a tragic world there is a reliable correlation between the degree of dreariness in childhood and the level of flourishing in adulthood. Would we really judge parents who impose enormous amounts of dreariness on their children compared to those who allow moments of enjoyment, even knowing that it affects the childâs future negatively, as the better ones? We agree that the answer to this question is not an obvious one; there is the strong intuition that the well-being of children seems to matter for its own sake, independently of its contribution to life as an adult. It is simply a good thing that a child lives a flourishing life, exercising and developing her capacities (Macleod 2010). Accordingly, we assume, in line with most theorists in the field, that childhood is intrinsically valuable and that children as the subjects of moral concern have a right to a good life. Second, the childâs condition, which includes a particular vulnerability, immaturity and dependency on others, makes her well-being an especially salient normative category. Children cannot be held responsible for their life choices as adults can, and therefore any harm to their well-being is particularly problematic from a moral point of view. Hence, a society that does not manage to sustain a certain level of well-being for its children cannot be a just one. This does not mean, of course, that the well-being of children is the only thing that matters for justice. The childâs future as an autonomous and thriving citizen â her becoming â is of importance as well, as is the well-being and well-becoming of all other members of society. However, since it often gets completely neglected in theories of justice, we want to stress clearly the importance that the wellbeing of children, qua children, should have for normative reasoning. Third, a comprehensive understanding of childrenâs lives must include a multitude of information. It just is not enough to know, for example, the economic situation of a child or her family in order to judge if she is indeed well off. There is more to disadvantage than can be expressed in monetary terms. In many approaches to the measurement of child poverty, this insight is well established. There, the well-being of children is judged in different dimensions, which are also set into relation with each other. When looking at the lives of children explicitly from a normative perspective, such a multidimensional approach is also requested. This is the case for the following reasons: First, if philosophy wants to develop an understanding of justice that is applicable in the real world and its non-ideal circumstances, it must work with a realistic picture of well-being. It should reflect our intuitive judgments about the subject and allow plausible assessments of the social position of an individual (Wolff and de-Shalit 2007, 21). It is obvious that different"
173,361,0.984,"intelligent Human Computer interaction : 9Th international Conference, Ihci 2017, Evry, France, December 11-13, 2017, Proceedings","The detection of affective user states is an emerging topic in the context of human-computer interaction (HCI) (cf. [19,24]), as it is known that besides the pure context additional information on the userâs feelings, moods, and intentions is transmitted during communication. For instance [1] discussed that such information should be used in HCI for a more general view on the human interlocutor. The detection of emotions from speech can be seen as a challenging issue since both, the emotions themselves as well as the way humans utter emotions, introduce variations increasing the difficulty of a distinct assessment (cf. [2,24]). Furthermore, many up-to-date classification methods analyse data based on the distances between the given sample points (cf. [24]). As a consequence of the c The Author(s) 2017 P. Horain et al. (Eds.): IHCI 2017, LNCS 10688, pp. 189â201, 2017. https://doi.org/10.1007/978-3-319-72038-8_15"
141,33,0.984,Cyber-Physical Systems of Systems: Foundations â A Conceptual Model and Some Derivations: The AMADEOS Legacy,"The duration between events that are temporally ordered can be smaller than the granularity of a single clock. This situation is even worse if two different globally synchronized clocks observe the two different events. It is therefore impossible to establish the true temporal order of events in case the events are closer together than 2g. This impossibility result can give rise to inconsistencies about the perceived temporal order of two events in distributed system. These inconsistencies can be avoided, if a minimum distance between events is maintained, such that the temporal order of the events, derived from the timestamps of the events that have been generated by different clocks of a system with properly synchronized clocks is always the same."
8,119,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Abstract In the latter half of the last century, it became evident that there exists an ever increasing number of different states of the so-called elementary particles. The usual reductionist approach to this problem was to search for a simpler infrastructure, culminating in the formulation of the quark model and quantum chromodynamics. In a complementary, completely novel approach, Hagedorn suggested that the mass distribution of the produced particles follows a self-similar composition pattern, predicting an unbounded number of states of increasing mass. He then concluded that such a growth would lead to a limiting temperature for strongly interacting matter. We discuss the conceptual basis for this approach, its relation to critical behavior, and its subsequent applications in different areas of high energy physics."
264,124,0.984,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"Telling how research is done opens another issue. At school, mathematics is traditionally taught as a closed science. Even touching open questions from research is out of question, for many good and mainly pedagogical reasons. However, this fosters the image of a perfect science where all results are available and all problems are solvedâwhich is of course completely wrong (and moreover also a source for a faulty image of mathematics among undergraduates). Of course, working with open questions in school is a difï¬cult task. None of the big open questions can be solved with an elementary mathematical toolbox; many of them are not even accessible as questions. So the big fear of discouraging pupils is well justiï¬ed. On the other hand, why not explore mathematics by showing how questions often pop up on the way? Posing questions in and about mathematics could lead to interesting answersâin particular to the question of âWhat is Mathematics, Really?â"
120,96,0.984,Genome Editing in Neurosciences,"will presumably have sufficient time to cut the targeted region until it can no longer be properly repaired. To rule out the existence of off-target effects, we also performed rescue experiments by transfecting a GluN1 cDNA. Re-introduction of the deleted subunit by co-transfection fully rescued the phenotype (Fig. 2). In a subsequent part of our project, our goal was to target multiplex genes. As a proof of concept we repeated the same experiment with the single GluN1 and GluA2 subunits, this time co-transfecting the two plasmids containing target gRNAs. We observed a complete deletion of both subunits with a complete rectification for AMPA receptors (due to the loss of GluA2) and no NMDA eEPSCs (Fig. 3). Another issue regards the possibility of studying an effect of a protein deletion in vivo. The advent of Cas9 opens a very exciting new conceptâwe can now inject gRNAs to target potentially any protein in a wild-type (WT) background (co-transfecting with Cas9 plasmids) or in Cas9 knock-in animals (Platt et al. 2014). For example the use of the AMPA receptors triple floxed mouse has been very important for understanding every single subunitâs contribution to the structure and function of glutamatergic excitatory synapses. Now we can reproduce these results in a few weeks (compared to years to create Cre-Flox lines and to cross them), optimizing time and cost enormously (Fig. 4). The possibility of expressing the protein of interest in a KO background enables one to study the function of specific domains in the synaptic context. This approach can be instrumental to the understanding of synaptic proteins that are involved in neurological diseases."
309,14,0.984,Food Anxiety in Globalising Vietnam,"concerns of food safety and security all touch uponâin one way or anotherâthe ambiguous nature of food itself, and also indicate discontinuities of broader social transformations. Food anxiety is the bookâs common lens through which such ambiguities are considered. The very act of incorporating food constitutes the moment and the process in which materially and culturally transformed matter crosses the boundary of the body, thereby dissolving the dichotomy between âinsideâ and âoutsideâ, between the self and the world (Fischler 1988, 279). In this understanding, boundaries are crossed at various scales, connecting but also disconnecting the eating body with the multiple contexts it lives in. It is exactly the linking and disrupting quality of foodstuffâa foodâs transgressive capacity (Goodman and Sage 2014)âthat accounts for its ambivalent nature and makes us speak of âfood anxietyâ. Therein, we understand the lens of food anxiety as constitutive for unravelling social theoretical insights into the relationship between the individual and society.3 What we observe in the vast field of food-related literature is that âfood anxietyâ is more often than not used without further conceptual explanation or becomes boiled down to paramount incidences of food scandals.4 We argue that âfood anxietyâ has more to offer than being a descriptive term for the emotional and institutional management of food safety risks, as we will detail below. This volume brings together authors from various disciplines and while their contributions are united in that they all deal with dimensions of food anxiety in Vietnam, the authors follow their own distinct methodological and theoretical approaches to food. Likewise, this introductory chapter should be seen as one, namely the editorsâ interpretation of what follows in the outline of the book parts. By applying our lens of food anxiety, we will frame the three thematic dimensions along which the book is organised: âBodily Transgressions: Identity, Othering, and Self â (Part I), âFood Safety: Trust, Responsibilisation, and Copingâ (Part II), and âThe Politics of Food Securityâ (Part III). When it came to naming this edited volume, food anxiety as the contributionsâ mutual lens quickly asserted itself into the title. Since all contributions focus on Vietnam, adding the countryâs name was also an easy decision. But then it took multiple attempts of testing the conceptual sound of âVietnam and beyondâ and the like to finally come down with"
321,182,0.984,Beyond the Limits to Growth : New Ideas for Sustainability from Japan,"elements, such as a solar cell, a heat insulation system, and an EcoCute unit. Through such decomposition, we can create constructive information for other regions. Using such information, one might be able to conclude, for example, that Komiyama-san uses this heat-insulation system, and that such a type would be better in Okinawa. Another might say that Komiyama-sanâs solar cell could be used also in that region. Komiyama House is only one of many cases. There may be few cases where Komiyama House can apply directly. It is then important to âfactorizeâ each case into individual elements and abstract common elements to create a universal solution. The paper-medium Platinum Handbook has a similar structure. It is not a collection of examples of experiments themselves, but it is structured, with the table of contents giving shape to the structure. The Platinum Handbook, based on IT, permits users to make a table of contents freely from their own perspective. The Platinum Society Network expects local university students to play an important role in not only participating in the establishment of a platinum society in coordination with their local government, but also in taking the initiative in the evolution of the Platinum Vision Handbook. The Platinum Vision Handbook serves as a platform for realizing âstructuring objectives and activitiesâ and âsharing structured knowledge,â which are requirements for networks."
259,48,0.984,The Little Book of Semaphores,"Since mutex is initially 1, whichever thread gets to the wait first will be able to proceed immediately. Of course, the act of waiting on the semaphore has the effect of decrementing it, so the second thread to arrive will have to wait until the first signals. I have indented the update operation to show that it is contained within the mutex. In this example, both threads are running the same code. This is sometimes called a symmetric solution. If the threads have to run different code, the solution is asymmetric. Symmetric solutions are often easier to generalize. In this case, the mutex solution can handle any number of concurrent threads without modification. As long as every thread waits before performing an update and signals after, then no two threads will access count concurrently. Often the code that needs to be protected is called the critical section, I suppose because it is critically important to prevent concurrent access. In the tradition of computer science and mixed metaphors, there are several other ways people sometimes talk about mutexes. In the metaphor we have been using so far, the mutex is a token that is passed from one thread to another. In an alternative metaphor, we think of the critical section as a room, and only one thread is allowed to be in the room at a time. In this metaphor, mutexes are called locks, and a thread is said to lock the mutex before entering and unlock it while exiting. Occasionally, though, people mix the metaphors and talk about âgettingâ or âreleasingâ a lock, which doesnât make much sense. Both metaphors are potentially useful and potentially misleading. As you work on the next problem, try out both ways of thinking and see which one leads you to a solution."
355,333,0.984,"Agile Processes in Software Engineering and Extreme Programming: 19th International Conference, XP 2018, Porto, Portugal, May 21â25, 2018, Proceedings (Volume 314.0)","As the impact of the adoption of Agile approaches gains traction within the organisation these Primary & Secondary contradictions are supplemented by Tertiary and Quaternary ones as the implications and effects of the adopted Agile practice extend beyond the project delivery team and impact on other organisational activities and practices. Interestingly Tertiary contradictions represented the lowest (5) level of occurrences of all the contradictions. This is similar to instances in the literature where for example a study by Rauf and Al Ghafees [58] indicated that most organisations do not follow any agile method completely. They adopt a mix of agile practices and traditional approaches. This could be indicative of signiï¬cant Tertiary contradiction where the âold mode of operation is rebelling against the new oneâ [49]. Similarly, case study analysis of agile implementations, undertaken by Rose [60] indicates that some organisations embrace agile principles without the wholesale abandonment of the already established traditional approaches. His research also noted that there was some symbolic re-labelling of some traditional elements using agile terminology. He notes that this was detrimental to moving forward with agile approaches as labelling acts as a departure point for organisational transformations and notes that âthe path to innovation is not navigable when labels do not accurately reflect either the status quo or the transformed stateâ [60]. Such maneuverings can be viewed from a Tertiary contradiction perspective and as Rose [60] indicates there is a further opportunity for research. Of particular interest would be an understanding as to why the occurrence of Tertiary contradictions is low in comparison with the Primary & Secondary contradiction. Is it because software teams are unaware of what âculturally more advanced formsâ of agile practice are?"
147,4,0.984,Trailblazing in Entrepreneurship : Creating New Paths For Understanding The Field,"This description of bushwhacking is the Australian version of trailblazing, and metaphorically, entrepreneurs can also be trailblazersâmake a path through new or unsettled terrain upon which others may follow. That is, rather than follow the established path created by others, entrepreneurs often challenge the status quo by attempting to chart a new direction through the creation of new products, services, and/ or processes. However, this book is not about trailblazing in coastal reserves or in product markets but about trailblazing in the field of entrepreneurship. We believe that scholars can be trailblazers, and in doing so, they can create new knowledge that others can build on to create additional knowledge and inform practice. Although this trailblazing may not have to deal with the poisonous snakes and spiders of Australia, it certainly has its fair share of obstacles, requires considerable effort, and may also lead to dead ends. Along with the challenges of creating a new trail are the intrinsic rewards from the process and the extrinsic rewards from the outcomes of making substantial contributions to knowledge. For scholars traveling along a well-worn path or a semi-worn path, the research outcomes are replication and incremental research, whereas trailblazing creates new knowledge through more radical ideas. Important in trailblazing is knowing where to start and having some knowledge about the terrain to be covered, the tools to help clear the path, and the potential âgemsâ that might be encountered along the way. The purpose of this book is to provide some insights into where trailblazing may be best directed, how, and with what potential outcomes. Specifically, this book offers a series of frameworks from which or within which we believe important research will emergeâresearch that will have a substantial impact on our understanding of an entrepreneurial phenomenon and, thus, on the way we progress with subsequent entrepreneurship research. We emphasize trailblazing (as opposed to taking existing paths) because we strongly believe that the future of the entrepreneurship field is promising but only if our research itself continues to be entrepreneurial. That being said, continuing to be entrepreneurial in our research may be more difficult than it initially appears. The success we have had thus far may lead us into a competency trap (Levitt & March, 1988) that rewards us in the short term but is detrimental to the field in the long term. That is, entrepreneurship researchers sometimes decide to âplay it safe,â using âacceptedâ theories and methods to answer progressively narrower research questions"
264,1257,0.984,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"A static ï¬gure is used in the learning of plane ï¬gures. In contrast, it is important to present three-dimensional shapes and dynamic movements, when a teacher teaches spatial ï¬gures. There are the following advantages in using pop-up card creation as teaching material. When making a card, a three-dimensional card is completed by trial and error, making a cut in a plane (card) plan, and opening and closing a card repeatedly. In this process, the instruction that connected the plane ï¬gures and the spatial ï¬gures is attained. In particular, a pop-up card called âorigami architectureâ is effective as teaching material from this respect. When you open a card that is folded in two to 90Â°, the three-dimensional spatial object appears. When you fold this card, this card is returned to its original state (Figs. 1â3). When people see a pop-up card, they often wonder how a solid is made from one sheet of plane paper. Students can observe the spatial motion of the work ï¬rst and understand that a three-dimensional work can be made from a card by opening and closing it repeatedly. They will want to make an original pop-up card. They will consider how to write a plan on a flat card while imagining the state a card will be in while opening and shutting it. They will infer how a line on a plane changes into a solid edge. They may come to observe opening and shutting of a card from the front, then view it from various directions and observe. In particular, they will notice that it is important that they observe the state of the transformation of the section of the card from the side. They can learn to understand projection view through this activity."
232,423,0.984,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"Injunction is a communication triggering action as the addressee should adapt his behavior regarding its message (conformity). This communication comes from an authority and is both binding and relying on its addressee subjectivity [5]; as the addressee is linked to the expected action or to its aim regarding responsibility criterion. Fundamentally, injunction implies a tension between what comes from oneself (autonomy) and what is implemented by external sources (heteronomy) [6]. This phenomenon affects oneâs identity as no one can predict how far a subject will integrate external things to his subjectivity [7] and experience. By saying so, one would conclude right by stating that safety injunction is not always or completely deï¬ned in time, space and form as shown by its legal evolutions from British Equity system to its 19th to early 20th variations in the United States of America (Stewart 1895; Gregory 1898; Mc Murry 1961) cristalized in the Pullman strikes repression through the Omnibus Indictment, and to its actual uses demonstrating that the term âinjunctionâ has no ï¬xed deï¬nition but is determined by its practice (Preston 2012, p. 5). That is why, prevention posters from Oak Ridge Laboratory dating from the Manhattan Project times are still quite relevant for any worker exposed to radiation sources, even though some military elements might have lost some sense since [8]. What is also interesting about injunction as a management device is that there is a wider array of potential issuers than in the order case. So far, three kinds of authorities have been identiï¬ed as relevant to make an injunction. The ï¬rst authority observed is derived from the recognized power one has to direct someone else, such as in hierarchy case. This typical authority has been widely analyzed since management studies beginning, particularly with Henri Fayol description of administrative skills use [9]."
235,296,0.984,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","By reduction to the halting problem it can be argued that the algorithmic information content I in general is incomputable. Because computability of the algorithmic information content I (sn ) would require that it would be possible to compute whether or not particular programs of length up to n + O(n) halt (after output of sn ). But this is clearly impossible for large enough (and even for small) n; see also the busy beaver function discussed later. It is therefore impossible that in general it is possible to prove (non)randomness or (in)computability of a particular individual infinite sequence. (Any particular finite sequence is provable computable, as by the earlier mentioned tabulation technique, an algorithm outputting it can be constructed by putting it directly into a program as a table. This is no contradiction to the earlier definition of randomness of a finite sequence because this is means relative and not absolute.) That is, all statements (e.g., Ref. [589]) such as âthis string is irreducibly random,â at least as far as they relate to ontology, are provably unprovable hypotheses. Epistemically they are inclinations at best, as expressed by Bornâs statement [68, p. 866] (English translation in [570, p. 54]) âI myself am inclined to give up determinism in the world of atoms.â At worst they are ideologies which remain unfalsifiable. In a quantitative sense one could go beyond the GÃ¶delâTuring reduction. Let us follow Chaitin [124] and employing Berryâs paradox, as reported by Russell [273, p. 153, contradiction (4), footnote 3]: âBut âthe least integer not nameable in fewer than nineteen syllablesâ is itself a name consisting of eighteen syllables; hence the least integer not nameable in fewer than nineteen syllables can be named in eighteen syllables, which is a contradiction.â Chaitinâs paradoxical construction which is based upon the Berry paradox can be expressed by the following sentence [124, 127] which cannot be valid: âthe program which yields the shortest proof that its algorithmic information content is much greater than its length, say, 1 billion bits.â"
317,20,0.984,Transitions in Mathematics Education,"I mentioned earlier that âpieces and processesâ study of mathematical learning is rarer than for science. This example is an exception. (For a related example, see Pratt and Noss 2002.) While it is too early to generalize to all of mathematics learning the example suggests that, at the microgenetic grain-size, there is no substantial discontinuity in learning. Wagner (2006) used clinical interviews to study learning of the statistical law of large numbers. One of the theoretical tools developed by KiP shows how concepts may not be monolithic, but actually composed of many pieces each of which works only in certain contexts. Then, learning may proceed incrementally, one context at a time. In Wagnerâs study a student gradually linked in intuitive schemes (similar to the science example, above) that were approximations of normative ideas. The idea that âlarger samples are more accurateâ occurred to her speciï¬cally in the context of surveys, and demonstrably changed the range of contexts in which she could effectively apply the law of large numbers. In general, context-speciï¬c intuitive ideas seemed necessary to extend her capacity to apply the law of large numbers across new contexts, and it appeared that linking those ideas in was distributed in time, with no categorical breakthroughs (gestalt switches)."
113,193,0.984,Pentecostalism and Witchcraft : Spiritual Warfare in Africa and Melanesia,"ATRs) see kindoki as material as well. Kindoki here references a material substance located in the belly. Vomiting and physical extraction are unsurprisingly some of the more common purifying practices in these ATR churches, while the majority of PCCs claims that speaking out the name of Jesus and the discursive chasing off evil spirits are the only âChristianâ practices for healing the afflicted. Therefore, I propose the concept of âthe witchcraft complexâ in order to acknowledge the variety of Pentecostal understandings of witchcraft, strategies for cleansing and repairing, and entanglements with local and global scales of reference. A âcomplexâ is at once (a) a composite bringing together many interconnected parts, an assemblage of associated things; (b) a fixed idea or obsessive notion; and (c) as used in psychology,13 a core pattern of emotions, memories, perceptions, and wishes in the personal unconscious organized around a common theme, such as power or status. The notion of the âcomplexâ thus allows us to acknowledge the juxtaposition of objects and types of materiality that contrast âtraditionâ and âmodernityâ in the imagination of evil. Smartphones and nkisi statues can appear side by side in testimonies about kindoki. Planes, television sets, and Rolex watches are all potentially dangerous, just as the ritualistic paraphernalia of a customary chief. Objects assumed to be âtraditionalâ seem to be quite hybrid as well. In her overview of demonization discourses in Africa, Hackett shows (2003: 62f) that the so-called âtypically Africanâ patterns of witchcraft often have trans-local origins. In particular, the demonology can be traced back from Westand East-Africa to the US, South Korea, and Brazil. The âTraditionalâ is âinventedâ, a transnational blend of images, names, props, and practices (Ibid.). The second meaning of âcomplexâ does justice to the fixation of PCC on witchcraft and the occult. In the Kinois context, PCC places greater emphasis on deliverance and the spiritual battle than on the gospel of health, wealth, and success. PCC is rejected by other Christian churches precisely because of Pentecostal communitiesâ consistent emphasis on kindoki. These same critics of PCC claim that the witchcraft confessions, sermons about how kindoki began and how one should protect oneself against witches, actually initiate Christians into the realm of the occult. The third semantic layer of the notion of âthe complexâ draws attention to the affective dimensions of kindoki. For many types of kindoki, fear, jealousy, envy, desire, anger, hatred, and other anti-social emotions are considered to be at once constitutive of witchcraft and the outcome"
311,859,0.984,The Physics of the B Factories,"17.3.1 Introduction B meson decays into all hadronic ï¬nal states containing open charm or charmonium account for almost three quarters of all B decays. Despite constituting the majority of ï¬nal states, these decays pose a challenge to both experiment and theory. The large available phase space in a B meson decay means that there are hundreds of possible ï¬nal states all with rather small branching fractions, typically a few tenths of a percent. Therefore to study in detail any particular ï¬nal state a very large sample of B mesons is necessary as well as a detector capable of measuring the energy, momentum, and identity of the ï¬nal state particles to high precision. Since these are all hadronic ï¬nal states, decay rate calculations must be done using non-perturbative QCD. For the majority of ï¬nal states, a quantitative prediction with controlled theoretical uncertainties remains out of reach. Only the decay rates of the simplest hadronic decays to charm, such as B 0 â D+ Ï â , can be calculated from ï¬rst principles using QCD. In spite of the above drawbacks, hadronic B decays to charm play an important role in the more glamorous aspects of B physics, i.e. the determination of the CKM parameters, measurements of CP violation, and search for physics beyond the Standard Model. If for no other reason these decay modes must be measured in order to understand the possible backgrounds involved in a measurement of a CKM parameter. Although the branching fractions here are small, it is still possible to collect very clean samples of B events using modes such as B â DÏ, B â Dâ Ï, etc. Two-body decays such as D0 Ï + and Dâ Ï + provide important detector calibration tools for determining momentum resolution (Ï Â± , K Â± ; see Sections 2.2.2, 6.2), electromagnetic energy resolution (Ï 0 , Î·; see Section 2.2.4), mass resolution (D, B; see Chapter 7), secondary vertex location (D, Ks0 ; see Chapter 6), and particle identiï¬cation eï¬ciency and rejection (Ï/K; see Chapter 5). Finally, precision measurements of modes such as B â D(â) Ï, D(â) ÏÏ may serve as standard candles for QCD calculations. In this section we are mainly concerned with decay rates and not the speciï¬cs of how the ï¬nal states are reconstructed and the techniques involved. These techniques are described in detail in Chapters 7 (B reconstruction), 12 (angular analysis), and 13 (Dalitz analysis)."
134,193,0.984,Curriculum Reform in The European Schools : Towards a 21St Century Vision,"(2009), in developing this framework, identify three broad types: motivational, where the intention is to influence the beliefs of the learner and their willingness to participate in the learning activities; reinforcing, where the aim is to reward or to punish specific behaviours; and informational, where the purpose is to change the performance of the learner in a particular direction. They point to the importance of being able to develop knowledge through a transfer of learning so that it is applicable in new contexts. Thus feedback is understood as multi-functional in relation to different learning environments, the needs of the learner, the purpose of the task and the particular relation feedback has to the learning theory being employed. A directive approach to feedback fits better a cognitivist perspective where it is understood as corrective with the expert providing information to the passive recipient. Alternatively, facilitative feedback identifies more closely with a socio-constructivist view where feedback is seen as a process that takes place within a learning environment, without determining what those understandings will be. Significantly, these two perspectives should be seen as reinforcing rather than as opposite ends of a continuum. The socio-constructivist view is highlighted in the need to see feedback as an integral and iterative part of the learning context and within formative assessment frameworks that emphasise interactions between teachers, pupils and subjects within communities of practice. Furthermore, within a co-constructivist approach, it is also accepted that the teacher learns from the student through dialogue and participation in a range of shared experiences (cf. Lave and Wenger 1998). Within such environments, feedback is understood as iterative, adaptive and dynamic, with different learners receiving different types of feedback and this varies at different stages of the learning process, though this principle is sometimes neglected in classrooms in European system schools."
311,211,0.984,The Physics of the B Factories,"4.4 Methods Statistical methods and tools of increasing sophistication used to optimize analyzes are described in the remainder of this chapter. Beforehand, it is important to stress that for many methods to be successful, two mandatory steps are required : training and validation. There are a few exceptions to this rule, where one can analytically compute the parameters required to perform an optimization. It is dangerous to optimize a selection with the actual data that is to be used in the measurement. Such an approach is prone to tuning on ï¬uctuations and the production of biases. For a simple example, suppose we are tuning an analysis for a particular signal, using the actual data. If we try to optimize S/B, say, we will ï¬nd selection criteria that tend to favor signal-like events, tuning on any upward ï¬uctuations. This will tend to bias our measurement of the signal strength toward high values. Nevertheless, this has been done extensively in particle physics, sometimes successfully, but sometimes with disastrous results. With an awareness of the issues, BABAR and Belle have gone to some length to avoid relying on the measurement data for the optimization. Note that these issues are discussed in a somewhat diï¬erent context in Chapter 14. Thus, BABAR and Belle take the approach of using a training dataset for the optimization. This could be simulated data, sidebands to the data that will not be used in the measurement, or a dataset that has similarities with the measurement data. A feature of the training dataset is"
192,88,0.984,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"The objective of the âdiscourse of the analystâ is to give the floor to the divided subjects ($) in order to put them on the track of their (object of) desire (a). In the context of psychotherapy, this is the function of the famous couch, a bourgeois contraption (contrasting with machinery and instruments of the industrial revolution) which generates a particular genre of discourse, namely the case history or Fallgeschichte, although Freud and his followers also use plays and novels as case histories, while Freudian case histories âread like novelsâ, as we have seen. It would be a mistake, moreover, to identify novels with one particular type of discourse. Rather, the novel is a kind of BÃ¼hne where various forms of discourse are mutually exposed and played out against one another. According to the famous Russian philosopher Michael Bachtin, this especially applies to the novels by Dostoevsky, an important source of inspiration for Freud as well (1928/1948). A"
8,390,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","I know of only two realizations of this aspect: the â system [97, 98] and SBMâboth remaining infinitely far behind the ambitious bootstrap program. In spite of a large number of other achievements obtained in and around the program (dispersion relations, Regge poles, Veneziano model, even string theories), one sees today a strong resurrection of Lagrangian field theories, which have now taken the lead in the race toward a âtheory of everythingâ. I believe that the bootstrap philosophy and the Lagrangian fundamentalism must complement each other. Each one alone will never obtain complete success."
385,506,0.984,Advanced R,"The next step up in complexity is to modify the output of a function. This could be quite simple, or it could fundamentally change the operation of the function by returning something completely diï¬erent to its usual output. In this section youâll learn about two simple modiï¬cations, Negate() and failwith(), and two fundamental modiï¬cations, capture_it() and time_it()."
378,59,0.984,The Great Mindshift : How a New Economic Paradigm and Sustainability Transformations Go Hand in Hand,"When considering the redesign of energy systems, urbanization trends and land use patterns, a study of the wider setting of correlations and side effects will enable people to not only think about more efï¬cient cars but also to explore how mobility can be delivered in the most sustainable way (WBGU 2011a: 342â343). Here of course we ï¬nd a strong link with the emphasis on mind-sets in this book, and my call to integrate political economy stems from what I ï¬nd to be a slightly naÃ¯ve conception of the origin and roles that ideas and paradigms play in political processes and their relation with power. While hardly anyone would explicitly argue against values like the protection of future generations or our environment, the devil lies instead in the detailâin this case the worldviews held. The same value set might lead to very different proposed solutions, given differing worldviews and their focuses. Mainstream economic mind-sets tend not to support the regulation of markets precisely because this would hamper individual freedom, happiness, creativity and meritocracyâvalues to which most people would subscribe. In one paragraph, the report mentions that Karl Polanyiâs interpretation of the industrial revolution, The Great Transformation (1944), describes how âattitudes and considerations inspired by personal beneï¬t maximisation have established themselvesâ and that with mass production, âthe âgood lifeâ has increasingly become synonymous with material wealthâ (WBGU 2011a: 67). The proposals for change in the report, however, leave this deep cultural wiring and its mental path dependencies unchallenged. Hence, it does not say how the observed value shifts can be implemented if there is no discussion of the paradigm behind the evidence and narratives used to argue which policies are suitable to embed the shift."
117,80,0.984,Care in Healthcare : Reflections On Theory and Practice,"Responsiveness In the light of the above, the distinguishing feature of care ethics is that it is defined less by initiative than by responsiveness. It responds or reacts to the needs of the person who is dependent on help. Care ethics is primarily response-focused. It is the other who calls for care. Thus care ethics is linked to the attitude and gestures of âturning toâ somebody and necessitates the capacity to approach the other. This requires an attitude of listening, of receptiveness, of understanding, essentially of close attention. Here, too, we can see a similarity with hermeneutic ethics, although care ethics involves more than just understanding; it contains the impulse to change, to realise care (Maio 2015). This impulse to realise care can be understood as the impulse to implement the response we are urged to give by the urgent situation of the other. In this context, Emmanuel Levinas defined care as âbeing called onâ by the other."
217,896,0.984,Finite Difference Computing With Pdes : a Modern Software Approach,"We shall later refer to the strategy of taking one Picard step, or equivalently, linearizing terms with use of the solution at the previous time step, as the Picard1 method. It is a widely used approach in science and technology, but with some limitations if Ât is not sufficiently small (as will be illustrated later). Notice"
82,306,0.984,Fading Foundations : Probability and The Regress Problem,"For then goodness is transferred lock, stock and barrel along the chain, and the no starting point objection, or rather Gilletâs more general Structural Objection, applies in full force. However, we have been arguing that the situation changes radically if the claims take on the form âif Y is good then there is a certain probability that X is goodâ and âif Y is bad then there is a certain (lower) probability that X is goodâ, and so on. For now goodness is not transferred in its entirety along the series. Rather it slowly emerges as we progress from the links Z to Y and Y to X. In this probabilistic scenario the original question would be how probable it is that a certain action, X, is good. And this question can indeed be answered; as we have seen, with the numbers chosen, it is 32 ."
305,251,0.984,Quantum Computing for Everyone,"you obtain a Josephson junction.** These junctions are now used in physics and engineering to create sensitive instruments for measuring magnetic fields. For our purposes, the important fact is that the energy levels of the Cooper pairs in a superconducting loop that contains a Josephson junction are discrete and can be used to encode qubits. IBM uses superconducting qubits in its quantum computers. In 2016, IBM introduced a five-qubit processor that they have made available to everyone for free on the cloud. Anyone can design their own quantum circuit, as long as it uses five or fewer qubits, and run it on this computer. IBMâs aim is to introduce quantum computing to a wide audienceâcircuits for superdense coding, for Bellâs inequality, and a model of the hydrogen atom have all been run on this machine. A primitive version of Battleships has also been run, giving the coder the claim of constructing the first quantum computer multiplayer game. At the end of 2017, IBM connected a twenty-qubit computer to the cloud. This time it is not for education, but it is a commercial venture where companies can buy access. Google is working on its quantum computer. It also uses superconducting qubits. Google is expected to announce in the near future that it has a computer that uses 72 qubits. What is special about this number? Classical computers can simulate quantum computers if the quantum computer doesnât have too many qubits, but as the number of qubits increases we reach the point where that is no longer possible. Google is expected to announce that it has reached or exceeded this number, giving them the right to claim quantum supremacyâthe first time an algorithm has been run on a quantum computer that is impossible to run, or simulate, on a classical computer. IBM, however, is not giving up without a fight. Its team, using some innovative ideas, has recently found a way to simulate a 56-qubit system classically, increasing the lower bound on the number of qubits needed for quantum supremacy. As work continues on building quantum computers, we are likely to see spinoffs into other areas. Qubits, however we encode them, are sensitive to interactions with their surroundings. As we understand these interactions better we will be able to build better shields to protect our qubits, but we will also be able to design ways our qubits can measure their surroundings. ** Brian David Josephson received the Nobel Prize in physics for his work on how Cooper pairs can flow through a Josephson junction by quantum tunneling."
264,574,0.984,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"Strand A: Functional Thinking If you understand functional thinking as âa way of thinking that is typical for the working with functions,â the knowledge about deï¬nitions, properties, related concepts, representations, and examples and counter-examples of functions is crucial for the development of functional thinking in mathematics. The understanding of the function concept is a long-standing process, which starts in kindergarten and primary school and can be seen as an open-ended process even in university mathematics. In Strand A we concentrated on two aspects in the frame of the function concept: basic ideas of the function concept and problems with real numbers represented in a number line."
378,88,0.984,The Great Mindshift : How a New Economic Paradigm and Sustainability Transformations Go Hand in Hand,"concepts in the second and third column of this table can best be understood by remembering that they were born in an era when â¢ the emphasis lay on the human intellect and its capacity to dissect complex processes and investigate them empirically; â¢ natural scientism and its law-like predictions of developments replaced religion as the prime source of explanations of the world; â¢ discourse around natural limits to population size met an energy revolution that fuelled the managerial-industrial drive to improve productivity; and â¢ the securing of private property and pursuit of self-interest became basic tenets of citizensâ freedom and were declared important drivers of progress. Modernity and neoclassical economic developments of the twentieth century have continued to employ the same mechanistic-additive view and basic concepts, pushing quantiï¬ed modeling and its extrapolating predictions into yet more dominance when computation made massive calculations possible. There is no emphasis on a deep or diversiï¬ed understanding of the ends that transactions should serve (human need satisfaction) or the scarce means that are required (natureâs resources). These are subsumed into the container terms âutilityâ and â(natural) capital.â This is in line with Robbinsâ deï¬nition and understandable when reflecting back on the context in which these concepts were born. Saturation with goods and services was reserved for a very small part of the population and poverty was widespread. It was rational to equate more with better. Meanwhile, in a world of one billion people with plenty of âundiscoveredâ territories, there was simply no expectation that more effective exploitation of nature would threaten its health and existence. From a transformation point of view it is thus easily understood why the Enlightenment movement claimed the term âliberalismâ: its ideas inspired collective action toward overcoming an old system that no longer delivered (as far as Enlightenment protagonists were concerned). The premise was to overcome the âdarkâ epoch of the Middle Ages. These ideas were key aspects of a paradigm shift that ï¬rst challenged the old order and its legitimizing narrative and later, as Polanyi showed, served as the gelling consensus between philosophers, scientists, businessmen, politicians and even church representatives working on alternative institution-building and rule-formulation. Polanyi also concluded that imagining all of society as one big market system and treating humans, nature, and money as ï¬ctitious commodities inevitably leads to sustainability problems. So here we come to an interesting question: if Robbins says that the application of economic concepts should be limited to situations of exchanges and choice making but Polanyi argues that all aspects of the planet have been subsumed under the imaginary and logic of a market system, where does the application of economics end? Having analyzed current discourse and observed the marketization and privatization trends of the last 30 years I would say that their application is almost ubiquitousâand that is precisely the problem. It means that neither ends nor means become the center of attention and investigation. Instead, it focuses only on the choice-making of selï¬shly calculating and insatiable individuals. The 250-year-old"
16,620,0.984,"Autonomous Control For a Reliable internet of Services : Methods, Models, Approaches, Techniques, Algorithms, and Tools","We believe that, exploring the architectural components is a very useful way to see the complete picture and understand IoT environments better. In Fig. 1, we outline a generic IoT architecture which is based on the general architectures previously proposed in [24,57,59]. The only difference of our architecture from the reference works is that we separated the IoT Access Network Layer from the IoT-Internet Connection Layer, whereas the reference studies combine them into a single layer called either as Network Layer or Transport Layer."
275,37,0.984,Foundations of Trusted Autonomy,"The history framework of UAI is more general than MDPs in the following respects: â¢ Partially observable states. In most realistic scenarios, the most recent observation or percept does not fully reveal the current state. For example, when in the supermarket I need to remember what is currently in my fridge; nothing in the percepts of supermarket shelves provide this information.2 â¢ Infinite number of states. Another common assumption in standard RL is that the number of states is finite. This is unrealistic in the real world. The UAI framework does not require a finite state space, and UAI agents can learn without ever returning to the same state (see Sect. 2.3.2). â¢ Non-stationary environments. Standard RL typically assumes that the environment is stationary, in the sense that the transition probability P(s â² | s, a) remains constant over time. This is not always realistic. A car that changes travelling direction from a sharp wheel turn in dry summer road conditions may react differently in slippery winter road conditions. Non-stationary environments are automatically allowed for by the general definition of a UAI environment Î¼ : (A ÃE )â ÃA  E (Definition 2). As emphasised in Chapter 11 of this book, the non-stationarity and non-ergodicity of the real world is what makes truly autonomous agents so challenging to construct and to trust. â¢ Non-stationary policies. Finally, UAI offers the following mild notational convenience. In standard RL, agents must be represented by sequences of policies Ï1 , Ï2 , . . . to allow for learning. The initial policy Ï1 may for example be random, while later policies Ït , t > 1, will be increasingly directed to obtaining reward. In the UAI framework, policies Ï : (A Ã E )â â A depend on the entire interaction history. Any learning that is made from a history Ã¦<t can be incorporated into a single policy Ï ."
213,14,0.984,Collider Physics Within The Standard Model : a Primer,"between two particles with total center of mass energy up to 2E  2pc . 7â14 TeV. These machines can, in principle, study physics down to distances Âx & 10 18 cm. Thus, on the basis of results from experiments at existing accelerators, we can indeed confirm that, down to distances of that order of magnitude, electrons, quarks, and all the fundamental SM particles do not show an appreciable internal structure, and look elementary and pointlike. We certainly expect quantum effects in gravity to become important at distances Âx  10 33 cm, corresponding to energies up to E  MPlanck c2  1019 GeV, where MPlanck is the Planck mass, related to Newtonâs gravitational constant by GN D âc=MPlanck . At such short distances the particles that so far appeared as pointlike may well reveal an extended structure, as would strings, and they may be described by a more detailed theoretical framework for which the local quantum field theory description of the SM would be just a low energy/large distance limit. From the first few moments of the Universe, just after the Big Bang, the temperature of the cosmic background gradually went down, starting from kT  MPlanck c2 , where k D 8:617  10 5 eV K 1 is the Boltzmann constant, down to the present situation where T  2:725 K. Then all stages of high energy physics from string theory, which is a purely speculative framework, down to the SM phenomenology, which is directly accessible to experiment and well tested, are essential for the reconstruction of the evolution of the Universe starting from the Big Bang. This is the basis for the ever increasing connection between high energy physics and cosmology."
375,238,0.984,Musical Haptics,"Interestingly, none of the participants complained about an implausible concert experience. Still, one could question whether the 5.1 reproduction situation can be compared with a live situation in a concert hall or church. Because test participants preferred generally higher acceleration levels, it is hypothesized that real halls could benefit from amplifying the vibrations in the auditorium. This could be achieved passively, e.g., by manipulating floor construction, or actively using electrodynamic exciters as in the described experiments. Indeed, in future experiments it would be interesting to investigate the effect of additional vibration in a real concert situation. Also, the vibration system could be hidden from participants in order to avoid possible biasing effects. During the experiments, the test participants sometimes indicated that the vibrations felt like tingling. This effect could be reduced by removing higher frequencies or shifting them down. However, this processing also weakened the perceived tactile intensity of broadband transients. The question arises, what relevance do transients have for the perceived quality of music compared with steady-state vibrations? One approach to reduce the tingling sensations for steady-state tones and simultaneously keep transients unaffected would be to fade continuous vibrations with a long attack and a short release using a compressor. This type of temporal processing appears to be promising based on an unpublished pilot study and should be further evaluated. Another approach for conveying audio-related vibration would be to code auditory pitch information into a different tactile dimension. For example, it would be possible to transform the pitch of a melody into the location of vibration along the forearm, tongue, or back using multiple vibration actuators. This frequency-to-place transformation approach is usually applied in the context of tactile hearing aids, in which the tactile channel is used to replace the corrupt auditory perception [20, 40]. However, in such sensory substitution systems, the transformation code needs to be learned. It has been shown in this study that it might not be necessary to code all available auditory information into the tactile channel to improve the perceived quality of music. Still, there is creative potential using this approach, which was applied in several projects [10, 11, 15]. Another interesting effect is the influence of vibrations on loudness perception at low frequencies, the so-called auditory-tactile loudness illusion [33]. It was demonstrated that tones were perceived to be louder when vibrations were reproduced simultaneously via a seat. This illusion can be used to reduce the bass level in a discotheque or an automobile entertainment system [29] and might have an effect on the ideal low-frequency audio equalization in a music reproduction scenario."
385,613,0.984,Advanced R,"The original update() has an evaluate argument that controls whether the function returns the call or the result. But I think itâs better, on principle, that a function returns only one type of object, rather than diï¬erent types depending on the functionâs arguments."
311,690,0.984,The Physics of the B Factories,"which would erase any matter-antimatter asymmetry. 3. The universe must have been out of thermal equilibrium. Under the assumption of locality, causality, and Lorentz invariance, CP T is conserved. Since in an equilibrium state time becomes irrelevant on the global scale, CP T reduces to CP , and the argument of point 2 applies. In order to illustrate the ï¬rst two Saharov conditions, we employ a very simplistic example. Assume that in the early universe, there was a particle X that could decay to only two ï¬nal states |f1  and |f2 , with baryon numbers NB and NB respectively, and decay rates Î (X â f1 ) = Î0 r"
13,31,0.984,Feeling Gender : a Generational and Psychosocial Approach,"who are engaged in non-conforming gender and sexual practices and who do not feel like they fit into whatever âweâ is being articulated as a norm (StormhÃ¸j 2013: 65). However important this approach is, it also has a tendency to conceptualise the dominant norm as monolithic and undynamic (McNay 2004; StormhÃ¸j 2013). The dominant norm emerges as a static background to non-normative gender performances and the question of what motivates some people to adhere to this norm moves out of focus (Hollway 1984; Layton 1998). But gender norms are neither deterministic nor monolithic or static. They vary between men and women and between classes and generations, and this creates internal incoherencies and contradictions within prevailing gender norms. The norms also change historically, and often also within the life course of individual people, without necessarily being dependent on destabilising discursive interventions from non-normative groups. Thus, the âdominant normâ is a moving target, and some of the movement may be explored by looking into the feelings of gender in those groups that adhere to these norms or through their behaviour modify them or create new ones. Feminist approaches departing from Bourdieuâs practice theory have a structurally and historically based understanding of change, combined with an emphasis on the ways in which such change also takes place as âlived relationsâ. Practice is motivated by peopleâs perceptions, feelings and representations, not just abstract social structures and economic forces (McNay 2004: 184). It is necessary to enter the âphenomenology of social spaceâ, a space that is relational in its structure and tied to experience in specific contexts, in order to understand how reflexivity and agency work as elements in both societal reproduction and change. Such reflexivity could be understood as an ongoing transformative practice âsimultaneous with the normal course of daily life, but also constitutive of how life is lived in history, across generations and in personal interactionsâ (Silva 2005: 96). By producing gender in ever-new ways, new ânormalitiesâ also come into being. Thus, change does not necessarily imply normative constraints, individual resistance or collective mobilisation, but can be located âin regard to a shift in the conditions of social reproduction itself â (Adkins 2004a: 9). Gender does not dissolve through this reflexivity, but is constantly in a process of reconfiguration. It is reflexivity itself that becomes âa habit of gender in late modernityâ (Adkins 2004b:"
310,139,0.984,Online Hate Speech in The European Union : a Discourse-Analytic Perspective,"CÃ©sar Arroyo LÃ³pez and Roberto Moreno LÃ³pez ICTs and internet access are a deï¬ning element in the lives of young people. In Spain alone, daily internet use among young people aged 16â35 is above 90% (INE 2016). The online experience, however, is not always positive, and as some recent studies indicate, traditional school bullying has taken a leap into the digital world, to the point that the number of incidents in both settings is practically the same (Calmaestra et al. 2016). While it is true that cyberhate, the online variety of hate speech, and cyberbullying are not the same thing, as the former targets certain groups on the basis of a common characteristic and the latter targets individuals usually in the setting of a particularly community (like a school or a workplace), the two concepts are deï¬nitely intertwined in the mind of young people. Bullying can be deï¬ned as âa type of behaviour aimed at doing harm, repeated over time and occurring in the midst of an interpersonal relationship characterised by an imbalance of powerâ (Olweus 1999: 25). Cyberbullying resembles bullying in that it, too, is intentional, aggressive and repeated over time, but with the particularity that those who engage in it do so through the use of electronic means. As Del RÃ­o et al. (2010) note, cyberbullying, as a mode of harassment, has characteristics that make it particularly intense, like the absence of temporal limits, the imperishability of online content, its capacity to be instantaneously witnessed by a vast audience and the perceived anonymity of its instigator. What is more, the consequences of cyberbullying have been analysed in several studies (cf. Garaigordobil 2011), and their connection with the effects that hate speech has on its victims is clear (cf. Ayto. Barcelona 2017). During the C.O.N.T.A.C.T. interviews in Spain,19 as many as six interviewees directly linked their experiences of hate speech to (cyber)bullying: (124) Interviewer Have you had any experience with hate speech? Interviewee 2 Yes, what I said about bullying in the ï¬rst years of high school with my nose problem, adenoids, they harassed me about my tone of voice and such things. Although in the particular case in (124) the motivation for the incident was not some protected characteristic of the victim, as is typically the case in instances of hate speech, in other reported cases, the cause of the harassment was precisely that: (125) Interviewer Maybe it was done to people around you? Interviewee 4 Yes (â¦) Interviewer Did they harass them about something particular or just because (â¦)?"
117,82,0.984,Care in Healthcare : Reflections On Theory and Practice,"Giving Preference to Emotional Knowledge The above criteria show that care ethics differs from other forms of ethics above all in the way in which problems are perceived. It perceives the ethical problem in different terms, which are not just related to the above basic elements, but rest more fundamentally on a wider concept of knowledge. For care ethics, knowing the objectifiable and formalisable facts does not suffice; care ethics also draws on what could be called âimplicit knowledgeâ. The critical role of relationships, the demand for an adequate perception of the situation, and the prioritising of creative solutions over deductive inference necessitate implicit forms of knowledge such as experiential knowledge, situational knowledge, and relationship knowledge. Valuing these forms of knowledge, which go beyond the confines of a formal-logical approach, is the essence of care ethics. They are forms of knowledge that cannot be learnt by heart but must be practised. According to care ethics, competence could be described as skill in dealing with ambiguity. The ability to cope with complexity plays a significantly more constitutive role here than in other forms of ethics. This perhaps also explains why the medical community continues to give little importance or support to care ethics. Care ethics represents a counterpoint to operational rationality because it practices a rationality of its own, in which feelings, intuition, and sensations are just as important as calculations, and in which experience is ascribed an epistemological value which is overlooked in the structural logic of modern medicine. In this regard, care ethics is more progressive than many forms of principlism, because it does away with the prejudice of the irrationality of feeling, because it takes the knowledge content of feelings seriously and in this respect constitutes an implicit plea to place more value on emotional knowledge. The specific challenge of care ethics, on the other hand,"
234,183,0.984,Mobile Professional Voluntarism and international Development : Killing Me Softly?,"account for this qualitative difference between types of action goals, the authors deï¬ne âinnovative intentionalityâ as âthe will to conceive or imagine realities with the purpose of making them effectiveâ (CaÃ±ibano et al. 2006: 319). Economic agents, as individuals or organisations, can in turn be qualiï¬ed as operating with higher or lower levels of innovative intentionality. This approach and the weighting given to intentionality as the key driver can be contrasted both with the behavioural science model and, as MuÃ±oz et al. explain, with the economic literature which, âargues that knowledge is the only foundation of capabilitiesâ (2011: 194). For MuÃ±oz et al., economic evolution (systemic change) does not come about as a result of the growth of knowledge per se. Intention, not capability, is the starting point. The concepts that immediately captured our imagination and resonated most starkly with our experience in Uganda included the explicit engagement with agency and the emphasis the model places on existing knowledge. Learning is not something that happens to people as passive actors/ victims (or empty vessels) but something that is essentially relational, interactive and cumulative. The individuals concerned are not devoid of (or lacking in) knowledge per se; indeed, it is their experiential (tacit and highly contextualised) knowledge â âtheir perceived realitiesâ â that shapes their response to new learning and imported knowledge. Agency is explicitly recognised in this model, which assumes that âhumans have sufï¬cient intelligence and incentives to anticipate and avoid the selection effectsâ associated with evolutionary, Darwinian theories (MuÃ±oz et al. 2011: 194 citing; Witt 2004: 128). This approach to intentionality helps us to see motivation as contextualised and as much about extrinsic as it is about intrinsic factors. In the ï¬rst instance, health workers are heterogeneous and this heterogeneity is ânot only a matter of differences in knowledge [or innate ability], but also of differences in action goals and intentionsâ (CaÃ±ibano et al. 2006: 319) and their subjective responses to means (opportunities) or plans will shape their approach to new learning. Furthermore, a health workerâs motivation will vary over time and space and in relation to diverse plans. A person, according to this model, cannot be placed on a linear motivationâdemotivation or innate intelligence continuum: their degree of motivation will necessarily vary according to speciï¬c plans (places, conditions and relationships). Seeing motivation as fundamentally contextdriven helpfully removes some of the essentialising (and potentially racist) elements of the behavioural science model."
360,325,0.984,Compositionality and Concepts in Linguistics and Psychology,"constructions like sports that are not games is exceptional in that it (indirectly) tests potential typicality clashes, here between sports and non-games. 12 For linguistic work, see Cresswell (1976), Kamp (1975), Kamp and Partee (1995), Klein (1980), Kennedy (2007). For experimental work, see Hansen et al. (2006), Kubat et al. (2009). Because of contextual effects, it is likely that some of the participants in Leeâs experiment would accept the hair in Image 1 as red but reject the car in Image 2, even if the nouns would not be mentioned, relativizing redness to the visual instances of the concepts HAIR and CAR. Leeâs work did not try to factor out possible effects of the visual stimuli, which are also central for concept composition (see Barsalou, 2017). However, Leeâs effects in distinguishing gradable-concept categorization in complex expressions with âneutralâ versus âbiasedâ categorizes are much stronger than in previous work on categorization with simple color terms. Therefore, we can maintain the assumption that the interpretation of an adjective is affected both by its visual context and its linguistic context. Below I ignore effects of the visual context because they are less directly relevant to the compositionality problem in linguistics. Most importantly, effects coming from the non-linguistic context are not immediately testable when analyzing reciprocity and distributivity, because these concepts, unlike adjective concepts, are not easily studied in isolation: unlike category names such as RED, reciprocity and distributivity are bound to appear as sentence parts."
93,346,0.984,Nordic Mediation Research,"As described, restorative justice theory often entails a triangular approach to an offence with the corners made up by offender, offended party, and community. In the practices I have observed, especially the involvement of the âcommunityâ seems to pose a challenge. What is the community? How can/should it be involved? In my observations there are different takes on this matter: In Northern Irish conferences, the âcommunityâ is involved if relevant. In the restorative meetings of the Norwegian sanctions it seems to be the public ânetworkâ of the young offenderâfor instance, a teacher and/or a local police ofï¬cerâwho largely stand in for the role of âcommunityâ. In the Florida conferences, the role of âcommunityâ was played by a board of civil volunteers from the area who had no connection to either offender, offended party or offence. To me the diverse interpretations of âcommunityâ in the three models suggest that the triangular model of the restorative theories might be just thatâtheoreticâbut often difï¬cult to put into a meaningful large scale practice. And as I see it, with no offended party present, and with the estranged âcommunityâ boardâhaving no direct relations to the offender of offenceâthe restorative/conï¬ict theories of Zehr, Braithwaite, Christie, and VindelÃ¸v seem extra hard to recognise in the Florida programs."
337,313,0.984,Understanding Society and Natural Resources : Forging New Strands of Integration Across the Social Sciences,"made those models explicit. Making processes, rules, and parameters explicit can be an illuminating, rewarding, and challenging exercise. For example, for a group to work well together requires a understanding of required terms and some baseline desire and ability to communicate to scholars in other disciplines. Reaching common understanding on what the most salient components of a system may be and how those will be represented in a simulation promotes team building across disciplines (Axelrod 2006). We agree with Epstein (2008) that the assumption many make of models is that their goal is to make predictions. Predictions can be made but often the assumptions of such models are so simplifying as to have little purchase in the real world. Myriad interactions and unforeseen changes make detailed predictions about future system states all but impossible in all but trivial circumstances. Prediction is rarely the goal of our work. Instead, we often seek to identify the magnitude and direction of change that may be expected in a system, for example, given the changes a particular policy or land management decision may make on the environment and for human wellbeing. Other work by ourselves and others uses hypothetical landscapes, and tests theory without being encumbered by specific circumstances (Griffin 2006). More generally, simulation can explain relationships, which is distinct from prediction (Epstein 2008). Alternative core dynamics may be incorporated in simulations, and those dynamics treated as hypotheses to be tested in experiments (Peck 2004; Grimm and Railback 2006). For example, the influence of topography on animal behavior may be quantified by using the observed topography in simulations, then substituting a flat landscape. Simulation can guide data collection, with sensitivity analyses (i.e., varying a parameter across its reasonable range of values and exploring changes in output) identifying new questions and uncertainties and allowing data collection efforts to be prioritized. Gaps in understanding can be suggested if an application that incorporates current theory is unable to generate the expected responses. Complex patterns can be shown to have simple underpinnings (e.g., the classic graphic of the Mandlebrot set used to demonstrate the nature of fractals) and simple patterns may be shown to be produced by relationships more complex than assumed (Epstein 2008). Simulation is helpful where analytical, differential equation-based approaches may become mathematically complex and intractable. Lastly, simulation is helpful when manipulations to real systems would be too costly, disruptive, or unethical (Peck 2004)."
311,225,0.984,The Physics of the B Factories,"In Fig. 4.4.1, top (for ÏâK separation), we see that the non-exhaustive ECOC performs similarly with the likelihood selector. When we go to an exhaustive ECOC selec4.4.8 Error correcting output code tion we ï¬nd a notable improvement in mis-identiï¬cation for the same eï¬ciency. In the bottom plot (for e â Ï sepWe may consider the situation with multiple output classes, aration) the non-exhaustive ECOC is tuned to somewhat but where one is still interested in the binary question higher eï¬ciency, but yields much poorer mis-identiï¬cation of determining whether the event belongs to a particular than the likelihood selector. Note that this is in contrast class or not. For example, suppose we have the classes e, with the situation for the Ï â K separation: relative clasÏ, K, p. There may be discriminants among all of these, siï¬er performance can depend substantially on the proband we may train classiï¬ers to distinguish among binary lem. Finding the optimal approach may require extensive partitions of this set of classes. That is, we might have study, including consideration of systematics as well as a classiï¬er that preferentially returns a 1 for classes e or performance. However, in this case tuning an exhaustive Ï and a â1 for K, or p. We could train diï¬erent classi- ECOC to the same eï¬ciency as the likelihood selector ï¬ers for every such partition of the classes, resulting in an again provides a lower misidentiï¬cation for the same eï¬âexhaustive matrixâ. The aggregate of these classiï¬ers is ciency."
372,347,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"Generally, antennas can be adjusted so that the % terms are no more than % 1Ä± , and here we have assumed that they are small enough that their cosines can be approximated by unity, their sines by the angles, and products of two sines by zero. Instrumental polarization is often different for the antennas even if they are structurally similar, and corrections must be made to the visibility data before they are combined into an image. Although we have derived expressions for deviations of the antenna polarizations from the ideal in terms of the ellipticity and orientation of the polarization ellipse in Eq. (4.29), it is not necessary to know these parameters for the antennas so long as it is possible to remove the instrumental effects from the measurements, so that they do not appear in the final image. In calibrating the antenna responses, an approach that is widely preferred is to specify the instrumental polarization in terms of the response of the antenna to a wave of polarization that is orthogonal or oppositehanded with respect to the nominal antenna response. Thus, for linearly polarized antennas, following the analysis of Sault et al. (1991), we can write vx0 D vx C Dx vy and vy0 D vy C Dy vx ;"
38,362,0.984,The GEO Handbook on Biodiversity Observation Networks,"to users, but not necessarily a barrier. For example, it is not possible to estimate lake water quality from a mixed pixel because it is impossible to say how much of the chlorophyll in the pixel is coming from the water column and how much is coming from the riparian/upland vegetation on the edge of the lake. However, with spectral unmixing techniques or other sub-pixel analyses it is possible to estimate how much of that pixel contains water, which is useful for determining inundation and hydro-period."
306,8,0.984,"Early Geometrical Thinking in the Environment of Patterns, Mosaics and Isometries","Accumulated experience enables children to create a data set that is used by them to build up their mathematical knowledge. The use of materials for manipulation (e.g., stacking ready-made elements) has been considered to be the most effective learning environment. In addition, childrenâs drawings also have a high research value (Swoboda 2007). Patternsâ arranged with blocks, folded with puzzles, made from plasticine, lined from small pictures and ï¬gures, or drawnâare a friendly environment for children as they are close to their natural, spontaneous activity. They give the pleasure of creation without worrying about the outcome, create a chance to speak out without the fear of criticism, enable the realization of oneâs own ideas, and give motivation for manual and intellectual work. Research conducted in the United States has shown that more than 94 % of children beginning their education are able to count to 10 and identify basic shapes (Ginsburg 2004). Additionally, children between 5 and 10 can act in the âworld of regularitiesâ by discovering them. During the creation of geometrical compositions, constructing buildings with blocks, or decorating carpets, children not only better learn geometrical shapes (by comparing the lengths of the sides or recognizing the size of the angles). They may also feel the need for such arrangements that an adult can describe using the language of geometrical relationships. Symmetry, illustrated either in a broader or narrower sense, is an idea that has been used by people to describe beauty, order, and perfection. These arrangements may appear accidentally, by trying and checking, until the child considers them to be sufï¬ciently pretty. A sense of order tends to be veriï¬ed visually by children. Hence, propaedeutic of geometrical ï¬gure-to-ï¬gure relationships may reside in the sense of a certain order or harmonyâa speciï¬c arrangement of a surface or available fragment of space. The idea of engaging children in the world of rhythms and regularities is a welcome phenomenon. The preschool period is already a good time for children to become interested in building shapes and ï¬nding patterns (Clements 2001). Generally, it has been stated that creating their own patterns is a good starting point for childrenâs understanding of geometrical transformations. It is a long way to go from building a mosaic to creating geometrical concepts, but the connection between both is clear. In some handbooks for teachers, there are suggestions to do exercises with changing a ï¬gureâs position, such as drawing patterns and mosaics where translation, rotation, and mirror symmetry are used (Jones and Mooney 2003). The creative process included in childrenâs activities is regulated by perception. Theories concerning the development of geometrical reasoning stress that, at ï¬rst, understanding is passiveâconsisting of attracting attention to a particular phenomenon: the shape of a ï¬gure or line or the mutual arrangement of objects in"
355,61,0.984,"Agile Processes in Software Engineering and Extreme Programming: 19th International Conference, XP 2018, Porto, Portugal, May 21â25, 2018, Proceedings (Volume 314.0)","Denotes that an object of Class2 is shown or can be manipulated through the underlying element in the UI. However, this element is obtained navigating from an association called associationName from another element of class Class1. Subclass is a type of Superclass"
360,233,0.984,Compositionality and Concepts in Linguistics and Psychology,"small and not statistically reliable (F(2, 255) = 0.82, n.s.), conï¬rming our expectation that an atypical color would not make participants hesitate on the category membership of the object shown. In Pretest (b), we asked the same group of participants from Pretest (a) for their color-shift judgment along a color spectrum, in order to conï¬rm that they did indeed see a color change in our stimuli, and that the locus of this change was not skewed too much toward one end of our color manipulation spectrum. In this task, the participants saw the entire spectrum of 11 colors of each object category on a single screen and indicated the manipulation level at which they thought there was a color shift by typing in the corresponding number (see Fig. 2). The direction of the spectrum on the screen (from Level 1 to Level 11, or from Level 11 to Level 1) was randomized for each trial. With Level 6 being the midpoint on the scale of 1â11, participants reported a perceived color shift around an average level of 5â7 for all our Main categories, as we expected (see Table 2)."
360,90,0.984,Compositionality and Concepts in Linguistics and Psychology,"be such a separate component of a theory of language. Often, although not always, this component amounts to a truth-theoretic account of the values of syntacticallycharacterized sentences. This typically involves a translation of the natural language sentence into some representation that is âintermediateâ between natural language and a truth-theoryâperhaps an augmented version of first-order logic, or perhaps a higher-order intensional language. And other times it involves a further level of syntax, LF (sometimes this is inaccurately called âlogical formâ), which itself is then interpreted by some truth- or information-theoretic method. The Essentialists who study semantics in such ways usually agree with Chomsky in seeing little role for pragmatics within linguistic theory. But their separation of semantics from pragmatics allows them to accord semantics a legitimacy within linguistics itself, and not âjustâ in psychology or sociology. This group are known by the general name of Formal Semanticists. And perhaps this is Evansâ group of âtraditional linguistsâ who adopt âliteralismâ? So for these Formal Semanticists, such features as the individual differences between speakers on their beliefs about âthe worldâ will become a part of pragmatics, and not a part of semantics. And hence, not a part of linguistics. (But rather, of some social psychological theory, or perhaps a theory about an individualâs cognitive life.) This is a view they share with the philosophers of language who were surveyed earlier. Chierchia and McConnell-Ginet (2000) and Heim and Kratzer (1998) are leading textbooks in this general framework, and both adopt a âlogic-orientedâ approach to semantics that employs a possible world semantics and other formal tools. Their differences reside in their length (Chierchia and McConnell-Ginet is much longer than Heim and Kratzer) and to some extent in their differing underlying syntactic frameworks. (Chierchia and McConnell-Ginet 2000) put their case in the preface and beginning of the book as follows: [W]e focus on what has come to be known as logical, truth-conditional, or model-theoretic semantics. This general approach to meaning was developed originally within the tradition of logic and the philosophy of language and over the last twenty years or so has been applied systematically to the study of meaning in natural languages, due especially to the work of Richard Montague. (Preface) Whatever linguistic meaning is like, there must be some sort of compositional account of the interpretation of complex expressions as composed or constructed from the interpretations of their parts and thus ultimately from the interpretations of the (finitely many) simple expressions contained in them and of the syntactic structures in which they occur. . . . . An important test of a semantic theory is set by compositionality. Can the theory generate the required interpretations for complex expressions from a specification of interpretations for the basic items? As we will see, explicit specification of how word meanings are combined to produce sentential meanings is not a trivial task. (pp. 6â7)"
192,389,0.984,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"When it comes to lessons learned, the results seem fairly disappointing. Abmaâs most concrete remedy is to enforce a maximum number of academic publications (2013, p. 162), which probably would result in a proliferation of pseudonyms, but the real problem of course is not the number of publications but rather the quality of many of these publications, the ways in which the data are procured, etc. The proposal would result in (yet another) top-down rule, but fails to explain or address what really drives this issue: the cupido scribendi â the academic will (desire, urgency, Trieb, etc.) to publish (a drive which also applies to Abma himself no doubt, as author of his book)."
245,1018,0.984,The European Higher Education Area : Between Critical Reflections and Future Policies,"Expressed in this form, as a list of typical indicators of what may be considered excellent or exemplary practice (in which respect it parallels the typical form, used in Anglo-Saxon contexts, of presentation of professional standards for speciï¬c workforces, such as teaching, e.g. AITSL 2011; DfES 2004; Scottish Executive 2005; Welsh Government 2011), this model of the âextendedâ researcher may serve as an aspirational guide and, by extension, as a potential motivator, for researchers at any stage of their careers. I do not propose this precise model, with the speciï¬c researcher characteristics or âstandardsâ listed above, as the one that the European research community should adopt; rather, I present it as indicative of the kind of yardstick of researcher excellence that could be formulated and promoted. The detail of the content must be discussed and agreed, so that as many stakeholders as possible will have ownership of it. This could be done at European level, with the aim of agreeing a model of the characteristics of excellent European researchers generally, or it could be speciï¬c to disciplines, or to national contexts, or even to institutions. Yet it is also important to present developing researchers with both ends of the yardstick against which they should be evaluating their own practice, indicating not only standards of practice that are considered to represent, but also those considered to fall far short of, excellence. The latter help elucidate the former and encourage introspection on the part of the researcher."
360,566,0.984,Compositionality and Concepts in Linguistics and Psychology,"nouns in these comparisons (4.70 and 2.97), as expected. But its score in the single-subject between-predicate comparison This ï¬lm is more Italian than American, 5.24 (1.68), was similar to the noun scores (4.00). These comparisons appear, indeed, to be especially suited to nouns. Moreover, this comparison type manifested the least signiï¬cant correlation. This may stem from the existence of a second reading, as suggested in the discussion of (6d), whereby similarity of an entity to two nominal prototypes is directly compared. This reading does not involve dimension counting, and thus, its contribution to the naturalness judgments is expected to be independent of the naturalness of exception phrases. Its existence is compatible also with the relatively high naturalness of nouns in this construction. At any rate, the clear preference of additive nouns over multiplicative nouns in this comparison does support the existence of a reading based on counting of accessible dimensions, as the paraphrases in (6câd) suggest. An anonymous reviewer observed that the between-natural-noun comparisons involved taxonomical hierarchies, e.g., snake and lizard were predicated over reptile and wolf and tiger over predator, whereas some between-social-noun comparisons didnât, e.g., champion and celebrity were predicated over football player. A second reviewer suggested that failure of an implicature may have confounded the results. The suggested implicature is that one of the two compared predicates (e.g., rose and dandelion) applies to the entity in question, but the speaker does not know which. Naturalness may have been reduced by the difï¬culty of imagining a speaker being unsure of whether a flower is a rose or a dandelion. These potential confounds deserve investigation (e.g., by presenting sentences in contexts where the implicature is either satisï¬ed or not), but they cannot explain why differences between natural and social nouns occurred also in several other constructions, speciï¬cally within-noun comparisons and exception phrases, of which the between-noun comparisons were actually more acceptable. A number of additional issues merit attention. Recall that the embedded clause in the within-predicate comparisons was as in This football player is more a champion than that football player (is), rather than than that one (is). The idea was to block interpretations of that one as relating to a property, which would turn the comparison into a between-noun one, but this feature may have reduced the naturalness of these comparisons, and thus should be eliminated in future studies. Moreover, in the exception phrase conditions, each noun was matched with but one dimension. This fact may have added noise that reduced the signiï¬cance of the results. Future research should assess judgments for exception phrases with at least 3â4 dimensions per noun or adjective (cf., Sassoon 2012. and Shamir 2013), as well as with other quantiï¬cational constructions, such as P in every respect, some respect, and most respects. Nouns and adjectives of additional domains can be tested (for instance, abstract nouns: problem, love), as well as additive natural kind nouns and multiplicative social nouns, assuming such exist. The role of mediating particles (cf., of in (3a)) should also be addressed. The present study has broader implications for our understanding of the adjective-noun word class distinction, including in particular ânounyâ adjectives and adjective-like nouns. Consider in particular, nationality concepts such as American"
264,1041,0.984,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"and began to problematize this representation by noting all the ways that empirical work could go wrongâe.g., (arrow 1) for decades of medical research, the âhuman populationâ was represented by male experimental subjects, and the results of many studies later turned out not to apply to women. Similarly, (arrow 2) what one decides to consider important in learning makes a big differenceâwe noted that nations rank differently on TIMSS and PISA, because the two mathematics tests capture different aspects of what might be considered mathematically important. Group members similarly (arrow 3) discussed alternative interpretations of statistical ï¬ndingsâlooked at one way, certain results seemed signiï¬cant, but from another perspective, they were not. Nor were challenges limited to the quantitative realm; questions of how to be conï¬dent about interpretations of discourse analyses received attention, as did issues of triangulation and the use of multiple methods. Likewise mapping back from analyses to the âreal worldâ was an issue: the fact that many âsigniï¬cantâ laboratory studies did not translate to meaningful learning gains in real classrooms was raised as an issue of concern. Perhaps the most interesting, and passionate, part of the conversation dealt with what has been called the âpolitical turnâ or âsocio-political turnâ in mathematics education. Members of the group noted that the conceptual analytic models in the second ï¬gure often were shaped by tacit social biasesâthat the âcleanâ analytic descriptions in the ï¬gures could obscure various forms of racism, sexism, gender bias, and more. There was a general concern among the group that the ï¬eld needs to attend in more explicit ways to the possibility of such bias, and address it in our work. At the same time, there was appreciation for the fact that, although there is a lot more to be done, the ï¬eld has made tremendous progress since the ï¬rst ICME in 1969. Open Access Except where otherwise noted, this chapter is licensed under a Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons. org/licenses/by/4.0/."
228,239,0.984,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"The mandala factor defuzzification operator is inspired by mandala. Let A be a given fuzzy number shown in Fig. 6.2. Let it assume the shape of a trapezoid in Fig. 6.7a. A trapezoid can in a particular case come down to atriangle, but we remain at a trapezoid, which makes our analysis more universal. Then one must fill in the outline marked by the sides of the number and the OX axis with virtual grains of sand in Fig. 6.7a. A number of virtual sand grains are collected in this way. Then one must construct a rectangle, the base of which is equal to the support value of the fuzzy number. The rectangle built in such a manner should be filled with virtual sand grains, starting from the outermost left side in Fig. 6.7b. The filling process should be done vertically in columns until all grains are used. A real number obtained as a result of defuzzification is the value above which the last filled column was finished. Mathematical formalism (6.54) of the above-described mandala factor visualization is shown below. Calculation of the R value uses the mandala factor M for the rising edge, falling edge, and core set function integral. Then the obtained value should be scaled from the center of the coordinate system by adding it to the start of the support value of the fuzzy number. When defuzzification is performed in the OFN arithmetic, then in the case of a positive order, one should proceed as described below, whereas in the case of a negative order, one should deduct the calculated value"
32,208,0.984,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","Where G.; x/ denotes the normal distribution with its deviation . After settling to the system, the species will experience either obtaining a new link from a newly introduced species or loosing a link during the extinction of interacting species. Those processes change the fitness of the species, and hence the distribution function. This change in FDF is found to be the one step of random walk with negative drift whose strength is proportional to 1=m. Therefore, writing this process by an operator DO , the (not normalized) FDF of species those have been experienced a loss or addition of one incoming link can be calculated from F0 as, O 0 .m; x/; F1 .m; x/ D EO DF"
269,44,0.984,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"disciplinary expertise. Thus a grant proposal crossing disciplines A and B is likely to be reviewed by an expert on A and an expert on B â neither of whom is necessarily likely to feel favourably towards the other, nor towards the interdisciplinary proposal as a whole. Below, in providing examples of peer reviewer comments we have received on some of our interdisciplinary grant applications, we dwell on what such comments might tell us about the possibility â or impossibility â of intervening in the field of interdisciplinary research on the mind and brain. The quotation that begins this section comes from our first example. With some of our neuroscientific collaborators, we had proposed to run a series of brain-imaging experiments whose design would not be specified in advance, but which would emerge iteratively from a series of interdisciplinary workshops that we would run. This got short shrift: âIn my field (scientific and medical neuroimaging)â, one reviewer wrote, âit is imperative to describe the proposed experiment in detail, e.g. paradigms, number and type of subjects, analysis plan ... . I did not feel sufficient information was provided to indicate the quality of outputs.â Here, we rub up against keenly guarded disciplinary norms surrounding robust research. But if this neuroscientist found our proposed experiment too loose and underspecified, another reviewer (likely someone from the social sciences) found it too concrete: our proposal demonstrated a ânaÃ¯ve realism about the experiment and its ambitions for experimental convergence ... [while the] ambition to establish a more general protocol to be followed by others is epistemically naÃ¯ve and unreflexively normativeâ. The broader problem becomes clear here: try to design an interdisciplinary experiment that puts together the empirical rigour of neuroimaging and the conceptual openness of the humanities, and there is a good chance that you will be condemned by neuroscientists for your ambiguity, and by humanities scholars and/or interpretive social scientists for your empiricism. In a second example â from a peer review of a different grant application â we were informed that while our project had âpotentialâ, we should not only do more to test our proposed approach (presumably prior to applying, again, for funds), but specifically provide âevidence of potential to engage neuroscientists in a reciprocal relationshipâ. The reviewers did not see sufficient evidence of âreal collaboration with the community [we were] targetingâ. Note, first, how the injunction to engage our targeted community (âneuroscientistsâ) betrays the assumed direction of travel: it seems inconceivable, for example, that the field might be too entangled"
90,118,0.984,"Informatics in the Future : Proceedings of the 11th European Computer Science Summit (ECSS 2015), Vienna, October 2015","reason and logic. If everyone lied all of the time, or many people much of the time, it would be very hard for society to function. This matter provides an illustration of the Snow Two Cultures thesis: computer scientists trying to address fault tolerance and safeguard computer networks have made considerable progress towards understanding how a system can work correctly even when some or possibly many of its components, consistently or occasionally, deliver wrong information. Every ethicist should have read the Paxos paper. On the other hand, the most obvious lesson, once you have learned from such work how to deal with systems that lie, is that if there is a choice truth always wins. We deal with lies because the world is imperfect and TCP/IP loses the odd packet or two, but things would be so much easier if Alice could always trust Bob. The rational arguments are not enough; truthfulness in human affairs is in the end an ethical issue. What better evidence could there be, for the last example of a discussion arising from an informatics conference, than the titanic Volkswagen lie of 2015? The software engineers, and their managers, and most likely the managers of their managers, apparently thought that results-faking software would gain VW a few points of market share. The company will, it seems, recover; but only after providing, through its egregious violation of the Principle of Truth, a textbookquality example of unethical behavior, sure to serve many generations of future ethicists."
173,235,0.984,"intelligent Human Computer interaction : 9Th international Conference, Ihci 2017, Evry, France, December 11-13, 2017, Proceedings","maximum of 15 min. As a video progresses, there are certain checkpoints that an instructor breaks a topic into and these checkpoints serve as the keynotes for the topic at hand. For example, a video about supervised learning of machine learning would typically discuss about the common examples in which it is used, then explain the algorithm employed, plot the points representing the features and interpret it, differentiate with other machine learning algorithms and finally conclude the scenarios and advantages where the algorithm applies. These checkpoints, although important to the MOOC taker at that instant, seem to fade away when the next video starts. The MOOC taker is reluctant on obligating to memory rather tend only to remember those parts which are needed to pass the quizzes. To address this issue, we propose a novel model whereby the MOOC taker can take notes on the fly when they are taking the course through watching videos. For the MOOC taker, the parts of the course which they intend to take note, it happens to be certain points in the video. It is assumed that the video is accompanied by an interactive transcript that scrolls and highlights what the instructor is saying at that moment of the video. DurFig. 1. MOOKBook work flow ing the video, there may happen to be equations, diagrams, graphs and example scenarios that explains the topic from various perspectives. To take the corresponding notes by hand, it would take stopping the video, taking the conventional note book up and writing or drawing whats on the video screen at that instant. This would take up the valuable time that the MOOC taker has invested already. The proposed on the go note taking, while the MOOC taker watches the video is a meta description extraction using a client side scripting on the browser that the learner is currently using to access the materials. The parts of the lecture which catches the attention of the learner are simultaneously displayed in the transcript. A recurrence script extracts transcript with the screen and add the portions to the notebook on events initiated by the user. The learner can save a considerable amount of time which they would otherwise be using for taking the notes conventionally. The user can view the updated note in the browser itself so that it gives a better perspective of what has been learnt (Fig. 1)."
208,143,0.984,Actors and the Art of Performance,"finished, completed. But, and this is whatâs fantastic about theater, on the next night, the next performance date, it can be repeated and new life can be breathed into it at each repetition. It can be repotentialized. What does that mean? Each individual performance is saved in the actorâs memory as a result of the rehearsal process â all directions, all the right and wrong turns, thoughts, feelings, texts, contexts, appearances, entrances, exits â the entire fabric of scenes and dialogues. They have been inscribed within him and memorized. He can draw from them and play them again and again, and each performance lays down another memory pathway, so that his archive continually becomes fuller and richer. But only if the actor risks what has been before and frees it can he again electrify it. Only if he time and again and once more risks opening his acting to the uncertainty of his movements does a performance take off. This act of creative repetition is what makes acting so electrifying. It is its aesthetic desire, for the actors and for the audience. It opens all involved to a temporal piece of art that defies common sense, the reason of the everyday. Or perhaps it opens them to the gift of the muses that allows the dawning of an era in which the law of chronology no longer holds. The actor looks to the remembered past. He brings it into the present word for word, situation for situation and at the same time sends it into the future word for word, situation for situation by taking all that has happened and again exposing it to the openness of the present. In this way, he secures the future of his acting. There is no closure, because it is reopened in every performance. The actor may be chained to the chronology of the plot and to a certain setting, but in the kairos of time â in the present, past, and future â he can again find, recognize, develop, and remember new and ever more complex meanings in the play and its performative form. He can make good on something he maybe owes the play. He can go back to what has been in time and make up for lapses after the fact. For this reason the difference in each repeated performance is always also an act of freedom and of liberation, an act of regeneration.51 He overturns the past and present because the future acts within him â always unique, always singular. For this reason it is not the same performance that is given each evening in each show with the same name, but each 51 Arno BÃ¶hler, âNietzsche â Vom regenerativen Charakter des GemÃ¼ts,â psycho-logik 2, Existenz und GefÃ¼hl (2007)."
108,65,0.984,Bordieuan Field Theory as an Instrument for Military Operational Analysis,"trafficking of people, weapons, drugs etc., was established as in all conflicts, with various actors using violent means to establish their territories (meaning here primarily not physical territory).2 Partly, because of this professor, Mary Kaldor has described this form of conflict and this specific conflict as a new form of war. Kaldor thus writes that she has identified a new form of war which has emerged since the Cold War, involving more or less organised violence between different parties (often not states). The motives behind this phenomenon are many and varied. As I put a theoretical perspective of my own on the conflict, I would like at this point to dwell a little on Kaldorâs ideas, ideas which have met some criticism.3 Her view of the informal economy as the driving force behind the war is essentially correct; she makes a fundamental point when highlighting the presence of the criminal world as an influential force. In bourdieuan terms, one could refer to a changing social field. That said, I would like to take issue with many of the theories she promotes. One problem is that history shows us plenty of examples of the fact that the parameters for the ânewâ wars already have existed and still exist; it is easy to recognise the phenomenon from history. The informal economy is not new, certainly not where war is being waged. People in the West today are used to regarding war as something that takes place between sovereign states. However, one does not need to look too far back in time to see the processes that Kaldor describes as part of just about every nation-building process that has occurred in Europe (this is probably also true for other parts of the world, but certainly holds generally throughout Europe). She highlights globalisation as a new contributing factor to this new type of war. It must be said that the consequences of war now have a more wide-ranging influence, but that this should affect events in a qualitatively new fashion remains to be proved. Breeding grounds for unrest have taken on a new significance within criminal circles, when all sorts of illegal transport make its way through the lawless country like electricity conducted through copper. There has,"
372,1767,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"The negative sign indicates that the bending is toward the Sun, which is the opposite sense of bending by interplanetary medium. Measurements of the solar general relativistic bending are discussed in more detail in Sect. 12.6."
119,375,0.984,Contemporary Issues in Human Rights Law : Europe and Asia,"Firstly, as a matter to be attended to when we introduce a German issue to Japan to review its constitutionality, Professor Tetsuharu Matsumoto37 notes that in Germany the idea of applying a strict test to the regulations on freely expressing opinions has not been adopted. According to him, there are differences between the thinking in Germany and the United States, since the US Supreme Court has applied a test mainly based on the First Amendment, and judicial reviews are incidental and concrete. In contrast, in Germany, he mentions how, when a review is conducted, it involves human dignity at its core, and, furthermore, the Constitutional Court can also conduct an abstract review. After adding that he does not have details of Germanyâs in-depth discussions, Professor Matsumoto speculates that it is difï¬cult under the German approach to describe freedom of speech as a dominant material value, because the concept of human dignity is at its core. In his view, whereas dealing with the conflicts between judicial reviews and democracy has been an issue in the United States, it might not arise in Germany, based on the Constitutional Courtâs competence in dealing with abstract reviews. The result of that conclusion for him is that, âit might be difï¬cult for the thought to arise that the courts should strictly review freedom of speech, when democracy is accepted as legitimate.â However, his observation is likely to require two considerations. The ï¬rst question is whether, in Germany, a conflict did not arise between judicial reviews and democracy.38 The other question, dealt with in this article, is whether there exists a German view on applying a strict test to regulations on the right to freely express opinions. With regard to this question, Professor Matsumoto refers to the views of Professor Shigenori Matsui, which are presented next. Professor Matsui says that there is no one approach to applying different tests to each aspect of fundamental human rights in Germany, and he asserts that the"
85,113,0.984,Bayesian Methods in the Search for MH370,"Although it is unlikely that any given trajectory will travel for more than six hours with no manoeuvre, those that do all end up in roughly the same place. Figure 7.4 shows a contour representation of the pdf of latitude and longitude at 00:19 that was generated using 200,000 particle draws from the prior model with no measurements and an isotropic kernel. The red diamond in the figure shows the mean of the prior. There is a clear peak corresponding to paths that make no manoeuvre. This is smeared around an arc corresponding to paths that turn once but maintain a steady speed and also radially from the start point corresponding to paths that do not turn but do change speed. The model samples a hyperparameter for the time between manoeuvres from a prior that covers 0.1 to 10 h. When a particle samples a very high value for the time constant it is unlikely to choose to manoeuvre. Where manoeuvres were selected it was again unlikely to choose to change speed and direction, leading to the contours shown. The altitude behaviour of the model is not visualised in the figure. There is an approximately circular region around the initialisation point where the pdf is elevated. This corresponds to particles that have sampled a very small time between manoeuvres: these turn so much that they circle back on themselves repeatedly and are effectively trapped close to their starting location. Beyond this circle, the pdf is"
51,273,0.984,How Generations Remember,"Minelaâs narrative is full of ambivalence towards Yugoslavia. As we have seen, she speaks highly of Yugoslaviaâs progressiveness and the multinational coexistence it enabled and nurtured. She emphasises that people were all the same and that most people did not care about the nationality of their compatriots. She claims that only the Bosniaks have kept this spirit while the others in the Mostar context (referring primarily to Croats) have tried to distinguish themselves from the others. Despite this positive view of Yugoslavia and her values, Minela simultaneously argues that multinationality is dangerous when it affects personal spheres like marriage because it leads to loss of identity. When she says, âI believe that this war had only one positive outcome: that we are no longer ashamed of ourselvesâ; she is referring to the strengthening of the Bosniak identity as a consequence of the war. We find both contrary discourses present in Minelaâs narrativeâabout the sameness of people and about a primordial national difference that one cannot and should not attempt to overcome. On the one hand, we could reason that the ambivalences found in her narrative are connected to the ambivalent position Bosniak political elites hold (see Chap. 3). To a certain extent this may be true but I argue that this ambivalence is characteristic for the Last Yugoslavs regardless of their nationality. This will become clear with Å½eljko, a Croat informant. As discussed earlier in the book, the present dominant Croat public discourse condemns Yugoslavia much more openly than the Bosniak one does. Å½eljkoâs positive memories of Yugoslavia thus disturb the negative picture of Yugoslavia that he ideologically supports."
75,5,0.984,"Opening Science : The Evolving Guide on How the Internet is Changing Research, Collaboration and Scholarly Publishing","Abstract In this introductory chapter we establish a common understanding of what are and what drives current changes in research and science. The concepts of Science 2.0 and Open Science will be introduced. As such we provide a short introduction to the history of science and knowledge dissemination. We explain the origins of our scientific culture which evolved around publication methods. Interdependencies of current concepts will be elucidated and it will be stated that the transition towards Open Science is a complex cultural change. Reasons as to why the change is slow are discussed and the main obstacles are identified. Next, we explain the recent changes in scientific workflows and how these cause changes in the system as a whole. Furthermore, we provide an overview on the entire book and explain what can be found in each chapter. Nicole Forsterâs goal as a researcher is to enhance cancer treatment. That is why she and her colleagues in the laboratory of Leif W. Ellisen at Massachusetts General Hospital Cancer Center in Boston, Massachusetts, study tumors on individual levels and search for cancer causes. In March 2012 Forster was trying to isolate ribonucleic acid (RNA)âthe genetic blueprint for proteins within the cellâwithin mouse cells. To prepare the cells for her experiment she mixed them with a special gel that provided them with all the nutrients to grow and proliferate, even outside the body, for a short period of time. Yet in the following step, she had to get rid of the gel to get to the information she needed: the RNA. And therein lay S. Bartling (&) German Cancer Research Center, Heidelberg, Germany e-mail: soenkebartling@gmx.de S. Bartling Institute for Clinical Radiology and Nuclear Medicine, Mannheim University Medical Center, Heidelberg University, Mannheim, Germany S. Friesike Alexander von Humboldt Institute for Internet and Society, Berlin, Germany e-mail: friesike@hiig.de S. Bartling and S. Friesike (eds.), Opening Science, DOI: 10.1007/978-3-319-00026-8_1,  The Author(s) 2014"
224,452,0.984,Ester Boserupâs Legacy on Sustainability : Orientations for Contemporary Research,"âGoverning the Commonâ (Ostrom 1990). Ostrom was subsequently honoured with the Nobel Prize in Economics in 2009. GâG stands for Gender or Gender Order. It is a point that I have added to the Ecological Complex. The crosscutting space of the rhombus and the core reflects the interplay between PETO components. This is a space to demonstrate and visualise what Boserup has called the âstatus of womenâ by analysing different components. With this concept, gender order as a social and cultural construction can be based on the components of the ecological complex. I will revisit the concept in my conclusions and the following illustrations. The status of women has often been explained by referring to culture and socio-cultural backgrounds of societies. It was and mostly still is treated as a black box, somehow inaccessible to scientific investigation and analysis. However, with G in the centre of the Ecological Complex, a new space for scientific research can be discovered within human ecological studies and the related fields of research."
372,1853,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"known from other observations, and the distance of the screen was taken to be half the distance of the pulsar. It was deduced that the angular separation of the images was # 3:3 mas, corresponding to a spacing of #1 AU (astronomical unit) between the refracting structures. In effect, the refracting structures constitute a two-element interferometer, with fringe spacing # 1 )as. For comparison, the angular resolution of a baseline equal to the diameter of the Earth at 430 MHz would be 44 mas. The particular conditions that resulted in this observation lasted for at least 19 days, and during that period, observations of other pulsars did not show similar scattering. This strongly suggests that the observed phenomenon resulted from a fortuitous configuration of the interstellar medium in the direction of the pulsar. Apart from cases of scattering such as that described, there are essentially no clear cases of spatially coherent astronomical sources, although coherent mechanisms may occur in pulsars and masers (Verschuur and Kellermann 1988). Fully coherent sources are not amenable to synthesis imaging using the van Cittertâ Zernike principle and thus do not fall within the area of principal concern of this book. Further material on coherence and partial coherence can be found, for example, in Beran and Parrent (1964), Born and Wolf (1999), Drane and Parrent (1962), Mandel and Wolf (1965, 1995), MacPhie (1964), and Goodman (1985). Open Access This chapter is licensed under the terms of the Creative Commons AttributionNonCommercial 4.0 International License (http://creativecommons.org/licenses/by-nc/4.0/), which permits any noncommercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made. The images or other third party material in this chapter are included in the chapterâs Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapterâs Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder."
213,48,0.984,Collider Physics Within The Standard Model : a Primer,"In this case there is a continuous orbit of lowest energy states, all with the same value of jMj, but different orientations. A particular direction chosen by the vector M0 leads to a breaking of the rotation symmetry. For a piece of iron we can imagine bringing it to high temperature and letting it melt in an external magnetic field B. The presence of B is an explicit breaking of the rotational symmetry and it induces a nonzero magnetization M along its direction. Now we lower the temperature while keeping B fixed. Both  and 2 depend on the temperature. With lowering T, 2 goes from positive to negative values. The critical"
32,288,0.984,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","the continuous relaxation of the discrete minimization problem of the so-called RatioCut. The RatioCut is lower for a partition with a sparse cut, while it penalizes for unbalanced partitions in the sense of the number of the vertices within a module; there may always exist a better cut in the sense of the RatioCut than the planted partition in the graph when is large. Finally, let us compare our estimate with results of studies in the literature. In the following, we focus on the case of equal size modules, i.e., p1 D p2 D 0:5. Let the total degree within a module be Kin and let the total degree from one module to the others be Kout . Since we have K D cN D 2.Kin C Kout / and Kout D N, Eq. (12.15) reads"
62,466,0.984,"Agile Processes in Software Engineering and Extreme Programming: 17th International Conference, XP 2016, Edinburgh, UK, May 24-27, 2016, Proceedings (Volume 251.0)","I think that part of the question regarding the role of an architect emerges when agile is compared to waterfall. In a waterfall environment, there is an explicit stage called the design phase. This phase is led by the architect. He designs and documents the soluâ tions. The design is prescriptive. The assumption is that if the developers follow the design, then the software will be OK. This design phase is the most demanding and solemn of the entire software phase. If you get the design phase wrong, you will end up paying a high price for years to come. On the other hand, in an agile environment, there often is no design phase. In conseâ quence, the reasoning often goes: âIf in an agile environment, there is no design phase, so there is no designer hence no architect.â So we must clarify. No design phase does not mean âno designâ. No design phase means that there is not an explicit phase during which design happens exclusively. Rather, design happens all the time. Likewise, when we say that the architecture is emergent, that should not mean that we do not need architects because architecture just happens. That is not true. It means something much more subtle. When we say that the architecture is emergent, we are saying that as the system evolves, a certain structure emerges. Someone needs to keep an eye on that emerging structure. When it is emerging according to plan, the architect will strengthen it, and when it is starts diverging, the architect will act accordingly. Maybe adjusting"
272,702,0.984,Reconsidering Constitutional Formation Ii Decisive Constitutional Normativity : From Old Liberties To New Precedence (Volume 12.0),"For an overall view of the main questions on the theme please see Miceli (1902). Many examples taken from parliamentary acts are included in Ferrari Zumbini. Tra norma e vita. Il mosaico costituzionale a Torino 1846â1849 cit. The author notes how the Statute norms were reference parameters which changed according to cultural and social awareness. Generally, in the Italian case there had been a constitutionalisation without a hierarchicalisation of norms. With reference to the relationship between hierarchicalisation and constitutionalisation Dieter Grimm noted: Â«The hierchicalization of legal norms does not by itself produce a constituzionalizationÂ». Cf. Dieter Grimm, The origins and transformation of the Concept of the Constitution. In Constitutionalism cit., 6. In his renowned volume dedicated to English public law, Albert V. Dicey speciï¬ed, referring to the concept of sovereignty of the Parliament, that in England no juridical distinction existed between Constitution and the other laws and that no power existed to nullify an Act of Parliament (86). Later the author clariï¬ed that Â«The expression âunconstitutionalâ has, as applied to a law, at least three different meanings varying according to the nature of the constitution with reference to which it is used: (1) The expression, as applied to an English Act of Parliament, means simply that the Act in question, as, for instance, the Irish Church Act, 1869, is, in the opinion of the speaker, opposed to the spirit of the English constitution; it cannot mean that the Act is either a breach of law or is void. (2) The expression, as applied to a law passed by the French Parliament, means that the law, e.g. extending the length of the Presidentâs tenure of ofï¬ce, is opposed to the articles of the constitution. The expression does not necessarily mean that the law in question is void, for it is by no means certain that any French Court will refuse to enforce a law because it is unconstitutional. The word would probably, though not of necessity, be, when employed by a Frenchman, a term of censure. (3) The expression, as applied to an Act of Congress, means simply that the Act is one beyond the power of Congress, and is therefore void. The word does not, in this case, necessarily import any censure whatever. An American might, without any inconsistency, say that an Act of Congress was a good law, that is, a law calculated in his opinion to beneï¬t the country, but that unfortunately it was âunconstitutionalâ that is to say, ultra vires and voidÂ». Cf. Dicey (1889). Appendix, Note VâThe meaning of the âunconstitutionalâ law, 427â428. Â«The epithet un-constitutional is applied to breaches of conventions as well as of law, meaning that the public opinion condemns (or should condemn) the actÂ». Cf. Wade and Phillips (1931, 8)."
249,311,0.984,Advances in Proof-Theoretic Semantics (Volume 43.0),"on identity of content, [4, p. 20f]. This paragraph could be used as further support of our account here, yet, Frege, by the time of the Begriffsschrift, didnât bring forward the notions of sense or mode of presentation. 8 In the discussion of this example, âstarâ is, of course, to be understood as a folk term."
132,127,0.984,AiREAS: Sustainocracy for a Healthy City: Phase 3: Civilian Participation â Including the Global Health Deal Proposition,"ms: SD2 (global HRV: higher values = long period of tension)âweekly average (just this case): 128.6 ms In the reference case above, personalized feedback is provided in the sense of: The average heartbeat of just above 70 beats/min is perfectly normal. It is interesting to see that no large deviations are detected over a signiï¬cant period of 10 h. In the middle graph (RMSSD), we see the most used HRV numbers. This is just one of the ways to look at the balance between the interaction of stress and relaxation. In your particular graphic, we see more fluctuations there than in the one of the heartbeat. Around noon, there is a clear moment of rest. Also, between 15:15 and 16:45, we see this period of relaxation. The same returns in the other graphs below the middle one, but is less prominently visible."
315,157,0.984,Situating Children of Migrants Across Borders and origins : a Methodological Overview,"The quantitative analysis of life course trajectories implies the use of retrospective data, or, in the ideal case, of panel data that provides information on the same individuals over an extended period of time. If trajectories are conceived as sequences of statuses between different points in time, a first goal could be to reconstruct those sequences in order to describe trajectories. Following Lesnard (2006), one needs methodological approaches that âcast light on social rhythms, on the social structuration of the timing of eventsâ (Lesnard 2006: 21). Sequence analysis is a method As an example here, Portes and Zhou (1993) see the school success of the children of Punjabi Sikhs in California despite a strong prejudiced experienced by this group as a consequence of the strong influence of the ethnic community in the neighborhood over the second generation."
228,140,0.984,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"Certainly this is just one of the possibilities to generalize the standard notion of a support onto the case of OFNs. Depending on interpretation, one might also think about defining supp A as an ordered interval [8] or as an interval (ordered or nonordered) taking into account only the quantities of s A and e A . Nevertheless, any reformulation of support should be equivalent to standard support whenever an OFN can be interpreted as a standard convex fuzzy number."
311,622,0.984,The Physics of the B Factories,"forbidden decay KL0 â Î¼Â± eâ . The experiment deï¬ned a signal region in two kinematic variables, the Î¼Â± eâ invariant mass (MÎ¼e ) and the KL0 candidateâs transverse momentum squared (PT2 ). The signal region was subsequently âblinded,â i.e., events falling within this region were not selected for viewing, while all selection criteria were ï¬nalized. Only after these criteria were ï¬nalized was this region unblinded and signal events counted. A similar technique was used by BNL E787 (Adler et al., 1996), which searched for the rare decay K + â Ï + Î½Î½, and by BNL E888 (Belz et al., 1996a,b), which searched for a longlived H dibaryon. The method was subsequently adopted by the Fermilab KTeV experiment (Alavi-Harati et al., 1999), which measured Ç«â² /Ç« in the K 0 -K 0 system; Fermilab E791 (Aitala et al., 1999b, 2001a), which measured rare/forbidden D meson decays; and the CERN NOMAD experiment (Astier et al., 1999), which searched for neutrino oscillations. As mentioned, the principle of a blind analysis is to not look at potential signal events before ï¬nalizing analysis criteria in order to avoid biasing the result. There are three main types of measurements this applies to: setting an upper limit, in which one wants to avoid selection criteria that bias one against signal events; measuring a branching fraction, in which one wants to avoid selections that bias one against background events (this can âsculptâ a signal peak); and precision measurements such as that of measuring mixing or CP -violation parameters, in which one wants to avoid selections or ï¬tting procedures that bias the result in a preferred direction. Some general examples of these cases are discussed below, followed by speciï¬c examples from Belle and BABAR. Not every measurement requires a blind analysis: usually when one searches for new particles and does not know a priori where to look, one inspects relevant distributions in an unblind manner. However, one still must be careful not to adjust selection criteria to increase or decrease the signal yield while looking at the signal events for feedback. A blind analysis is typically more time-consuming than an unblind one and, in the case of setting an upper limit, can produce a poor result (see below)."
141,136,0.984,Cyber-Physical Systems of Systems: Foundations â A Conceptual Model and Some Derivations: The AMADEOS Legacy,"receivers will only be able to observe object data which is (more or less closely) related to the original data sent and needs to be correctly interpreted to avoid property mismatch. A model of the environmental dynamics able to represent the processing and modiï¬cations performed on data would be paramount in the understanding and mastering of stigmergic information exchange. In cyber space data is represented by a bit-pattern that can be generated by the processing of stored Itoms or by some data acquisition process, e.g., by a sensor. For data acquisition, the design of the sensor determines how the acquired bit pattern has to be interpreted, i.e., provides for the explanation of the object data. Since an Itom is a higher-level concept than the sole object data in an Itom, we propose to use Itoms in the speciï¬cation of Relied-Upon Interfaces (RUIs) among the Constituent Systems (CSs) of a CPSoS (see Chap. 2). According to [23] the full speciï¬cation of an Itom has to provide answers to the following questions: â¢ Identiï¬cation: What entity is involved? The entity must be clearly identiï¬ed in the space-time reference frame. â¢ Purpose: Why is the data created? This answer establishes the link between the raw data, the reï¬ned data and the purpose of the CPSoS. â¢ Meaning: How has the data to be interpreted by a human or manipulated by a machine? If the answer to this question is directed towards a human, then the presentation of the answer must use symbols and refer to concepts that are familiar to the human. If a computer acquires data, then the explanation must specify how the data must be manipulated and stored by the computer. â¢ Time: What are the temporal properties of the data? Real-time data must include the instant of observation in the entity. In control applications it is helpful to include a second timestamp, a validity instant that delimits the validity of the control data as part of the Itom [22] (p. 4). Message-based Information Flows: A message-based information flow is present if one CS sends a message to another CS. In many legacy distributed systems only object data is contained in a message while the explanation of the data is derived from the context. In a CPSoS the involved CSs can be operating in differing contexts, e.g., in the US and Europe. For example, in the US temperature is represented by degrees Fahrenheit, while in Europe temperature is represented by degrees Celsius. As a consequence, the same data (bit-patterns) can convey a different meaning if the contexts of the sender differs from the context of the receiver of the message, causing a property mismatch. Such property mismatches have been the cause of severe accidents. Stigmergic Information Flows: A stigmergic information flow is present if one sending CS acts on the physical environment and changes the state of the environment and later on another receiving CS observes the changed state in the environment with a sensor that captures the sensor speciï¬c aspect of the environment [24]. Consider, for example, the coordination of cars on a busy highway to realize a smooth flow of trafï¬c. In addition to the direct communication by explicit signals among the drivers of the cars (e.g., the blinker or horn), the stigmergic information flow based on the observation of the movement of the vehicles on the road (caused by the actions of other drivers) is a primary source of information for the assessment of a trafï¬c scenario. An"
113,38,0.984,Pentecostalism and Witchcraft : Spiritual Warfare in Africa and Melanesia,"Still, with Strongâs comments on an imagined and desired parallel consumer society in mind, a culture of consumerism is clearly outlined for certain African societies in ways that we have not yet seen in Melanesia (see Ferguson 1999; Roitman 2005; Weiss 2009). In places like urban Cote dâIvoire consumer objects and brands form entire symbolic systemsâso that, for example, Timberland shoes versus Sebago shoes, in Newellâs (2007, p. 473) example, becomes an existentially meaningful contrast. We get a sense that this is very different from the signification that consumer objects take in Melanesian societies. However, despite these contrasts, it is perhaps interesting to think more broadly about the way consumerism and witchcraft have become part of larger modes of moral signification. Indeed, this is the angle that Ruy Blanes takes in Chap. 4, when he describes sorcery in urban Angola. He goes one step further with an âoccult economyâ analysis in showing how sorcery is becoming a paramount index of value in urban Angola. Like the Dow Jones index that daily sets a standard for the universal stock exchange, ndoki sorcery is a similar index for measuring spiritual value in pentecostalized neighborhoods of Luanda. It orders and measures the moral parameters of social life and becomes a tool in the spiritual mapping of the city and in the ongoing processes of rumors, scapegoating, and accusations. There are also converging points between this value index and the value index of consumerismâsince money and goods that are withheld or redistributed are also valued or devalued by the ndoki index. Whereas in most traditional Melanesian examples the value index of witchcraft is negative, in the sense that it prohibits and destroys work and productivity, the ndoki index is a positive scale that sets the value of consumption independently of any production. This has many similarities to what Katrien Pype in Chap. 5 calls âa witchcraft complex,â by which she means a total institution that harbors references, connectivity, and contradictions along many planes. As we move from Luanda in Angola to Kinshasa in Congo, Pype finds processes very similar to those described by Blanes. Her point is that the very terms of Pentecostal witchcraft are changing. The prevalent idea among Pentecostals in Kinshasa is no longer that witchcraft is about âa break with the pastââ that is, a break with the rural, with kinship obligations, with traditions of fetishes, or with the older generationâas described in Birgit Meyerâs work two decades ago (Meyer 1998). Now, the Pentecostals formulate âa break with the futureâ and with technologies of connectivity. ICT"
173,319,0.984,"intelligent Human Computer interaction : 9Th international Conference, Ihci 2017, Evry, France, December 11-13, 2017, Proceedings","the following emotionsâhappy, sad, angry, scared, surprised, worried, and troubled. We arrived at these seven emotions by augmenting commonly observed emotions in counseling platforms with those that are widely accepted in psychological research [32]. These probabilities provide some âpriorâ information about the userâs emotions and hence serve as the priors in the Bayesian framework. Let the prior probability of an emotion i, be denoted by Pp (emoi ). Thus, there are multiple emotion variables, with each of these variables taking a value in the range [0, 1] indicating their probabilities. We leverage word synsets to obtain a rich set of words related to each of the emotions that we want to recognize. Synsets are defined as a set of synonyms for a word. Let the set of synsets across all the emotion categories be referred to as the emotion vocabulary. The words in a userâs text are then matched for co-occurrence with the emotion vocabulary and are weighted (normalized) based on their frequency of occurrence to obtain probability of an emotion. We found this simple approach quite reliable for our data. This will give the probabilities for various expressed emotions."
78,112,0.984,The Onlife Manifesto : Being Human in a Hyperconnected Era,"The web will certainly affect political forms; some people claim that it will give birth to a new participative democracy, under which the influence of the nation is diminished or where sovereignty no longer stands. Nevertheless, the recent emergence of populism in Europe shows the weakness of these assumptions. It goes the same, with the reinforcement of traditional Islamic parties after the Arab Spring, in Middle East and in many Arab countries (e.g. in Egypt, Libya, etc.) where authoritarian regimes have been overthrown. To conclude, we cannot extrapolate the exact nature of transformation from an analysis of the topology of the network, neither can we use it to further characterize the state of society as, in itself, the topology is static and consequently does not reflect social processes and the part they play in evolution of society."
134,175,0.984,Curriculum Reform in The European Schools : Towards a 21St Century Vision,"and other-examination, with these examinations being understood in both a personal and political sense, extending to Athenians and foreigners alike. Stoic cosmopolitanism in its various guises was enormously persuasive throughout the Greco-Roman world. Although the term cosmopolitan (ÎºÎ¿ÏÎ¼Î¿ÏÎ¿Î»Î¯ÏÎ·Ï, literally, world-citizen), originated earlier than the Stoic philosophers, it was these philosophers who gave meaning to the term, even if that meaning is somewhat different from the way it is used now. As early as 340 BC Diogenes the Cynic (1925a) described himself as âa citizen of the worldâ (in Greek, kosmopolites), and Antiphon (1965), a little later, wrote that âby nature we are all constituted alike in all things, both barbarians and greeks. â¦ This can be seen by consideration of those things which are essential by nature to all men. In these things no barbarian is set apart from us, nor any Greek. For we all breathe into the air through mouth and nostrils â¦â. Zeno (1925b), in his Republic, was reported by Plutarch as saying: â(m)oreover, the much-admired Republic of Zeno, the founder of the Stoic sect, may be summed up in this one main principle: that all the inhabitants of this world of ours should not live differentiated by their respective rules of justice into separate cities and communities, but that we should consider all men to be of one community and one polity, and that we should have a common life and an order common to us all, even as a herd that feeds together and shares the pasturage of a common fieldâ. Stoic philosophers later offered a dual notion of citizenship, that of the local polis, city-state or nation complemented by that of the kosmos (universe or world). This is redolent of the modern notion of layered citizenship, embracing local and global elements, including a notion of Europeanism. This sense of common humanity, reflected in our ability to reason, was later seen as a principle of natural law, and the philosopher, John Locke (2007 [1689]), at a much later point in time, used it to develop a notion of a universal code of justice and an idea that human beings have inalienable rights regardless of what governments said and did. Stoic cosmopolitanism made many people more receptive to the cosmopolitan ideal and thus contributed greatly to its widespread influence. Cosmopolitanism slowly emerged as a key theme of the European Enlightenment, exemplified in the writings of the renowned international"
155,358,0.984,New Vision 2050 : a Platinum Society,"It Is Innovation for Being Used at the Site Yoshikawa Perhaps this is whatâs called social engineering. In the case of Japan, even if something is achieved at the level of science or technology, whether or not that achievement gets introduced into society is a separate problem. There is a huge social problem. In that regard, I believe that there is still much room for improvement in Japan. I believe that there is also a large role to be played by the state and the government. For example, a historical example that was successful that I think of, though maybe somewhat out of the blue, is the Dojunkai Apartments. In the recovery after the 1923 Kanto Earthquake, the Home Ministry erected about twenty Dojunkai Apartment buildings in Tokyo and Yokohama. Up until then, collective housing had consisted of wooden buildings with at most two stories; however, the collective housing introduced with the Dojunkai Apartments consisted of concrete structures that were 3 to 4 stories, or, in some cases, even 6â7 stories high. The Dojunkai Apartments served as the model for post-WWII collective housing, and it is clear that they represent the prototype of the modern-day apartment complexes. The Home Ministry played a leading role in carrying out that experimental project. Because âseeing is believing.â it is extremely important to present a model in a form that people can see with their eyes. Especially nowadays, I think itâs even more necessary to make a model. This is true especially in the fields of medicine and caregiving. An easy to understand example would be that even if a caregiver robot were developed, it is not introduced into an actual caregiving facility. It is because they are not a part of the medical treatment fee structure. Aside from the case where there is a unique business manager who wants to experiment with the robot and pays out of his/ her pocket for it, there is no basis for introducing such robots into caregiving facilities. Thus, the state must play a leading role in spreading the use of caregiver robots. People have been talking about this for 10 years now, but it still isnât happening. Even in the field of energy, in which you are interested, there was a certain type of scheme that was created in order to foster the diffusion of renewables. In the same manner, the medical and caregiving fields will more than likely require some sort of public initiative before innovative technologies are adopted for widespread use. I guess this is what is called social innovation. I believe that there is a lot of room for progress there."
235,231,0.984,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","The physical theories of classical mechanics, electrodynamics and gravitation (relativity theory) have been developed alongside classical analysis. Thereby assumptions about the formal mathematical models for theoretical physics had to be made which were partly (to some degree of accuracy) corroborated empirically; and partly a mere convenience. In particular, classical continuum physics employed mathematical objects â the continuum of real and complex numbers â which, from a logical, recursion theoretic, and algorithmic point of view, has turned out to be highly nontrivial, to say the least. For instance, as is argued in the Appendix A, with probability one, an arbitrary real number turns out to be incomputable, and even algorithmically incompressible â that is, random. Stated differently, almost all elements of a continuum are not attainable by any operational physical process. They require unlimited (in terms of computation space, time et cetera) resources. When contemplating the use of nonconstructive means for physical models, two questions are imminent: (i) Are these nonconstructive continuum models a faithful representation of the physical systems in the sense that they do not underrepresent â that is, do they systems? include and comprise essential operational features of these physical systems. (ii) Are these nonconstructive continuum models a faithful representation of the physical systems in the sense that they do not overrepresent; that is, that they do not introduce entities, properties, capacities and features which have no correspondence in the empirical data? If they allege and suggest capacities â such as irreducible randomness and computability beyond the universal Turing-type â can these capacities be utilized and (technologically) harvested for âsupertasksâ [53, 187, 188, 188, 352, 530] which go beyond the finite capacities usually ascribed to physical systems? (iii) What kind of verification, if any at all, can be given for nonconstructive means? The term â(non)constructiveâ is used here in its metamathematical meaning [63, 77, 78, 354]. Â© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_16"
289,50,0.984,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","In order to show the correctness of the translation, we prove that our translation always produces well-typed expressions in Î»B. By Lammas 1 and 2, we have the following theorem: T heorem 2 (Type Safety). If Î¨ â¢ e : A  s, then Î¨ â¢B s : A. Parametricity. An important semantic property of polymorphic types is relational parametricity [19]. The parametricity property says that all instances of a polymorphic function should behave uniformly. A classic example is a function with the type âa.a â a. The parametricity property guarantees that a value of this type must be either the identity function (i.e., Î»x.x) or the undefined function (one which never returns a value). However, with the addition of the unknown type â, careful measures are to be taken to ensure parametricity. This is exactly the circumstance that Î»B was designed to address. Ahmed et al. [2] proved that Î»B satisfies relational parametricity. Based on their result, and by T heorem 2, parametricity is preserved in our system."
272,686,0.984,Reconsidering Constitutional Formation Ii Decisive Constitutional Normativity : From Old Liberties To New Precedence (Volume 12.0),"If it is true that in Italy constitutionalisation has been a âdownward processâ, this can be partly explained with the culture of the liberal-moderate party concerned about avoiding excesses, favouring compromise and shunning big breaks and upheavals. It is extremely appropriate to underline how the promulgation of a written constitution constitutes only the ï¬rst step of the slow and difï¬cult process of constitutionalisation, made of progresses but also of reactions, standstills. The other phase is that of experimenting which led to an evolution of the meaning of the Statute in the light of the socio-cultural context. In Italy, constitutionalisation of National uniï¬cation is a long-term phenomenon, constituting the key feature for reading the National Building.8 Speciï¬cally, the Albertine Statute is a starting point more than an arrival point.9 From the moment when the sovereign grants the constitution, we realise that the normative act deriving from the granting is insufï¬cient in itself, the constitutional law is simply an act which cannot be revoked, but the constitution in order to live must obtain consent and public opinion must believe in it."
275,619,0.984,Foundations of Trusted Autonomy,"tems and hold that this is a correct characterisation of the world, strong predictability remains the exception, not the rule.8 So the premise to the conclusion of predictability that we can rely on for, say, making investment decisions or for autonomously operating in a complex operational environment, is untenable in general, and we draw this conclusion without needing to reject the notion that some events may have causes, or even the stronger assertion that every possible event has causes. It collapses merely when we admit that causes may be only necessary but not sufficient for at least some events that matter to us in terms of our decision-making some of the time. Moreover, across a wide range of contemporary fields, including mathematics, computer science and physics, as well as economics, it appears increasingly clear that there are also events that just do not really have any cause at all [7, 8]. This ties in very closely with Keynesâ ontological notion of uncertainty."
113,417,0.984,Pentecostalism and Witchcraft : Spiritual Warfare in Africa and Melanesia,"7. Evesâs target in âSorceryâs the Curseâ is the Comaroffsâ introduction to Modernity and Its Malcontents which he reads as seeing âlocal responses as the passive product of exogenous forcesâ (2000: 454) rather than emphasizing the âdynamic interplay between the local and the exogenousâ (ibid.: 455). However, the Comaroffs clearly do emphasize the interplay between the local and the exogenous."
106,147,0.984,Community-Oriented Policing and Technological Innovations,"Relations of Inference (Yellow Coding) Before any evidential structuring and reasoning approach can be incorporated within an application design, it is imperative to understand how the users in the target domain think and reason. This has been researched by Wong and Kodagoda (2016) and their results can been expressed as a sense-making triangle (see Fig. 8.2) which encompasses of three interlinked triangles. The inner triangle is the inference-triangle and describes the process of inference making as a combination of deductive, inductive and abductive processes. Each inferential process type is interlinked and a combination of factors, such as the userâs experience, domain and situation knowledge as well as the availability of information, determines which inferential process type will be at the forefront of the userâs thinking and reasoning. The second triangle is the anchoring-triangle and describes the sense-making process in terms of anchoring, laddering and associative questioning (Wong and Kodagoda 2016). Gerber et al. (2016) added the third insight-triangle, by describing the role that intuition and leap-of-faith plays, in highly uncertain environments, in order to gain insight. All three triangles work together in a complex combination of processes and sequences and forms an integral part of the tacit processes of human thinking and reasoning. The work of Wong (2014) and Wong and Kodagoda (2016) relies greatly on the foundational research of Klein et al. (2007) and Kahneman (2011) on the human thinking processes."
289,92,0.984,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","Difference from Lens Combinators. As mentioned above, the idea of branch switching can be traced back to lens languages. In particular, the design of case is inspired by the combinator cond [7]. Despite the similarities, it is important to recognise that case is not only a more convenient syntax for cond , but also crucially supports the unrestricted use of Î»-bound variables. This more fundamental difference is the reason why we could define appendB in the conventional functional style as the variables x and y are used freely in the body of case. In other words, the novelty of HOBiT is its ability to combine the traditional (higher-order) functional programming and the bidirectional constructs as found in lens combinators, effectively establishing a new way of bidirectional programming."
395,105,0.984,Beyond Safety Training : Embedding Safety in Professional Skills,"Abstract This chapter focuses on the relationship between representations of work (rules, procedures, models, speciï¬cations, plans) and work as a situated practice, performed by real people in always unique contexts. Empirically, it is organized around two main examples, the ï¬rst one being a discussion of the compartmentalization of safety seen in shipping and the railway sector. It shows how safety, as an object of management, has become decoupled from practice, and how current discourses about safety disempower practitioners and subordinate their perspectives to more âtheoreticalâ positions. The second is based on a study of control room operators in a space research operations setting. Here safety in the sense of avoiding harm to people is not the main concern; rather it is the reliability and robustness of an experiment on the International Space Station that is at stake. This example serves as a starting point for discussing how the research and theory on industrial safety should address the different temporalities of different work situations. It also helps to discuss the role of rules and procedures to support safety, reliability and resilience within the ï¬eld of safety science. Finally, some propositions about the relationship between situated practice and the management of safety are provided: how invisible aspects of situated work might be important for safety yet hard to manage, how procedures and rules might be integrated parts of situated work as much as representations of it and how different temporalities of work situations should be included in the theorizing of safety and resilience. Keywords Reliability"
125,179,0.984,"Charismatic Christianity in Finland, Norway, and Sweden : Case Studies in Historical and Contemporary Developments","elaborated on by Pierre Bourdieu (2010, see also KÃ¶hrsen 2008). Bourdieu portrays a habitus as a durable disposition, a system produced by structural conditions, and as a historical product (1990, 53â54). The habitus more or less determines how a person or group presents itself, its taste, behavior, and performance, in order to produce a coherent image and an acceptable disposition of experience. Finnish sociologist J.P. Roos strongly relates habitus with lifestyle; a habitus is a general approach to life, and a lifestyle is the enactment of that approach with living practices (1988, 30â33). For Bourdieu, a habitus is not the result of subjective intention but of socialization. It is motivated by the intention to belong to something; in order to be included in a group, one has to learn and present a habitus compatible with the group (Bourdieu 1990, 62; 2000, 100; 2010, 166â167, 174). He furthermore makes a distinction between class habitus and individual habitus. While the individual habitus may be informed by multiple groups, a class habitus is shared by a socioeconomic segment of people (Bourdieu 1990, 60). Class cultures are cultivated in structural conditions, which means they are subject to change over time, and shaped by the participantsâ needs and resources, possibilities and identification (Bourdieu 2010, 373â393; see also Mantsinen 2014, 42â47). In my view, no group or class habitus fully determines a personâs individual habitus, as individuals may draw upon different habituses in different situations. This may result from conscious decisions or underlying rationales dependent upon socializations. Relationships between individual agency and habitus can be illustrated by looking at how individuals mobilize different habituses in different contexts, in order to position themselves in webs of power present in various groups (cf. Bourdieu 1990, 53â56; Mahmood 2005, 26â27; Vanberg 1993, 189â191)."
124,493,0.984,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"5 Summary To sum up, I have argued (1) that a certain subclass of FTA structures is identical with a certain subclass of BST structures; (2) that the feature of FTA which formally corresponds to the prior choice principle of BST is a fundamental principle of the unity of life; (3) that the basic theory of ancestry BTA may modalised in such a way that it incorporates the principle of the necessity of origin; (4) that an account of speciation along the lines I have suggested calls into doubt the idea that species-membership is always an essential property; (5) that retrospect is an ontological feature of reality; (6) that the beginning of life on earth, speciation, individual ontogeny, death and decisions involve retrospect. Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited. 37 Schopenhauer (1818), 152 [WWV I 1 Â§18]: âNur die AusfÃ¼hrung stÃ¤mpelt den Entschluss, der"
28,191,0.984,A History of Self-Harm in Britain,"intent to others, the circumstances in which the attempt occurred, the measures taken to either ensure or avoid discovery and the reactions of relatives.60 This is a revealing didactic practice for the consistent fabrication of a social environment around a presumed attempted suicide (rather than investigations of the patientâs constitution or brain chemistry, for example). Kessel also sees the dramatic nature of self-poisoning as requiring PSW assistance. He claims that GPs confronted with the phenomenon âwill need the services of a psychiatric social worker, so that an informantâs account can be obtained in all cases. Very often the patient himself will conceal important information ... so as to extract the last ounce of drama from a situation in which he holds the centre of the stageâ.61 The language of deceit solidifies the self-conscious character of intent and shows its reliance upon social work practices."
326,82,0.984,"Autonomes Fahren : Technische, Rechtliche Und Gesellschaftliche Aspekte","concepts, they demonstrate how different approaches to ethical reasoning can be turned into algorithms that make decisions for automated vehicles. The correct choice of an ethical framework for automated vehicles is far from obvious, however, and they argue that there are benefits to taking a more deontological, or rule-based, approach to dilemma situations and a more consequentialist, or outcome-based approach to operating in traffic. Interactions in traffic and societal acceptance depend not only upon the programming in the automated vehicles but how the automated vehicles are understood â or misunderstood â by the people around them. Ingo Wolf, in his chapter Wechselwirkung Mensch und autonomer Agent, discusses the psychological concept of a mental model and how such models can be critical in defining human interactions with automated systems. He outlines several possible mental models for interactions between humans and automated vehicles and, using the results of an online survey, shows which are closest to current perceptions of this technology. This section concludes with a look at the specific challenge presented by the informal communication channels that humans use to interpret the intentions to other road users or signal their own intentions. Berthold FÃ¤rber demonstrates the importance of nonverbal communication such as eye contact and gesture in his chapter Kommunikationsprobleme zwischen autonomen Fahrzeugen und menschlichen Fahrern. This raises a range of questions such as how eye contact with someone in the driver seat of an automated vehicle who is not actually driving may be interpreted and what possibilities for new communication modalities might exist."
124,191,0.984,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"6 The General Character of Models First of all, we note that a model for a logical system is always a model with respect to a language, and that this language is held fixed for the whole class of models for a given system. Second, we note that each model will have two distinguishable componentsâa structure and an interpretive scheme. The structure is our stand-in for the world; the interpretive scheme links the language to the structure and in doing so represents the way the language is understood to connect with the world. The structure in turn has two sub-components: an ontology, which varies from model to model, and a set of structural constraints which remain constant across models. The interpretive scheme also has two sub-components: a valuation, which varies from model to model, and a set of satisfaction conditions which is invariant across models. The ontology gives the array of types of entities that are taken to be explanatorily fundamental, for the level and style of explanation undertaken by the system. All other types of entities acknowledged by the system are constructed from, or 8 We may, but Belnap may not approve. There is, after all, only one world, our world, and the"
275,203,0.984,Foundations of Trusted Autonomy,"Such capabilities are essential for the agents embedded in many types of game or simulator. Games often provide multiple agents of some generic type â e.g. the settler type in the Civilization game â which must as individuals pursue differing activities that contribute to the success of the team rather than the individual. And those agents must be adaptive in their choice of activities, taking account of the game state, including the choices being made by their peers. Yet the scripted behavior of agents in commercial and open source games commonly fail at that requirement, making decisions that appear to take little account of context. For example, in strategy games, even when the agents are not autonomous and a higher-level AI is able to manipulate them according to some plan, individual agents are frequently observed to make piecemeal attacks that are suicidal due to a lack of supporting actions by their peers. To a human observer, such agents simply do not seem very intelligent. Appropriate computational intelligence methods should be able to take game intelligence beyond the brittleness, inflexibility, and narrowness of scripted activity, and for many games or simulators the context-awareness and adaptivity of the agents in an ATA will be a necessary part of any successful solution. In the field of evolutionary robotics, Floreano et al. also examined homogeneous teams controlled by ANNs evolved by team selection, in a study of hypotheses for explaining biological altruism [8]. Altruism does not play an explicit role in Legion II, but their study found that a homogeneous team evolved by team selection performed better than three other architectures examined, producing robust altruistic behavior in the process. Altruism, when appropriate, is an important facet of trusted autonomy in multi-agent environments, and can contribute to the appearance of intelligent behavior as well. It is interesting to note that the necessary adaptivity for our ATA was obtained using a simple feed-forward network for the legionsâ controllers. We know that artificial neural networks are powerful computing devices (see e.g. [7, 25]), and that genetic algorithms are able to train them to sophisticated behaviors (e.g. [1, 3, 15, 26, 27, 29]). To a first approximation it may be concluded that the Legion II controllers have been trained to partition the gameâs state space, as seen from an egocentric point of view, into two classes, and to choose a behavior on the basis of which class the current state observation falls in to. However, what they actually choose is one of seven atomic moves, none of which can be uniquely associated with either of the two behavior classes. For an agent to pursue a coherent higher-level behavior across many game turns â i.e., to give an appearance of intent-driven behavior â would seem to require access to some internal state, i.e. an ability to ârememberâ what it is doing. Conjecturally, the Legion II agents have learned a workaround whereby they effectively store their internal state in the external environment. I.e., in addition to whatever else they learn during training, they learn a mapping from their egocentric view of the environment to a virtual representation of whatever internal state information is necessary for ârememberingâ what they are doing. The flow of information is in fact recurrent: the fact that the agents move within their environment causes a transformation of their next view of the environment. In an otherwise static environment those transformations would be deterministic; the presence of other agents in the Legion II"
8,808,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Thus as PhVi ! 0, so must hNi, the number of clusters, for finite T. We record the astonishing fact that the hadron gas phase obeys an âidealâ gas equation, although of course hNi is not constant as for a real ideal gas but a function of the thermodynamic variables. The boundary given by '.Ë; / D '0 D ln.4=e/"
16,282,0.984,"Autonomous Control For a Reliable internet of Services : Methods, Models, Approaches, Techniques, Algorithms, and Tools","The purpose of the first study with regard to the bandwidth factor is to investigate how it influences the minimum advance time tadv in the case of context awareness, and how it influences the stalling probability in the conventional context unaware case. The results are presented in Fig. 5. As demonstrated in Fig. 5a, for very low data rates (e.g. a bandwidth factor of 0.2), the minimum required advance time gets higher, as the user would need a much greater timemargin to proactively fill the buffer in light of the outage, because the network is heavily congested. Moreover, the uncertainty in this case is also very high,"
297,1639,0.984,The R Book,"The important distinction in models with categorical explanatory variables is between cases where the data come from a designed experiment, in which treatments were allocated to locations or subjects at random, and cases where the data come from an observational study in which the categorical variables are associated with an observation before the study. Here, we call the ï¬rst case split-plot experiments and the second case hierarchical designs. The point is that their dataframes look identical, so it is easy to analyse one case wrongly as if it were the other. You need to be able to distinguish between ï¬xed effects and random effects in both cases."
49,448,0.984,Artificial Intelligence and Cognitive Science IV,"GA is also used for generating and selecting rules but the principal difference to Michigan approach is a fact that in each iteration step only one rule with the best strength is selected and added to the resulting RB, which is held separately from the learning system. After that it is possible to exclude all examples from the data set that are covered by this rule. In such a manner the data set is consecutively pruned until it will be cleared, which is the final stopping criterion for the learning process because the selected rules cover completely a given data set. Such an approach is very simple it requires only defining a criterion (or more criteria) for selecting the best rule for the final RB and eventually other stopping criteria being able to asses the completeness of RB. However, there is also a significant drawback. The rules (individuals) are evaluated individually regardless of a fact that rules create a unity and they cooperate in reality. Rules are separated, which leads to a redundant final RB showing over-fitting. From this reason a second stage of post-processing is still needed, which would simplify RB removing any redundancy. IRL approach thanks to its simplicity and ability to use various selection criteria has found a wide range of use and at present it is one of the most researched means in this scientific area. Especially popular it is in multi-objective problems, where mainly a trade-off between accuracy and interpretability (simplicity, comprehensibility) of the proposed RB grows on importance. The most known IRL systems in this area are MOGUL [7] and SLAVE [11]."
87,147,0.984,"Bioeconomy : Shaping The Transition To a Sustainable, Biobased Economy","interactions would need quantification. The more complex systems, the more direct interventions will induce side effects, and the less they are likely to succeed. Finally, one debate connected with systems approaches is that about the ontological status of a system. There is a position that systems are ârealâ. Thus, a system is understood as existing in the real world; it has ontological status, i.e. exists independent from an observer. The alternative viewpoint is that systems are analytical constructions by the observer. The elements, relations and boundaries of the system are defined by the observer, who has a certain interest in the analysis. Thus, systems can be considered as systems of interests. Science or any other societal community define system perspectives to analyse certain types of problems. In this sense, systems are socially constructed entities (by a group rather than by an individual). For example, from a biological perspective, it seems at a glance self-evident that the human is defined by the boundaries of the body. However, the body is settled by microbes that may be both dangerous (e.g. viruses) and helpful (e.g. millions"
224,475,0.984,Ester Boserupâs Legacy on Sustainability : Orientations for Contemporary Research,"Finally, Ringhofer et al. demonstrate very clearly that the Boserupian mechanism of increasing yields at the expense of labour input holds only for subsistence agriculture. As soon as fossil fuels come into play, it is necessary to resort to a more generalized concept of energy inputs in order to arrive at consistent explanatory models. In effect, it seems that Boserupâs gradualist model of development does not hold when it comes to the transition to fossil fuel based industrial society in which land is no more the key resource. This shift in energy regime (as described in Chap. 3) seems to be more relevant and powerful than Boserup would admit. It is not so clear whether similar limitations to her model hold as far as gender relations are concerned. All the chapters dealing with the role of gender relations recognize their often underestimated importance in development. Empirical confirmations arise mainly in those chapters that deal with early stages of development, such as in Chap. 10 for the Himalaya region in India, or in Chap. 11 for sub-Saharan Africa. In cases dealing with more advanced industrial situations, such as Chap. 12 about contemporary large scale land deals, or Chap. 13 about contemporary Mexican communities, the storyline appears to be more mixed: It is not so clear that females tend to be disadvantaged by development over their previous traditional roles, but sometimes it seems to work the other way round. Many of the contributions to this volume reflect that it seems more difficult for contemporary human-environmental scientists to share Boserupâs in principle positive and optimistic outlook into the future. Several environmental pressures appear to have evolved in a way that demonstrate a Malthusian rather than in a Boserupian pathway: more people on earth imply an accelerating rise in the exploitation of natural resources. In particular it has been noted that, if not so much land, so many other resource uses rise over proportionally to population (see Chap. 3). Today, it is much more apparent than at the time of Boserupâs writing that development has not been following the transition pathway she propagated, but in much of the world rather resembled a âgold rushâ leaving barren land behind. The âlimits to growthâ notion that Boserup would not take into consideration (although she was aware of Meadowsâ et al. 1972 publication) seems to deserve more attention nowadays. For several reasonsâsuch as her insistence in gradualism, her deep rooted trust in positive outcomes, and her neglect of energy sources as marking qualitative breaks in societal developmentâit is difficult to learn a lesson from her concerning a next transition to a more sustainable society. It seems she believed the society she inhabited made mistakes but was ok (or the only option) after all. Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
170,131,0.984,Impact of Information Society Research in the Global South,"subject at hand not only as another issue in a world of constrained budgets but as part of the solution to such issues. While the discussion on the policy cycle and locus might be more clearly stated, the discussion of the context for policies tends to seem elusive. As Avgerou (2010) has mentioned in the case of ICTs, oversimplifying context as a different âlocal cultureâ adds little value on understanding the interactions of people and technology in the developing world. With regard to policy context, it may involve understanding the interaction between policy and research, political systems, electoral processes, structure of governments, and so on. For the purpose of simplifying our understanding regarding the politics of implementing ICT4D initiatives and the role research can play in the process, I would argue for focusing on understanding the rational and value-driven aspect of policy problems. Hoppe (2010), for instance, looks into two dimensions of a policy problem: on the one hand, the level of certainty regarding relevant knowledge for the policy process and, on the other hand, the level of consensus on relevant norms and values. The first dimension is rational and relates to what is known about the problem at hand and how stakeholders react to such knowledge. Is knowledge valid, trustworthy, and relevant? The second dimension refers to values surrounding the problem and whether stakeholders agree or disagree on how the problem is defined and the values that should guide its solution. This way of thinking about the political context focuses on the relationship among the stakeholders involved in relation to their rational and value-based interpretation of the problem at hand. Some of the questions that could be seen from this perspective involve both the evidence and the value surrounding decisions on the role of the state and the provision and support of ICTs. Furthermore, it would be interesting to explore the perceptions of stakeholders. For instance, it could be interesting to examine how policy actors see technology, either as something that should be imported from the developed world or constructed locally or the result of the articulation of imported and local knowledge. It could also be interesting to explore if ICTs are seen as a disruptive or progressive force of development (Avgerou 2010), the expected uses of ICTs (Harindranath and Sein 2007), and how they would gauge the success of an ICT policy. Research in the realm of ICT4D-related policies and its politics is the basis to be able to plan research programmes that can respond to the challenges of public policy. This research allows understanding of the setting where ICT4D research would interact with policy and politics and sheds light on the complexities of policymaking and the adoption of ICTs in public programmes, projects, and regulations."
217,478,0.984,Finite Difference Computing With Pdes : a Modern Software Approach,"2.14 Applications of Wave Equations This section presents a range of wave equation models for different physical phenomena. Although many wave motion problems in physics can be modeled by the standard linear wave equation, or a similar formulation with a system of first-order equations, there are some exceptions. Perhaps the most important is water waves: these are modeled by the Laplace equation with time-dependent boundary conditions at the water surface (long water waves, however, can be approximated by a standard wave equation, see Sect. 2.14.7). Quantum mechanical waves constitute another example where the waves are governed by the SchrÃ¶dinger equation, i.e., not by a standard wave equation. Many wave phenomena also need to take nonlinear effects into account when the wave amplitude is significant. Shock waves in the air is a primary example. The derivations in the following are very brief. Those with a firm background in continuum mechanics will probably have enough knowledge to fill in the details, while other readers will hopefully get some impression of the physics and approximations involved when establishing wave equation models."
289,1363,0.984,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","where Q â P (P (Î£ +â )) is a partition of sequences of program states. More specifically, to reason about input data usage of a program P , we lift the trace semantics [[P ]] to P  by partitioning it into sets of traces that yield the same program outcome. The key insight behind this idea is that, given an input variable i, the initial states of all traces in a partition give all initial values for i that yield a program outcome; the variable i is unused if and only if these initial values are all the possible values for i (or the set of values is empty because the outcome is unfeasible, cf. Eq. 3). Thus, if the trace semantics [[P ]] of a program P belongs to the input data usage property NJ , then each partition in P  must also belong to NJ , and vice versa: we have that [[P ]] â NJ â P  â NJ , which is precisely what we want (cf. Eq. 5). denote the subset of the finite sequences of program states in T â Let To=v P (Î£ ) with value v for the output variable o in their outcome (i.e., their"
249,197,0.984,Advances in Proof-Theoretic Semantics (Volume 43.0),"1 Introduction Broadly speaking, there are two conceptions of meaning: the referentialist one based on truth conditions as advocated by Davidson [7], and the inferentialist one based on verification or use conditions as advocated by Dummett [10] or more recent Brandom [5]. Along the latter strand of the theory of meaning, proof-theoretic semantics undertakes the enterprise of accounting for the meaning of logical constants and inferences in terms of proof rather than truth, thus replacing Davidsonâs path âfrom truth to meaningâ by another Dummettian path âfrom proof to meaningâ; the term âproof-theoretic semanticsâ was coined by Schroeder-Heister (for a gentle introduction, see Schroeder-Heister [21]; he also coined the term âsubstructural logicâ with DoÅ¡en). It builds upon the proof-theoretic tradition of Gentzen, Prawitz, and Martin-LÃ¶f, tightly intertwined with developments of Brouwerâs intuitionism and varieties of constructive mathematics, especially the Brouwer-Heyting-Kolmogorov interpretation, and its younger relative, the Curry-Howard correspondence between logic and type theory, or rather the Curry-Howard-Lambek correspondence between logic, type theory, and category theory (see, e.g., Lambek and Scott [12]). Note however that Brouwer himself objected to the very idea of formal logic, claiming the priority of mathematics to logic (cf. Hilbertâs Kantian argument in [11] concluding: âMathematics, therefore, can never be grounded solely on logicâ). âHarmonyâ in Dummettâs terms and the justification of logical laws have been central issues in proof-theoretic semantics (see, e.g., Dummett [10] and Martin-LÃ¶f [14]). Combining proof-theoretic semantics with category theory (see, e.g., Awodey [2]), the present paper aims at laying down a foundation for categorical proof-theoretic semantics, proposing the principle of categorical harmony, and thereby shedding structural light on Priorâs âtonkâ and related paradoxical logical constants. Priorâs invention of a weird logical connective âtonkâ in his seminal paper [18] compelled philosophical logicians to articulate the concept of logical constants, followed by developments of the notion of harmony. In a way harmony prescribes the condition of possibility for logical connectives or their defining rules to be meaning-conferring, and as such, it works as a conceptual criterion to demarcate pseudo-logical constants from genuine logical constants. Let us recall the definition of Priorâs tonk. Tonk can be defined, for example, by the following rules of inference as in the system of natural deduction: Î¾ â¢Ï (tonk-intro.) Î¾ â¢ Ï tonk Ï"
289,500,0.984,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","It is well-known that Reynoldsâ parametricity [13] and Kreiselâs modified realizability [4] are two instances of the broader logical relation techniques. Usually, parametricity is used to derive theorems for free, while realizability constrains programs. In a surprising turn of events, we use Bernardyâs variant of parametricity on CIC [5] as a realizability trick to evict undesirable behaviours of TE . This leads to the parametric exceptional translation, which can be seen as the embodiment of Kreiselâs realizability in type theory. In this section, we first present this translation on the negative fragment, then extend it to CIC and finally discuss its meta-theoretical properties."
208,128,0.984,Actors and the Art of Performance,"inadvertently make you face the tragedy and riddle of non-identity into which it plunges you. It can also bring to light that which might otherwise have remained hidden and untouched in the dark, because it is confrontational, painful, and threatening. The power of the gaze can cause calamities. It can objectify others, betray, curse, and cut. As the saying goes, a look can even kill. One involuntary gaze into the eyes of Medusa can turn you to stone, and fear of the evil eye is found in almost all cultures reaching back to the beginning of history. Another momentary gaze is needed for interplay on stage. This is another desire altogether. Perhaps it has its roots in the âpenetrating eyesâ related to Dionysus,29 which inspire and are the source of the bottomless reservoir of creativity. It is a gaze meant to challenge, not harm, others, not even by the distortions of idealizing. It is open to and unafraid of the future, and is therefore not a slave to the prejudices that dazzle and delude us and judge Others without seeing what they can do. Instead, it is fundamentally welcoming to the Other and wants to open all options for him, make all avenues possible. Such a gaze is fundamentally generous and passionate, willing to risk a love-gaze and trusting that it will be able to distinguish strange from stranger, so that it does not expose itself naively to the destructive Other. And if Medusa does stare back â something that has been known to happen even in the most beauteous temples of the muses â the gaze is averted in time or lets itself come to the test. For who, in the kairos of time, has exhibited more potency â Eros the matchmaker or the demon Negativity? When the interplay goes well, Eros has a good chance. In the kairos of the moment, the gazes of homo ludens lock on stage in the shared eros of the creativity of the muses. And what kind of coupling would it be if one cut the other off in the name of his own pleasure and advantage? That would be a poor showing and not a felicitous act, even if one of the two, much acclaimed, believed himself to be the winner. Victorious moments, gazed upon in theater and smiled upon by the muses, have another look to them. They are not self-centered nor do they know self-denial. Rather they are fed by the understanding that 29 Walter Otto, Dionysus Myth and Cult, trans. Walter Palmer (Bloomington: Indiana University Press, 1965), 90."
310,85,0.984,Online Hate Speech in The European Union : a Discourse-Analytic Perspective,"implicatures as propositions that can, among other things, âbe communicated with varying degrees of strength, depending on the conï¬dence with which a hearer can assume that they form part of the speakerâs informative intentionâ (Assimakopoulos 2017: 319). So, even though a statement may not constitute hate speech in the eyes of the law, it might still reveal the discriminatory attitude of its producer, to the extent that it could also be considered to be detrimental to the feeling of self-worth of members of a minority group. What is crucial in this respect is, of course, to take into account not only the explicit content of a given statement, but also the particular context in which it has been produced. One of the most extensively researched categories of implicated meaning is that of irony, which can be easily identiï¬ed in the example that follows: (29) We need to thank the geniuses who agreed with us signing the Dublin 2 convention. They want us to ï¬ngerprint immigrants to make it easier for them to identify and deport them back to Malta.33 At face value, the comment above has a particularly positive undertone, as it starts off with positively charged words, like âthankâ and âgeniusesâ. Yet, when looked at in its particular context, it is clear that it is meant as a negative comment against the Maltese politicians who signed the âthe Dublin 2 conventionâ,34 thus agreeing to Malta being solely responsible for the examination of asylum applications by refugees who enter the EU through its territory. The implicitly communicated negative meaning of this comment is derived from the combinatorial meaning of the two sentences it comprises, with the second sentence providing an explanation as to why the ï¬rst one is intended as an ironical statement. In this second sentence, the user creates a distinction between âusâ (=the Maltese) and (the ï¬rst, exophoric) âthemâ (=other EU countries) to relay the information that âimmigrantsâ35 are unwanted and that the EU is using Malta as a dumping ground for this undesirable group of people. It is therefore evident that the user employs irony to communicate that the signing of the Dublin II convention was an unwise decision that has had negative effects on Malta. And even though the implicature at hand strongly communicates the userâs dismay at a particular political choice, it also carries a weaker negative stance toward migrants, since the irony of the ï¬rst sentence in"
62,365,0.984,"Agile Processes in Software Engineering and Extreme Programming: 17th International Conference, XP 2016, Edinburgh, UK, May 24-27, 2016, Proceedings (Volume 251.0)","could exist inconsistency between the real stage of a software startup and the perceived stage by its respondent. There are additional questions in the original survey (not included in this study) that could be used in the follow-up studies to triangulate the perceived life cycle stages. Lastly, regarding the Chi-square test, it is recommended that the categorical variable has a small number of categories. To improve the confidence of the results, the challenges could be classified into fewer groups. A meaningful and valid way to group these challenges is needed."
305,176,0.984,Quantum Computing for Everyone,"The inability to clone a qubit has many important consequences. We want to be able to back up files and send copies of files to other people. Copying is ubiquitous. Our everyday computers are based on von Neumann architecture, which is heavily based on the ability to copy. When we run a program we are always copying bits from one place to another. In quantum computing this is not possible for general qubits. So, if programmable quantum computers are designed they will not be based on our current architecture. At first, the fact that we cannot clone qubits seems like a serious drawback, but there are a couple of important comments that need to be made. Often we want to prevent copying. We want to secure our dataâwe donât want our communications to be tapped. Here, as we saw with Eve, the fact that we cannot clone qubits can be used to our advantage, preventing unwanted copies from being made. The second comment is so important it deserves its own section. Quantum Computation versus Classical Computation The qubits 0 and 1 correspond to the bits 0 and 1. If we run our quantum CNOT gate just using the qubits 0 and 1 , and not any superpositions, then the computation is exactly the same as running a classical CNOT gate with 0 and 1. The same is true of the quantum version of the Fredkin gate. Since the classical Fredkin gate is universal and the quantum Fredkin gate using just 0 and 1 is equivalent to the classical gate, we can see that a quantum circuit can calculate anything that can be calculated by a classical circuit. The no-cloning property may seem worrisome, but it doesnât restrict us from doing classical computations in any way. This is a deep result. It shows that if we compare classical and quantum computation, we shouldnât think of them as different types of computation. Quantum computation includes all of classical computation. It is the more general form of computation. The qubit is the basic unit of computation, not the bit. Now that we have seen some basic gates, we will start to connect them together to form circuits."
305,22,0.984,Quantum Computing for Everyone,"know that the photons coming through the first filter are polarized vertically. When measured by the second filter, half of the photons are found to be polarized in the 45Â° direction and half in the 135Â° directions. The ones with 45Â° polarization pass through the filter, and the others are absorbed. The third filter again measures the polarization in the vertical and horizontal directions. The photons entering have 45Â° polarization, and when measured in the vertical and horizontal directions, half will have vertical polarization and half will have horizontal polarization. The filter absorbs the vertically polarized photons and lets through those that are polarized horizontally. Conclusions We started this chapter by saying that classical bits can be represented by everyday objects like switches in the on or off position, but that qubits are generally represented by the spin of electrons or the polarization of photons. Spin and polarization are not nearly so familiar to us and have properties that are quite unlike their classical counterparts. To measure spin, you first have to choose a direction and then measure it in that direction. Spin is quantized: When measured, it gives just two possible answersânot a continuous range of answers. We can assign classical bits to these results. For example, if we obtain an N we can consider it to be the binary digit 0, and if we obtain an S we can consider it to be the binary digit 1. This is exactly how we get answers from a quantum computation. The last stage of the computation is to take a measurement. The result will be one of two things, which will be interpreted as either 0 or 1. Although the actual computation will involve qubits, the final answer will be in terms of classical bits. We have only just started our study, so we are quite limited in what we can do. We can, however, generate random strings of binary digits. The experiment that generated random strings of Ns and Ss can be rewritten as a string of 0s and 1s. Consequently measuring spins of electrons first in the vertical and then in the horizontal direction gives a random string of 0s and 1s. This is probably the simplest thing that we can do with qubits, but surprisingly this is something that cannot be done with a classical computer. Classical computers are deterministic. They can compute strings that pass various tests for randomness, but these are pseudorandom, not"
310,149,0.984,Online Hate Speech in The European Union : a Discourse-Analytic Perspective,"Apart from the identiï¬cation of this repertoire of strategies, which is arguably not exhaustive, we have also attempted in this volume to show that the general publicâs perception of what actually constitutes hate speech and how it should be regulated is far from uniform. So, even though the young people we interviewed are, quite expectedly, ardent supporters of freedom of expression, they still generally feel that hate speech is an issue that needs to be combatted. Yet, many do not seem to be sensitised towards what exactly hate speech is and also justify at times the negative sentiment of the general public towards a particular minority. To our mind, this carries two implications. On the one hand, even though the EU is pushing for the regulation of hate speech at a transnational level, it seems sensible for it to leave space for each member state to target hate speech within its national context, with its own particularities and needs. On the other hand, it seems that while legislation does help, it is not enough on its own to contain the situation, since most participants in our interviews showed ignorance of the relevant laws and repercussions for the expression of hate online. What emerged from the interviews, instead, was that the most effective weapon in the ï¬ght against hate speech is education, broadly construed. Against this background, when it comes to policy-making, the C.O.N.T.A.C.T. consortium can therefore make two recommendations. For one, it is necessary to conduct extensive research on the different forms that hate speech can take, both online and offline, as well as the underlying reasons for the emergence of such speech. It may sound banal to point this out, but it is only through the profound understanding of these reasons in the ï¬rst place, at both the national state and transnational levels, that effective policies of inclusion can be developed; and this is something that seems to be often disregarded by those in charge. Then, it is equally, if not even more important for the general public to develop an awareness on matters of discrimination. This is something that can only be accomplished through wide-reaching awareness-raising events, a responsible approach to the relevant issues by the media, and, of course, the establishment of an agenda that promotes inclusion and tolerance at all levels of education. The latter has also been pointed out in a very recent European Agency for Fundamental Rights press release, according to which, âpromoting inclusion and mutual respect through education and strong positive narratives are essential to prevent incitement to hatred and counter hate speech in the digital ageâ (FRA 2013: 1). In closing, we hope to have shown that linguists have an important role to play in this picture (cf. Olsson and Luchjenbroers 2013). Since it is intention that lies at the very core of most legal deï¬nitions of hate speech, contextualising and qualitatively analysing such speech seems central to not only tackling this complex phenomenon but also to safeguarding freedom of expression on the many platforms that the internet offers. We therefore believe that this is an endeavour that can only be accomplished by encouraging collaboration and constructive dialogue between policy makers, legal practitioners, linguists and computer scientists specialising in the automatic detection of hate speech, as well as involving higher education institutions more directly in the implementation of the relevant EU agency directives."
101,3,0.984,Finite Difference Computing with Exponential Decay Models (Volume 110.0),"(PDEs). It turns out that a range of concepts and tools needed for PDEs can be introduced and illustrated by very simple ordinary differential equation (ODE) examples. The goal of the text is therefore to lay a foundation for understanding numerical methods for PDEs by first meeting the fundamental ideas in a simpler ODE setting. Compared to other books, the present one has a much stronger focus on how to turn mathematics into working code. It also explains the mathematics and programming in more detail than what is common in the literature. There is a more advanced companion book in the works, âFinite Difference Computing with Partial Differential Equationsâ, which treats finite difference methods for PDEs using the same writing style and having the same focus on turning mathematical algorithms into reliable software. Although the main example in the present book is u0 D au, we also address the more general model problem u0 D a.t/u C b.t/, and the completely general, nonlinear problem u0 D f .u; t/, both for scalar and vector u.t/. The author believes in the principle simplify, understand, and then generalize. That is why we start out with the simple model u0 D au and try to understand how methods are constructed, how they work, how they are implemented, and how they may fail for this problem, before we generalize what we have learned from u0 D au to more complicated models. The following list of topics will be elaborated on.  How to think when constructing finite difference methods, with special focus on the Forward Euler, Backward Euler, and CrankâNicolson (midpoint) schemes.  How to formulate a computational algorithm and translate it into Python code.  How to make curve plots of the solutions.  How to compute numerical errors.  How to compute convergence rates.  How to test that an implementation is correct (verification) and how to automate tests through test functions and unit testing.  How to work with Python concepts such as arrays, lists, dictionaries, lambda functions, and functions in functions (closures).  How to perform array computing and understand the difference from scalar computing.  How to uncover numerical artifacts in the computed solution.  How to analyze the numerical schemes mathematically to understand why artifacts may occur.  How to derive mathematical expressions for various measures of the error in numerical methods, frequently by using the sympy software for symbolic computations.  How to understand concepts such as finite difference operators, mesh (grid), mesh functions, stability, truncation error, consistency, and convergence.  How to solve the general nonlinear ODE u0 D f .u; t/, which is either a scalar ODE or a system of ODEs (i.e., u and f can either be a function or a vector of functions).  How to access professional packages for solving ODEs.  How the model equation u0 D au arises in a wide range of phenomena in physics, biology, chemistry, and finance.  How to structure a code in terms of functions."
124,509,0.984,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"5 Ontological Competition In the past sections we have looked at structures of possibilia mainly under the aspect of co-possibility, investigating possible collections of possible beings that can (or even must) be grouped together. We now turn to the contradictory aspect of incompatibility. Let us have a look at three examples from the realm that is to provide the material interpretation to our formal theory, which we will consider as test cases for its power to model phenomena of incompatibility. (Example 1) A human couple, Elizabeth and Peter, actually have a lot of children and they could have had even more children, or a distinct lot of possible children, or another lot, or â¦ However, there clearly is an upper limit to the number of children they could have had, determined (roughly speaking) by the minimum length of a pregnancy and the maximum duration a woman can bear children. So, the number of the possible children of Elizabeth and Peter exceeds the upper limit of children they could have had by far. Any collection of possible children of a number that exceeds this upper limit is not compatible. (Example 2) Two possible mammals, A and B, are such that they not only have the same ancestors but result from the very same sperm and egg, while their dates of conception are a few seconds apart. For the scope of this example we understand beings of the kind of A and B to be individuated by their time of conception (among other things). Hence we must see A and B as incompatible individuals. (Example 3) A cell of a kind that reproduces by fission actually splits into the two daughter cells D1 and D2 , but it could have split in a different way (e.g., distributing its material in a different way) that would have led to the two possible daughter cells E1 and E2 . But every cell can split only onceâthough its daughter cells may split in turn, it is no longer there to split up again. Hence D1 is incompatible with E1 , D1 is incompatible with E2 , and so on. In fact, as there plausibly are many possibilities for a cell to split up, there will be a large plurality of incompatible pairs of possible daughter cells for each cell. These examples illustrate three general observations that are relevant to the task of modeling phenomena of incompatibility. Firstly, incompatibility need not be a twoplace relation. It may well be that each two of the many possible children of Elizabeth and Peter of (Example 1) are compatible, but clearly no collection of a hundred of them is a population of any possibility.19 Robert Brandom makes a corresponding observation with respect to sentences or claims: âthe claim that the piece of fruit in my hand is a blackberry is incompatible with the two claims that it is red and that it 19 Here we are bracketing the intuitions behind (Example 2) and (Example 3). But even taking into"
117,66,0.984,Care in Healthcare : Reflections On Theory and Practice,"2. Joan Tronto has provided a somewhat similar representation from a feminist-political perspective (cf. Tronto 1993). There are, however, several differences to our approach, particularly the fact that for Tronto, compassion with the other person and the understanding of care as a collective process play an important role. This is not the case in our proto-ethical approach, which is why we will not go into Trontoâs work in more detail here. A critical study of the fundamental aspects of Trontoâs approach is provided by Edwards (2009, pp. 233â238), an overview to different concepts of care (including Trontoâs approach) is provided by Kohlen and Kumbruck (2008). 3. Carl Amery undertook the interesting exercise of allowing the âminor charactersâ of the parable (the priest, the Levite, the innkeeper and even the leader of the robbers) to express themselves and to explain their respective actions (cf. Amery 1973). 4. It seems, as we wish to note only in passing, to be immaterial for the significance of the ought whether it was created as an existential factum, as we claim here, or whether it encountered me as a facticity. 5. Protest against the tendency of caring ethics to interpret care fundamentally as something essentially goodâan idea brought forth primarily by Carol Gilligan and Nel Noddingsâhas also arisen within the field of nursing studies, cf. for example Allmark (1995); Bradshaw (1996); Edwards (2009, pp. 232 ff). 6. âCare is an affectively charged and selective mode of attention that action, affection, or concern at something, and in effect, it draws attention away from other things. In practice, a person who cares is one who has already chosen an object to care about. Consider, however, that prior to securing a thing to care for, a person must have the capacity or willingness to respond, to be called into action, to be hailed by that object or phenomenon. In short, a person who cares must first be willing and available to be moved by this other.â 7. â[T]here was notable consistency between students and nurses in reasons for entering nursing affected by neither age nor level of experience. This finding along with high levels of innate personal traits that are conducive to a caring and cooperative nature suggests that individuals are drawn to nursing for similar reasons. There was a general consensus by participants that âall sorts of personalities make a good nurseâ and the dominant trait of a good nurse is that âdesire to careâ.â"
289,580,0.983,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","We have B = Î¨ â C for some C by Lemma 1. Instead of B, we can directly return C as the output type, since we can derive from the application context that e is of type Î¨ â C, and Î»x. e is of type (Î¨, A) â C. Thus we obtain the T-Lam-Alt rule. Note that the choice of the style of the rules is only a matter of taste in the language in Sect. 3. However, it turns out to be very useful for our variant of System F, since it helps avoiding introducing types like âa = Int.a. Therefore, we adopt the new form of judgment. Now the judgment Î  Î¨ â¢ e â A is interpreted as: under the typing context Î , and the application context Î¨ , the return type of e applied to the arguments whose types are in Î¨ is A."
229,55,0.983,Constructions of Cancer in Early Modern England,"his peers, however, Read identified the causes of adustion with more certitude than specificity. In general, medical practitioners positing a humoral explanation for cancer looked only so far inward â to the level of adust melancholy or atra bilis â before, like Read, they turned their gaze once more toward the environmental factors which aggravated that substance. They were therefore either unable, or saw no good reason, to supply details of exactly what happened inside the body to turn melancholy into these more harmful substances. The neo-Galenic model seems not to have fostered inquiry into the mechanics of each humourâs operation, but rather focussed upon their qualitative characteristics. One particularly interesting theory, however, which we can see fleetingly referenced in Readâs âburning of naturall melancholly and yellow cholerâ, was that adust or poisonous forms of melancholy might either have been comprised of several different humours, or of a different humour â choler, for example â which mutated into melancholy during the process of adustion.57 While this kind of âcompoundâ melancholy is not evident in most texts on cancer, it is present in a number of discussions of the maladyâs cause, where a posited link between adust melancholy and choler (yellow bile) often provides a logical bridge between the efficient causes and the characteristics of the disease.58 These discussions occurred over the sixteenth, seventeenth and eighteenth centuries, and may have been derived from ancient writings, though this remains unclear.59 In his 1684 Adenochoiradelogia, for instance, Browne asserted that âwhen [cancer] takes Adust Choler into its cognizance, and this gains better and nearer acquaintance therein, this in time masters the other, and makes the Patient feel the Vigour of its prevalency, by its corrosive, cruel and terrible pain which it brings along with itâ.60 Authors who discussed âcompoundâ melancholy were clear on the fact that yellow bile changed the character of resulting diseases for the worse. âHot, dry [and] bitterâ, choler was associated with anger and fierceness, and in his 1621 The Anatomy of Melancholy, Burton pinpointed choler as the root of âbrutishâ, ârash, ravingâ varieties of madness.61 Moreover, Jennifer Radden notes that, according to Galen, yellow bile was associated with acute diseases and black bile with those of long continuance.62 In theories of âcholericâ melancholy, therefore, one sees particularly clearly the marriage between discussions of cancerâs cause and its troublesome, âfierceâ character, alongside a ready explanation of how the disease could be both acute in effects and chronic in duration. Furthermore, the language in which such correlations were described once again makes obvious how readily early modern people embraced emotive discourses of the fierce, filthy and mutable nature of certain bodily substances."
58,93,0.983,Enabling Things to Talk,"In this figure, dashed arrows represent dependency, while solid arrows represent control flow (can be understood as either the next step or expressing a logical contingency of the target on the source). As you can see, the creation of the Physical Entity View and IoT Context View (see Fig. 6.1) are explicit activities in the architecting process. All other views are comprised in the activity âderive other viewsâ. Before we look at each of these activities in more depth, let us return to the question of architecture methodologies and how the IoT ARM relates to them."
363,165,0.983,History and Cultural Memory in Neo-Victorian Fiction,"This depiction of photography as magical is present in some of the earliest reviews of the medium. Thus, one English reviewer, writing for The Athenaeum in 1839, called its effects âperfectly magicalâ (qtd. in McQuire, 1998: 13). Mary Warner Marien argues that in the earliest stories of photographyâs origins the technology that made the science of photography possible is couched in mythical, magical language, making of photography a mysterious and hybrid form. She gives the example of stories about Louis Daguerre in which accident, or fate, becomes a character. Daguerre receives a vision of the camera obscura during a dream and a spoon in his cupboard is darkened by mercury fumes overnight. These serendipitous accidents, or fateful events, become as important to Daguerreâs role in the invention of photography as his technological successes engendered by hard work and scientific knowledge and experimentation. Such stories, suggests Marien, âpitch mystery, magic, and alchemy against banal technological accounts of photographyâs adventâ (Marien, 1997: 54).5 Green-Lewis calls this ârealismâs romance with photographyâ, and suggests that in a culture dominated by realism and the desire to reveal, the depiction of photography as magical allowed it to be romanced as the ultimate in proof, as unbiased truth (Green-Lewis, 1996: 9â10).6 It is the photographâs association with perfect representation that, for Linda Hutcheon, makes it attractive to historiographic metafiction. In The Politics of Postmodernism (1989) she links fiction and photography since âboth forms have traditionally been assumed to be transparent media which paradoxically could master/capture/fix the realâ. She argues that in historiographic metafiction photographic models become metaphors for âthe related issue of narrative representation â its powers and its limitationsâ, particularly for the telling of history (Hutcheon, 1989: 39). However, this focus upon the cameraâs perceived capacity for representational veracity elides the other capacity for which photography was enthusiastically welcomed and celebrated in the nineteenth century. The magic attributed to photography also allowed it to be romanced as a memorial. Barrett-Browningâs celebration of the photograph as âthe very shadow of the person lying there fixed foreverâ is echoed in the late twentieth century by Susan Sontag, for whom the photograph is âsomething directly stenciled off the real, like a footprint or a death mask â¦ never less than the registering of an emanationâ. More than realist representation, here the photograph contains the âtraceâ of its subject, is a âmaterial vestigeâ (Sontag 1977: 154). More than correspondence, the photograph inheres its subject. As Green-Lewis"
134,43,0.983,Curriculum Reform in The European Schools : Towards a 21St Century Vision,"skills and dispositions implicit within the curriculum, there are four forms of complexity that might signify progression. These are behavioural complexity, symbolic complexity, affective complexity and perceptual complexity. There is also a type of progression, abstracting, which involves moving from a concrete understanding of a concept to a more abstract one. A further type of progression is an increased capacity to articulate, explain or amplify an idea or construct, i.e. the learner retains the ability to deploy the skill, and in addition, they can now articulate, explain or amplify what they are able to do and what they have done. And finally progression can be understood as part of a process, and this refers to the way that the learner interacts with the learning object. An example could be moving from an assisted performance to an independent one. This suggests that curricula as they are presently conceived round the world are deficient if they employ extensional forms of progression exclusively at the expense of a range of other types. These forms of progression are not of the same order; however, they refer to different aspects of the process of learning. There is no category error here. They are linked by their capacity to affect different parts of the learning process, and in particular, where an individual moves from one state of being to another. For example, extensional forms of progression focus on the objects of learning, whereas process forms of progression focus on the learner and the way they can and do respond to these objects. Over the last fifty years, there has been a move away from traditional/ fragmented approaches towards networked approaches in some school curricula. There are implications of adopting either fragmented or networked approaches or taking up positions in between. A fragmented or traditional approach fits better with how universities, teachers, parents and students understand curricular divisions at school level; allows choice between subject options whilst retaining core subjects; better reflects current arrangements; and can be better accommodated within traditional pedagogic structures. A networked approach reduces choice because it implies that all aspects of the curriculum have to be covered in the teaching and learning arrangements that are put in place; and may better reflect the nature of subject knowledge. The key question is how to balance these imperatives when undertaking a reform of secondary education in a system such as the one we are focusing on in this book. Thus,"
173,317,0.983,"intelligent Human Computer interaction : 9Th international Conference, Ihci 2017, Evry, France, December 11-13, 2017, Proceedings","Let the conditional probability of experiencing an emotion emon given that an emotion emom is expressed be denoted by Pt (emon |emom ). We evaluate Pt (emon |emom ) using a Bayesian framework. These are then normalized over the space of all expressed emotions emom to obtain the probabilities of various experienced emotions emon . First, an emotion recognition algorithm is run on the end-userâs texts to determine the probabilities of various expressed emotions. These probabilities serve as priors in the Bayesian framework. Next, we leverage large datasets containing emotional content across many people (such as blogs, etc.) to measure the similarities between words corresponding to a pair of emotions. This information is computed across several people and is reflective of the general relatedness between two emotion-indicating words (for example, between the words âsadâ and âangryâ). This measure is then normalized (across all possible pairs of emotions considered) to constitute the likelihood probability in the Bayesian framework. The priors and likelihoods are then integrated to obtain Pt (emon |emom ). This conditional probability is specific to the end-user under consideration. This is then normalized over all possible choices of expressed emotions to obtain probabilities of experienced emotions for the end-user under consideration. While a variety of other approaches could be used for this computation, our choice of the Bayesian framework is motivated by the following facts. First, Bayesian models have been successful in characterizing several aspects of human cognition such as inductive learning, causal inference, language processing, social cognition, reasoning and perception [30]. Second, Bayesian learning incorporates the notion of prior knowledge which is a crucial element in human learning. Finally, these models have been successful in learning from limited data, akin to human inference [31]."
32,610,0.983,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","Furthermore, if we consider applicative use, it is difficult to find a solution of k for every st .X/, because the analysis on the typical case requires a tremendous number of operators who solve k in real time, if we aim to create the forecast system of Enjyo. Thus, we test to compare the results with those in random sampling for i. This random sampling would simulate if the Y.t/ by reduced data has the same tendency with Y.t/ of all of data or not. Furthermore, the random sampling will be applied for k because of the influence on k given by the subjective ideas of operators."
80,647,0.983,Innovations in Quantitative Risk Management (Volume 99.0),"2.7 Summary We summarize the second part of the paper. Provided a good model exists,8 we suggest to begin with calculating the bifurcations/threshold values of parameters. They are the pivoting points of possible trend switching. The distance between the current operation point of the real financial system and the bifurcation point must be observed. Large values of the risk index can be used as indicator, signaling how close the risk is. This can be used as a tool for a stress test. Acknowledgments The paper has benefited from discussions with Roland C. Seydel. Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
82,245,0.983,Fading Foundations : Probability and The Regress Problem,"made this choice: De Finetti 1974/1990; Jeffreys 1939/1961; Johnson 1921; Keynes 1921; ReÌnyi 1970/1998. One can define P(q|p) as P(q â§ p)/P(p) only if P(p) = 0. If one adopts this Kolmogorovian definition, one is unable to make sense of P(q|p) when P(p) = 0. The approach of the philosophers mentioned above is free from this difficulty."
117,67,0.983,Care in Healthcare : Reflections On Theory and Practice,"8. âDespite individual differences in perspectives of nurses and nursing, most studies [analysed in this meta-study] identified that nurses held some construction of an âidealâ nurse that usually focused on caring.â 9. The problem of care-givers not being able to meet the necessary demands (e.g. due to a high workload) is mentioned repeatedly in empirical investigations (cf. Price 2009, pp. 16 ff. (on the paradox of caring); Eley et al. 2010, 2012). 10. âNursing is a practice with an inherent moral sense.â 11. âNursing is by nature a moral endeavour.â 12. âCare can be considered simply an ethical task and thus a burden of one more thing to do, or it can be considered a commitment to attending to and becoming enthusiastically involved in the patientâs needs.â 13. See the contribution by Opgenhaffen in this volume (Chapter âRegulation as an Obstacle to Care? A Care-Ethical Evaluation of the Regulation on the Use of Seclusion Cells in Psychiatric Care in Flanders (Belgium)â). 14. There can, of course, also be other accommodations of care, but that is not of interest to me here. 15. Moreover, if nursing is recognised as care, it seems necessary to support and encourage it as care, and consequently to support and encourage the realisation of the constitutive moments of that careâappeal, concern, volition and practice."
185,124,0.983,The Essence of Software Engineering,"something of very limited âreal-estateâ (an index card or electronic equivalent) as the mechanism for capturing the headline information about what we want to build into the Software System. The Work Product is defined here on the card in terms â¢ A brief description. â¢ The Levels of Detail that we progressively elaborate â in this case indicating that initially we ensure that we have captured and communicated the associated value, and that we also need to continue on at some stage to list the acceptance criteria â the dotted outline of the third level of detail indicating that we may or may not capture associated conversations â for example in an electronic tool if we are a distributed team. â¢ The Alpha that the Work Product describes â a User Story in this case. Putting it all together We have now described a representative subset of the different types of card which are used in the User Story Essentials practice, so we will not describe the other cards because the story will rapidly become familiar and repetitious (which is part of the value of using a simple, standard language to express all our practice guidance). Now we understand what all the cards mean, we also need to understand at a high level how the whole practice works. The cards themselves give us all the clues we need as to how the elements fit together to provide an end-to-end story â which activities progress and produce which elements, but it is also here useful to tell the joined-up story in terms of end-to-end flow through the different activities. â¢ First we need to Find User Stories. This brings one or more User Stories into existence in the initial Identified state, each documented by a Story Card with just enough information to ensure that the User Story has its Value Expressed. â¢ On a Story-by-Story basis, we will select a User Story that we wish to get done next, and use the Prepare a User Story activity to progress the User Story to be Ready for Development, which involves ensuring that we have the Acceptance Criteria Listed on the Story Card, and during which we may also get any supporting Conversation Captured. As part of this same activity we also fully elaborate the associated Test Cases. â¢ The final activity that this practice describes is how we work to Accept a User Story, the successful completion of which moves the User Story to the Done state. Notice that this âchainingâ of Activities primarily via the state of the things that they progress does not over-constrain the overall flow. It does not, for example, imply a single-pass, strictly sequential flow. We might, for example, iterate around the different activities for different User Stories in different ways. Exactly how may be further constrained as part of adopting other practices. For example, if we use the User Story practice in conjunction with Scrum, as is very common, we may agree the following general rules as a team: â¢ Do the Find User Stories before we start our First Sprint, but also allow this to happen on an ad hoc basis ongoing."
311,2382,0.983,The Physics of the B Factories,"does not really work. One possibility is that these states are not canonical csÌ. However, the agreement with theory would be improved for the Ds0 (2317)+ , if other prominent decay modes existed in addition to the Ds+ Ï 0 . 19.3.4.6 Precision measurements of Ds1 (2536)+ properties BABAR measured the mass of the Ds1 (2536)+ with a significant improvement compared to the world average, and, for the first time, measured directly its decay width, instead of reporting an upper limit only (Lees, 2011d)."
65,156,0.983,Handbook of Ocean Wave Energy,"couplings between wave components, but in this case they are between sets of three wave components that cause energy transfer via non-linear interactions. When the waves are dispersive these interactions cannot be created, which is why they only occur in very shallow water. The effect of triad wave-wave interactions is to generate a second peak in the wave spectrum at twice the frequency of the original spectrum, which is bound to the main frequency peak in the sense that it travels with the same phase velocity. Unfortunately, currently third generation wave models are unable to correctly model these bound waves, and the triad wave-wave interaction source term is estimated based on the wave spectrum and water depth; however, these source terms have been found to be reasonable approximations in most cases. â¢ The ï¬nal source term typically included in wave models is the depth-induced wave breaking (surf-breaking) source term. This source term typically assumes that a ï¬xed proportion of the energy in any wave is lost when it breaks. Thus it is necessary to make an estimate on the proportion of waves that break at any particular water depth for any particular wave spectrum. This may be done by making some assumptions about the distribution of wave heights and the relative water depth in which these waves will break. Perhaps surprisingly, laboratory observations suggest that the spectral shape is not affected by wave breaking and so the energy removed due to wave breaking, and thus the strength of the depth-induced wave breaking source term, is typically assumed to be proportional to the wave energy spectrum, with the coefï¬cient of proportionality dependent on the proportion of breaking waves."
187,388,0.983,Managing The Complexity of Critical infrastructures : a Modelling and Simulation Approach (Volume 90.0),"CI [3]. (2) An emergency or disaster may emerge from a CI. A technical failure in the electricity infrastructure may lead to a blackout and further cascading effects [4]. (3) An infrastructure may be a resource for response and mitigation actions. This might not be immediately obvious, but may come as a late insight when this infrastructure fails or even gets destroyed [5]. An example is a bridge in eastern Germany that was washed away by a fluvial flood. The local responders were no longer able to send forces to the other riverside, and did not have an alternative plan for that situation. In most cases, it is not possible for crisis managers to revert a decision or an action already takenâin reality. However, in simulation it is possible to do exactly this: âgo back in timeâ and explore a different course of action. This allows answering hypothetical questions like âWhat would happen if I take a different decision or follow a different course of action?â. Therefore, this is also sometimes called âwhat-if analysisâ. Since this type of what-if analysis requires simulation, it is rather suited for training purposes. Providing such what-if analysis as a capability to end-users is the essential idea behind the training system CIPRTrainer, which we will present in detail in the main part of this chapter. CIPRTrainer is the software system that enables crisis managers to train decision-making in crises and emergencies involving cascading effects of CIs. CIPRTrainer constitutes an unprecedented training opportunity that complements standard command post, table-top, or physical exercises. The expected beneï¬ts would be increased awareness of crisis managers of the role and behaviour of interconnected CIs in disasters, emergencies, and crisis situations, and a better understanding of possible consequences of scenario evolution and the influence of own actions. The remainder of this chapter is structured as follows. We continue with briefly embedding the CIPRTrainer approach to what-if analysis into the state of the art and then proceed with characterising the types of complex crisis scenarios that we designed for the CIPRTrainer prototype. Then we will give an overview of the building blocks of CIPRTrainer, explaining how we technically realised it. The following section will then provide an overview of how we realised impact and consequence analysis (CA) for the global assessment of damages and how it is employed for what-if analysis. We continue with explaining how CIPRTrainer is actually used and with an example of a training session. An outlook on the next version of CIPRTrainer and a conclusion end this chapter. For reference, we included a list of acronyms and a bibliography."
230,305,0.983,Marine Anthropogenic Litter,"et al. 2004; Fig. 7.1). Subsequent research showed that similar sized particles were present in shallow waters around Singapore (Ng and Obbard 2006). However, while these early reports referred to truly microscopic particles they did not give a specific definition of microplastic. In 2008, the National Oceanographic and Atmospheric Agency (NOAA) of the US hosted the first International Microplastics Workshop in Washington and as part of this meeting formulated a broader working definition to include all particles less than 5 mm in diameter (Arthur et al. 2009). Particles of this size (i.e. <5 mm) have been very widely reported including publications that considerably pre-dated the use of the term âmicroplasticsâ (Carpenter et al. 1972; Colton et al. 1974). There is still some debate over the most appropriate upper size bound to use in a formal definition of microplastics, with perhaps a more intuitive boundary following the SI classification of <1 mm. The European Union have followed the US and adopted a 5-mm upper bound for categorization of microplastics within the Marine Strategy Framework Directive (MSFD, Galgani et al. 2010). There is a similar lack of clarity when considering the lower size bound for a definition of microplastics. Operationally, this, by default, has been assumed to be the mesh size of the particular net or sieve used to separate the microplastic from the bulk medium of sediment or water column (see review by Hidalgo-Ruz et al. 2012). However, as a necessity of construction, collection devices with meshes in the sub-millimetre size range have a high ratio of net/sieve material compared to apertures and as a consequence they will trap particles much smaller than the size of the apertures/mesh size. Hence, it is not sensible to define the minimum size captured on the basis of the mesh used to collect the sample. Within the EU MSFD a pragmatic approach has been taken based on that used by researchers sampling benthic infauna and sediments with sieves (e.g. Wentworth graduated sieves), where the organisms âretainedâ by a particular sieve are reported. In summary, there is no universally agreed definition of microplastic size, but most workers consider microplastic to be particles of plastic <5 mm in size. There is little consensus on the lower size bound. While defining parameters is essential for consistent monitoring, in the wider context of marine debris and concerns about the potential harmful effects of microplastic it may actually be unwise to specify the size definitions precisely at the present time. Differently sized particles are likely to have differing effects. For example, smaller particles could have consequences that are fundamentally different to larger particles, since the particles themselves can accumulate in tissues and/ or may cause disruption of physiological processes (Browne et al. 2008; Wright et al. 2013c). From a monitoring science, rather than a curiosity-driven perspective, a logical rationale for sampling is to consider abundance in relation to any associated impacts. Since our understanding of the potential impacts of microplastics is currently in its infancy it could, for the time being, be unwise to set a formal limit to lower size boundary and, until there is better understanding about which types/sizes of microplastics are of concern a sensible strategy could be to collect from the bulk medium any particles <5 mm and then quantify microplastics according to size categories."
390,348,0.983,The Hidden Language of Computer Hardware and SW,"The output of the flip-flop is itself an input to the flip-flop. Itâs feedback upon feedback! (In practice, this could present a problem. The oscillator is constructed out of a relay or other switching component thatâs flipping back and forth as fast as it can. The output of the oscillator is connected to the components that make up the flip-flop. These other components might not be able to keep up with the speed of the oscillator. To avoid these problems, letâs assume that the oscillator is much slower than the flip-flops used elsewhere in these circuits.) To see what happens in this circuit, letâs look at a function table that illustrates the various changes. Itâs a little tricky, so letâs take it step by step."
45,119,0.983,Measurement and Control of Charged Particle Beams,"It is possible to considerably extend this simple closed-orbit distortion scheme. For example, the response of all BPMs to every single steering corrector may be combined into a big matrix, which can be used as an input to a sophisticated statistical fitting program, such as LOCO [36, 37]. LOCO then varies the individual gradients of the quadrupoles in a computer model, e.g., using MAD [32], to find the modified quadrupole gradients that best reproduce the measured orbit response data. The advantage of using multiple data sets and multiple fits lies therein that the numerical solutions are overconstrained (which reduces the influence of systematic errors) and that multiple error sources, if present, can be more accurately ascertained. 2.4.3 Phase Advance Instead of fitting trajectories, one can also use (2.32) to compute the beta functions from the measured phase advance around the ring. Then either the quadrupoles fields may be adjusted in the model or the actual magnet settings of the accelerator may be changed to improve the agreement between the measured and predicted phase advance and so identify the source of the discrepancy. An example from PEP-II has been presented in Fig. 2.13 [38]. From top to bottom the improved agreement of model and measurement is clear as the strength of a quadrupole pair in the interaction region was changed by a total of 0.15%. For each quadrupole value, the left column shows the entire ring while the right column shown an expanded view of a particular section. As can be seen, the final quadrupole strength (bottom) yields a satisfactory agreement with the model."
170,233,0.983,Impact of Information Society Research in the Global South,"2.8 Limitations: Motivational Stage and Linkage with General Socioeconomic Inequality Aside from the (maybe too) general approach (which some may consider as a limitation, but we consider it one of its main strengths), the main weakness of Selwynâs model refers both to the lack of a motivational component and to the absence of a direct theoretical linkage with non-ICT inequalities. Despite some people conceptualizing a motivational stage as the first level of the divide (Van Dijk J. 2005), motivations or attitudes towards ICT are complex, multicausal and affect all of the three ICT stages of the proposed divide model in circular ways. From the household perspective, positive attitudes towards ICT could encourage the purchase of digital commodities, but on the other side, without the possibility of access, individuals will not be able to get to the usage stage even if absolutely motivated. Also, motivation could increase the chances of usage (Van Dijk 2005), but usage could lead to the acknowledgement of what can be achieved with ICT and, thus, motivation or appropriation or both. The second weakness of the version we adopted from Selwynâs approach is the lack of any direct linkage within ICT inequalities and socioeconomic-based ones (e.g. social class, Bourdieuâs capitals). It is not that Selwyn forgets to include these subjects (Selwyn 2004), but for the sake of simplification, we excluded them in the specific framework. Obviously, differences in access, usage, appropriation and also motivation related to ICT are determined, at least in part, by socioeconomic inequity (Dodel 2013), and at least some kind of theoretical linkage between the two was already stated in this chapter (the regressive effect of ICT without public policy intervention)."
275,478,0.983,Foundations of Trusted Autonomy,"16.2 Intrinsically Motivated Swarms At the heart of computational models of flocks, herds, schools, swarms and crowd behavior is Reynoldâs iconic boids model [35]. The boids model can be viewed as a kind of rule-based reasoning in which rules take into account certain properties of other agents. The three fundamental rules are: â¢ Cohesion: Each agent moves toward the average position of its neighbors; â¢ Alignment: Each agent steers so as to align itself with the average heading of its neighbors; â¢ Separation: Agents move to avoid hitting their neighbors."
378,66,0.983,The Great Mindshift : How a New Economic Paradigm and Sustainability Transformations Go Hand in Hand,"In The Great Transformation. The Political and Economic Origins of our Times, Polanyi described what he saw as the complete overhaul of the core operating principles of societies, which took place when feudal agriculture was replaced by the capitalist industrialist market model. He used the term âGreat Transformationâ because it demarcated the change from one civilization to another through a process of continuous change of values, knowledge, norms, rules and regulations, starting in the late eighteenth century (Polanyi 1957: 3). His analysis focuses on Great Britain as the origin of industrialization. While applying a historical point of view, his work does not reconstruct a sequence of events in a perfectly chronological manner, but seeks to identify trends in the emergence of institutions and social technologies and to track which philosophical and economic ideas or reasoning lay behind them. To shed some light on these ties, Polanyi describes real world developments as well as core theoretical concepts and the explanations of influential thinkers of the time. This account therefore paints a picture of how creative and reflective actors provide ideas and explanations for real world developments and in so doing influence sociopolitical responses, sometimes very explicitly. His analysis shows how the basic ideas of what I will describe as the mainstream paradigm started emerging in the eighteenth century, and have since underpinned a massive reorganization of the social technologies and institutions guiding human development. To Polanyi, the most powerful of those ideas was the substitution of the economic motive of subsistence with that of gain. Polanyi discusses how the philosophers and scholars of that time were instrumental in presenting this perspective as a more accurate description of reality, one that was even natural or at least desirable. He singles out Adam Smith as particularly influential with his argument that it is a deeply natural human inclination to barter, trade and exchange in order to maximize gain. Smith also made self-interest the fundamental human drive behind the pursuit of those activities (Polanyi 1957: 68â70). Polanyi adds frequent references to other influential thinkers like Thomas Malthus, Jeremy Bentham, David Ricardo and Joseph Townsend, who nurtured the view that this inclination would need to be unleashed fully if man were to escape the fetters of poverty and starvation. Over time the new concept of âinterestsâ replaced what the"
253,872,0.983,"Autonomous Driving : Technical, Legal and Social Aspects","The relation between data and information is too subtle and too complex to be sufï¬ciently explained within the limits of this chapter. All the same, considering data and information as roughly equivalent should sufï¬ce for the purpose of this chapter. Using only one term would put the paper at odds with some of the referenced literature."
307,191,0.983,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"In Fig. 4.6, we show the expected values of the calcium concentration for wild type data when the channel is open (solid, red) and closed (solid, blue), as well as mutant-type data . D 3/ when the channel is open (dotted, red) and closed (dotted, blue). In the computation using mutant data, we simply replace kco with kco . However, keep in mind that this affects the formulas defining the probabilities o and c as well."
37,28,0.983,The Making of Islamic Heritage : Muslim Pasts and Heritage Presents,"I believe this perspective to be truer to the nature of the development of ideas and practices pertaining to Islam. Read in this way, the mosque approximates to the trove of narrative materials pertaining to the past that has amassed over the course of more than a millennium. The density and variety of evidence available at the site of the Friday mosque in Isfahan makes it a case exceptional for its richness. However, I would like to suggest that principles regarding Islamic pasts that I have highlighted using it as an example are applicable more generally to materials and sites large and small. The ultimate point here is that heritage is an evaluative concept and is based on assessments of worth. Not all things from the past constitute heritage in all circumstances, and the ones that do differ between times and places. When assessing evidence pertaining to Islam, I suggest that we should begin with the presumption that Islamic understandings of the past are variable, both synchronically and diachronically. Traces at our disposal indicate a tremendous variety of ways of being and acting as Muslim. The diversity of meanings on display here has been inherent in Islamic discourses for all the contexts for which we have evidence. In tandem with these facts, Islamic valuations of heritage are also fundamentally diverse and changeable. As we can see in the case of the inï¬uence of the prestige of Italian restorers and modern organizations such as UNESCO, Islamic understandings need not be seen as exclusive or hermetic. Rather, the notion of heritage itself requires historicization in all contexts, which reveals investments held by us as well as people who created earlier material forms. I believe this perspective attunes us to pay the utmost attention to the particularities of the evidence we encounter while simultaneously avoiding reiï¬cation and being mindful of shadows cast by interpretive paradigms old and new. Appreciating and creating heritage are closely related, if not synonymous, matters, something that is as true today as it was for Muslims of the past whose effects we scrutinize to create our narratives."
14,285,0.983,Contested Childhoods : Growing Up in Migrancy,"Emerging Strategies and Paradigm Practices of childhood in different social ï¬elds and negotiation of identity are far from trivial. Our ï¬ndings show how many paradoxical ways of negotiating identity exist in everyday life among children of mixed parentage. The participants negotiate identity by combining various approaches, such as âhaving both feet in both campsâ and practicing situational ethnicity, choosing only one group or simply going beyond any categories in various contexts. Some reject describing themselves with a limited category. They seem to require a new category of their own that expresses having an in-between âmixedâ status. They contest the current categories and challenge this limited way of thinking, which hardly reflects real life. One participant stands out, as he simply declares himself a multi-cultural cosmopolitan and rejects narrow labels. Even the participant who declares herself to be Danish indicates a simultaneously positive awareness of multiple belongings. Our data shows that the participants overall construct their identity based on different negotiating strategies in everyday life (Root 1996). Our theoretical understanding takes into consideration the context and the realities of society which shape the identity of the child and enables us to show how interaction with parents, peers, siblings, and community are important factors in shaping an individualâs life. Along with examining the context, the theory places human existence in a speciï¬c historical time and space; the child can construct changeable identities while adjusting to a speciï¬c social environment. The three strategies mentioned in the theoretical framework bring into focus the changeable aspects of different contexts, as identity is understood as a constantly changing phenomenon throughout life. Such a multi-dimensional model gives an opportunity to have more memberships and multiple identities with different groups (Root 1992, 6; Spickard 1992, 22). In fact, today a child of mixed parentage can choose to embrace some or all aspects of his identity, although within societal constraints (Spickard 1992, 21â22). In this way, social ambiguity and fluid identities are seen as possible ways of viewing oneself as deï¬ned and united. The participantsâ narratives demonstrate that you can belong to multiple cultures. They see it as a strength and advantage, despite clear disadvantages. This notion challenges the simpliï¬ed perception of belonging to just one group, a contrast to the historical perception of mixedness as pathologized (described in the beginning of the chapter). In fact, the childrenâs self-narratives and identities are much more dynamic, having their own complex nature. Although most of the participants are proud of their mixed origin, they also report having suffered from discrimination and bullying. There is a big gap between how they see themselves and how the world views them due to continued ignoring of mixedness at the societal level. As there is no policy focus on the impact of mixedness in the educational and social"
58,53,0.983,Enabling Things to Talk,"role of the IoT ARM here in order to take full advantage of the IoT Reference Model and the IoT Reference Architecture in the chapters in-between. When applying the IoT ARM in designing systems, it is likely that in each individual case, different architectures will result. Thus, while Fig. 3.2 gives the impression that the process of translating the reference architecture into a concrete architecture is independent of the use case itself, this is, in reality, not so â the guidelines and the engineering practices chosen rely on a use case description and the requirements. This fact is reflected in Fig. 3.3. The role of the IoT ARM is to provide transformation rules for translating the rather abstract models into a concrete architecture. This step is strongly influenced by the use case and the related requirements. One entry point for this information is during the process of design choices, i.e. when the architect favours one avenue for realising the functionality or quality needed over another. The IoT ARM also recommends design patterns for such choices. The IoT ARM does not operate in a design vacuum but should be applied together with proven design process practices, which in themselves are contingent upon the guidelines provided and upon the use case and the requirements. In Chaps. 7 and 8 we describe how both the IoT Reference Model and the IoT Reference Architecture can be used in this design process. Even though we describe the design process in a linear fashion, remember that in practice this will not always be the case. Depending on the engineering strategies used, some of the steps can be done in parallel or may even have to be reiterated due to additional understanding gained during the process or due to changes in the requirements."
13,464,0.983,Feeling Gender : a Generational and Psychosocial Approach,"The characteristic switching in the youngest generation between gender as dichotomy and gender as irrelevant may illustrate a tension between, on the one hand, gender as a dichotomous structure in our language and thinking where it is almost unavoidable to automatically âgenderâ opposing qualities, and, on the other hand, an experience of increasing irrelevance of gender in practice. However, it may also testify to gender having become a more flexible dimension psychologically and through this also less substantial and threatening. It is difficult to see the appreciation of sexual difference within certain limits as only products of normative commands, backlashes or defensive reactions in the youngest generation. Gender as âsoft assemblyâ may be a historical product: âGender may in some contexts be thick and reified, as plausible real as anything in our character. At other moments gender may seem porous and insubstantialâ (Harris 2002: 104). Muriel Dimen argues that the solution to the problem of splitting is not merely remembering the other pole, but âbeing able to inhabit the space between, to tolerate and even enjoy the paradox of simultaneityâ (Dimen 2002: 56). What I would add to this is that there are not only new spaces between the two poles, but also non-gendered spaces where the poles disappear because they are simply no longer experienced as relevant. This addresses the question I posed in Chap. 1 about the relationship between destabilising a category and weakening its significance in different areas of life. Increased equality leads to a weakening of gender norms or makes the category of gender less important, constraining and exclusionary in some areas. This does not imply that sexual difference disappears, but different sexual preferences based on sexual difference may be experienced as more personal and less normatively constrained choices. The wish for sexual difference in heterosexual attraction expressed by many in the youngest generation is not claimed as general, unitary, recommendable or normative, but as a personal preference (until further notice). Within the youngest generationâs individualist frame of thinking, it is rather a claim that everyone should do as they like, and what one likes may vary. Thus, the connection between gender norms and desire is loosened. Heterosexual choice becomes one of a number of choices. This does of course not remove the still-prevalent discriminatory attitudes and structural disadvantages of non-normative groups in society with a magical touch, but it indicates that heterosexuality may also be understood and practised within a post-heter-"
8,193,0.983,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Cherenkov Radiation At the same time in 1979 I proposed [9] the idea about Cherenkov gluons which could produce jets (collimated groups of particles) in high energy collisions. That happened after our experimentalists showed me the emulsion plate with the newly registered cosmic ray event where distinct rings formed by secondary hadrons reminding us of the ordinary Cherenkov rings were easily seen. That new property could also serve as a signature of the properties of the medium. In analogy to the permittivity of the ordinary medium, the term âchromopermittivityâ was coined to describe the hadronic medium containing colored quarks and gluons. Its value was directly determined by the positions of the rings. For further work of our group, and in particular, on comparison with the data of RHIC experiments I refer to papers [10, 11] where our approach has been reviewed."
275,17,0.983,Foundations of Trusted Autonomy,"consider agents composing a swarm intelligence as âsimilarâ and ranging to identical, but not necessarily âsignificantâ capabilities, with the implications that resilience is a property of the collective rather than the individual. Harvey notes that swarm intelligence may relate to a category within the complexity and self-organisation spectrum of emergence characterised as weakly predictable. Swarms do not require centralised control, and may be formed from simple agent interactions, offering the potential for graceful degradation. That is, the loss of some individuals may only weakly degrade the effect of the collective. These and other âblessingsâ of swarm intelligence presented by the author are tempered by the shortcomings of weak predictability and controllability. Indeed, if they are identical, systematic failure may also be possible as any design fault in an individual is replicated. The author suggests a future direction for research related to the specification of trust properties, might follow from the intersection of liveness properties based on formal methods and safety properties based on Lyapunov measures. Swarm intelligence also brings into question the nature of intelligence. Perhaps it may arise as an emergent property from interacting simpler cognitive elements. If a social goal for autonomy is collaboration, then cooperation and competition (e.g. for resources) is important. Furthermore, interdependent autonomy must include machines capable of social conflict. Conflict exists where there is mutually exclusive intent. That is, if the intent of one agent can only be achieved if the intent of the other is not achieved. Machine agents need to recognise and operate under these conditions. A structured approach to framing competition and conflict is in games. Michael Barlow, in Chapter 7 examines trusted autonomous game play. Barlow explains four defining traits of games that include a goal (intent), rules (action bounds), a feedback system (awareness), and voluntary participation. Voluntary participation is an exercise of agency where an agreement to act within those conditions is accepted. Barlow examines both perspectives of autonomy for games and games for autonomy. Autonomous entities are usually termed AIs in games, and may serve a training purpose or just provide an engaging user experience. So, improving AIs may improve human capabilities. Autonomous systems can also benefit from games, as games provide a closed-world construct for machine reasoning and learning about scenarios. These chapters take us on a brief journey of some unique perspectives, from autonomy as individual computational intelligence through to collective machine diversity."
238,272,0.983,Nanoinformatics,"with values equal to the threshold were joined to give crisp isosurfaces. Figure 7.14b shows the same isosurface with the inclusion of uncertainty. Uncertainty is a function of spatial distribution of atoms. Distribution of atoms is less at distances away from the isosurface and thus no effect was observed. Near the surface, the uncertainty of the isosurface decreases. From the image, it is observed that there is an increased level of intensity as the surface is approached."
264,64,0.983,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"arriving on surf beaches come from more than one storm, and if we add two similar sine functions we get a curve with three peaks. Everyone has their own examplesâICMIâs Klein Project is a multilingual collection of contemporary mathematics written for teachers. It would be nice if the pleasure that we get from mathematics imbued the whole of mathematics education, but we know it does not. Why not? How do we manage to take the pleasure out of mathematics? This question underlies all that follows. Let me now return to Ubiratan DâAmbrosio. It is an honour to be following up Ubiratan DâAmbrosioâs thinking, so let me briefly, and with a broad brush-stroke, go over what he was on about. He questioned inequity within mathematics education in a very fundamental way, and gave us some models for working towards creating a fairer world through a mathematics education that really paid attention to social and cultural issues. Many, many people have worked very strongly in this area, and I do not intend to give a summary of the comprehensive work that has been done. In more recent years, Ubiratan DâAmbrosio started to talk about mathematics as a dorsal spine. I want to highlight this metaphor because it is a very nice way of thinking about what has happened. He sees mathematics as the dorsal spine of civilization, the basis of science and technology (DâAmbrosio, 2007, 2015). The trouble is that you may have a spine and skeleton on which an animal may be built, but that animal sometimes turns into a monster rather than a beautiful creature. This has happened within mathematics, and, I would argue, within mathematics education. DâAmbrosio suggests that our essential goals are responsible creativity and ethical citizenship. What he did was highlight the role of mathematics and mathematics education in achieving both of those goals. In other words, he was pointing us to the wider reasons for our work as mathematicians and mathematics educators. But how? How do we do this? What is it I am supposed to do to engender responsible creativity and ethical citizenship? When I walk up the steps and go into my ofï¬ce, what actions will I take? I can presumably do some things in the way I behave, but how do I help to engender appropriate actions in the students that I teach? How are we to build a beautiful creature and not a monster. I think that DâAmbrosioâs essential message is that we should reinstate cultural processes within mathematics education in order to build beautiful creatures. I wish to think about what other things we might do. To develop a basis for making possible actions more explicit I would like to invoke ecological systems theory, which was developed in the context of child development by Urie Bronfenbrenner in 1979, a couple of years after DâAmbrosio introduced the ethnomathematical approach. The two theories have some overlapping principles (Bronfenbrenner, 1992)."
116,46,0.983,Moral Reasoning At Work : Rethinking Ethics in organizations,"towards yourself, then you should be helpful towards them. Presumably, if helpfulness is not something you particularly treasure in others, you have no moral obligation to be helpful towards others. Kant meant the categorical imperative to be stricter than a hypothetical one in that it is universally binding for all rational beings, and not contingent upon individual or cultural differences. It seems that he considered the Categorical Imperative to be an improvement on the Golden Rule in the sense that it avoided subjectivity and added universality as a requirement of moral considerations. The principle of equality puts demands on the justification of choices. It requires that a decision-maker can back up a difference in treatment of two cases with an identification of a morally relevant difference between them, but does not single out one particular moral outlook or ethical foundation to be uniquely right. It does not favour duty ethics over utilitarianism, or vice versa, but remains neutral regarding the tension between them. As we saw in the discussion of the trolley problem, duty ethics considers the fact that a person is used as mere means to indicate that it is a morally unacceptable option, even though this option maximizes utility. Utilitarianism, on the other hand, considers consequences as the only morally relevant features of the situation, and thus comes to different conclusions about what to do. Both traditions acknowledge the principle of equality, but part company on the issue of what constitutes a morally relevant difference. Looking back on previous examples from this book through the lens of the principle of equality, we can see how the justifications can take the form of finding reasons to make exceptions in the application of ordinary moral norms. In the blackmail case, Anne can claim that she does not give in to blackmail or other kinds of pressure that can occur in a corrupt economy, but that she makes an exception in this case, due to the colossal economic stakes that are involved. In the reference case, Ben can argue that he normally is truthful towards others, but that he is making an exception in this case, since he needs to restore harmony in his unit, and can do so effectively by hiding truths about the employeeâs quarrelsome behaviour. In the second trolley case, a person can argue that he or she would normally not kill an innocent person, but that the current situation warrants an exception, since it is thereby possible to save five lives. Whether we accept these appeals to morally relevant differences depends on how well they fit with our moral convictions and beliefs. DOI: 10.1057/9781137532619.0007"
330,112,0.983,Dynamics of Long-Life Assets : From Technology Adaptation to Upgrading the Business Model,"The design research community has yet to clearly deï¬ned design thinking (Dorst 2011), but according to Brown (2009), âdesign thinking functions within a framework of three intersecting âconstraints.â They are âfeasibilityâ, which is what can be done; âviabilityâ, what you can do successfully within a business; and âdesirabilityâ, what people want or will come to want.â The principle underlying the intersection of desirability, feasibility, and viability is an iterative process. This process includes the development of visualized prototypes, then demonstrating them to customers and observing the customers to learn what they really desire (Maurya 2012). Although this process leads to more failures than successes, it tent to reveal customersâ current needs (bootcamp bootleg 2015). To navigate through this process requires a different mind-set and also a high level of empathy for people, hence a human centred approach. The objective of design thinking is to improve the rate at which successful product, service, and business model innovations are brought to the market (Harvard Business Review 2015)."
170,225,0.983,Impact of Information Society Research in the Global South,"agency: these refer to a âpoolâ of performances to which an individual can access at any given time (Alkire 2005: p. 1; Zheng 2007: p. 2). Nevertheless, effective functionings are the only things researchers would probably be able to assess in the majority of studies: well-paid jobs, formal education achievement, income, etc. This would be the area or dependent variable in which ICT may have an impact. However, Fig. 1 shows that CAâs framework is even more complex, in the way that not all individuals are able to convert or generate capabilities in equal rates from the same features. This is due to differences in their conversion factors, which may be personal (i.e. literacy, cognitive ability, gender), social (i.e. culture, norms, values) and environmental (Zheng 2007: p. 2). Conversion factors may also be considered capabilities themselves, which mediate the conversion of ICTâs characteristics (Alampay 2006: p. 9; Garnham 1997: p. 32): literacies, knowledge on the use of ICT and the understanding of the implications of using information as a resource, to name just a few of them. Moreover, ICT commodities may also act as conversion factors or conversion factor enablers (Heeks and Molla 2009: p. 34). At this point, it is probably clear to the reader that the problem with CA is that as strong and comprehensive its theoretical and philosophical framework is, it is a very difficult framework to understand and apply, especially to a non-ICT researcher. As Heeks and Molla (2008: p. 33) argue, CA is âquite a dense set of ideas that can be hard to understand and translate into practical evaluation termsâ. For the sake of lowering barriers to entry to the ICT field, while CA provides a strong theoretical model on ICTâs impact on wellbeing, it is not the best candidate to use as a firstentry analytical framework."
187,299,0.983,Managing The Complexity of Critical infrastructures : a Modelling and Simulation Approach (Volume 90.0),"that uses the outcomes of the V&V. V&V Users/Sponsors face the problem of having to make a judgment on the development and suitability of the M&S system or results for an intended use. The key issue here is that it is not possible to demonstrate with absolute certainty that the M&S system or results will meet the Real World needs prior to its actual use. Consequently, there is always a probability that the M&S-based solution is not successful when used (i.e., fails). Such a failure would result in an undesirable impact (i.e., a risk) on the operational environment. Therefore, an M&S system or result is only acceptable to the V&V User/Sponsor if he or she has sufï¬cient conï¬dence that the use of an M&S system or result satisï¬es the Real World needs without posing unacceptable risks (e.g., costs, liabilities). This M&S acceptability is something relative to different V&V Users/Sponsors: what is acceptable to one V&V User/Sponsor may not be acceptable for another. The V&V User/Sponsorâs decision-making process therefore requires appropriate evidence-based arguments to justify his or her acceptance decision. The basic premise of GM-VV is that V&V are performed to collect, generate, maintain and reason with a body of evidence in support of the V&V Users/Sponsors acceptance decision-making process. Here, validation is referred to as the process that establishes the V&V User/Sponsorâs conï¬dence as to whether or not they have built or procured the right M&S system or result for the intended use (i.e., M&S validity). In other words âDid we build the right M&S system?â. To ensure that the M&S system or results at delivery can be demonstrated to be valid, it is necessary to ensure that the M&S system is built and employed in the right manner. Here veriï¬cation is referred to as the process of establishing V&V User/Sponsors conï¬dence in whether the evolving M&S system or result is built right (i.e., M&S correctness). In other words âDid we build the M&S system right?â. The GM-VV considers V&V as a speciï¬c problem domain of M&S with its own needs, objectives and issues. This domain is referred to as the V&V World (Fig. 6). The V&V world groups the products, processes and organizational aspects that are needed to develop an acceptance recommendation that can be used by the V&V User/Sponsor in his or her acceptance decision procedure(s). This recommendation included in a V&V report is the key deliverable of a V&V effort and contains evidence-based arguments regarding the acceptability of an M&S system or results. Here the GM-VV premise is that the acceptance decision itself is always the responsibility of the V&V User/Sponsor and decision procedure(s) may involve trade-off aspects beyond the V&V effort scope. The development of an acceptance recommendation in the V&V world is driven by the V&V needs that are traceable to the V&V User/Sponsorâs acceptance decision or procedure(s) needs (e.g., budget, responsibilities, risks, liabilities). Therefore, the extent, rigor and timeframe of a V&V effort depend on these needs. Depending on these needs, the V&V effort could span the whole or speciï¬c M&S lifecycle phase of the four worlds; could focus on one speciï¬c or multiple (intermediate) M&S products; and should match the development paradigm that was used (e.g., waterfall, spiral). Each case may require a separate acceptance recommendation with its own scope and development timeline. Moreover, the way the V&V effort interacts with the four M&S-based problem worlds also varies from"
113,128,0.983,Pentecostalism and Witchcraft : Spiritual Warfare in Africa and Melanesia,"to ideas about growth, whether we speak of vibrant vital energies of the high mountain forest, thickly concealed with vegetation (Biersack 1982), or if we are thinking of the deliberate secrecy or opacity associated with many ritual activities in Melanesia, including especially forms of male initiation that grew young boys into strong men by removing them from the sight of their mothers and others, as was the case in the upper Asaro and its male cult (Read 1952). Dialectics of sight and sociality, display and domination, exhibition and egalitarianism, are key qualities of the happening of social life in Melanesia. Yet this ethnographic literature on the social effects of tactics of concealment and display has focused largely on the motivations and intentions of social actors, on their personal and political purposes. The problem of witchcraft, however, involves dynamics of the concealed or unseen in contexts where people might wish to see but cannot. In the discourse of witchcraft, we find ideas about those who deceive without being perceived, and people who feel watched but cannot witness. Witchcraft in an important sense is about the disruption of the interpersonal economy of sight. During my interviews with lawyers, police, and legal experts in Port Moresby and Goroka in 2013 and 2014, witchcraft was often said to be a âspiritual problem.â When asked to describe what they mean by âthe spiritual,â people refer to things which cannot be seenâthe invisible (cf. Blanes, this volume). Witchcraft is believed to be an especially troubling phenomenon because it is both everywhere and unseen. For those in PNGâs law and justice sector, then, it is a problem precisely because it cannot leave evidence in its wake, making it impossible to prosecute, and contributing to the sense that witchcraft is increasing. A further connotation of what is meant by âspiritualâ refers to the spirits or souls of persons and the corruption therein. Law and justice sector discourse characterizing witchcraft as a âspiritual problem,â then, is also a discourse that locates the best means of addressing witchcraft in those institutions that address the souls of persons: churches. Most PNG police, scholars and others with whom I have worked would agree that the solution to PNGâs problem of witchcraft is ministry. Ministry alone is thought to be capable of redeeming the community. But the irony of âPentecostal witchcraftâ reemerges, for the ministry that might solve the social problem of witchcraft can only do so by finding it over and again in its congregations."
311,2755,0.983,The Physics of the B Factories,"where MR , ÎÎ³Î³ and Î are the mass, two-photon decay width, and total width of the meson resonance R, respectively. B(R â final state) is the branching fraction for the decay of the meson R. Here we assume that the resonance shape is represented by a conventional relativistic Breit-Wigner function (see Chapter 13). If we know the branching fraction of the mesonâs decay mode used in the measurement, then we can extract the two-photon decay width; otherwise, we measure the product of the twophoton decay width and the branching fraction. The twophoton partial decay width is a fundamental and direct observable to explore the qq or exotic nature of the neutral meson. Even for a non-exotic meson, the two-photon decay width is useful to study the quarksâ quantum state inside the meson and to test QCD models (Munz, 1996). In a zero-tag measurement, we can make the signal events almost free from other processes by applying a rather stringent transverse-momentum balance. This is a great advantage in searches for new resonances as well as new decay modes of known hadrons. The requirement for the transverse-momentum balance also restricts the Q2 of"
257,175,0.983,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","Having presented our language, we now turn to the reasoning principles that it supports. Intuitively, these principles allow us to analyze the effect of a piece of code by restricting our attention to a smaller portion of the program state. A first set of frame theorems (1, 3, and 4) describes how the execution of a piece of code is affected by extending the initial state on which it runs. These in turn imply a noninterference property, Corollary 1, guaranteeing that program execution is independent of inaccessible memory regionsâthat is, those that correspond to block identifiers that a piece of code does not possess. Finally, in Sect. 3.3, we discuss how the frame theorems can be recast in the language of separation logic, leading to a new variant of its frame rule (Theorem 6)."
375,25,0.983,Musical Haptics,"Abstract The dynamical response of a musical instrument plays a vital role in determining its playability. This is because, for instruments where there is a physical coupling between the sound-producing mechanism of the instrument and the playerâs body (as with any acoustic instrument), energy can be exchanged across points of contact. Most instruments are strong enough to push back; they are springy, have inertia, and store and release energy on a scale that is appropriate and well matched to the playerâs body. Haptic receptors embedded in skin, muscles, and joints are stimulated to relay force and motion signals to the player. We propose that the performer-instrument interaction is, in practice, a dynamic coupling between a mechanical system and a biomechanical instrumentalist. We take a stand on what is actually under the control of the musician, claiming it is not the instrument that is played, but the dynamic system formed by the instrument coupled to the musicianâs body. In this chapter, we suggest that the robustness, immediacy, and potential for virtuosity associated with acoustic instrument performance are derived, in no small measure, from the fact that such interactions engage both the active and passive elements of the sensorimotor system and from the musicianâs ability to learn to control and manage the dynamics of this coupled system. This, we suggest, is very different from an interaction with an instrument whose interface only supports information exchange. Finally, we suggest that a musical instrument interface that incorporates dynamic coupling likely supports the development of higher levels of skill and musical expressiveness. S. OâModhrain (B) School of Information & School of Music, Theatre and Dance, University of Michigan, 2051 Moore Building, 1100 Baits Dr, MI 48109-2085 Ann Arbor, MI, USA e-mail: sileo@umich.edu R. B. Gillespie Mechanical Engineering, University of Michigan, 3450 GG Brown Building, 2350 Hayward Street, MI 48109-2525 Ann Arbor, MI, USA e-mail: brentg@umich.edu Â© The Author(s) 2018 S. Papetti and C. Saitis (eds.), Musical Haptics, Springer Series on Touch and Haptic Systems, https://doi.org/10.1007/978-3-319-58316-7_2"
188,404,0.983,Responsive Open Learning Environments : Outcomes of Research From The Role Project,"teacher-created artefacts are the norm (using âteacherâ to mean anyone, including a team of designers or fellow learners, that intentionally or otherwise helps another to learn). So it is with interest that I read this chapter reporting on personal learning environments, but talking about them in the context of intentional teaching, courses, workshops and other planned processes. Self-regulation can occur at many scales. We may choose to control different aspects of the learning process but almost always delegate control to others at many stages, whether to the author of a chapter or learning object, the leader of a workshop, our PLE or the widgets within it. Some tools described in this chapter such as Etherpad and Flashmeeting hinge on social engagement, which entails a need to be at the very least mindful of the schedules, needs and goals of others. This highlights a tension that exists in nearly all PLE implementations, that they support our social learning activities, but that those social learning activities themselves, with our fellow learners and teachers, provide shape and form to our learning. For instance, I was not surprised to read that relatively little use was made of Etherpad and Chat in the events described: given that participants were collocated it would not normally be very useful to provide alternative real-time collaboration tools, especially as the tasks did not appear to focus on production of a permanent artefact but were simple part of some active experimentation to use the toolset. At the heart of all my reflections on this chapter is the fact that PLEs are more than just a way to keep things organized in our learning lives. Done well, they are generative toolsets that can act in some ways like a teacher, offering guidance, inspiration, motivational support and structure to the learning experience. But, at the same time, they seek to provide freedom from such a teacher role, to be soft tools to support self-regulated learning. They are thus both teachers and not teachers at the same time. Their innate softness is perhaps the reason that the evaluations performed in this chapter focused on helping people to use the tools in a manner that is anything but self-regulated and explains why it is so hard to pin them down. A PLE is personal: every individual builds processes and methods around them, configures his or her own space but, at the same time, that space is shaped and influenced by the people, resources, learning objects, tools and expertise that are available. This tension lies at the heart of education. When we educate ourselves we choose the parts that we delegate to others more than those who follow a more guided path but, through the shape of our tools, the people around us and simple path dependencies, we have many of our decisions made for us and, at a finer granularity, always delegate at least some of the teaching process to others. Getting the right balance is a tough task to perform well and partly explains why case studies like the one presented here have a vital role to perform in helping us to understand that better."
333,19,0.983,Uses of Technology in Upper Secondary Mathematics Education,"Even apparently individual co-action becomes social as learners work together to process the meaning of representations. However, the social dimension can become even more pronounced through collective work with distributed representations. Our second example of co-action involves students interacting collaboratively with the representation and communication infrastructure (Hegedus and Moreno-Armella 2009) of a classroom network of graphing calculators. Within that setting, we can give each student control of a single point in a Cartesian environment, which she can move using the calculatorâs arrow keys. In real time, the points of all the students in the class are displayed in a shared Cartesian space, which is projected at the front of the classroom. The following activity was created by a teacher to support the idea of the perpendicular bisector of a segment as the locus of points equidistant from the segmentâs endpoints. As students move their point (point C), they see it represented on their calculator screen as the third vertex of a triangle with the segment AB as its opposite side, where the measures of the variable sides of the triangle are also shown (Fig. 2a, c). The teacher asks the class to search for points where the distances from point C to points A and B are the same.2 As students locate points that satisfy the condition, a pattern emerges in the shared space, indicating the perpendicular bisector of AB as a locus of points, with ever-increasing clarity (Fig. 2b). Of course, a dynamic geometry environment can provide this representation on an individualâs screen. However, the socially distributed nature of the locus of points in this activity provides an important experience and tool for thinking for the"
8,463,0.983,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Let us nevertheless make a little numerical analysis of Eq. (19.56). For the average mass of âfireballsâ, we may expect a value of perhaps the nucleon mass. This is in accordance with our observation that most of all energy in jets must be contained in the kinetic energy of the longitudinal component and only a little in the excitation (= mass) of fireballs. We rewrite Eq. (19.56), putting x0 D m =T? and V0 D .4=3/.a=m / :"
269,121,0.983,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"Here then, even as the three accounts diverge markedly in what happened, there is, at least to our eyes, a way in which the discipline of psychology, here, is positioned â or imagines itself to be â as more pragmatic than the other disciplines. We are reminded that, even in Hubbub, which is an endeavour in many ways defined by its successes, disagreement abounds. Feelings can run high. Very different ideas of the stakes of the project, as well as the modes of investigation appropriate to it, run headlong into one another. If the split in the above meeting was temporary, and indeed productive, we believe, for the research of Hubbub as a whole, we remind ourselves of the thin sutures that continue to hold interdisciplinary collaborative projects together. We remain deeply alive to our own capacity, at any moment, to come undone."
271,81,0.983,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"From todayâs perspective, we can still agree with Park that we should treat the city more as a specific context for different processes of communitization than necessarily a coherent community; although constructions of the city as a community are of course quite possible. But all the same, they should be treated more at the level of public communication, where their function can range from appeal to âsocial controlâ. By contrast, what we understand by âthe mediaâ has fundamentally changed in 100 years. Todayâs urban media environment no longer consists of a few printed media for public communication, plus the letter and the telephone. With the deep mediatization of the city, this environment has become more varied, complex and also contradictory. But this, in turn, takes us back to Parkâs starting point concerning the role of media in the âmosaic of little worldsâ. Recent study of the role of media in urban life is likewise concerned with this role. There is particular interest in the analysis of the city as a diverse and transcultural living space that is also in many respects segregated and gentrified. In this context, Myria Georgiou talked of the âmediated cityâ (2013: 41). This concept draws attention to the fact that our experience of the city today can no longer be treated as separate from the media of communication. She is likewise interested in seeing the relationship of media and city âfrom street levelâ (2013: 3; see also Lane 2016); substantively, this means in terms of peopleâs âconsumption, identity, community, actionâ. Shopping malls are in particular locations of urban consumption, locations that are comprehensively saturated medially, while also being an expression of gentrification (cf. Bolin 2004). It can be observed that the medial construction of identity in the city takes place in a relation of tension between very different cultural definitions (Christensen and Jansson 2015: 130â152), given that the city is itself a transcultural space in which people of many different backgrounds live together (cf. Hepp 2015: 120â123). With respect to the formation of community, it follows from this that the city cannot be simply seen as a single community. Instead, we have a variety of diverse and local communities that are in part opposed to each other, as well as diasporas with a very weak sense of community (Georgiou 2013: 92â116). Here, then, media open opportunities for political action in the city, examples being âurban gardeningâ or âreclaim the streetsâ, the organization of both of which is supported via digital media (Bridge 2009; Rauterberg 2013: 97â128). From this perspective, it is also necessary to describe this complexity of the city critically, as a medially saturated space"
252,13,0.983,The Ethics of Vaccination,"social media and paid a visit to the school to personally thank the students. She rightly wanted to give visibility to a behaviour which, she suggested, should serve as a model for others to follow. Many, including all the newspapers that reported the news, had the same reaction as the Italian Minister of Health. In a note on the high school website, the class described their decision to be collectively vaccinated as an âact of solidarityâ towards Simone. There is no doubt the classâ decision was motivated by noble sentiments and that, considering that many of them would not otherwise have got vaccinated, it was in fact an act of solidarity. This nice story is particularly suited to introducing a book on the ethics of vaccination for three reasons. First, it clearly illustrates, on a small-scale scenario, the practical application of a concept with great ethical relevance when applied on a large scale, namely, that of herd immunityâa concept I will return to later in this chapter and throughout the book. Second, the story shows why we need to develop an âethics of vaccinationâ, as the title of this book suggests: being vaccinated is a decision that not only could benefit the vaccinated individual but alsoâand indeed more importantlyâcontributes to protecting other people around us, thus raising the distinctively ethical question of whether and to what extent we should do something that is not only or even primarily in our self-interest (actually, the individual benefit of vaccination will be minimal or even negligible in some cases, as we will see in Chap. 2). Third, the story suggests that protecting vulnerable people through herd immunity is a collective enterprise, that is, something individuals cannot do alone but need to do together. The collective nature of the effort gives rise to a collective action problem and a tension between collective and individual responsibility. Such tension calls for a philosophical inquiry that can yield precise ethical and, ideally, political prescriptions. The philosophical inquiry around collective and individual responsibilities will be dealt with in Chap. 2. The policy implications, viewed in light of a principle of least restrictive alternative in public health policy, will be the subject of Chaps. 3 and 4. In this first chapter, I will discuss some of the sources of the ethical problems raised by vaccination and some of the ethically relevant facts about vaccination, clarifying the exact scope of the present discussion and what important ethical issues will be left out. This book will be successful if, at its conclusion, it will have convinced the reader that in a world where people simply behave in a minimally ethical wayânot heroically, only decentlyâa case like that of the Italian high school class should not be seen as particularly praiseworthy. On the"
113,154,0.983,Pentecostalism and Witchcraft : Spiritual Warfare in Africa and Melanesia,"statue in itself of being a product of sorcery18 (compare similar processes of iconoclasm with Strong, this volume). They claimed to have received the prophetic message to do so in the Muxima. Similarly, another particularly notorious and âextremeâ version of this kind of appropriations of sorcery takes place in a church located in the Hoji Ya Henda neighborhood, known as Combat Spirituel (the French name deriving from the fact that it is a movement originated in the DRC, and indicating that its public is mostly composed of regressados). This church is led by two pastors that are a married couple. As a Bakongo friend of mine explained, their success is based on their strong emphasis on demonology and the organization of âspiritual crusadesâ (cruzadas espirituais) that engage in deliverance. Through a âconfessional regimeâ (Badstuebner 2003), believers in this church are required to âleave everything behindâ, including their material possessions, in order to be released from the effects of ndoki. According to several comments I collected from people from other churches, the point of abandoning your money and possessions (and handing them to the pastors) in order to be healed was central. The heightened expression of such a logic takes place in their well-known extreme fasting sessions (so long that many people are reported to have died in them). Through this operation of bodily and material dispossession promoted by the Combat Spirituel, we see a kind of appropriation of the traditional conception of ndoki as an object inside the body, complemented by the fact that the believer who wants to be released must âbecome invisibleâ, in the sense of abandoning his material possessions and entering the church compound. Interestingly, I was never âallowedâ by my Angolan friends to visit the church. The interesting point here is that, more than seeing ndoki as an agent of corruption, Pentecostal churches seem to insert it within a logic of production of alterity, of âcosmological strangenessâ (Kwon 2008: 29; see also Bertelsen, this volume), through a Manichean reasoning (whereby ndoki is a necessary and convenient corruption) that also emphasizes the economic transaction, similar to what the traditional kimbandeiros performed (so there is more a continuity than a rupture, from this perspective). But here, the economic paradigm is inverted: instead of the traditional redistribution, there is a logic of dispossession versus accumulation. Thus, we can speculate about the material and symbolic role of money in the process. Heonik Kwon (2007) talks about âdollarized ghost moneyâ to describe how, in Vietnam, mourning rituals became affected by changing political and economic relations in the country."
280,302,0.983,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"Early workers used major genes in butterfly mimicry as an argument for major mutations driving evolution, but Fisher countered that mutations with a large effect on the organism will virtually always be deleterious (Fisher 1930). More recently Orr has shown that during an adaptive walk, we expect an exponential distribution of mutational effect sizes (Orr 1998, 2005). Early in the process, there is a high likelihood of mutations that move the population a large distance relative to the optimum. Later on, smaller effect mutations are more probable, that act to âfinetuneâ the adaptation. To some extent this modern view therefore reconciles the two camps. The theory developed by Orr and others hypothesised a population evolving towards a single adaptive peak. However, the frequency-dependent nature of mimicry and warning colour means that these traits have a different dynamic. If a population of butterflies has a bright warning colour pattern (hereafter the âmimicâ), predators will learn this pattern, and the population will generally be well protected from predation. There may be other butterfly species locally that are perhaps more abundant or more toxic (the âmodelâ) and therefore have a better-protected wing patterns, so the mimic species would gain in fitness by evolving mimicry of the model pattern. However, an individual âmimicâ that deviates from the rest of the population would be selected against, even if it becomes slightly more similar to the model. The two patterns would have to be very similar for predators to generalise between them, in order for gradual evolution towards the model to be possible (Turner 1981). Most current Heliconius patterns in different mimicry rings are sufficiently different from one another that gradual convergence seems unlikely. There is a valley of low fitness between the model and mimic which would seem to prevent gradual evolution of mimicry. This difficulty can be overcome if a single mutation causes a large change, sufficient to induce enough similarity to the model in one step that overall fitness is increased. This initial mutation is unlikely to produce a perfect mimic, so subsequent mutations will then be needed to perfect the phenotype. This argument was first outlined by Nicholson (1927) and termed the âNicholson two-step modelâ by John Turner (1977, 1984, 1987). Mimicry may therefore have a different genetic architecture to traits evolving under a singlepeak-climbing model (Baxter et al. 2009)."
233,382,0.983,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"but rather by sample completeness (Alroy 2010; Jost 2010; Chao and Jost 2012). Completeness, when measured by a statistic known as coverage (Good 1953), is the proportion of individuals in a community that are represented by species in a sample from that community (Chao and Jost 2012). When samples differ in their coverage, they should be standardised to equal coverage before a âfairâ comparison can be made. Much like expected species richness, the coverage of a sample can be estimated from the sample size and the distribution of individuals among the species in the sample (Chao and Jost 2012). Given that standardisation by sample completeness has been shown to yield a less biased comparison of species richness between communities (Chao and Jost 2012), it would be desirable to have a similar method of standardisation for PD. Since rarefaction of coverage is mathematically related to rarefaction of sample size, the recent work on estimating PD from sample size will no doubt form the basis from which estimated PD for sample coverage will be developed. Finally, a general issue when considering any PD measure is uncertainty regarding the length of branches and the topology (branching pattern) of the tree. All PD measures (including those presented here) assume that the branch lengths and their arrangement in the tree are perfectly known. This is obviously an abstraction, although PD can be surprisingly robust to this source of variation (Swenson 2009). One solution to this dilemma is to calculate PD, including rarefied PD, for a large number of possible trees and report the mean and confidence limits. The output from a Bayesian phylogenetic analysis is a large number of trees, each with their own topology and corresponding branch lengths (see for example Jetz et al. 2012) and so lends itself well to this approach. However, when the possible trees number in the thousands and tens of thousands, this is obviously computationally intensive. An analytical solution, directly incorporating uncertainty into the calculation, would therefore be desirable. This is not an easy extension of the PD rarefaction solution because both variation in branch length and topology (affecting the probability of encountering internal branches) would need to be taken into account. It is worth remembering that phylogenetic relationships are not the only source of uncertainty when investigating real ecological communities â neither the abundance, nor even the presence (occupancy), of species are necessarily known with precision."
116,97,0.983,Moral Reasoning At Work : Rethinking Ethics in organizations,"Denial of responsibility The decision-maker claims that one or more of the conditions for responsible agency are absent. Forces beyond his or her control rule out genuine decision-making and the freedom to choose. In business, this technique can take the expression of the person presenting himself as a pawn on a checkers board, move around by top management or the dynamics of the competitive environment. The person claims to act out of necessity, and not from free will and personal control. It is a matter of survival. Natural forces are at play, and moral criticism makes no more sense here than if we were morally critical of a storm, a fight amongst animals, or some other natural phenomenon."
117,83,0.983,Care in Healthcare : Reflections On Theory and Practice,"is to take this emotional knowledge seriously in such a way that it is not set in opposition to cognitive knowledge. A healthy balance must be struck between both forms of knowledge, placing more value on emotional knowledge as a creative factor while cognitive knowledge remains present in the same way as a constant check and balance. Care ethics can only truly bear fruit when it draws on emotional knowledge to enable unique and creative approaches without being absolved of the obligation to justify such creative solutions with transparent and comprehensible arguments."
297,1524,0.983,The R Book,"Evidently, a logarithmic transformation of the explanatory variable is likely to improve the model ï¬t. We shall see in a moment. The question is whether increasing population density leads to a signiï¬cant increase in the proportion of males in the population â or, more brieï¬y, whether the sex ratio is density-dependent. It certainly looks from the plot as if it is. The response variable is a matched pair of counts that we wish to analyse as proportion data using a GLM with binomial errors. First, we use cbind to bind together the vectors of male and female counts into a single object that will be the response in our analysis: y <- cbind(males,females)"
316,17,0.983,Executing Magic in the Modern Era,"Abstract This chapter examines historic views on the potency, power and agency of the living criminal body in the early modern and modern periods as a way of understanding the potency of the criminal corpse. The main section of the chapter focuses on the witch as the most powerful of living criminal bodies. There is discussion on phrenological interpretations of criminality and the work of Cesare Lombroso on the âborn criminalâ. The meaning of cruentation, or the ordeal by bleeding corpse, is also explored. Keywords Witch Â· Phrenology Â· Humours Â· Bleeding corpse Lombroso In the medieval and early modern period, it was widely thought that God left his imprints on all living things, and it was an aspect of natural magic for humans to try and interpret their meaning to understand better the world He had created. With regard to human bodies, this meant that the lines on the hand, the wrinkles on the forehead, the shape of the nose, the colour of hair, the number of moles and other visible bodily features, signified how God moulded each person and imbued him or her with an individual character, identity and destiny. This art or science of physiognomy drew on concepts from the ancient world that expounded all-encompassing theories regarding the interconnectedness Â© The Author(s) 2017 O. Davies and F. Matteoni, Executing Magic in the Modern Era, Palgrave Historical Studies in the Criminal Corpse and its Afterlife, DOI 10.1007/978-3-319-59519-1_2"
245,678,0.983,The European Higher Education Area : Between Critical Reflections and Future Policies,"(4.2) Conceptions of knowledge and knowing. Hofer (2004), Hofer and Pintrich (1997) developed a model that organizes epistemic beliefs in four dimensions, each seen as a continuum between two poles: the certainty of knowledge, ranging from deï¬nitive to evolutionary; the simplicity of knowledge ranging from individual concepts added one to another, to concepts seen to be interrelated; the source of knowledge, ranging from it being transmitted by an external authority, to it being produced by the person him or herself; the justiï¬cation of knowledge, ranging from it being due to an authority, to it resulting from proof via a rigorous procedure. Automatically activated, epistemic beliefs would influence the goals constructed by the learner, the metacognitive processes and the choice of learning strategies (Muis 2007). The learner not only makes judgments about learning (Do I know?), but also makes what could be called epistemic judgments: How do I know? (Hofer 2004). The importance of these judgments can be seen in the trivialization of internet search, where queries using Google are in most cases the ï¬rst step of a literature search (Biddix et al. 2011). The learner is confronted with a multitude of information sources, the reliability of which needs to be assessed. In this regard, (BrÃ¥ten et al. 2005: 154) note that âin open and global information networks, anyone can publish anything, and the difï¬cult task of checking the relevance and accuracy of information traditionally done by publishers, is now transferred to the students themselvesâ. Finally, the analysis in terms of structural equation modelling carried out by Cano (2005) in a survey of 1600 Spanish students conï¬rmed the direct and indirect influence (via learning approaches) of epistemic beliefs on school performance. (5) Personality characteristics. One of the most influential characterisations of personality is the âBig Fiveâ model (Costa and McCrae 1992), so called because it organises personality in ï¬ve traits: extraversion (active, sociable versus silent, shy); pleasantness (nice, cooperating versus nasty); conscientiousness (meticulous, applied versus disordered, distracted); emotional stability or neuroticism (calm, relaxed versus anxious, irritable); openness to experience (openness, curiosity versus conformity, conventional). In a research in the UK with Bachelor students, (Chamorro-Premuzic and Furnham 2008) observed that conscientiousness, and to a lesser extent openness to experience, have a signiï¬cant impact on academic success. The recent meta-analysis of the psychological correlates of academic achievement conducted by Richardson et al. (2012) conï¬rms that conscientiousness is signiï¬cantly associated with academic achievement. In contrast, openness to experience does not seem to exercise signiï¬cant influence. However, to our knowledge these features have not been linked to learning outcomes such as âthe disposition to understand for oneselfâ (Entwistle and McCune 2013). In conclusion, as far as characteristics of students are concerned, it seems necessary to consider a whole range of features related to previous training experience, and the level of knowledge acquired to enter the program. This level can be assessed in various ways on the basis of past academic experience or, more speciï¬cally, via an initial assessment of knowledge about the area to be learnt. In addition, the impact of epistemic beliefs and conceptions of learning on the learning process now seems sufï¬ciently documented through research for us to include them. With regard to"
8,243,0.983,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","(NA35, NA36 and WA85) were specifically equipped, (b) high temperatures, which my experimental collaboration WA80 took on to measure via direct photon spectroscopy, (c) J/psi suppression which NA 38 set out to measure. Experiment NA34 addressed spectra of dileptons, a signature of dense hadronic matter. All experiments had to master measuring the global event character, like impact parameter, multiplicity of charged particles, transverse energy flow, etc., which some did only after many years of trial and error. Hope was that we should find in the QGP temperatures above âHagedornâs limiting temperatureâ, recognized as the freeze-out temperature for hadrons from a cooling down QGP. In nearly all SPS experiments, Rolf Hagedornâs limiting temperature was found in various measured hadron spectra. The strangeness QGP signature was particularly successful. Strangeness and more specifically strange antibaryons were recognized and developed by J. Rafelski, at times in collaborations involving R. Hagedorn, B. Muller, M. Danos, as the key to the QGP discovery. In the early years, both QGP and abundant strangeness, were very âexoticâ topics. As an example, Johannâs strangeness presentation was relegated to the âexoticaâ section of the LBL conference proceedings in 1983 in company of âAnomalonsâ, a long forgotten false discovery. With a strong experimental program and clear objectives at SPS, very strong and diverse evidence for QGP was discovered in study of strange hadrons, in particular strange antibaryons. My WA93 collaboration discovered flow phenomena in 200 A GeV S+Au collisions at a level of 5â10 times weaker than at the Bevalac. As mentioned earlier, this became then a real industry of v1 and v2 measurements, now extended to much higher orders. My groupâs experimental series WA80/93/98 was keen to measure direct photons, developed exquisite technologies and methods to do so. Some predicted QGP-temperatures of up to 1 GeV in early stages of the SPS collisions. In retrospect we can say that this was impossible, as this would require huge compressed energy densities and very short thermalisation times. This is indeed impossible to achieve even at the LHC. In our initial optimism we hoped to measure these extreme conditions, but were realistic enough to prepare for low thermal photon yields of thermal = ratios of a few percent only. However, the extreme values never showed up, and we could only measure an upper limit for thermal photons from a plasma with a temperature of about 220 MeV. At RHIC, our WA98 photon spectrometer was employed again and could measure indeed direct photons telling a temperature of about 280 MeV of the QGP."
297,1753,0.983,The R Book,"Suppose that our data are X and our model contains the parameters Î¸ . We might ask what is the probability of observing our data given that the model is true? This is called the frequentist approach to maximum likelihood (see p. 390): p(X |Î¸ ). Alternatively, because we are usually a lot more certain about our data than we are about the truth of our model, we might ask what is the likelihood of our model, given the data: l(Î¸ |X ). These two quantities are different ways of expressing the same idea, but they embody a fundamentally different approach. You need to think about this last paragraph until the penny drops. The fundamental part of Bayesian statistics is that the posterior distribution p(Î¸ |X ) is proportional to the product of the prior and the likelihood: p(Î¸ |X ) â p(Î¸ ) Ã l(Î¸ |X ). This tells us how to modify our existing beliefs in the light of the newly available data. Note the proportionality: we can multiply the likelihood by any constant without affecting the posterior, but we need our posterior probability distribution to integrate to 1."
294,105,0.983,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"The variables x_pos and y_pos could then have been printed or used in other ways in the code. There are possibilities for having a variable number of function input and output parameters (using *args and **kwargs constructions for the arguments). However, we do not go further into that topic here. Variables that are defined inside a function, e.g., g in the last xy function, are local variables. This means they are only known inside the function. Therefore, if you had accidentally used g in some calculation outside the function, you would have got an error message. The variable time is defined outside the function and is therefore a global variable. It is known both outside and inside the function(s). If you define one global and one local variable, both with the same name, the function only sees the local one, so the global variable is not affected by what happens with the local variable of the same name. The arguments named in the heading of a function definition are by rule local variables inside the function. If you want to change the value of a global variable inside a function, you need to declare the variable as global inside the function. That is, if the global variable was x, we would need to write global x inside the function definition before we let the function change it. After function execution, x would then have a changed value. One should strive to define variables mostly where they are needed and not everywhere. Another very useful way of handling function parameters in Python, is by defining parameters as keyword arguments. This gives default values to parameters and allows more freedom in function calls, since the order and number of parameters may vary. Let us illustrate the use of keyword arguments with the function xy. Assume we defined xy as def xy(t, v0x=0, v0y=0): g = 9.81 # acceleration of gravity return v0x*t, v0y*t - 0.5*g*t**2"
275,424,0.983,Foundations of Trusted Autonomy,"14.3.1 Phase 1: Human-Human Studies The purpose of the Human-Human study phase is to elicit behaviors on the part of humans in a collaboration so that they can be characterized and understood, and then replicated on a robotic platform. For this work, a study of non-verbal interaction between human dyads performing a car door assembly task was performed. The door is instrumented in seven locations"
62,139,0.983,"Agile Processes in Software Engineering and Extreme Programming: 17th International Conference, XP 2016, Edinburgh, UK, May 24-27, 2016, Proceedings (Volume 251.0)","We designed our algorithm to receive a series of snapshots as input. We define a snapshot as a copy of the code and tests at a given point in time. In addition to the contents of code and tests, the snapshot contains the results of running the tests at that point in time. Our algorithm uses these snapshots to determine the developersâ changes to the program. It then uses these changes to infer the TDD process. In this paper, we use a corpus of data where a snapshot was taken every time the code was compiled and the tests were run. It is important that the snapshots have this level of detail, because if they do not, we do not get a clear picture of the development process."
378,237,0.983,The Great Mindshift : How a New Economic Paradigm and Sustainability Transformations Go Hand in Hand,"least will not seem inevitable. The term âmindâ captures all of those less intellectual aspects of human existence, too: sense, meaning, soul, intention, or spirit. The seeds of imagination, belief, and argumentative ammunition for becoming a change agent have been planted. The emphasis in this book lies on exploring the transformational potentials of a Great Mindshift in mainstream economics for the agenda of sustainable development. Of course one could also open up the blind spots and contingencies in other dominant paradigms of the development agenda, like nationalism and sovereignty or human rights and individualistic justice systems. But none of these are built on ideas or âscientiï¬c conceptsâ that involve such a degree of flawed assumptions about the things to which they are applied: human-need satisfaction and natural resource governance. Sustainable development is about integrating social, environmental, and economic goals in the short and long term. So while the monetized numbers and mathematical equations appear to provide a high degree of scientiï¬c certainty and predictability, they do not say much about the trade-offs behind the costâbeneï¬t weighting that happened in the quantiï¬cation process. The models running predictions of growth, employment, productivity, and competitiveness are equally intransparent and based on the assumptions that nature and humans can be freely substituted and should move around in the correct amounts needed for efï¬cient markets. This is very unhelpful for informed decision-making. For democratic decision-making, it is a real problem. It means one can present computational graphs and numbers instead of having to make serious ethical and moral judgments explicit because they might be politically risky or detrimental for the justiï¬cation of oneâs privileges. Concepts such as utility, capital, market price, and growth are, as discussed, laden terms. Whether we like it or not they include many value judgments. Also, according to the mainstream economic theory, only more is better. Any idea of enough or sufï¬ciency necessarily translates into limiting and unsatisfying results. Any vision of arriving at steady-state equitable prosperity is ex ante excluded from the imaginary. This is at least ideological. When looking at the triple crisis in environment, social equity, and economic stability today it seems future-foreclosing. History is an open-ended process and the security-, justice-, and well-being-providing potentials of sufï¬ciency strategies become imperative for a world of nine billion, in particular with regard to future generations. They should not be qua theory excluded from the choice set of rational actors. Interestingly, some important economic thinkers like Mill (1806â1873) and Keynes (1883â1946) also had sufï¬ciency ideas for the scenario in which economic output growth led to a certain degree of material saturation. At levels of sufï¬cient supply, they reasoned, humans would work and produce at a constant level and efï¬ciency or technology improvements would lead to more time with family and friends, cultural events, education, recreation, and so on. These thinkers also always limited the realm of an economy, and therefore economics. Neither the governance framework nor the paradigm were foreseen to impact all of human existence and natural life."
270,187,0.983,The Huawei and Snowden Questions : Can Electronic Equipment From Untrusted Vendors Be Verified? Can An Untrusted Vendor Build Trust into Electronic Equipment? (Volume 4.0),"11.1 Overview Many of the chapters in this book are concerned with fields of research that are motivated by problems besides ours but which still have some bearing on the problem of untrusted vendors. This chapter is different, in that it is motivated by an envisaged solution to the problem rather than a research field. This envisaged solution â containment of the equipment that we do not trust â consists of two parts. The first part is the detection of misbehaviour. This problem area is discussed thoroughly throughout the book. In separate chapters, we have discussed detection through formal methods, reverse engineering, the static analysis of code and hardware, the dynamic analysis of systems, and software quality management. The other part of the solution is that of replacing or somehow handling misbehaving equipment. For this part of the solution, we are in the fortunate situation in which the mechanisms we need have been studied and developed over decades. This work has been conducted under the headline of fault tolerance rather than security, but the developments work well in our situation nevertheless. In the upcoming sections, we touch upon some of this work. The field is rich and solutions have been developed for Â© The Author(s) 2018 O. Lysne, The Huawei and Snowden Questions, Simula SpringerBriefs on Computing 4, https://doi.org/10.1007/978-3-319-74950-1_11"
192,167,0.983,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"society at large. Scientists see the âcrisis of scienceâ as âa reflection of the crisis that besets our whole societyâ¦ How increasingly instable the integrations in physics are becomingâ (p. 151). The âgolden sphere of harmonyâ had already ceased to exist long ago and even the basic convictions (the philosophemes) of classical physics became untenable. Indeed, quantum physics exemplifies the âshakiness of our metaphysicsâ (p. 151), but also the âawareness of the need for synthesisâ. But this, according to Bloch and his followers, would require a shift, on the level of the philosophemes (beneath the bar: S1), towards dialectic materialism as the new foundation, turning society as such into a kind of university world, with physicists as guardians of the atomic age. According to Marxism, only a classless society may solve the crisis that science (S2 above the bar) experiences today (p. 151), abolishing philosophy by absorbing it into science. Initially, the gadget has a positive impact on the subjects involved in terms of individuation and personal growth. It prompts and enables Sebastian to face difficulties that would have âoverwhelmed him 3 years beforeâ, that he would have been âutterly incapable of coping withâ (p. 178). Thanks to the gadget, he had âbroken through that magic but sterilising cocoon of solipsism that had enveloped him â¦ He had become a human beingâ (p. 179). Yet, increasingly, this process of individuation becomes frustrated and Sebastian eventually emerges as a tormented subject ($ in the lower-right position). Working on the bomb project, Sebastian experiences himself as âa man dividedâ, outwardly self-possessed, but inwardly distraught and tormentedâ (p. 211). A ânameless anguishâ possesses him (p. 213). His new symptoms ($) are no longer the symptoms of egocentrism and unworldliness that troubled him in the past, for the âThingâ changes everything and engenders a series of more unsettling symptoms. He becomes increasingly distant and unapproachable. A sinister change comes over him (p. 256) due to his involvement in âthis fearful Thingâ (p. 257), turning him more and more into a stranger. In the confrontation with the Thing, the position of the scientific expert (S2) becomes destabilised, and the research site becomes an ethical and epistemological clinic ($), sometimes literally, when Sebastian is forced to keep his bed because of exhaustion (clinic is derived from ÎºÎ»Î¯Î½Î·, = bed). According to Tanya, his unsettling tasks make him inhuman and estrange him from humanity. His appalling responsibilities become âa screen separating themâ (p. 260). The Monster creates âa gulf between themâ (p. 288). âWeâve hit on the gadgetâ¦ the Monsterâ¦ the Thingâ (p. 148), Sebastian argues, and this âThingâ, this instrument of wholesale destruction, without parallel or precedent in history, is now relentlessly âchallengingâ him (p. 147). Indeed, his whole being becomes wrapped up in the Bolt; he eventually becomes devoured by it (p. 344). He is âconsumedâ by his gadget, his object a. The Monster will be born; nothing can stand in its way. His whole being seems to rebel against it, but he cannot stop the Monster. He senses that he is âin the grip of a necessity that transcended his own willâ (p. 336). The object a is pulling him towards his destiny. Face to face with the Bolt, he is no longer master in his own house, as Freud once phrased it: He had known for a long time, even before the premonitory shadow of its shape first loomed before him, that his commitment to it was inescapableâ¦ His whole being rebelled, [but] the most important decision of his life was one that he had already unwittingly made (202)."
21,146,0.983,intertwingled : The Work and influence of Ted Nelson,"In the introductory pages of Dream Machines, Nelson makes it clear why CL/DM is a book with two sides. He does not aim for broader understanding of computers, through the information found in Computer Lib, simply because our society is becoming more computational in general. Rather, as he writes: My special concern, all too tightly framed here, is the use of computers to help people write, think, and show. But I think presentation by computer is a branch of show biz and writing, not of psychology, engineering or pedagogy. This would be idle disputation if it did not have far-reaching consequences for the designs of the systems we are all going to have to live with. [10, DM2]"
257,386,0.983,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","of a use-case, for brevity and concreteness, here we describe directly the formalization of a real use-case scenario that involves mHealth (mobile health) apps. All the concepts presented apply in general to every solution based on mID(OTP) (apart from a trivial renaming of the entities). Only the steps and instance-factors related to the particular OTP generator used are specific for this use-case. In Sect. 4.1, we describe the entities and the steps of the OTP-generator approach for this use-case. In Sects. 4.2 and 4.3, we detail the mapping between the assumptions and their formal specification. In Sect. 4.4, we give the formalization of the security goals. In Sect. 4.5, we present the results of our security analysis."
249,324,0.983,Advances in Proof-Theoretic Semantics (Volume 43.0),"Abstract In this paper we discuss a proof-theoretic foundation of set theory that focusses on set definitions in an open type free framework. The idea to make Cantorâs informal definition of the notion of a set more precise by saying that any given property defines a set seems to be in conflict with ordinary modes of reasoning. There is to some extent a confusion here between extensional perspectives (sets as collections of objects) and intensional perspectives (set theoretic definitions) that the central paradoxes build on. The solutions offered by Zermelo-Fraenkel set theories, von Neumann-Bernays set-class theories and type theories follow the strategy of retirement behind more or less safe boundaries. What if we revisit the original idea without making strong assumptions on closure properties of the theoretical notion of a set? That is, take the basic definitions for what they are without confusing the borders between intensional and extensional perspectives. Keywords Set theory Â· Foundations Â· Proof theory Â· Definitional reflection Â· Partial inductive definitions Â· Functional closure"
378,99,0.983,The Great Mindshift : How a New Economic Paradigm and Sustainability Transformations Go Hand in Hand,"Manfred Max-Neef developed a helpful tool to understand human needs that increases the creative imaginary space for sustainable innovations that target experienced utility and well-being. This popular Chilean development economist has called himself the âbarefoot economistâ after spending 12 years with the poorest communities in Latin America. His moment of revelation came when understanding that none of the mainstream market-based economic concepts were of any relevance to them. Much of his work has thus been built on proper empirical experience in non-marketized societies. It therefore provides some fascinating reflexive systemic insight into the effect that living with certain institutional setups has on the way actors see and experience the world. In an interview with Democracy Now! Max-Neef explained that his return to immersing himself in experience rather than deducing theoretical models made him conclude that we need an entirely new language in order to understand better what people really need. According to him, mainstream economics has nothing to say in support of the poorest people in the world if strategies are supposed to emerge from local systems rather than disrupting them. If you live in poverty, behaving like the selï¬sh, insatiable atomic accumulator of mainstream theory wonât get you very far, says Max-Neef: âYou cannot be an idiot if you want to survive; you need networks of cooperation and mutual aid.â In those communities, he observed, competition and the promise of monetary gain are not required for people to demonstrate enormous creativity, innovation and willingness to collaborate (Max-Neef 2010). To illustrate which fundamental human needs he observed instead, Max-Neef has developed a matrix whose key messages I summarized in Table 3.2. The matrix limits the number of existential human needs to approximately nine. He does not argue that the list of nine existential needs is deï¬nitive or set in stone, but he says he is conï¬dent that a change in those basic needs would at best occur at a very slow pace. Also, these needs should be understood as interrelated and without hierarchies, except for the need of subsistence or survival, which comes ï¬rst (Max-Neef 1992: 204â205). Here Max-Neef differs from other need-satisfaction approaches, many of which place self-actualization at the top and social, as well as material"
348,124,0.983,Control Theory Tutorial : Basic Concepts Illustrated By Software Examples,"7.2 Uncertainty: Distance Between Systems Suppose we assume a nominal form for a process, P. We can design a controller, C, in a feedback loop to improve system stability and performance. If we design our controller for the process, P, then how robust is the feedback system to alternative forms of P? The real process, P â² , may differ from P because of inherent stochasticity, or because of our simple model for P misspecified the true underlying process. What is the appropriate set of alternative forms to describe uncertainty with respect to P? Suppose we defined a distance between P and an alternative process, P â² . Then"
75,470,0.983,"Opening Science : The Evolving Guide on How the Internet is Changing Research, Collaboration and Scholarly Publishing","Aspects of the Social Web in Open Science Users particularly welcome innovations when they hold the promise of an advantage of some kind. Individual features embedded in otherwise linear platforms will be just as unsuccessful as those that regularly fail nowadays in consumer applications. The intelligent consolidation of information to create an outcome that is of broad use to all stakeholders involved will assert itself unless an inferior, but high-reach solution achieves exclusive prominence in the eyes of the users. For this reason, the aspects set out below can only realize a fraction of their collective, self-multiplying effect. They are nevertheless being included individually for the sake of maintaining the linear structure of the text."
209,132,0.983,Deliberative Public Engagement With Science : An Empirical investigation,"In a separate manuscript that uses the data from Study 5, we develop a statistical model for parsing out the distinct types of attitude change and polarization, which may be of use to those concerned about multiple possible outcomes of deliberation (Muhlberger, Gonzalez, PytlikZillig, Hutchens, & Tomkins, 2017). Although the model is easily estimable using OLS regression, the model remains somewhat involved, so we do not use it in this chapter."
311,901,0.983,The Physics of the B Factories,"Due to the presence of cross-feed events, the fit for the branching fraction for any one channel uses as inputs the branching fractions from the other channels. Since these branching fractions are not a priori known, BABAR employs an iterative procedure to obtain the 22 branching fractions. It has been shown that D(â) D(â) K events contain resonant contributions (Aubert, 2008bd). In order to measure the branching fractions inclusively without any assumptions on the resonance structure of the signal, BABAR estimates the eï¬ciency as a function of location in the Dalitz plane of the data. BABAR uses this eï¬ciency at the event position in the Dalitz plane to reweight the signal contribution. To isolate the signal contribution eventper-event, BABAR uses the s Plots technique (Pivk and Le Diberder, 2005) (see Chapter 11). The s Plots technique exploits the result of the mES fit (yield and covariance matrix) and the p.d.f.s of this fit to compute an eventper-event weight for the signal category and background category."
78,159,0.983,The Onlife Manifesto : Being Human in a Hyperconnected Era,"of a (crude) atomismâis thematic in phenomenology as well as other twentieth century currents of thought. As but one example in phenomenology: in her analysis and descriptions of various forms of sexual experience, Sara Ruddick first of all critiques more dualistic understandings of sexualityâi.e. as something that occurs solely between âbodiesâ as somehow radically separated from their âownerâsâ sense of selfhood and identity (1975). Rather, a phenomenological account of our most intense experiences (such as experiences of playing sports) foregrounds how in such experiences, there is no felt mind-body dualism, but rather an immediate unity of self and body. Not all sexual experiences count (or need to count) for Ruddick as involving such direct unity: but she argues that those that do are morally preferable first of all because in such experiences, our own personhood and autonomy cannot be separated from our bodies, and hence these experiences foster the Kantian duty of respect for the Other as a person. Ruddick further argues that such sexual experiences thereby foster two additional virtuesânamely, the norm of equality and the virtue of loving (Ruddick 1975, p. 98 ff.; cf. Ess 2014). To recall Natanson, finally, our erotic engagement with an Other is at once the exemplar and a primary instantiation of our inextricable relationality with one another as co-constituting our identities as embodied beings (1970, p. 47 f.). At the same time, at least to some degree, this focus on interactions more than isolated entities is already at work in Kantâs epistemology. Broadly, Kant makes clear that science, as resting on both mathematical and empirical foundations, thereby focuses on the law-like relationships ( VerhÃ¤ltnisse) between entities. This becomes perhaps most prominent in his Critique of Judgment, with its focus on the sensus communis as an intersubjectively shared sense of aesthetic judgment (Thorseth 2012). As yet another example: in his âtheory of communicative action,â Habermas develops a phenomenological notion of a âlife-world,â one âbounded by the totality of interpretations presupposed by the members as background knowledgeâ (1985, p. 13). Such a life-world, with its background of shared assumptions, is then the context for the communicative practices Habermas takes as paradigmatic of rationality. As characterized by his expositor, Thomas McCarthy, Habermas focuses on morality as intertwined with a socialized intuition that further brings into play the (equally Aristotelian) recognition that self-identity, as shaped by the society in which one finds oneself, ââ¦is from the start interwoven with relations of mutual recognitionâ (1994, p. 47). This interdependence, moreover, ââ¦brings with it a reciprocal vulnerability that calls for guarantees of mutual consideration to preserve both the integrity of individual persons and the web of interpersonal relations in which their identities are formed and maintained.â ( ibid) The phrase âthe web of interpersonal relations,â finally, echoes and reinforces especially feminist emphases on ethical decision-making within âthe web of relationships,â beginning with the work of Carol Gilligan (1982). At the same time, as we will explore more fully below, one of the most significant contemporary philosophical theories of privacyânamely, Helen Nissenbaumâs account of privacy as âcontextual integrityâ (2010)ârests precisely on such relational notions of"
62,400,0.983,"Agile Processes in Software Engineering and Extreme Programming: 17th International Conference, XP 2016, Edinburgh, UK, May 24-27, 2016, Proceedings (Volume 251.0)","for each of those implementations and one âsuperâ mind map that links all of them together. For more complex modules, we may have the mind map of the module which has âsubâ mind maps for different areas or sections of the module. This concept of linking mind maps really revolutionized the way we used this tool. Most importantly it helped us create something that was missing in our project: a high level vision of our product. Using this powerful tool, we centralized all the relevant information about our product that would otherwise be scattered all around our AMT. Mind maps started being used not just by testers but also by product owners, developers or any other person who wants to understand the expected behaviour of any module in our application. We would often see someone asking a question like: âIs this module supposed to appear in this page?â or âDo you remember the rules for this module we implemented a few months ago?â being answered two or three clicks after opening the root mind map. Now we can confidently say that this tool is in fact an Oracle for the behaviour of our application."
141,188,0.983,Cyber-Physical Systems of Systems: Foundations â A Conceptual Model and Some Derivations: The AMADEOS Legacy,"Eliciting dynamicity behavior of different nature that applies to different portions of an SoS is not enough to have a full understanding of the dynamic behavior. With this aim, along with the dynamicity package, we have considered interaction diagrams in order to focus on the message interchange between a number of lifelines: Sequence Diagrams. We propose a methodology to be used to represent dynamicity as it follows: â¢ Making use of Sequence Diagrams to represent the system behavior in terms of a sequence messages exchanged between parts; â¢ Selecting the constituent systems involved in the communication; â¢ Describing the most common interactions. This type of representation helps a system designer to understand which are the properties of an SoS that are constantly changing and how the SoS can change and rearrange its components. The dynamic introduction, modiï¬cation or removal of constituent systems can introduce new system behaviors that need to be analyzed."
8,440,0.983,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","conservation of total energy could be left to the longitudinal component. Thus Eq. (19.36) would hold true whatever the actual number of particles is. We would expect such a law to govern, not only processes like p C p ! A C B, but also the transverse momentum distribution of the many particles produced in cosmic ray jets. Our model would thus explain the hitherto obscure fact that experimentally the transverse momentum distribution in high-energy events is independent not only of the primary energy, but even of the number of particles involved. Since T0 depends only on the range of interaction, cosmic ray jets and large angle scattering must show the same behaviour There might, of course, be some slowly varying factors (powers of p and/or E) in front of the exponential which differ from case to case, but the asymptotic behaviour should be dominated by Eq. (19.36). Orear [14] points out that Eq. (19.36) is a rather good fit to many processes. He quotes experimental results on p p elastic scattering, p C p ! C d, C p ! C p, and finds that they are all well fitted [the C p data are rather meagre and can only be said not to disagree with Eq. (19.36)] by our Eq. (19.36) if one takes T0 D 158 MeV (p p elastic), 160 MeV (p C p ! C d). Figure 19.2 may illustrate how good the Ë fit actually is. Orear (from whose paper [14] the figure is taken) plots E2 del =d! Ëpp as a function of p? D p sin . The fit d! Ë 2 d Ë"
134,52,0.983,Curriculum Reform in The European Schools : Towards a 21St Century Vision,"regards to mathematics). Nevertheless in both science education and mathematics education there is a growing body of evidence-informed work about what makes for a good curriculum. Perhaps the most fundamental issue is that of content. In science education there has been a growing acknowledgement in recent times that many school curricula are overloaded. Too much time is spent covering a myriad of specific, often isolated, pieces of content with the result that the larger picture is lost. It is clearly important to have a curriculum that facilitates, or at the very least enables, students to progress in their learning as best they can. Studies on studentsâ progression in learning (whether in mathematics, in science or more generally) have often been interpreted as though learning progresses up a ladder or in stages, so that each rung of the ladder (or stage) needs to be reached before subsequent progression can occur. Unsurprisingly, fine-grained observations of studentsâ learning, such as those by Shapiro (1994), reveal that learning is rarely like this. Not only do learners sometimes regress, they also at times miss a stage (or rung on the ladder). The implication for curriculum developers is that concepts need to be ordered in a logical sequence that facilitates learning but it should not be assumed that learning proceeds inflexibly along such a route. Learning can be more like putting together the pieces of a jigsaw, where this can be done successfully in a number of ways rather than in one predetermined order. It is generally agreed in curricula round the world that mathematics and science should be core subjects. Given this, there are a number of important considerations as to which subjects should be taught in the European Schoolsâ curriculum. Parents and students will invariably bring their own understandings about curriculum planning to any discussion of a reform process. This means that if parents hold traditional views about subjects within a curriculum, for example, that there needs to be three separate sciences (i.e. physics, chemistry and biology), then it follows that, as far as they are concerned, a general science curriculum is going to appear incomprehensible or, in their view, represent a simplification and thus reduction in the quality of this important area of the curriculum. It doesnât matter whether parents are correct in their judgements about the subject make-up of the curriculum, their beliefs are significant factors in any decisions made by European school curriculum-makers, and need to be taken into account accord-"
342,30,0.983,Semiotics in Mathematics Education,"To account for the process that leads from a collective form of behavior to an intra-psychological function Vygotsky introduced the concept of internalization. He wrote: âWe call the internal reconstruction of an external operation internalizationâ (Vygotsky 1978, p. 56). To illustrate the idea of internalization Vygotsky (1978) provided the example of pointing gestures: A good example of this process may be found in the development of pointing. Initially, this gesture is nothing more than an unsuccessful attempt to grasp something, a movement aimed at a certain object which designates forthcoming activity. The child attempts to grasp an object placed beyond his reach; his hands, stretched toward that object, remain poised in the air. His ï¬ngers make grasping movements. At this initial stage pointing is represented by the childâs movement, which seems to be pointing to an objectâthat and nothing more. When the mother comes to the childâs aid and realizes his movement indicates something, the situation changes fundamentally. Pointing becomes a gesture for others. The childâs unsuccessful attempt engenders a reaction not from the object he seeks but from another person. Consequently, the primary meaning of that unsuccessful grasping movement is established by others. Only later, when the child can link his unsuccessful grasping movement to the objective situation as a whole, does he begin to understand this movement as pointing. (p. 56)"
228,263,0.983,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"7.2.5 Applications Initially, OFNs were designed to deal with optimization problems when data are fuzzy [14, 15, 17, 20]. When Kacprzak and KosinÌski observed that a subspace of OFNs, called SOFN, may be equipped with a lattice structure, it turned out that OFNs have a much wider field of application. The ability to define Boolean operations such as conjunction, disjunction, and, more important, diverse types of implications, has become the basis for creating a new logical system. In consequence, it turned out that SOFNs can be used not only for evaluation of linguistic statements such as, âA patient is fatâ or âA car is fast,â but also for approximate reasoning on such imprecise notions. One of the important applications is employing SOFNs in multiagent systems for modeling agentsâ beliefs about fuzzy expressions [27]. This can be helpful in"
311,995,0.983,The Physics of the B Factories,"does not wholly eliminate the correlations. Systematic uncertainties that are unique to the Dalitz Plot are: the asymmetries in the background; limited statistics from the sidebands used to form the continuum histograms (if histograms are used); the mass rejection regions; diï¬erences in the continuum shape between the sideband and the signal region; and charge bias introduced either by the detector response or the selection criteria. A model dependent error derived from performing fits with an alternative set of resonances is sometimes quoted either in quadrature with the systematic error or its own. As with quasi-twobody modes, an important systematic is associated with uncertainty on the parameters that are fixed in the fit. If a resonance is deemed to be significant, the mass and width may still not be well known. Rather than float the mass and width, a series of fits can be performed with the mass and width fixed at diï¬erent values and the change in the likelihood used as a guide to the best values. Even so, it may be necessary to modify a model after unblinding, particularly to remove resonances that are not significant. 17.4.7 Three-body and Dalitz decays Approximately seven B 0 and eleven B Â± Dalitz Plots have been investigated by BABAR and Belle. It is impossible to do justice to the wealth of information available. Decays involving three pions, particularly B â ÏÏ, are important for the measurement of Ï2 and are considered in Chapter 17.6. Decays with an Î·, Î· â² , Ï, f0 (980), or K â in the final three-body state are itemized in the tables and figures of this section but are not described in detail. Instead, this section concentrates on modes with one or more kaons in the final state. B meson decays to three-body final states B â Khh proceed predominantly via b â u tree-level diagrams (T and C diagrams in Fig. 17.4.1) and b â s(d) penguin diagrams (P in Fig. 17.4.1). The other diagrams can contribute but are expected to be much smaller. Final states with an odd number of kaons (s-quarks) are expected to proceed dominantly via b â s penguin transitions as the b â u transition is color-suppressed. If there are two kaons, the decay proceeds through the color-allowed b â u tree diagram and the b â d penguin decay with no b â s penguin contribution. As a result, these Dalitz decays provide an excellent opportunity to understand the relative contribution of tree and penguin amplitudes in charmless decays. This is shown in Fig. 17.4.18 where the extracted values of sin 2Ï1 in b â s penguin transitions are compared to b â ccs decays. Table 17.4.10 summarizes the reported branching fractions and asymmetries. In many cases, no resonances have been found in a Dalitz Plot and so consequently it has only been possible to give a branching fraction (or upper limit) and a CP asymmetry for the whole Dalitz Plot. Figure 17.4.19 shows the relative values of the reported branching fractions so far measured."
249,58,0.983,Advances in Proof-Theoretic Semantics (Volume 43.0),"Abstract The goal of this paper is to consider the prospects for developing a consistent variant of the Theory of Constructions originally proposed by Georg Kreisel and Nicolas Goodman in light of two developments which have been traditionally associated with the theoryâi.e. Kreiselâs second clause interpretation of the intuitionistic connectives, and an antinomy about constructive provability sometimes referred to as the Kreisel-Goodman paradox. After discussing the formulation of the theory itself, we then discuss how it can be used to formalize the BHK interpretation in light of concerns about the impredicativity of intuitionistic implication and Kreiselâs proposed amendments to overcome this. We next reconstruct Goodmanâs presentation of a paradox pertaining to a ânaiveâ variant of the theory and discuss the influence this had on its subsequent reception. We conclude by considering various means of responding to this result. Contrary to the received view that the second clause interpretation itself contributes to the paradox, we argue that the inconsistency arises in virtue of an interaction between reflection and internalization principles similar to those employed in Artemovâs Logic of Proofs. Keywords BHK interpretation Â· Intuitionistic logic Â· Theory of Constructions Â· the Kreisel-Goodman paradox Â· Logic of Proofs"
337,1,0.983,Understanding Society and Natural Resources : Forging New Strands of Integration Across the Social Sciences,"This book covers a wide range of subjects which have enormous relevance to the interface between human society and the use and conservation of natural resources. This is a theme on which perhaps much more could have been done by researchers and academics, but possibly the integration of various disciplines, particularly through those dealing with the physical sciences and researchers involved in the social sciences, does not take place with adequate facility in most parts of the world. This volume is clearly an important contribution to the literature with a proper blending of different disciplines that would help us understand the interface between human society and natural resources in an integral manner. The very first pages beginning with the introduction set out the case for transdisciplinarity. This theme is then dealt with elaborately in subsequent chapters in a manner that would appeal to all the disciplines represented in the chapters of the book. I would hope that this effort can also be replicated through integration of disciplines dealing with the subject of climate change. As was logical, the initial work of scientists dealing with climate change focused largely on the biophysical and geophysical aspects of this problem. This, of course, was essential because it was important for society to understand what really was happening with changes in the physical system given that emissions of greenhouse gases have been increasing, and as a result the concentration of these gases going up significantly since industrialization. It was also essential to understand the physical nature of impacts of climate change, such as those involving the entire water cycle and how it would be affected as a result, as well as to assess the physical impacts of climate change in the form of extreme events and disasters. The Intergovernmental Panel on Climate Change (IPCC) brought out a special report in 2011 on Managing the Risks of Extreme Events and Disasters to Advance Climate Change Adaptation. This provided an in-depth assessment not only of extreme events and how their frequency and intensity would change as a result of climate change but also various human dimensions of the problem. One of the observations that was brought out in the report stated that between 1970 and 2008, 95 % of the fatalities that took place around the world as a result of all kinds of disasters occurred in the developing countries. There was also an elaboration of several other implications for human society from increase in"
118,582,0.983,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"17.2.5 Risk Analysis and Public Confidence, by Naomi Kaida, the University of Tokyo In this summer school, lecturers and students proposed various arguments. In this essay, however, I would like to focus on two points: one is an answer to the question posed by Professor Kastenberg, and the other is an extension of the discussion among the students. The construction of this essay is as follows. Firstly, a response to the question is proposed. The question is about improvement of risk analysis and avoiding loss of public confidence. Secondly, further thoughts about the discussion are suggested. The main point of the argument is the relationship between social decision-making and nuclear engineers. One of the students said that it was society that would make a decision about whether to stop using nuclear power, and he would obey the social determination as an engineer. However, this essay suggests that the social/technical dichotomy is meaningless. Finally, an integrated idea of the whole is demonstrated: to construct or reconstruct public confidence, arguments in more detail among nuclear engineers are needed. Professor Kastenberg posed some interesting questions, and one of them is, âWhat would it take to improve the quality of risk analysis and emergency planning"
84,103,0.983,Eye Tracking Methodology,"fication of neural regions which respond to these features. How certain features are perceived, particularly within and beyond the fovea, is the topic covered in the next chapter. For an excellent review of physiological optics and visual perception in general, see Hendee and Wells (1997). For an introduction to neuroscience, see Hubel (1988) very readable text. For a more recent description of the brain with an emphasis on color vision, see Zeki (1993). Apart from these texts on vision, several âhandbooksâ have also been assembled describing current knowledge of the brain. Arbib (1995) handbook is one such example. It is an excellent source summarizing current knowledge of the brain, although it is somewhat difficult to read and to navigate through.1 Another such well-organized but rather large text is Gazzaniga (2000)."
235,232,0.983,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","Pointedly stated, if some theory is the double of some physical system (or vice versa [21]) we have to differentiate between properties of the theory and properties of the physical system. And we have to make sure that we do not over-represent physical facts by formalisms which contain elements which have no correspondence to the former. Because if we are not careful enough we fall pray of Jaynesâ Mind Projection Fallacy mentioned in Sect. 9.5 (p. 42). Another issue is the applicability of mathematical models or methods which somehow implicity or explicitly rely on infinities. For instance, Cantorâs diagonalization technique which is often used to prove the undenumerability of the real unit interval relies on an infinite process [79] which is nonoperational. Again the issue of supertasks mentioned earlier arises. It may not be totally unjustified to consider the question of whether or not theoretical physics should allow for such infinities unsettled. The issue has been raised already by Eleatic philosophy [253, 331, 440], and may be with us forever."
28,248,0.983,A History of Self-Harm in Britain,"kind of external, social gratification.72 Explicitly then, this dissertation constitutes an argument against the socially embedded self-harm analysed throughout this book. It is influenced by a number of North American studies on self-cutting that promote internal, psychological, emotional needs as the roots of self-cutting, especially tension release â studies that continue to influence current models of self-harm. He refers many times to North American studies by Pao (1969), Crabtree (1967), Graff and Mallin (1967) and Grunebaum and Klerman (1969). Indeed, he also mentions Offerâs and Barglowâs sociologically influenced study of 1960 which, he admits, focuses more upon the role of imitation in epidemics of self-cutting. He concedes that psychoanalytic authors such as Pao, Crabtree and Graff and Mallin do not give the role of imitation muchconsideration.73 Imitation implies a social field, and the idea that the point (underdeveloped in the American psychoanalytic studies) shows how a division is opening up between the internal and external ideas of causation. This division has the potential to separate any selfcutting that might present at A&E departments from the overwhelming mass of socially embedded and understood self-poisoners with whom they are combined in the 1970s analyses of those such as Hugh Gethin Morgan. However, Waldenberg does not make this split according to method. He calls the group âcuttersâ but argues that some of these patients can distinguish between the feelings that precede a frankly suicidal overdose and those that precede cutting and/or a less serious overdose.74 This equates cutting and trivial overdosing and implies that they are prompted by the same state of mind. Thus, the strong differentiation between cutting and overdosing does not seem to stem from here. However, the emphasis on cutting and on internal motivations â explicitly against sociological or epidemic ones â is highly significant. Self-cutting is cast as internally rather than externally motivated, but this internal motivation is also ascribed to trivial overdoses. Ping-Nie Paoâs study of âdelicate cuttersâ from Chestnut Lodge, Maryland, is praised for the clarity of its descriptions, especially the patientsâ subjective experience of cutting. Waldenberg quotes Paoâs account, which uses the words tense, tension, and tenseness in a single sentence. As well as this internal emotional state, Waldenberg does acknowledge the social setting, mentioning interruptions in interpersonal relationships as possible factors that might precipitate cutting.75 His literature review is ambivalent about the internal/external divide. He writes that others have noted the relationship between an episode of cutting and interpersonal disturbances, such as the end of visits; othersâ works might start with a view of cutting as a purely internally focused"
275,70,0.983,Foundations of Trusted Autonomy,"2.5.1 Optimality and Exploration What is the optimal behaviour for an agent in any unknown environment? The AIXI formula is a natural answer, as it specifies which action generates the highest expected return with respect to a distribution M that learns any computable environment in a strong sense (Theorem 1). The question of optimality is substantially more delicate than this however, as illustrated by the common dilemma of when to explore and when to instead exploit knowledge gathered so far. Consider, for example, the question of whether to try a new restaurant in town. Trying means risking a bad evening, spending valued dollars"
13,426,0.983,Feeling Gender : a Generational and Psychosocial Approach,"she and her mother are of the same gender, and is more abrupt for the boy as he is of a different gender. Femininity will be constructed in the generational dimension: the girl is little, the mother is big, but they are of the same kind. This gives the girlâs gender identity a safe ground, and her subjectivity becomes more clearly relational in its character and with good capabilities for intimacy and empathy. However, the development of autonomy and establishing psychological borders between herself and others may become restrained. For the boy, on the other hand, separation takes place in the dimension of gender, which implies a more dramatic relational cut-off from his primary identificatory object. This may give him a better capacity for autonomy, but constrains his relational capacity. Chodorow summarises the gender identity development in a way that emphasises the advantages for the girl and the problems for the boy: âgrowing girls come to define and experience themselves as continuous with others; their experience of self contains more flexibility or permeable ego boundaries. Boys come to define themselves as more separate and distinct, with greater sense of rigid ego boundaries and differentiation. The basic feminine sense of self is connected to the world, the basic masculine sense of self is separateâ (Chodorow 1978: 169). The other element of the gender identity model is the sociological framing of the separation-individuation process in the post-Second World War family arrangement (white, middle-class) family, where the mother is the primary carer for the child and the father is a more distant figure. For the boy, the establishing of a masculine identity becomes more precarious when he does not have a model at hand to show him what masculinity implies. He does not know exactly what a man is; he only knows that a man is not a woman. Thus, masculine identity becomes abstract and negatively defined, and based on a repudiation of femininity. He will fear and denigrate everything connected with the feminineâ closeness, weakness, careâand deny its existence in himself. Care may be received as long as it takes the form of service and does not turn him into a baby. For the girl the problem is rather that the closeness she has with her mother also makes her vulnerable to the motherâs psychological conflicts, which again stem from the motherâs own restricted agency. This means that the already ambivalent relation between mothers and daughtersâwhere the girl both wants to stay close and have freedomâ"
346,40,0.983,"History Education and Conflict Transformation : Social Psychological Theories, History Teaching and Reconciliation","aim of history teaching according to kello and wagner (Chap. 8). The situation is more problematic in Belarus where an authoritarian administration is clearly using history teaching for political purposes in a very centralized way. The result of such pressures for the actual teaching practice is that teachers trying to balance a romantic and enlightened way of history teaching end up making use of communicative styles that Moscovici described as âpropagationâ, a communicative style which is a middle road between propaganda and diffusion (Moscovici 1961/2008). Indeed the denial of citizenship rights to a significant number of inhabitants of the Baltic states of Russian origin should not come as a surprise given the link between essentialist representations of the past, ethnic identity and exclusionary notions of citizenship (kadianaki and Andreouli 2015; kadianaki et al. 2016). The fact that the collective action paradigm is premised on predetermined roles of oppressor and oppressed, majorityâminority, perpetrator and victim can be also challenged in that groups historically can pass from both roles and thus it is rather unlikely that there will ever be a clear case of a group being constantly in the same position. This problem is very clear in Cyprus, for example, not only because at different times in history both groups were oppressed and oppressors, victims and perpetrators, minorities and majorities but also because Greek Cypriots can always claim that they are the victims of a huge country like Turkey and the Turkish Cypriots at the same time claim that they are the victims of the 80% of the population (Greek Cypriots) in Cyprus. So in fact there is an interaction of social representations of the Cyprus issue with representations of the past (the main tension being whether it is a problem of intercommunal conflict vs a problem of violation of international law by Turkey which invaded Cyprus) which is not very far from the spirit of competitive victimhood already discussed for its pernicious effects."
360,375,0.983,Compositionality and Concepts in Linguistics and Psychology,"PiÃ±ango and Deo (2015) propose that aspectual verbs lexically select for complements that are structured individuals, rather than events. This proposal is motivated by the goal of offering a uniï¬ed analysis for both âcoercion conï¬gurationâ sentences like John began the book and sentences like The chapter on global warming began the book (similar examples were given (4)â(6), not part of the traditional coercion set). The intuition underpinning a structured individual is an ordinary entity that maps onto a one-dimensional directed path structure (one-dimensional DPS) along a range of dimensions. A one-dimensional directed path structure is deï¬ned, following Krifka (1998), as a totally ordered structure whose adjacent (non-overlapping) parts bear the precedence relation along some dimension (temporal, spatial, eventive, etc.). For instance, one may construe an"
393,325,0.983,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 2,"Figure 7-1. The basic simulation algorithm. The algorithm repeatedly removes the event with the smallest occurrence time from the event list, advances the simulation time to that occurrence time, and executes the corresponding event handler. This approach is called event driven, since the simulation time advancement is based on the occurrence time of the pending events. For the sake of completeness, let us mention that there is also another algorithm which can be used to generate the state trajectory, known as time stepped simulation. Here, the simulation time is advanced by small constant steps (increments). The simulation algorithm repeatedly advances the simulation time by one step and simulates all pending events which fall into the current step. This method has some drawbacks when used with discrete event systems, thus we will not consider it any further in this survey. As can be now seen, a simulation program computing a state trajectory consists of code which can be divided into two parts, as follows:"
124,240,0.983,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"8 Open Future Semantics and Supervaluations The reader may complain that open futures semantics for PL is nothing new. The existence of non-classical interpretations for classical propositional logic has been well-known since the invention of supervaluation semantics (van Fraassen 1969). Supervaluations may be used to show how a sentence of the form Av â A can be validated in three-valued scheme that allows the values of A and âA to be unsettled. So, supervaluations can already serve the role of providing for a logic of an open future. Although it is granted that ||PL|| and supervaluations have some strong points of similarity, there are crucial points of difference, and these argue for the superiority"
185,79,0.983,The Essence of Software Engineering,"2 Understanding Change Since change is connatural to software, it is important to understand where it comes from so that we can handle it properly. The foundational work by Zave and Jackson [37] on requirements engineering, which sheds the light on software and change, is briefly summarized in Sect. 2.1. This work leads to a useful distinction between evolution and adaptation, discussed in Sect. 2.2. We will also argue that adaptation can often be anticipated through careful design, and this can lead to development of self-adaptive software."
13,93,0.983,Feeling Gender : a Generational and Psychosocial Approach,"Reading Feelings A last methodological issue to be considered is how it is possible to read âfeelingsâ out of research interviews. This question is especially pertinent when the theoretical perspective on feelings draws on psychoanalytic theory, which is developed for and validated in a clinical setting. An in-depth study of emotional meanings is admittedly hard to accomplish in empirical research outside of clinical practice. This has been a target of much debate in the field of psychosocial studies, where some argue that it is not only a possible but also a vital resource for social researchers to make use of transference/countertransference processes in the reading of the informantsâ unconscious processes, while others have been critical to this (for the discussion on this, see, for instance, Walkerdine 1997; Hollway and Jefferson 2000; Bornat 2004; Frosh and Emerson 2005; Roseneil 2006; Frosh and Baraitser 2008; Hollway 2015; Woodward 2015). My position on this is that using psychoanalytic perspectives in social research does not imply that the researcher should relate to their informants as psychoanalysts to their patients. One issue to consider is that even though qualitative interviews as compared to surveys, experiments and tests may be seen as rich materials and âthick descriptionsâ, they are indeed very âmeagreâ, as Chodorow (1999) puts it, when compared to clinical data that stems from often many years of clinical encounters between the patient and the analyst. Another issue is that most social researchers lack proper training in handling and interpreting what goes on in the transference in the interview situation, so the danger of âwild analysisâ cannot be excluded. A third issue is that the analyses of interview transcripts are not (and in my opinion should not be) validated in an interpersonal space that includes the informant. What social researchers basically work with are texts, and texts are not persons (and definitely not when analysing interviews that are 25 years old). However, when psychosocial methods are implied in social research, it is not primarily the psychobiographical specificity of individual stories"
58,332,0.983,Enabling Things to Talk,"Description of Virtual Entities The Virtual Entity is the key concept of any IoT system as it models the Physical Entity or the Thing that is the real element of interest. As specified in the IoT IM, Virtual Entities have an identifier (ID), an entityType and a number of attributes that provide information about the entity or can be used for changing the state of the Virtual Entity, triggering an actuation on the modelled Physical Entity. The modelling of the entityType is of special importance. The entityType can be used to determine what attributes a Virtual Entity instance can have, defining its semantics. The entityType can be modelled based on a"
187,56,0.983,Managing The Complexity of Critical infrastructures : a Modelling and Simulation Approach (Volume 90.0),"very difï¬cult to collect such detailed data due to the reluctance of the operators to provide such sensitive data, and also because of the huge quantity of highly time-varying data that should be collected. In the middle, we have the networked based approaches, which, for some aspects, share some of the advantages and weaknesses of both previous classes. Indeed, their most simple formulation (that referred as âstructuralâ) is quite easy to set-up, since only the topological structure of the involved systems is required. Conversely, when there is the need to consider also the âfunctionalâ properties of the network, the complexity of the model grows fast and it becomes comparable with simulation-based approaches. The topological approach, in a scenario composed of two infrastructures, where there is a single predominant (e.g., physical) dependency mechanism, is able to provide more âobjectiveâ measurements rather than holistic models of comparable effort. Unfortunately, the extension to more complex scenarios is not straightforward and requires to collected huge quantity of resources. In the rest of the chapter we illustrate in more detail the ï¬rst two classes (which will be further analyzed in Chap. 6), while the other chapters of the book are dedicated to illustrate the different elements and aspects related with the simulation based approach. In [17] the authors have catalogued about 150 approaches using a six classes taxonomy where the methods are not split on the level of granularity of the data but on the type of information used in the six classes: â¢ Empirical approaches: The analysis of the dependencies is performed on the base of historical accidents or disaster data and past expert experiences. These methods allow to identify frequent and signiï¬cant failure patterns, to quantify (inter)dependency indicators and perform empirically-based risk analyses; â¢ Agent based approaches: These approaches follow a bottom-up method assuming that the complex behavior emerges from many individual and"
287,72,0.983,Theories in and of Mathematics Education,"distinguished, identifying and realizing, which can be initiated by tasks. Taking pattern or ï¬eld orientations into account allows for foreseeing what kinds of tasks the students might solve successfully. Initiating a learning activity does not only focus on the current task situation but requires also taking past learning experience and future goals into account. Tools, e.g., diagrams, do not belong to the kernel of the theory. Its kernel encompasses the concept of activity and how a learning activity can be shaped, initiated by tasks, and created by the learner with the help of the teacher. The teacherâs role is crucial. Referring to the example above, one intended application is concerned with the problem of which further tasks the teacher can choose in order to assist the students in building the concept of Â¾ to be represented by various shapes. The strength of this approach is its prescriptive nature for initiating learning activities, while diagrams may serve one kind of resource among others. While both approaches share the sensitivity towards acting, the core concepts (e.g., diagram) of the one theory lie more in the periphery of the other (e.g., as a resources for a learning activity). If we take a networking of theories view and coordinate the analyses by using the two theoretical views, the empirical situation presented may be investigated according to two complementary questions: (1) what and how can acting with diagrams express mathematical ideas and (2) how can a task with certain goals be designed to initiate basic actions, such as identifying and realizing in a speciï¬c stage of the course of instruction, that are built on prior knowledge and preparing future goals to achieve cultural knowledge. Thus, both approaches complement each other and may enrich each other to inform practice (see TME program): coming from the learning activity we may zoom into (see Jungwirth 2009 cited by Prediger et al. 2009, p. 1532) diagram use, and coming from diagram use we may zoom out (ibid., p. 1532) to embed the diagram use into the whole course of the learning activity. Open Access This chapter is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, duplication, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, a link is provided to the Creative Commons license and any changes made are indicated. The images or other third party material in this chapter are included in the workâs Creative Commons license, unless indicated otherwise in the credit line; if such material is not included in the workâs Creative Commons license and the respective action is not permitted by statutory regulation, users will need to obtain permission from the license holder to duplicate, adapt or reproduce the material."
233,68,0.983,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"as a character and (2) that all characters across all clades are of equal signiï¬cance or contribute equally to biodiversity. To make this more concrete, we would have to assume that there is a fact of the matter as to how many characters contribute to the evolution of human cognition and that the biodiversity represented by the evolution of human cognition is of the same magnitude as the evolution of an equivalent number of characters in some other clade(s) for some other purpose(s)."
253,702,0.983,"Autonomous Driving : Technical, Legal and Social Aspects","20.3.2 State Uncertainty The state uncertainty of a detected object is described, in accordance with Bayesâ theorem, by means of a probability density function which can be used to determine the most probable total and individual state and also, with a certain probability, possible variations from this. In the case of a multi-dimensional, normally distributed probability density function, the state uncertainty is completely represented by a covariance matrix. In estimating static variables, such as the vehicle dimensions, their state uncertainty can be reduced progressively by means of repeated measurements. The estimated value based on the available measurements converges with the true values, as long as there is no systematic sensor error, e.g. in the form of an offset. For the estimation of dynamic, time-changeable states such as the object position or the object speed, due to the movement of the object between the measuring times, there is no convergence with a true value. Therefore, for the evaluation of the quality of the state estimation, it is stipulated that the mean error is zero and the uncertainty as low as possible."
28,178,0.983,A History of Self-Harm in Britain,"Kesselâs âself-poisoningâ: similarities and modifications Kesselâs self-poisoning is different in three main ways from Stengel and Cook and Batchelor and Napier. The self-conscious nature of the appeal is the strongest and simplest notion of intent yet seen, and the archetypal behaviours and gender stereotypes are explicitly discussed. Further, Kesselâs self-poisoning is rooted in an amorphous category of distress. This emotional state is thought common to all self-poisoning episodes, through which point of view it becomes a distinct, coherent clinical object. Thus in all three ways, Kesselâs self-poisoning is more definitely, more precisely and more securely established: the intent is self-consciously to appeal; the stereotypes of young women and overdose are explicit; and interpersonal, present-centred stress and distress hold the object together at a deep conceptual level. However, much remains the same under this new term. Much of the intense scrutiny focuses upon the familiar issues of lethality and intent. These function as part of a debate between therapeutic regimes. Kessel and two social-work colleagues make it very clear in 1963 that physical danger to life and psychiatric pathology are to be assessed separately: [N]o simple relationship exists between the degree of danger to life and the seriousness of any psychological disorder present. Many people who have been deeply unconscious we allow to go home after physical recovery because they require only a minimum of psychiatric supervision afterwards; on the other hand, a sixth of the patients who had not risked their life at all needed admission to a psychiatric hospital, and many more needed extensive out-patient care.16 There is a complex relationship between danger to life and a psychological disorder. Although they concede that âon the whole ... the more [physically] âseriousâ cases are more likely to call for active psychological"
28,282,0.983,A History of Self-Harm in Britain,"ignoredâ.19 To him, it is clear (in 2011) that cells genes and neurons are at the forefront of conventional explanations. In a similar vein, sociologists Adler and Adler are clear about their desire to âdemedicaliseâ selfcutting, understanding it instead through sociological concepts such as deviance and social reinforcement.20 It should be noted that sociological and psychological explanations persist â based upon learning and peergroup influence, and yet remain based upon ideas of emotional regulation. This is not a simple dichotomous split. However, over the past decade there have been many efforts to understand self-harm through neurochemical and neurological frames of reference. Health communications scholar Warren Bareiss concludes that media narratives of self-injury consistently downplay possible social causes of self-injury in favour of a model that understands self-injury as a personal choice.21 This idea of an individualised, personal choice meshes well with neurochemical understandings, as well as with market-based ideology that is centred upon a rational, autonomous consumer. This is a complex and nuanced picture, where social â and sociological â explanations can co-exist with ideas of internal tension and can also feed into neurological explanations. There is no easy way to sum them up. However, we can be more certain about the shifts at the heart of this book: that the archetypes of self-damage from the 1930s to the 1980s have undergone radical transformations. This corresponds to local, mundane and administrative innovations, but also feed off and feed into much broader political constellations. It is to these that we now turn."
289,1616,0.983,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","Several session type works study exception handling [7,9,16,30]. However, to the best of our knowledge this is the first theoretical work to develop a formalism and typing discipline for the coordinator-based model of crash failure handling in practical asynchronous distributed systems. Structured interactional exceptions [7] study exception handling for binary sessions. The work extends session types with a try-catch construct and a throw instruction, allowing participants to raise runtime exceptions. Global escape [6] extends previous works on exception handling in binary session types to MPSTs. It supports nesting and sequencing of try-catch blocks with restrictions. Reduction rules for exception handling are of the form Î£ â¢ P â Î£ â² â¢ P â² , where Î£ is the exception environment. This central environment at the core of the semantics is updated synchronously and atomically. Furthermore, the reduction of a trycatch block to its continuation is done in a synchronous reduction step involving all participants in a block. Lastly this work can only handle exceptions, i.e., explicitly raised application-level failures. These do not affect communication channels [6], unlike participant crashes. Similarly, our previous work [13] only deals with exceptions. An interaction p â q : S â¨ F defines that p can send a message of type S to q. If F is not empty then instead of sending a message p can throw F . If a failure is thrown only participants that have casual dependencies to that failure are involved in the failure handling. No concurrent failures are allowed therefore all interactions which can raise failures are executed in a lock step fashion. As a consequence, the model can not be used to deal with crash failures."
297,427,0.983,The R Book,"into R as character strings, then convert them into dates and times using the strptime function to explain exactly what the elements of the character string mean (e.g. which are the days, which are the months, what are the separators, and so on; see p. 103 for an explanation of the formats supported)."
125,144,0.983,"Charismatic Christianity in Finland, Norway, and Sweden : Case Studies in Historical and Contemporary Developments","grace that had initially been so important for establishing charismatic authority itself seems to have become corrupted, causing not only the charismatic authority it had served to support to consequently crumble, but also the very status of prophecy itself. This is coherent with the prevailing interpretation of Weberian charisma being solely in the eye of the beholder(s), as quoted earlier. In the words of David Setley and Douglas Gautsch, charisma is âbased solely on the evaluation of the follower on a leaderâs traits, not on any absolutes or skills the leader actually hasâ (Setley and Gautsch 2015, 23â24). While this studyâs findings are consistent with that interpretation, they also show how abuse of a charisma-generating practice can turn against itself. Such an interpretation of transformations and the respondentsâ views on prophecy is also helpful for an understanding of how the congregation acquired a cautionary approach to prophecy. From this angle, it follows that transforming the foundations of authority was a necessary step in order to rebuild trust in the emerging leadership, as well as in wider fellowship within the church. So far, we have seen that these transformations entailed a process of rationalization in favor of which âthe purely personal character of leadership is eliminatedâ (Weber 1947, 364, italics added). The keyword here is purely; it is significant that the charismatic element of authority has not been replaced. Rather, it seems to have been toned down, while legalrational elements have been added as mechanisms for guarding against the abuse that may come with a purely charismatic leadership. The language of prophecy is conspicuously absent in illustrating the reconstructed charismatic element of authority, which could mean that charisma itself has been transformed. Its previous conditions for authenticity have been replaced by new ones. Such a revamp of conditions for authority also bears on questions of authenticity, detectable in relation to other gifts and manifestations, among them âfalling in the Spirit.â"
96,155,0.983,Networks in the Russian Market Economy,"shoved aside completely in the name of non-instrumental friendship (Boltanski 1990; Kharkhordin 2005, 2009). In the remaining text of this section, justification theory is first described in short and then, in the next section, applied to the Russian managersâ interview data. The basic idea of justification theory is that the normal, conventional course of action â for example, running a business â tends from time to time to drift into a dead end. Justification theory focuses on these âcritical momentsâ â crises, conflicts, and disputes â which force the disagreeing parties to argue and justify their actions by referring to âa common goodâ recognized and accepted by both parties.7 In order to settle the dispute, the parties have to establish a principle of equivalence, against which the arguments presented in the dispute can be evaluated, and the âworthâ (grandeur) of the disputants can be measured. Boltanski and ThÃ©venot describe six different orders of worth, each of them referring to a different principle defining the âworthâ, âsizeâ, or âgreatnessâ (grandeur) of the disputing parties. They distinguish six common worlds based on these principles and on the beings (persons or things) that inhabit these worlds.8 First, in the market world, the greatness (grandeur) of an actor is defined by wealth and ultimately measured by markets. The greatness of a physician in this world, for example, could be measured by her commercial success in medical business. Second, in the industrial world, to continue the example, the same physician may be valued â irrespective of her commercial success â by her efficiency and measured in concrete terms, for example, by the number of patients handled per day. Third, in the domestic world, the greatness of the physician is evaluated by her position in the system of mutual dependency. Valued or âworthyâ in this world is oneâs trusted family doctor who has been treating all the members of the family for years and with whom one can always jump the queue to get an appointment. Fourth, in the civic world, a doctor is evaluated by her willingness to treat all patients equally as citizens. Fifth, in the world of fame, a great person would be a well-known media figure (such as Dr. Phil), whereas in the inspired world such a figure would be a genius surgeon who is the only one able to conduct certain operations because of her unique, God-given artistic capabilities.9 In this book the focus is mostly on the domestic and market orders of worth and the tensions between them. It is important to note that economic relations are not to be identified with market worth since a firm, for example, may be analyzed as a âcompromising deviceâ between the market and industrial worth (ThÃ©venot 2001)."
335,399,0.983,"Open Source Systems : Towards Robust Practices 13Th Ifip Wg 2.13 international Conference, Oss 2017, Buenos Aires, Argentina, May 22-23, 2017, Proceedings","Expression (3) shows the rate at which developer i makes ministeps, multiplied by the probability that he changes the arc variable Xij , if he makes a ministep. Our Markov chain can be simulated by following the steps explained in [28]. 4.2.4 Markov Chain Monte Carlo (MCMC) Estimation The described statistical model for longitudinal analysis of open source software development communities is a complex model and cannot be exactly calculated, but it can be stochastically estimated. We can simulate the longitudinal evolution, and estimate the model based on the simulations. Then we can choose an estimated model that has a good fit to the network data. For details of the simulation and estimation procedures please refer to [28]. The desirable outcome for the estimation is the vector parameter Î²Ì for which the expected and the observed vectors are the same."
271,114,0.983,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"The city as community therefore plays a role with centrists if their dominant interest has a connection with it. If this is so, then this will be reflected in the communicative network. For Dirk, this last element is the music scene, including related websites and magazines for which he also writes. Our material shows that even multi-localists can feel attached to a particular city. Here we can detect clear parallels with what we have already"
281,625,0.983,Stochastics of Environmental and Financial Economics (Volume 138.0),"Markov processes for which the logarithm of the characteristic function of the process is affine with respect to the initial state. Affine processes on the canonical state space m Ã Rn have been investigated in [3, 5, 13, 14]. Based on the exponential-affine structure of the JCIR, we are able to compute its characteristic function explicitly. Moreover, this enables us to represent the distribution of the JCIR as the convolution of two distributions. The first distribution is known and coincides with the distribution of the CIR model. However, the second distribution is more complicated. We will give a sufficient condition such that the second distribution is singular at the point 0. In this way we derive a lower bound estimate of the transition densities of the JCIR. The other problem we consider in this paper is the exponential ergodicity of the JCIR. According to the main results of [10] (see also [12]), the JCIR has a unique invariant probability measure Ï , given that some integrability condition on the LÃ©vy measure of (Jt , t â¥ 0) is satisfied. Under some sharper assumptions we show in this paper that the convergence of the law of the JCIR process to its invariant probability measure under the total variation norm is exponentially fast, which is called the exponential ergodicity. Our method is the same as in [9], namely we show the existence of a Forster-Lyapunov function and then apply the general framework of [16â18] to get the exponential ergodicity. The remainder of this paper is organized as follows. In Sect. 2 we collect some key facts on the JCIR and in particular derive its characteristic function. In Sect. 3 we study the characteristic function of the JCIR and prove a lower bound of its transition densities. In Sect. 4 we show the existence of a Forster-Lyapunov function and the exponential ergodicity for the JCIR."
257,316,0.983,"Principles of Security and Trust : 7th International Conference, POST 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","Trace equivalence of two processes is a property that guarantees that an attacker observing the execution of either of the two processes cannot decide which one it is. Previous work [28] has shown how trace equivalence can be proved statically using a type system combined with a constraint checking procedure. The type system consists of typing rules of the form Î â¢ P â¼ Q â C, meaning that in an environment Î two processes P and Q are equivalent if the produced set of constraints C, encoding the attacker observables, is consistent. The typing environment Î is a mapping from nonces, keys, and variables to types. Nonces are assigned security labels with a confidentiality and an integrity component, e.g. HL for high confidentiality and low integrity. Key types are of the form keyl (T ) where l is the security label of the key and T is the type of the payload. Key types are crucial to convey typing information from one process to another one. Normally, we cannot make any assumptions about values received from the network â they might possibly originate from the attacker. If we however successfully decrypt a message using a secret symmetric key, we know that the result is of the keyâs payload type. This is enforced on the sender side, whenever outputting an encryption. A core assumption of virtually any efficient static analysis for equivalence is uniform execution, meaning that the two processes of interest always take the same branch in a branching statement. For instance, this means that all decryptions must always succeed or fail equally in the two processes. For this reason, previous work introduced a restriction to allow only encryption and decryption with keys whose equality could be statically proved."
8,375,0.983,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","toy model, but turns out to be the same in all (non-cutoff) realistic SBM cases [49, 51]. Moreover, it is independent of: â the number of space-time dimensions [54], â the number of âinput particlesâ (z becomes a sum over modified Hankel functions of input masses), â Abelian or non-Abelian symmetry constraints [55]. What is wanted is of course G.z/, given implicitly by Eq. (17.48). Solutions are reviewed in Chapter 25 [2]. Simplest is its graphic solution: we draw z.G/ according to Eq. (17.48) and exchange the axes (see Fig. 17.5). One sees immediately that (universally!)"
261,250,0.983,The Poetics and Politics of Alzheimerâs Disease Life-Writing,"paintings can be read as a window onto her increasing preoccupations and changing mind set. âSortirâ [Leaving] is composed of pastel, though still bright colours, and artistically witnesses Couturierâs decline, because its impressionistic style focuses less on detail. Likewise, her motif anticipates the advanced condition, given that â[l]andscapes present a simple and satisfying themeâ. In fact, since this painting depicts a stony road winding past a protruding panorama of trees in bloom, towards towering mountains, the onlooker perceives of Couturier as picturing her experience in a manner, reminiscent of McGowinâs presentation, as a journey towards the unknown, fully aware of her daily struggles and ambivalence.35 It is noteworthy in this context that the daughter Kim Howes Zabbia portrays her motherâs condition in expressionistic terms, relying on mute and deep colours.36 This choice may well suggest her inability to understand the motherâs state. Couturier, by comparison, demonstrates continued realistic agency mostly relying on impressionistic style. This discrepancy, again, highlights the difference in viewpoint and perception between caregiver and patient. Ultimately, Couturier and Bryden demonstrate that Alzheimerâs disease affects each individual in different ways, even releasing originality and creativity. They both embody the deeper meaning of Toledanoâs âriverâ, as they live out their identity beyond the diagnosis, unconcerned about the distance to its âmouthâ. Admittedly, both patients witness to their strong faith, but â in comparison to the believers Rose, Schneider and Davis â they do not devise eventually only partly enabling explanatory strategies in the ï¬rst place. Instead, they almost come to see their condition and its consequences as integral parts of their lives. Audacious as it may sound: in the same way as the Alzheimerâs condition imposes emotionality, passivity and dependence on its âvictimsâ, these characteristics are moved beyond their traditional classiï¬cation as only female. The female Alzheimerâs story becomes the synthesis of gender- and illness-related conceptualisation. And the female individual with Alzheimerâs disease, who demonstrates activity and independence, surmounts both the gender- and illness-related prejudice, to be perceived as doubly strong â exactly like the female caregiver who rises"
275,56,0.983,Foundations of Trusted Autonomy,"In the pictured example, P(1) = 2/3 would be a reasonable prediction since in two thirds of the cases where the context 0011 occurred before it was followed by a 1. (Laplaceâs rule gives a slightly different estimate.) Humans often make predictions this way. For example, when predicting whether I will like the food at a Vietnamese restaurant, I use my experience from previous visits to Vietnamese restaurants. One question that arises when doing induction with contexts is how long or specific the context should be. Should I use the experience from all Vietnamese restaurants I have ever been to, or only this particular Vietnamese restaurant? Using the latter, I may have very limited data (especially if I have never been to the restaurant before!) On the other hand, using too unspecific contexts is not useful either: Basing my prediction on all restaurants I have ever been to (and not only the Vietnamese), will probably be too unspecific. Table 2.2 summarises the tradeoff between short and long contexts, which is nicely solved by the CTW algorithm. The right choice of context length depends on a few different parameters. First, it depends on how much data is available. In the beginning of an agentâs lifetime, the history will be short, and mainly shorter contexts will have a chance to produce an adequate amount of data for prediction. Later in the agentâs life, the context can often be more specific, due to the greater amount of accumulated experience."
79,22,0.983,Cosmic Ray Neutron Sensing : Estimation of Agricultural Crop Biomass Water Equivalent,"determination of BWE. The main limitation of this form of sampling is its timeconsuming nature and therefore is limited to a few fields at a time. This works well for stationary CRNS locations where the BWE must be calculated for one singular field but becomes difficult when mobile versions of the CRNS technique are employed in which the BWE must be determined for many fields (see Franz et al. (2015) and Avery et al. (2016) for more details on the mobile aspects of this technology [4, 5])."
100,234,0.983,Migration in The Southern Balkans : From Ottoman Territory To Globalized Nation States,"light of this crisis, andâneedless to repeatâstigmatization by the host societies, the des-identification, that is, the effort to differentiate oneself from the rest of the (stigmatized) group, has been a highly practised option at least among immigrants in Greece. The absence of any physiognomic markers that would render their difference from the dominant society visible, together with their identity management, seems to have suggested the âsocial inconspicuousnessâ of the Albanian immigrants in Greece. Probably every Greek sought to distinguish her or his Albanian neighbours; but, outside this local scale of personal relations and close vicinity, the Albanians were collectively visible in very few cases. For Thessaloniki, the field of my study, the Albanians were distinctively perceptible as a different group only in the piazzas for job-seeking. Compared to their numeric weight in the cityâs population, however, this visibility can be considered negligible. As I have tried to demonstrate in this chapter, the Albaniansâ âsocial inconspicuousnessâ is reflected in the way in which their physical setting is contained within the Greek city. The remarkable absence of ethnic infrastructure and the dispersion of Albanian households throughout the city suggest the âspatial invisibilityâ of the most important immigrant population of Thessaloniki. As maintained in this chapter, despite their great numbers, Albanians do not reveal any visible trace of their ethnicity in the urban space. As such, apart from being socially inconspicuous, Albanians are also spatially âinvisibleâ. It is important to note, however, that both outside forces and the preferences of the migrant group may lead to residential dispersion. This latter is not a sign of weakening of the groupâs identity. Besides, it would be wrong to interpret the counter-practice of clustering as a refusal to integrate, while understanding the residential dispersion as an enthusiastic volition to inclusion. In its way, each option expresses a form of response appropriate to the circumstances created by the migration situation and from the specific resources available to each group of migrants in order to deal with the situation (Barou 2003, p. 263). In this respect, Albanian migrantsâ relative invisibility in Greece, and more particularly in Thessaloniki, reflects a specific adaptation strategy among other optionsâa strategy which is not, anyway, an option for all Albanians in Greece. Moreover, apart from representing the Albanian immigrantsâ adaptation to the conditions encountered in Greece, these strategies respond too to the process of adaptation of Albanians to the post-communist eraâmeaning the realities that emerged after the discovery of the outside world, namely Albaniaâs position in this latter and the questionings that have arisen on what it may mean to be Albanian. These strategies also make use of the cultural legacies of a more distant past. As such, I argue that the practices of inconspicuousness respond to a complex situation that draws on both the Greek context of reception and the Albanian background. Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
139,250,0.983,Programming for Computations - MATLAB/Octave (Volume 14.0),"hence qualitatively wrong, because one can prove that the exact solution of the differential equation is monotone. (However, there is a corresponding difference equation model, NnC1 D rNn .1 Nn =M /, which allows oscillatory solutions and those are observed in animal populations. The problem with large t is that it just leads to wrong mathematics â and two wrongs donât make a right in terms of a relevant model.)"
192,176,0.983,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"Gregg sees Oppenheimer/Bloch as an enigma who manages to exert a âpoisonous influenceâ (p. 165) on âhundreds of peopleâ (notably students and young researchers), and the investigation aims to reveal the secret of his magnetic personality. To achieve this, Gregg wants Ampter to build up a file. Modern wars, Gregg explains, call for complete mobilisation of manpower and resources; and this especially involves mobilisation of âbrain powerâ (p. 38). Modern wars are won or lost in laboratories, and this creates wholly new security problems. For while statesmen are trained in the game of international politics and the military in the game of war, âthese science fellows know nothing about either. And some of them have some pretty wild-eyed notionsâ. The first objective is to find out whether Sebastian is a communist. But the target is not a naÃ¯ve subject. From the very beginning, Sebastian was âconscious of another presence of whose attention he was the objectâ (p. 221). This dimension of the novel likewise reflects the dynamics of university discourse. The secret service agent is an expert, a researcher, specialised in accumulating and analysing files (with the help of recording equipment, typewriters, archives, mnemonic devices, etc.). The name Ampter is reminiscent of the German word Amt, originally spelled as Ambt. Etymologically speaking, the word basically means servant. Mark Ampter is the agent (a promising professional, yet replaceable, in principle) who is expected to spend all of his time on investigating Bloch, in order to discover the mysterious factor X somehow at work in him, by producing a huge pile of material about his target. Years of work result in an impressive file, the Bloch archive, âa stack of folders ten or more inches highâ (p. 157). As Gregg later explains, it is a unique procedure, âsomething of an experimentâ (p. 239). Via Mark Ampter (his instrument, his eyes and ears), Gregg aims to become connected with, to establish a window into the doings of Sebastian Bloch. The Chief from now on perceives everything Ampter says as a possible access to âthe mind of Sebastian Blochâ (p. 73). Gregg is basically a psychologist who has designed an N = 1 experiment, keeping his distance, working from a distance, using Ampter as his one-way screen: his âone direct line right into the heart of the enemy territoryâ (p. 159). Ampter is entrusted with the actual work, but Gregg reads his reports quite care-"
390,15,0.983,The Hidden Language of Computer Hardware and SW,"The lengths of time that the flashlight remains on and off arenât fixed. Theyâre all relative to the length of a dot, which depends on how fast the flashlight switch can be triggered and also how quickly a Morse code sender can remember the code for a particular letter. A fast senderâs dash might be the same length as a slow senderâs dot. This little problem could make reading a Morse code message tough, but after a letter or two, the person on the receiving end can usually figure out whatâs a dot and whatâs a dash. At first, the definition of Morse codeâand by definition I mean the correspondence of various sequences of dots and dashes to the letters of the alphabetâappears as random as the layout of a computer keyboard. On closer inspection, however, this is not entirely so. The simpler and shorter codes are assigned to the more frequently used letters of the alphabet, such as E and T. Scrabble players and Wheel of Fortune fans might notice this right away. The less common letters, such as Q and Z (which get you"
360,178,0.983,Compositionality and Concepts in Linguistics and Psychology,"1 Concepts and Prototypes In The Compositionality Papers, Fodor and Lepore (2002) return frequently to a âknock-downâ argument against the suggestion that concepts might be prototypes. Concepts, they argue, must be compositional. It must be possible, if one accepts the representational theory of mind, to explain how the meaning of a complex phrase is based solely on the meaning of the elements from which it is constructed, plus the syntactic structure into which they are placed. To account for our ability to understand the meaning of sentences such as (1) and (2) and countless other similar sentences the semantic system needs a set of ï¬xed symbols to represent the conceptual atoms in the sentences (John, Mary, Bill, loves, hates) which can then be inserted into suitable syntactically structured sentence frames to yield the appropriate meaning for the sentence as a whole."
375,46,0.983,Musical Haptics,"2.6 Conclusions In this chapter, we have placed particular focus on the idea that the passive dynamics of the body of a musician play an integral role in the process of making music through an instrument. Our thesis, namely that performer-instrument interaction is, in practice, a dynamic coupling between a mechanical system and a biomechanical instrumentalist, repositions the challenge of playing an instrument as a challenge of âplayingâ the coupled dynamics in which the body is already involved. The idea that an instrument becomes an extension of the playerâs body is quite concrete when the coupled dynamics of instrument and player are made explicit in a model. From"
360,111,0.983,Compositionality and Concepts in Linguistics and Psychology,"And so each particular situation uses the word in the sense appropriate exactly to such a situation, and not in its general sense. Objectivists complain that the Subjectivists are so enthralled with the particularity of an experience that they have blinded themselves to the fact that language is full of generality in the description and reporting of that experience and refuse to acknowledge that this is precisely what happens on particular occasions. Indeed, they say, thatâs the whole point of general terms. You use X to say something, then it is what X is true of that counts. . . not some hidden (or even public) idiosyncratic interpretation of X that you might have. If I say that I met someone on the train yesterday, an Objectivist would claim that it is just silly to think that I âreallyâ meant that I met a woman, just because I know that it is true that the person is a woman! Objectivists think the Subjectivist theory is completely bewitched by the view that what is being communicated is just exactly what one is aware of and what oneâs par-"
299,416,0.983,Happiness Is The Wrong Metric : a Liberal Communitarian Response To Populism,"recommendation is based on a human-made algorithm that calculates the shortest route, or that which will take the least amount of time to travel, or some other such criteria. A significant amount of autonomy occurs when the machine is given a large number of guidelines, some that conflict with each other, and is ordered to draw on information it acquires as it proceeds, to draw conclusions on its ownâsuch as a more advanced GPS system, which identifies upcoming traffic, or an accident, and reroutes accordingly. Machines equipped with AI are held to be able to act much more autonomously than those not so equipped. Monica Rozenfield (2016) writes: Deep learning is a relatively new form of artificial intelligence that gives an old technologyâa neural networkâa twist made possible by big data, supercomputing, and advanced algorithms. Data lines possessed by each neuron of the network communicate with one another. It would be impossible to write code for an unlimited number of situations. And without correct code, a machine would not know what to do. With deep learning, however, the system is able to figure things out on its own. The technique lets the network form neural relationships most relevant to each new situation."
32,509,0.983,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","22.4 Binary Scattering Study In this section, we give a simple explanation to understand the mechanism that underlies the characteristic ordering behavior shown in previous section. Let us limit our discussion only to the binary particle collision process [15]. Here we assume the system is dilute enough ( ! 0) so that only the uncorrelated, binary collisions take place, and both the velocity and the polarity are fully relaxed before each collision. If the damping is weak, the polarities of two particles remain unchanged, so the directions of motion are temporally changed by the collision but eventually restored to the original direction. Here the relative angle between the velocities does not change before and after the collision. By contrast, if the damping is strong, the polarities rotates themselves quickly to align to the directions of motion, so the particles moves as if they have exchanged their momentum. Here again the absolute"
69,355,0.983,"Fundamental Approaches to Software Engineering : 21st International Conference, FASE 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","In this section, we describe the algorithms for the analysis of partial components, which we have implemented on top of LTSA [25]. Checking Realizability. Realizability of a property Ï is checked via the following procedure. Let E be the environment of the partial component C, and C B be the LTS resulting from removing all black-box states and their incoming and outgoing transitions from C. Check C B E |= Ï. If Ï is not satisfied, the component is not realizable: no matter how the black-box states are specified, there will be a behavior of the system that does not satisfy Ï. Otherwise, compute C E (as specified in Definition 1) and model-check it against Â¬Ï. If the property Â¬Ï is satisfied, the component is not realizable. Indeed, all the behaviors of C E satisfy Â¬Ï, i.e., there is no behavior that the component can exhibit to satisfy Ï. Otherwise, the component may be realizable. For example, the realizability checker shows that it is possible to realize a component refining the one shown in Fig. 3c while satisfying property P2. Specifically, it returns a trace that ensures that after a userReq event, the offer is provided to the user (the event offerRcvd ) only if the furniture service has confirmed the availability of the requested product (the event inforRcvd ). Theorem 2. Given a component specified using an IPLTS C, its environment E, and a property of interest Ï, the realizability checker returns ânot realizableâ if there is no component C â² obtained from C by integrating sub-components, s.t. (C â² E) |= Ï. Checking Well-Formedness. Given a partial component C with a black-box state b annotated with a pre-condition pre(b) and its environment E, the wellformedness checks whether pre(b) is satisfied in C as follows. (1) Transform post-conditions into LTSs. Transform every FLTL post-condition post(bi ) of every black-box state bi of C, including b, into an FLTL formula post(bi )â² as specified in [13]. This transformation ensures that the infinite traces that satisfy post(bi )â² have the form Ï, {end}Ï , where Ï satisfies post(bi ). For each black-box state bi , the corresponding post-condition"
249,204,0.983,Advances in Proof-Theoretic Semantics (Volume 43.0),"categorical, i.e., the concept of morphisms with their domains being a terminal object (or monoidal unit in the case of substructural logic). Some authors have discussed how logical constants can be derived from logical consequence relations (see, e.g., WesterstÃ¥hl [25]). To that end, category theory allows us to derive logical constants from abstract proof-theoretic consequence relations (i.e., categories) through the concept of adjunctions, which even give us inferential rules for the derived logical constants, and hence a proof system as a whole (in the case of intuitionistic logic, for example, the proof system thus obtained is indeed equivalent to standard ones, such as NJ and LJ). Given a category (abstract prooftheoretic consequence relation), we can always mine logical constants (if any) in the category via the generic criteria of adjunctions. In such a way, category-theoretical logic elucidates a generic link between logical constants and logical consequence, without focusing on a particular system of logic. In the following, let us review the concept of adjoint functors in the simple case of preorders, which is basically enough for us, apart from occasional exceptions. A preorder (L , â¢ L ) consists of a set P with a reflexive and transitive relation â¢ L on L. Especially, the deductive relations of most logical systems form preorders; note that reflexivity and transitivity amount to identity and cut in logical terms. It is well known that a preorder can be seen as a category in which the number of morphisms between fixed two objects in it are at most one. Then, a functor F : L â L â² between preorders L and L â² is just a monotone map: i.e., Ï â¢ L Ï implies F(Ï) â¢ L â² F(Ï). Now, a functor F : L â Lâ²"
192,324,0.983,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"For indeed: the mouseâs body is actually a window into the human body, and its biochemistry can be extrapolated in principle to human biochemistry. The biochemical letters or elements (ÏÏÎ¿Î¹ÏÎµá¿Î±): the noumenal, symbolical essence of all mammal bodies is basically the same. The mouseâs body is a kind of elementary textbook or manual containing the elements of human biochemistry. These symbols, these letters and numbers, suddenly seem to speak out to him, and to speak for themselves. Meanwhile, a second line of research is opened up. While Cliff himself focusses on this virus and his mice, Robin decides to change her perspective and to secretly monitor Cliff. Instead of his viruses, she decides to study his practices. Cliff the scientific subject becomes her âcaseâ."
346,266,0.983,"History Education and Conflict Transformation : Social Psychological Theories, History Teaching and Reconciliation","This statement sets the stage for this chapter. wilschut points both to the dynamic nature of the contexts of history teaching and to the fact that the particular constellation of the factors is to some extent open to interpretations. The constellation can change quite quickly even in the same country, and history educators even in the same time and space can perceive the relative weights of contextual factors quite differently (see kello 2016). Educators face a multiplicity of understandings and expectations from the different fields, and often they need to navigate between contradictory understandings and expectations. There is a continuous discussion and dialogue between the fields over aims, contents and functions of history teaching (wilschut 2010). In lay and political representations, serving national identity and patriotism is still perceived as the main function of history teaching in many countries. However, since its beginnings, the school subject has always served what Carretero and Bermudez (2012) call âenlightenedâ approach aiming at the more general education of the students. The compatibility of the âpatrioticâ and âenlightenedâ tasks depends on how the latter are understood. âEducating studentsâ, if conceived as transmitting information without much reflection, need not interfere with the patriotic aims. In contrast, âcritical enlightenedâ history teaching demands recognition of divergent experiences and perspectives, critical (self-)reflection and contesting celebratory myths and narratives (Carretero and Bermudez 2012). The present chapter is set on the backdrop of such variety of understandings and expectations of history as a school subject. we take a look at different positions that history teachers take towards their subject and its contexts using material from in-depth interviews with Estonian and Latvian history teachers. Viewing the history classroom as a communicative space, we discuss how the three styles of communicationâdiffusion,"
192,307,0.983,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"To some extent the novel can be said to individualise the problem, addressing plagiarism in the form of a case history, but the systemic ambiance is addressed as well. It is in the contemporary academic arena that individuals are spurred into selfexhaustion, and Perlmannâs crisis can be seen as symptomatic of transformations within the scientific production system as such. In other words, Mercierâs novel amounts to a diagnostics: not only of individual deviance, but also of the current academic crisis. At the same time it is clear that, as an academic individual, the protagonist dramatically fails to live up to the challenge of re-establishing himself as author within a certain discursive constellation, although in principle he could have done so, for instance by actively taking up the role of steward of an absent voice. Perlmannâs position is captured by a term already discussed in Chap. 4, namely kenosis (ÎºÎ­Î½ÏÏÎ¹Ï,13 i.e. âemptyingâ) in the sense that he suffers (like Sebastian Bloch) from an experience of emptiness, reflecting a profound crisis of academic authorship (perhaps even of authorship as such). But dialectically speaking, precisely such an experience of crisis and self-contradiction (M2) is valuable, because it may give rise to new discursive practices of the Self, to a shift in discursive position, for instance in the direction of productive collaboration (consciously giving the floor to the voice of the absent Other and acknowledging this otherâs priority) as an alternative scenario for plagiarism (i.e. appropriating the voice and picking the brain of the other as a misguided strategy to fill the gap, as misconduct). Science novels contribute to the research integrity debate neither by condoning nor by denouncing plagiarism (or other forms of misconduct) on the basis of established but perhaps questionable or outdated conventions (S2), but rather by forcing us to reconsider some basic conceptions and challenges of academic authorship from multiple (epistemological, political and normative) perspectives. Via this oblique detour we may explore feasible scenarios that may help us to address (as individuals and as research communities) the current crisis of academic authorship ($), perhaps resulting in the establishment of a new plateau of normativity (âM3)."
311,2114,0.983,The Physics of the B Factories,"obtained from the unbinned likelihood fits are listed in Table 19.1.27 with statistical and systematic uncertainties. Only systematic uncertainties associated with the signal and background p.d.f.s are included in the systematic uncertainty for the yields. The curves representing the fits are overlaid in the figures. The most significant signal is + â seen in the distribution for Î+ c â pÎ¼ Î¼ ; the signal yield has a statistical-only significance of 2.6Ï as determined from the change in log-likelihood with respect to zero assumed signal events. With 35 diï¬erent measurements, a 2.6Ï deviation is expected with about 25% probability. 19.1.10 Summary of charmed meson decays Charm decays open the road to investigate the flavor physics of up-type quarks, which is complementary to the weak interactions of the (bottom and strange) down type quarks. Since the B decays are dominated by the b â c transitions, and the e+ eâ â cc cross section is comparatively high, the B factories also generated plenty of charm which allowed detailed measurements with highly competitive precision. FCNC processes of up-type quarks are predicted to be heavily GIM suppressed, which motivated measurements and searches of rare and even forbidden decays of charm at the B factories. Although for many FCNC processes it is diï¬cult to make a precise theoretical prediction, the B factories added a lot of new information on these decays, constraining significantly the limits on physics beyond the Aside from the weak interactions also many studies of QCD related issues have been performed. The charm quark is neither heavy enough to be cleanly treated within"
208,101,0.983,Actors and the Art of Performance,"âLanguage belongs in its origin to the age of the most rudimentary form of psychology: we find ourselves in the midst of a rude fetishism when we call to mind the basic presuppositions of the metaphysics of language â which is to say, of reason. It is this which sees everywhere deed and doer; this which believes in will as cause in general; this which believes in the âegoâ, in the ego as being, in the ego as substance, and which projects its belief in the ego-substance on to all things [ ... ].â Friedrich Nietzsche, Twilight of the Idols, trans. R. J. Hollingdale (London: Penguin 1990), 48. Italics in the original."
192,260,0.983,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"with the first person singular? Who is this âweâ? From a Lacanian-Bachtinian perspective, it is clear that the âweâ functions as a grammatical operationalisation of S2: the replaceable, un-subjective, decidedly impersonal subject of science. But it also covers up the exploitation and expropriation of the servant by the Master (âWe, the Master, did thisâ). And Leahâs therapeutic intervention proves effective, for from now on, Stafford begins to pay attention to Cantorâs use of the term âweâ (or âourâ), which suddenly may give way to âIâ or âmineâ. For instance, he now notices that Cantor informs him that the Krauss team is having troubles repeating âyourâ experiment, and that there may be something the matter with âyourâ notebooks, while he consistently speaks about âourâ Nature article. In the latter case, there is âno ambiguity about ourâ (p. 89). In other words, Cantor uses the âweâ in such a way that he may safeguard his intellectual property rights, while attributing any experimental flaws to his assistant. From now on, Stafford begins to pay attention to (and even count) Cantorâs uses of the signifier âweâ (p. 83). Indeed, the use of the signifier âweâ proves highly symptomatic and, from the point of view of critical discourse analysis, a fascinating object of research. In response to Leahâs intervention, Celestineâs supervisor makes a telling confession. At a certain point in her career, she decided to change her name from Yardley to Ardley, in view of the importance of alphabetic order in the listing of author names: Let me confess something to you â¦ but promise not to spread it around â¦ When I was a senior at Brown [University] â and a very ambitious one, almost unpleasantly so â I paid very much attention to where my name would ultimately appearâ¦ To my fatherâs shock, I announced one day that I would change my name from Jean Yardly to Jean Ardleyâ¦ Yes. I went to the courthouse and did it legally. Itâs best to be first, itâs been true since prehistoric times (p. 51)."
337,232,0.983,Understanding Society and Natural Resources : Forging New Strands of Integration Across the Social Sciences,"Although the Bloomington School is often criticized for the general absence of power in related studies, Ostrom (2005) offers a clear and concise definition of power in her seminal work on the IAD framework, Understanding Institutional Diversity. According to Ostrom: the âpowerâ of an individual in a situation is the value of the opportunity (the range in the outcomes afforded by the situation) times the extent of control. Thus, an individual can have a small degree of power, even though the individual has absolute control if the amount of opportunity in a situation is small. The amount of power may also be small when the opportunity is large, but the individual has only a small degree of control. (2005, p. 50)"
271,403,0.983,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"already been made from their own perspective. By telling a slightly different version of the story from their own perspective, actors seem to be able to integrate the proposed decision into their own, differing, background assumptions. In addition, the multipleâ although revisedârepetitions of the suggested solution seem to add substance to the idea of a joint solution that the actors as a group can adopt as their decision. Through their non-verbal behaviour, participants signalize the growing coherence of the group, their increasing approval of the story that is told again and again in slight variations. â¢ Practices of self-authorization. Practices of self-authorization are sequences in which participants reassure each other of their authority as decision-makers. They define and reinforce their position to legitimately make the actual decision at hand. These practices serve to establish, or re-establish, the self-image of the participants as possessing authority and responsibility in the respective case/issue. These are only four examples of practices of relatedness that we identified in an analysis of actual committee meetings (Nullmeier and Pritzlaff 2009; Pritzlaff and Nullmeier 2011). All of these practices, however, have one thing in common: they rest on a complex, triadic structure of interaction (Lindemann 2006a, 2006b, 2010, 2012) that includes verbal and non-verbal forms of communication and requiresâto adapt Essâs above-cited termâa context in which participants can âreadâ each other. In order to do this, referring back to Berger and Luckmann, the availability of a âmaximum of symptomsâ is of crucial importance to the participants. Triadic interaction sequences are characterized by relational dynamics that differ from dyadic types of interaction. And although these sequences in which a speaker âis defining his or her relationship with two other people simultaneouslyâ (Heatherington and Friedlander 2015: 109) have been analyzed mainly in the area of family therapy, it is important to point out that it is âequally beneficial to capture these types of relational dynamics in other interactional settings involving three or more participantsâ (Escudero and Rogers 2015: 33). If one wants to analyze these kinds of relational dynamics within figurations of political decision-making, it is extremely important to move beyond the analysis of language use and to include non-verbal, bodily elements of interaction."
84,544,0.983,Eye Tracking Methodology,"Using string editing similarity measures, Privitera and Stark evaluated six different attentional algorithms, some sharing similarities with the above models of Wolfe, Osberger and Maeder, and Itti et al.. Although the algorithms tested were not the actual ones proposed by the latter group of authors, it appears that a multiresolutional strategy, such as that of Itti et al., seems to be very efficient for several classes of images. In general, although the set of algorithms picked by Privitera and Stark was only a small representative sample of many possible procedures, this set could indeed predict eye fixations. The problem of computationally modeling human eye movements, and indeed human visual search, is far from being solved. No current model of visual search is as yet complete. Recent progress in algorithmic sophistication is encouraging. Models such as those of Wolfe, Osberger and Maeder, and Itti et al., show definite promise. Certainly the capability of tracking human eye movements has yet to play a crucial role in the corroboration of any model of visual search. Carmi and Itti (2006) recently evaluated the saliency of dynamic visual cues, noting that these cues play a dominant causal role in attracting attention. Peters and Itti (2006) suggest heuristics sensitive to dynamic events as predictors of human attention in next-generation immersive virtual environments and games. Privitera and Starkâs approach to the comparison of human and algorithmic scanpaths is one of the first methods to appear to quantitatively measure not only the loci of ROIs but also the order of ROIs. Undoubtedly future evaluation of human and artificial scanpaths will play a critical role in the investigation of visual search."
77,276,0.983,A Life Course Perspective on Health Trajectories and Transitions (Volume 4.0),"Multivariate Extensions By definition, a statistical model for longitudinal data must involve multiple variables, in that the outcome of interest must be observed at least twice (at times i D 1 and i D 2). Despite this technicality, it is common to call a longitudinal model applied to a single outcome univariate, denoting that a single Y outcome is analyzed. It follows that a multivariate longitudinal model is one that analyzes at least two outcomes (Y1 and Y2 ). In its simplest specification, a multivariate LMEM supposes an intercept and a slope growth factor for each outcome, which covary freely (MacCallum et al. 1997). This allows assessing the degree of communality of multiple change processes. Note that the functional form of growth need not be the same across the outcomes. In the previous illustration it would be possible to study change in both self-rated health and satisfaction with personal relationships. A multivariate LMEM might specify an intercept and a slope for both outcomes and estimate all six covariances among the four growth factors. The LCM can also be easily adapted to multiple outcomes. Here, the association between the growth factors is not limited to symmetrical effects (i.e., covariances), but may be freely specified by the analyst. For instance, it is possible to let the growth factors of an earlier process predict those of a later process (rather than simply covary with them; Singer and Willett 2003). Such a relation cannot be tested within a multivariate LMEM. In the previous illustration we might want to assess"
28,278,0.983,A History of Self-Harm in Britain,"small way, when patient testimony is used and deployed by psychiatrists as evidence. This is especially useful when patients confound expectations (as in Watsonâs 1970 study), or requires significant intellectual work to make it fit (as in Waldenberg, 1972). However, this is principally a study of specific hospital practices, a certain set of psychiatric ideas about the social setting, and how these might resonate with a wider political context: a shift from a welfare-based, socially interventionist consensus to one of individuated, market-oriented competition. Roy Porter champions ideas of the âpatientâs voiceâ as central to the history of medicine, but this is not my principal area of interest.9 I am far more concerned with how ideas and research practices interact and produce the concepts and shorthand that humans use to understand themselves and others. Basing this book on the experience of the patient would make it a very different project. In addition, Joan Scott writes persuasively: When experience is taken as the origin of knowledge, the vision of the individual subject (the person who had the experience or the historian who recounts it) becomes the bedrock of evidence on which explanation is built. Questions about the constructed nature of experience, about how subjects are constituted as different in the first place, about how oneâs vision is structured â about language (or discourse) and history â are left aside.10 I am most interested in how âvision is structuredâ, in how ideas and practices come to influence what is possible and explicable behaviour, and how these change. This is not to demean patients or their stories, experiences or identities, but to say that this history attempts something different. The patients and their experiences recede in this telling, as do the psychiatrists to an extent. What is left are practices, arrangements, ideas, concepts â all the things that recur in psychiatric journal articles and government documents. This, like all history, must resemble its sources, but remains useful â hopefully to people other than myself â because it enables new connections to be made around self-harm, society, psychology and politics. It might make the various individuals involved in the story less visible (in terms of their experiences), or flatten them out to their research contributions, but it also allows new links: between categories of identity and the rise of professional groups; between broad political contexts and clinical categories; between an intellectual climate in psychology and psychiatry and the ways in which we understand selfdamaging behaviour; between politics and the ways in which people understand themselves and their identities."
297,420,0.983,The R Book,"Suppose that you read a ï¬le from data, then attach it: murder <- read.table(""c:\\temp\\murders.txt"",header=T,as.is=""region"") The following warning will be produced if your attach function causes a duplication of one or more names: attach(murder) The following object(s) are masked _by_ .GlobalEnv: murder The reason in the present case is that we have created a dataframe called murder and attached a variable from this dataframe which is also called murder. This shows the cause of the problem: the dataframe name and the third variable name are identical: head(murder) state population murder region Alabama 15.1 South Alaska Arizona Arkansas 10.1 South 5 California 21198 Colorado This ambiguity might cause difï¬culties later. A much better plan is to give the dataframe a unique name (like murders, for instance). We use the attach function so that the variables inside a dataframe can be accessed directly by name. Technically, this means that the dataframe is attached to the R search path, so that the dataframe is searched by R when evaluating a variable. We could tabulate the numbers of murders by region, for instance: table(region) region North.Central"
249,99,0.983,Advances in Proof-Theoretic Semantics (Volume 43.0),"Finally, we observe that it follows that the derivability of â¤ â¡ â¥ from no premises in T + entails that all equations are derivable from no premises in this system. But this is precisely how inconsistency is traditionally defined for systems based on the lambda calculus."
95,179,0.983,Elements of Robotics,5.9.2 Gyroscopes A gyroscope (âgyroâ) uses the principle of Coriolis force to measure angular velocity. This concept is explained in textbooks on physics and we will not go into it here. There are many types of gyros:
78,362,0.983,The Onlife Manifesto : Being Human in a Hyperconnected Era,"1959, p. 157). In her view, the public is loaded with more ontological dignity than the private, because it is where freedom can be experienced. The private, as still visible in the etymology, meant originally to be deprived from being among equals. Nowadays, privacy is hardly understood as âbeing deprivedâ from anything! On the contrary: freedom is more on the side of the private, and the rule of law on the side of the public. Property is associated with wealth and accumulation, while property and wealth used to be only the pre-condition for engaging in the public realm. Action has been substituted by behaviours, or by fabrication. In tyranny, as in mass society, âmen have become entirely private, that is, they have been deprived of seeing and hearing others, of being seen and being heard by themâ (Arendt 1959, p. 53). Freedom has changed sides: privacy is now perceived as one way to protect freedom, while publicity is more perceived as the realm of constraints (rule of law, accountability, transparency, justification, surveillance, etcâ¦), than as the realm of enjoying plurality and freedom. It is interesting to note that Arendt attributes the dissolution of the public/private distinction and the profound change of their meaning to political modernity. In her terms, the invasion of the social in the public realm, in the form of the nation state, which can be seen as a huge household, joined up with the ancestral âgreat temptation, for men of action no less than for men of thought, to find a substitute for action in the hope that the realm of human affairs may escape the haphazardness and moral irresponsibility inherent in a plurality of agentsâ (Arendt 1959, p. 197). The Arendtian axiomatic reset is not about going back to the Greek polis: pushing labour and work out of the public sphere, and concentrating politics on action only is not a credible option in the twenty-first century. However, the Arendtian tripartition of the vita activa in labour, work and action remains inspirational as it reminds us that labour (and necessity) and work (and causality) cannot account for the totality of the human experience: action (and plurality and freedom) has to have a place! For Arendt, the meaning of politics is freedom. If indeed, omnipotence and omniscience were a possibility, there would be no room for politics, as politics is precisely the place where we experience the noblest part of the human condition, i.e. plurality and natality. In the vita activa of the twenty-first century, the labour-work-action tripartition should not be seen hierarchically, i.e., with action, the public and the agora on the top and labour, the private and the home on the bottom: instead, labour, work and action form a trio generating a 3D-space. Failing to recognize action as a third dimension ends up in a degenerative perception of the human condition and a flattening of the human experience. Arendt mapped the private/public distinction with idealized representations of the home and the agora as they were supposed to be in Greek Antiquity. There and then, the private was the household, the place where women and slaves took care of the necessities of life, while the public was the space where men, freed from the necessities of life, could experience freedom, among equals. It is obvious that the public/private distinction does not correspond anymore to the distinction agora/home. It is my view that the public/private distinction can most usefully be redescribed in the twenty-first century by indexing it primarily on the freedom/necessity polarity, and"
8,733,0.983,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Such a type of equation has only exponential solutions. Actually, the arguments are much more subtle and the equation for .m/ is not as simple as Eq. (26.28), but the conclusion remains the same:  is of the exponential type .m/ D g.m/em=T0 ;"
124,158,0.983,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"definitions. So it is assumed here that what an agent is obliged or is forbidden does not depend on what it is doing or on what others are doing. Before discussing the moral character of the prohibitions and obligations defined, let us look at some example formulas. We discuss these normative formulas by interpreting them in the model of Fig. 4, which adds violation constants to the earlier discussed model of Fig. 3. Our first example formula, which is satisfied in dynamic state s1 , h 3 â in Fig. 4 is For b[Ag1 xstitâ¥0.4 ]Ï â [Ag1 xstitâ¥0.4 ]Ï â X Â¬Ï. The formula expresses that agent Ag1 is forbidden to choose in such a way that it believes to have a risk of at least 0.4 to obtain Ï, while at the same time agent Ag1 is actually doing such an action, but, where it is lucky since what is actually happening is that Â¬Ï is obtained (state s6 in the model). The second example formula is very closely related. It is also satisfied in dynamic state s1 , h 3 â of Fig. 4. The formula is For b[Ag1 xatt]Ï â [Ag1 xatt]Ï â X Â¬Ï and it expresses that agent Ag1 is forbidden to attempt Ï, while at the same time that is actually what it is doing, but, where it is lucky since what is actually happening is that Â¬Ï is obtained (again, state s6 in the model). So, for both example formulas state s6 is a state of luck. The condition Ï the agent according to the normative part of the formula is supposed to avoid, is indeed not true in it, but that is not due to the determination of the agent, but due to the lucky coincidence that the other agent took the top row as its choice. The agent is lucky in both example formula situations; the first formula specifies how the agent deliberately took a significant risk for Ï and the second formula specifies how the agent even explicitly attempted Ï. So definitely, the state s6 is a state of luck. But, it"
48,56,0.983,Time Predictions : Understanding and Avoiding Unrealism in Project Planning and Everyday Life (Volume 5.0),"for most real-world tasks, are meaningless values that are impossible to derive from experience or historical data. As an alternative to the current approaches, such as the PERT model, we developed a new three-point prediction tool (a spreadsheet model) that provides predictions of mean outcomes based on user-determined confidence levels.9 We believe that the tool has several important features that make it different from and perhaps better than other approaches. First, it forces the person making the prediction to look back and use historical information. Neglecting historical information may be one of the main reasons for poor time usage predictions in many domains [18]. Second, the tool does not require the prediction of extreme outcomes, such p1 or p95 predictions. Third, it does not require a particular meaning of the time prediction used as reference (median, most likely, p85, etc.), as long as the meaning is the same used for previous time predictions. The steps to calculate the mean time prediction are as follows: 1. Predict the time usage. The prediction, the reference, may be a prediction of the most likely use of time or any other type of time prediction. 2. Assess the accuracy of similar past predictions. Select two prediction accuracy points for which you have historical information or can make a qualified judgement. Each accuracy point should include (a) the prediction error (the deviation of actual outcomes from the prediction) and (b) the frequency of occurrence. For example, you may know that, for about seven out of 10 (70% occurrence) previously completed tasks similar to the one being predicted, you spent less than 130% of the predicted time. This means that your p70 prediction is 130%. You need one more such accuracy point. The second assessment could, for example, be that you spent less than 90% of the predicted time in three out of 10 cases (30% occurrence), meaning that your p30 prediction is 90%. 3. Input the accuracy points into the spreadsheet, which calculates the uncertainty distribution, the pX values and the mean value. Example: Assume that you have predicted the most likely time usage to be 30 minutes. You know from similar situations that, in about 90% of the cases, the actual time usage was less than twice (200%) your predicted time usage and that, in about 50% of the cases, the actual time usage was less than 130% of your predicted time usage. This means that you have a p90 prediction of 200% the original prediction and a p50 prediction of 130% the original prediction. Using the spreadsheet, assuming a lognormal distribution of time usage,10 yields a mean time prediction of 41 minutes. The time usage distribution is displayed in Fig. 3.5, showing, for example, that the most likely value is around 33 minutes. The pX distribution is displayed in Fig. 3.6, showing, for instance, that the p95 prediction is a bit less than 70 minutes. For more details on this method for making realistic pX predictions, including more examples, see Chap. 6."
385,689,0.983,Advanced R,"To understand Râs performance, it helps to think about R as both a language and as an implementation of that language. The R-language is abstract: it deï¬nes what R code means and how it should work. The implementation is concrete: it reads R code and computes a result. The most popular implementation is the one from r-project.org (http://rproject.org). Iâll call that implementation GNU-R to distinguish it from R-language, and from the other implementations Iâll discuss later in the chapter. The distinction between R-language and GNU-R is a bit murky because the R-language is not formally deï¬ned. While there is the R language deï¬nition (http://cran.r-project.org/doc/manuals/R-lang.html), it is informal and incomplete. The R-language is mostly deï¬ned in terms of how GNU-R works. This is in contrast to other languages, like C++ (http://isocpp.org/std/the-standard) and javascript (http: //www.ecma-international.org/publications/standards/Ecma-262.htm), that make a clear distinction between language and implementation by laying out formal speciï¬cations that describe in minute detail how every aspect of the language should work. Nevertheless, the distinction between R-language and GNU-R is still useful: poor performance due to the language is hard to ï¬x without breaking existing code; ï¬xing poor performance due to the implementation is easier. In Section 16.3, I discuss some of the ways in which the design of the Rlanguage imposes fundamental constraints on Râs speed. In Section 16.4, I discuss why GNU-R is currently far from the theoretical maximum, and why improvements in performance happen so slowly. While itâs hard to know exactly how much faster a better implementation could be, a >10x improvement in speed seems achievable. In Section 16.5, I discuss some of the promising new implementations of R, and describe one important technique they use to make R code run faster. Beyond performance limitations due to design and implementation, it has to be said that a lot of R code is slow simply because itâs poorly written. Few R users have any formal training in programming or software development. Fewer still write R code for a living. Most people use R to understand data: itâs more important to get an answer quickly than to develop a system that will work in a wide variety of situations. This means that itâs relatively easy to make most R code much faster, as weâll see in the following chapters."
238,239,0.983,Nanoinformatics,"Fig. 7.2 Example of how the deï¬nition of a cluster or atomic scale feature is largely dependent on user selection of parameters. In this case, based on a difference in nearest neighbor distances, two very different clusters are deï¬ned. This is a signiï¬cant problem in APT, where this same issue can result in two totally different microstructural characterizations. For example, multiple boundaries of a precipitate can be reasonably deï¬ned. Through the use of topological methods, we propose to address this issue and deï¬ne a bias-free approach to reconstruction and data analysis in APT and, therefore, provide believable and sufï¬ciently robust results not provided with geometry-based approaches. Reproduced from Ref. [22] with permission from The Royal Society of Chemistry"
58,222,0.983,Enabling Things to Talk,"(âA brain is part of a studentâ -> composition), whereas the lifetime of objects of class B is independent from class A (âA student has a schoolâ). Finally, an âopen arrowâ is used to denote a âone-wayâ association. The notation shown in Fig. 7.5 indicates that every object in class A is associated with zero or more objects in class B, and that every object in class B is associated with exactly one object in class A. However more importantly, this notation indicates that a class A object will âknowâ class B objects with which it is associated, and that a class B object will ânot knowâ the class A object with which it is associated, ref. Sensor and Physical Entity in Fig. 7.7. The cardinalities (âasteriskâ, â1â, etc.) are to be read as follows: from the source read the relation and the cardinality on the target gives the multiplicity with which the source can be in that relation with the target. For the inverse relation, the cardinality at the source is relevant. For example (see Fig. 7.7), a Tag identifies no or one (0..1) Physical Entity â whereas a Physical Entity may be identified by 0 or more Tags. A Virtual Entity may contain 0 or more other Virtual Entities, whereas a Virtual Entity can optionally be contained in at most one other Virtual Entity. Concepts depicting hardware are shown in blue, software in green, animate beings in yellow, and concepts that fit into either multiple or no categories in brown."
103,400,0.983,Solar Particle Radiation Storms Forecasting and Analysis : The Hesperia Horizon 2020 Project and Beyond (Volume 444.0),"10.4.1 Inversion of Spacecraft Data to the Sun Numerical simulations of the propagation of SEPs along the IMF are a useful tool to understand SEP events and their sources. We currently have a good theoretical understanding of the transport processes that affect SEPs in the interplanetary medium (see Chap. 4). In Sect. 10.3.1 we discussed a model that simulates the processes undergone by SEPs during their propagation from their source to the observer with the critical parameters summarized in the gray box on page 186. The approach introduced by Agueda et al. (2008) is to utilize the computed response of the âsystemâ to an impulsive (delta) injection at the Sun, i.e. the Greenâs function of particle transport. A convolution of some delta injections allows us to compute different pitch angle dependent proton intensity time profiles that are used as input to the second step in the chain (see Sect. 10.3.2). Given a system impulse response, g.t/, and the input injection profile q.t/, the output, I.t/, is the convolution of g.t/ and q.t/: I.t/ D"
259,20,0.983,The Little Book of Semaphores,"Because the two threads run concurrently, the order of execution depends on the scheduler. During any given run of this program, the output might be âyes noâ or âno yesâ. Non-determinism is one of the things that makes concurrent programs hard to debug. A program might work correctly 1000 times in a row, and then crash on the 1001st run, depending on the particular decisions of the scheduler. These kinds of bugs are almost impossible to find by testing; they can only be avoided by careful programming."
8,402,0.983,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","was not evident to the reader of the SBM paper. SBM is presented in this book both conceptually and with mathematical detail as relating to hot nuclear matter in Chap. 23, and the relation to phase transition to quark matter is further developed in Chap. 27. A historical SBM perspective that discusses the role of distinguishable particles is offered in Chap. 17. The unpublished âDistinguishable Particlesâ paper, motivating SBM, is published here for the first time in the following Chap. 19."
78,139,0.983,The Onlife Manifesto : Being Human in a Hyperconnected Era,"memory (Benjamin 2006), which had been introduced by Marcel Proust and Henri Bergson (Bergson 1926) to characterize a type of remembering that is both contemplative and unconscious, and that contrasts with an intellectual and active access that is implemented in the voluntary memory. According to Benjamin, works of art are received and valued on different planes that stand between two polar opposites; on the one, the accent is mainly put on the cult value, that is associated with the contemplation, which requires concentration; on the second, the accent is put on the exhibition value of works of art that are designed to distract the mass of spectators and that no longer demand them to be absorbed. With mechanical reproduction, the cult value of works of art that requires concentration and efforts tend to decline while the exhibition value, which distracts the mass, becomes more and more prominent. As a consequence, the aura, which is attached to the cult value and to traditions, vanishes. This loss of the aura is not only negative. It has aesthetic consequences. New forms of art that no longer refer to traditions and that eliminate cult value are emerging among which one can note Baudelaireâs poetry, Cubism or Dadaism. But it has also less positive consequences that led political regimesâespecially, the twentieth century totalitarian regimesâto use new media and works of art for their propaganda. Lastly, it has economical consequences that lead works of art to focus only on the exhibition value."
143,572,0.983,Digital Kenya : An Entrepreneurial Revolution in The Making,"true for Kenyans. Not only is the language of techpreneurs with all its buzzwords completely different from that of other sectors, but also what is considered to be âhipâ and âcoolâ differs. The devil is in the details!â and admittedly, learning the details is difficult and time-consuming. A training program for both Kenyans and foreigners, however, could facilitate a controlled exposure to the culture of oneâs counterpartâa unique opportunity to learn how the game is played. Not everyone picks up these peculiarities right from the start. Rather than going through a painstaking six-month period in which everyone only scratches the surface, this solution can be a unique opportunity to learn the meaning of and reasoning behind each othersâ behaviors and terms. As always, there are also downsides to this mindset. The Innovator is met with opposition, resistance, and a small peer group to work with. Counterintuitive and inherently new solutions face an uphill battle until they become recognized and fully accepted. The bearers of this mindset will therefore find only few supporters who fully understand and support the solution. So being able to take a long breath when working through potential failures and to bear with comments such as âI told you it ainât worth it!â are assets."
208,43,0.983,Actors and the Art of Performance,"âWhoâs to say that the passion for the literal can be controlled? That gaping and scarring will not break through to the real at any given moment.â14 Was it this imposition of felicitous acting that made the actor break into tears and stop, that provoked a stubborn ânoâ to her former desire to become an actor? To find yourself beside yourself. Childâs play, incidental. As if youâd always been there. Not artificially forced and without any hysterics. No exaltedness, no fake theatrical aftertaste. No crutches of specious talent. None of that deceptive, mostly self-serving, affectedness. Let out of the cave of habitual perception into the surplus of play. By chance. As if by accident. In one instant pushed to the margins, the seams. An unnamed in-between. Between the lines, between the cracks, between the borders. Traveling in an imaginary Charonâs boat?15 Jean-Luc Nancy says in Corpus, The a-part-self as departure is whatâs exposed. âExpositionâ doesnât mean that intimacy is extracted from its withdrawal, and carried outside, put on display. âExposition,â on the contrary, means that expression itself is an intimacy and a withdrawal. The a-apart-self is [ ... ] this vertiginous withdrawal of the self from the self that is needed to open the infinity of that withdrawal all the way up to self. The body is this departure of self to self.16"
305,91,0.983,Quantum Computing for Everyone,"When the first person measures her or his qubit, the second personâs qubit immediately jumps to one of two states. These states depend on the result of the first personâs measurement. This is quite unlike our everyday experience. Later we will see clever ways of exploiting entangled qubits, but first we consider superluminal communication. Superluminal Communication Superluminal communication is communication faster than the speed of light. Two apparently contradictory inferences seem to be able to be deduced concerning this. The first is that Einsteinâs special theory of relativity tells us that as you travel faster, approaching the speed of light, time slows down. If you could travel at the speed of light, time stops. And if you"
289,514,0.983,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","Independence of premise (IP) is a semi-classical principle from first-order logic whose CIC equivalent can be stated as follows. Î (A : ) (B : N â ). (Â¬A â Î£n : N. B n) â Î£n : N. Â¬A â B n (IP) Although not derivable in intuitionistic logic, it is an admissible rule of HA. The standard proof of this property is to go through Kreiselâs modified realizability interpretation of HA [4]. In a nutshell, the interpretation goes as follows: by induction over a formula A, define a simple type Ï (A) of realizers of A together with a realizability predicate Â·  A over Ï (A). Then show that whenever â¢HA A, there exists some simply-typed term t : Ï (A) s.t. t  A. As the interpretation also implies that there is no t s.t. t  â¥, this gives a sound model of HA, which contains more than the latter. Most notably, there is for instance a term ip s.t. ip  (Â¬A â ân. B) â ân. Â¬A â B for any A, B. Intriguingly, the computational content of ip did not seem to receive a fair treatment in the literature. To the best of our knowledge, it has never been explicitly stated that IP was realizable because of the following âbugâ of Kreiselâs modified realizability. Lemma 35 (Kreiselâs bug). For every formula A, Ï (A) is inhabited. In particular, Ï (â¥) := unit. We show that this is actually not a bug, but a hidden feature of Kreiselâs modified realizability, which secretly allows to encode exceptions in the realizers. To this end, we implement IP in TEp by relying internally on paraproofs, i.e. terms raising exceptions, while ensuring these exceptions never escape outside of the locally unsafe boundary. The resulting TEp term has essentially the same computational content as its Kreiselâs realizability counterpart. In this section we suppose that E := unit, although assuming E to be inhabited is sufficient. To ease the understanding of the definition, we rely on effectful combinators that can be defined in TE ."
378,50,0.983,The Great Mindshift : How a New Economic Paradigm and Sustainability Transformations Go Hand in Hand,"1. Pre-development, in which a system is in a dynamic state of equilibrium and changes slowly but unobtrusively. 2. Take-off or point of ignition during which more coordinated niche activity and regime reactions gain momentum. These may lead to the tipping points followed by anâ¦ 3. Acceleration or navigation phase in which structural changes become possible and visible but hard to control. Eventually, at the stage ofâ¦ 4. Stabilization, a new dynamic system setting emerges. This can be a transformed system in which the overall development trajectory is different, because it is informed by many niche elements and ideas. However, this can also be an adapted version of the old dynamic in which most of the challenges have been absorbed or subjugated by the old regime structures, so that some aspects are amended but the general development trajectory stays the same. In a third alternative, the system can collapse when it falls out of the order imposed by the former dynamic, should restabilizing feedbacks and activities be insufï¬cient (Grin et al. 2010: 3â7). The phase pattern was originally observed in natural systems but proves very insightful for social changes as well. It is part of all the reports reviewed in this chapter. Figure 2.2 provides an example illustration drawn up for a climate change ï¬nancing project by myself and my colleagues at the Wuppertal Institute"
124,502,0.983,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"With our basic theory, we have already achieved some similarity to Belnapâs BST. This becomes evident by contrasting the present TPAcarve to the TPAknit of Strobachâs paper. While the latter starts out with given possibilities, each with its own relation of ancestry, which have to be made to match each other by certain requirements on those relations to enable the next step of knitting them together, the former needs no corresponding requirements because possibilities are carved out of a single pre-existing object, the structure of possibilia. However, we yet have no natural one-to-one correspondence between the possibilities of TPAcarve and the possibilities of TPAknit . This is so because our basic theory leaves out many of the requirements, especially of cardinality and connectedness, which are implemented in TPAknit (which it in turn had inherited from the non-modal formal theory of ancestry, FTA). So, although we already have captured a few basic metaphysical intuitions, there is still some way to go for our TPAcarve to model the biological realm in a satisfactory way."
285,716,0.983,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","Despite more than a century of study, there is no consensus regarding the basic nature of pitch, causing a palpable level of frustration among hearing researchers. The wealth of behavioral research on pitch perception contrasts with the paucity of physiological research into its neural basis beyond the level of the auditory nerve (AN). Processing in the central nervous system (CNS) is expected to be fundamentally different for the various temporal vs. spectral schemes that have been proposed, so physiological insights have the potential to reveal which (combination) of the two classes of schemes underlies human pitch perception. A robust but implicit code for pitch exists in the AN in the form of an across-fiber pooled interspike interval (ISI) distribution (Cariani and Delgutte 1996a, 1996b; Meddis and Hewitt 1991a, b), which resembles the stimulus autocorrelation. An unsolved question is how and whether this implicit temporal representation is transformed into a more explicit representation. Following an early model (Licklider 1951), various autocorrelation-type schemes have been proposed, in which periodicity-tuning is generated by some combination of coincidence detection and a source of delay. It is generally assumed that such a computation is implemented at a brainstem level, where responses are temporally precise over a broad range of frequencies. However, recordings have not revealed convincing evidence for levelinvariant, periodicity-tuned neurons or sources of delay that cover a sufficiently broad temporal range (Neuert et al. 2005; Sayles et al. 2013; Sayles and Winter 2008a, 2008b; Verhey and Winter 2006; Wang and Delgutte 2012). Here, simple properties of the early central auditory system are brought into focus and it is argued that the relevant representation is fundamentally different from autocorrelation-type schemes Ã  la Licklider (1951). The key proposal is that entrained phase-locking (contracted to âentrackingâ) generates a scalar rate code for pitch early in the brainstem."
385,174,0.983,Advanced R,"Note that when creating the function, you have to put the name in backticks because itâs a special name. This is just a syntactic sugar for an ordinary function call; as far as R is concerned there is no diï¬erence between these two expressions: ""new"" %+% "" string"" #> [1] ""new string"" `%+%`(""new"", "" string"") #> [1] ""new string"""
372,309,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"4.4 Visibility Frequencies As explained in Sect. 3.1, the phase of the complex visibility is measured with respect to that of a hypothetical point source at the phase reference position. The fringe-frequency variations do not appear in the visibility function, but slower"
93,111,0.983,Nordic Mediation Research,"The developmental work done in project Fasper was a collaborative effort to create new concepts for family mediation. In his analysis of the dynamics of concept formation, EngestrÃ¶m (2014) found two main directions: concept formation with the name in the lead and concept formation with the practice in the lead. In the ï¬rst case, there exists a name (a concept), but only a vague idea of what it represents. Collaborative concept formation is a search for contents for the name. In the latter case, concept formation moves the opposite order with the novel practice in the lead, but no name for it (EngestrÃ¶m 2014). In our previous study on the concept formation (Haavisto et al. 2016), we found that, in general, project Fasper was an example of concept formation with a name in the lead. The concept of family mediation was recognised and deï¬ned by the law, and in the project, new mental and material content for it was created. The ï¬rst question of the multi-professional and multi-organisational learning network was âwhat family mediation is about?â When the concept formation proceeded, the practical question was âhow family mediation is organised?â Later, when the models for family mediation process and service organisation were deï¬ned, there emerged a need to ask more speciï¬cally âhow and with what tools family mediation is conducted?â Proceeding with the name in the lead does not, however, mean that concept formation was only mental, i.e., producing textual deï¬nitions of the phenomena and abstract content for the concept. In fact, family mediation could not be conceptualised as suchâthat is making universal deï¬nitions at a deskâbut it"
275,494,0.983,Foundations of Trusted Autonomy,"The motivation and PSO components may potentially run in parallel, for example on different processors, or they may be interleaved on a single centralized processor. In either case, the motivation and PSO components do not have to step at the same rate. The PSO component simply works with the current version of the fitness function available. In Algorithm 2, the ratio of motivation to optimization is controlled by the parameter Z . It is further assumed that the environment is piecewise dynamic, that is, it changes slowly enough for the PSO component to converge on an optima before its location changes again. In this algorithm all agents are motivated by a single, shared, but dynamic fitness function. The approach in the next section incorporates different models of motivation into different agents to allow the agents to exhibit different characteristics in task selection."
186,194,0.983,Dignity in The 21St Century : Middle East and West,"constantly renewed. Therefore, the closed circuit of material$material production and consumption is accompanied by waste and dissipation (Koran 17:53; 6:121; 4:76). Since it is impossible for everyone to participate in the competition for mass consumption, poverty is constantly increasing, both in human communities and in the natural environment. Hence, we are observing the erosion of human communities, the great majority of them by poverty and a small minority by mass consumption. Certain methods for harmonising material and spiritual needs were recommended at a time when the communities on the Arabian Peninsula were living in poverty. The teachings noted that any consumption which negatively affects the natural balance of the body and deranges the mind is wastefully extravagant and fatal. It taught people that if they do not try to develop and actualise their talents, the energy used for these talents cannot stay unoccupied and hence will be used for domination and transgression (Koran 96:6). However, the question is: how do we know that we are on the path to development and not domination (and fozoon talabi or greed)? There are 14 ways to measure this. 1. Human development should be accompanied by the prosperity of nature. Hence, any development which is accompanied by environmental destruction should not be seen as development but an act of dominance and greed, which eventually will lead both humans and the environment to the valley of death. According to the Koran, the elimination of ethnic groups and cities resulted from their deviation from the path of development.9 There are two approaches to the relationship between human beings and their environment. One perceives nature as an active entity and humans as passive subordinates to nature, and the other sees humans as a dominant entity which has to conquer nature and ferociously exploit its resources for the wholesale consumption of its resources. How is it possible, one can ask, that the Koran provides us with the solution to encroaching environmental disaster, when it was written 1,400 years ago in the deserts of the Arabian Peninsula? How is it possible that such a warning should have come from anyone but God? 2. If a right creates a zero-sum relation in which one will reap beneï¬t at the expense of someone elseâs loss, then this cannot be seen as a right. Even if a ârightâ is seen as an entity which one has and the other does not, this also is not ârightâ but falsity. If we can see that human rights are not exercised in their totality in any country, then will it be wrong to assume that the reason for this is that those who are aware of these rights feel themselves to be entitled to them but do not see as their duty the defence of these rights in regard to others? If so, then one can only explain this discrepancy by arguing that the guiding principles of these peopleâs thoughts and actions are based on power relations. On this ground, as pointed out already, there is a zero-sum relationship between self"
311,2282,0.983,The Physics of the B Factories,"The sources of systematic error that have been considered are the fit model, the particle identification, the fit bias measured on MC and the selection based on the likelihood ratio. The procedure to evaluate the systematic error follows the same strategy of the D0 analysis. A slightly diï¬erent behavior is observed for the ATP and ATP asymmetries in D+ and Ds+ . Even if the final state is the same, a diï¬erent resonant sub-structure may be responsible for this diï¬erence (Gronau and Rosner, 2011). The CP violation results obtained at the B Factories in four-body D decays are consistent with zero to a precision of 0.5%. These results are in agreement with expectations from SM predictions. Nevertheless, the relative simplicity of the analyses based on T -odd correlations and the results that can be obtained allow one to consider this tool as fundamental to search for CP violation in four-body decays. Furthermore, the study of T -odd correlations allows one to probe FSI in four-body D decays. 19.2.6.4 Summary Numerous t-integrated searches for CP violation in charm meson decays have been performed by the B Factories and these show no significant intrinsic eï¬ect at the levels of sensitivity achieved. These diï¬er from a few percent for the DCS decays to over a few tenths of a percent for SCS decays to 0.1% for CF decays (see Table 19.2.5). Because of the various detector induced and physics (forwardbackward) asymmetries the measurements require careful calibration of the data using control samples and hence represent one of the most demanding measurements performed at the B Factories. BABAR and Belle developed"
346,206,0.983,"History Education and Conflict Transformation : Social Psychological Theories, History Teaching and Reconciliation","critically. Seixas and Peck (2004) distinguish six main elements composing this ability of historical thinking: significance, epistemology and evidence, continuity and change, progress and decline, empathy and moral judgment, and historical agency (Seixas and Peck 2004). Although obviously relying on all these elements in order to be effective, the use of parrhesia when teaching historical wrongdoings of the in-group covered up by a literal social denial (Cohen 2001) addresses in a specific way the dimensions linked to empathy and moral judgment. It has to be stressed that, in this description of Seixas and Peck (2004), empathy is evoked not as a psychological construct, yet as an ability to perspective taking that is historically based. It means that, although trying to âimagineâ ourselves in the position of older generations when facing difficult choices, this kind of empathy is not based on âpresentism,â i.e., a cognitive short cut assuming that all people react in a similar way under different historical and cultural situations. On the contrary, this perspective taking is based âon a rich base of information about the fundamental structures and processes of everyday life during those (past) timesâ (Seixas and Peck 2004, p. 115), making it clear for students feeling empathy with past generations of the in-group that there are basic differences and changes between their life and life of their ancestors. Being empathic and aware of anachronistic abuses of imposing present-day moral standards to past situation, however, does not imply the impossibility to morally judge on past crimes. In fact, âexactly as with the problem of historical empathy, our ability to make moral judgment in history requires that we entertain the notion of an historically transcendent human commonalityâ (Seixas and Peck 2004, p. 115). Speaking fearlessly about moral transgressions committed by the in-group and then denied in the following social discourse, parrhesia specifically address this capability to recognize this urge to morally judge the past inherited by previous generations, without nourishing a relativism that disallows any condemnation, also when it is largely deserved. In the second part of the chapter, results of a case study on contemporary history teaching about colonial crimes committed by the Italian Army during the Ethiopia invasion (1935â1936) will be presented, in order to observe how conveying this knowledge, although referring to remote facts, may produce considerable effects on present-day young Italians. This study explores how such a kind of historical teaching, narrating in-group misdeeds formerly denied in the social discourse, may help young descendants of perpetrators to better understand their"
235,127,0.983,"Physical (A)Causality : Determinism, Randomness and Uncaused Events (Volume 192.0)","12.9.1 Booleâs Conditions of Possible Experience Already George Boole, although better known for his symbolic logic calculus of propositions aka Laws of Thought [66], pointed out that the probabilities of certain events, as well as their (joint) occurrence are subject to linear constraints [45â50, 66, 67, 163, 181â183, 221, 257, 258, 328, 421, 424, 524, 541â543]. A typical problem considered by Boole was the following [67, p. 229]: âLet p1 , p2 , . . . , pn represent the probabilities given in the data. As these will in general not be the probabilities of unconnected events, they will be subject to other conditions than that of being positive proper fractions, . . .. Those other conditions will, as will hereafter be shown, be capable of expression by equations or inequations reducible to the general form a1 p1 + a2 p2 + Â· Â· Â· + an pn + a â¥ 0, a1 , a2 , . . . , an , a being numerical constants which differ for the different conditions in question. These . . . may be termed the conditions of possible experience.â Independently, Bell [40] derived some bounds on classical joint probabilities which relate to quantized systems insofar as they can be tested and falsified in the quantum regime by measuring subsets of compatible observables (possibly by EinsteinâPodolskyâRosen type [196] counterfactual inference) â one at a time â on different subensembles prepared in the same state. Thereby, in hindsight, it appears to be a bitter turn of history of thought that Bell, a staunch classical realist, who found wanting [41] previous attempts [552, 554], created one of the most powerful theorems used against (local) hidden variables. The present form of the âBell inequalitiesâ is due to Wigner [572] (cf. Sakurai [439, pp. 241â243] and Pitowsky [397, Footnote 13]. Fine [215] later pointed out that deterministic hidden variables just amount to suitable joint probability functions. In referring to a later paper by Bell [42], Froissart [143, 227] proposed a general constructive method to produce all âmaximalâ (in the sense of tightest) constraints on classical probabilities and correlations for arbitrary physical configurations. This method uses all conceivable types of classical correlated outcomes, represented as matrices (or higher dimensional objects) which are the vertices [227, p. 243] âof a polyhedron which is their convex hull. Another way of describing this convex polyhedron is to view it as an intersection of half-spaces, each one corresponding to a face. The points of the polyhedron thus satisfy as many inequations as there are faces. Computation of the face equations is straightforward but tedious.â That is, certain âoptimalâ Bell-type inequalities can be interpreted as defining half-spaces (âbelow-above,â âinside-outsideâ) which represent the faces of a convex correlation polytope. Later Pitowsky pointed out that any Bell-type inequality can be interpreted as Booleâs condition of possible experience [396â400, 407]. Pitowsky does not quote Froissart but mentions [396, p. 1556] that he had been motivated by a (series of) paper(s) by Garg and Mermin [235] (who incidentally did not mention Froissart either) on Farkasâ Lemma. Their concerns were linear constraints on pair distributions, derivable from the existence of higher-order distributions; constraints which turn out to be Bell-type inequalities; derivable as facets of convex correlation"
8,890,0.983,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","neglecting for the time being the perturbative corrections. The mass ms of the strange quarks in the perturbative vacuum is believed to be of the order of 180â300 MeV.1 Since the phase space density of strangeness is not too high, the Boltzmann limit is used in Eq. (31.1). Similarly, there is a certain light antiquark density (q stands for either u or d):"
49,58,0.983,Artificial Intelligence and Cognitive Science IV,"""Exception cells"" should remain off during a learned sequence. Hawkins' novel prediction is that certain cells are inhibited during a learned sequence. A class of cells in layers 2 and 3 should not fire during a learned sequence, the axons of these ""exception cells"" should fire only if a local prediction is failing. This prevents flooding the brain with the usual sensations, leaving only exceptions for post-processing. ""Exception cells"" should propagate unanticipated events. If an unusual event occurs (the learned sequence fails), the ""exception cells"" should fire, propagating up the cortical hierarchy to the hippocampus, the repository of new memories."
117,105,0.983,Care in Healthcare : Reflections On Theory and Practice,"of autonomy is needed. On the one hand, this account must understand will-formation as a process that is guided by ethical reasons. On the other hand, this must be an open process in which certain solutions cannot be flagged as right or wrong without a deliberative, intersubjective exchange. Such an exchange should be held together by a justified sense of having a shared aim, but, at the same time, it must be assumed that right and wrong cannot be inferred from this shared aim without intersubjective deliberation. Kantâs use of the criterion of universalisability points in a different direction. The way in which he treats the question of whether it is ethically justified to take oneâs life if oneâs future life does not appear to be worth living is a case in point. He does not imagine this to be a weighing of multiple points of view or a process involving intersubjective social support. Rather he assumes that individually applying the test of universalisability inevitably leads to the conclusion that suicide is not ethically allowed. In other words, the shared aim of acting in accordance with the test of universalisability does not leave room for interpretation or different forms of concretisation in specific contexts as part of a process of intersubjective reason-guided communication. What is more, critics have pointed out that contrary to Kantâs own supposition, planned actions cannot be ethically justified or prohibited on the basis of the criterion of universalisability. These critics claim that Kant relies on implicit hidden assumptions that render planned actions non-universalisable. Therefore, the criterion of universalisability as such is empty. To take the two examples given above, someone who is willing to accept that he will not receive help in cases where he might need it might not see any contradiction if his preference not to help others were universalised. In the same vein, it can be argued that the assumed fact that a preference for suicide, if universalised, undermines the possibility of there being any wills is only a contradiction if one regards the existence of wills as undoubtedly desirable.2 It might be possible to save Kant from this criticism and perhaps also to develop an interpretation of his theory that can resolve the autonomy dilemma.3 However, since there are other philosophical accounts of autonomous will-formation that are better suited to resolve this dilemma from the start, it makes sense to turn to these approaches instead."
77,322,0.983,A Life Course Perspective on Health Trajectories and Transitions (Volume 4.0),"As such, if we know the value of two of these variables, we will automatically know the value of the third. Consequently, when the true underlying process affecting a dependent variable includes linear effects of some or all of APC, there is a risk that we will pick the wrong combination, given that we could swap a term for the combination of the other two terms without changing the data. For example, take a contrived hypothetical process in which, say, an individualâs level of health is affected by all of age, period and cohort, each with an effect size of 1: Health D .1  Age/ C .1  Period/ C .1  Cohort/"
249,301,0.983,Advances in Proof-Theoretic Semantics (Volume 43.0),"The proofs can obviously be worked through also for standard natural deduction, along the lines of my paper (von Plato 2011 [6]). Two more applications of explicit composition can be noted here: 1. The interpretation of arbitrary cuts in natural deduction: A comparison of natural deduction in sequent calculus style with sequent calculus proper shows that a non-normal instance of an E-rule corresponds exactly to the case of a cut in which the right premiss of cut has been derived by a corresponding left rule. In the translation from sequent derivations with cuts to natural deduction, such cuts turn into nonnormalities. The rest of the cuts are translated as explicit delayed compositions. What corresponds to cut elimination is seen from the admissibility of composition in natural deduction: An uppermost instance of Comp is permuted up until it either reaches an assumption and vanishes or hits a normal instance of an E-rule and gets turned into a non-normality. After the delayed compositions have been eliminated, there remain the proper non-normalitites and these can be eliminated in any order whatsoever. When in the normal derivation the major premisses are left unwritten, a sequent derivation is obtained. The overall procedure gives strong cut elimination in precisely the same sense in which there is strong normalization in natural deduction. Details are found in Sect. 13.4 of von Plato (2013) [7]. 2. Normalization and strong normalization of Î»-terms: Any proof of normalization and strong normalization can be turned into a corresponding proof for typed Î»-terms. The term structure is particularly transparent with general elimination rules, for the selector terms have now, with implication elimination as an example, the following structure (von Plato 2001 [5, p. 566]): [x :. B] c: AâB a: A d:C gap(c, a, (x)d) : C A selector term is normal if its first argument is a variable, in particular, for the above âgeneralized applicationâ as it is called in von Plato 2001 [5], the nested âtowerâ of applications, met with the standard application function, does not occur for normal terms. Permutative conversions reduce a suitably defined notion of depth of selector terms, and detour conversions reduce to substitutions. A Hilfssatz is used to prove that strong normalizability of Î»-terms is maintained under such substitution. Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
275,548,0.983,Foundations of Trusted Autonomy,"17.4 Computational Simulations In this section, we simulate TACU in action by building up three micro-simulations encoded in De CEC. As discussed above, De CEC is a first-order modal logic that has proof-theoretic semantics rather than the usual possible-worlds semantics. This means that the meaning of a modal operator is specified using computations and proofs rather than possible worlds. This can be seen more clearly in the case of Proves (Î¦, Ï). The meaning of Proves (Î¦, Ï) is given immediately below. Î¦ â¢ Ï â {} â¢ Proves(Î¦, Ï)"
275,30,0.983,Foundations of Trusted Autonomy,"A more modern approach to AI shifts the focus from reasoning to learning. This inductive approach has become increasingly popular, both due to progress in machine learning and neural networks, and due to the failure of deductive systems to manage unknown and noisy environments. While it is possible for a human designer to construct a deductive agent for well-defined problems like chess, this task becomes unfeasible in tasks involving real-world sensors and actuators. For example, the reaction of any physical motor will never be exactly the same twice. Similarly, inferring objects from visual data could potentially be solved by a âhard-codedâ deductive system under âperfect circumstancesâ where a finite number of geometric shapes generate perfectly predictable images. But in the real world, objects do not come from a finite number of geometric shapes, and camera images from visual sensors always contain a significant amount of noise. Induction-oriented systems that learn from data seem better fitted to handle such difficulties. It is natural to imagine that some synthesis of inductive and deductive modules will yield superior systems. In practice, this may well turn out to be the case. From a theoretical perspective, however, the inductive approach is more-or-less selfsufficient. Deduction emerges automatically from a âsimpleâ planning algorithm once the induction component has been defined, as will be made clear in the following section. In contrast, no general theory of AI has been constructed starting from a deductive system. See [67] (Sect. 1.1) for a more formal comparison."
213,300,0.983,Collider Physics Within The Standard Model : a Primer,"representation of SO.10/ which is anomaly free. So GUTs can naturally explain the cancellation of the chiral anomaly. An important implication of chiral anomalies together with the topological properties of the vacuum in non-Abelian gauge theories is that the conservation of the charges associated with baryon (B) and lepton (L) numbers is broken by the anomaly [336], so that B and L conservation are actually violated in the standard electroweak theory (but B L remains conserved). B and L are conserved to all orders in the perturbative expansion, but the violation occurs via nonperturbative instanton effects [87] [The amplitude is proportional to the typical non-perturbative factor exp. c=g2 /, with c a constant and g the SU.2/ gauge coupling.] The corresponding effect is totally negligible at zero temperature T, but becomes relevant at temperatures close to the electroweak symmetry breaking scale, precisely at T  O.TeV/. The non-conservation of B C L and the conservation of B L near the weak scale plays a role in the theory of baryogenesis that aims quantitatively at explaining the observed matterâantimatter asymmetry in the Universe (for reviews and references, see, for example, [115])."
10,96,0.983,Governance For Drought Resilience : Land and Water Drought Management in Europe,"The Governance Assessment Tool is rooted in a theory of policy implementation that is labelled Contextual Interaction Theory (Bressers 2004, 2009; De Boer and Bressers 2011). It views implementation processes not top down, as just the application of policy decisions, but as multi-actor interaction processes that are ultimately driven by the actors involved. Thus it makes sense to explain the course and results of the process from that simple starting point and to place these actors and their main characteristics central stage in any analytical model. This is also relevant because in the history of implementation research hundreds of crucial success factors were proposed and used to analyse all kinds of different cases. This can be theoretically interesting when one can try to carve out the impact of a single factor from those of all the others. In practical reality however practitioners must deal with situations in which all factors are around simultaneously, and thus with combinations of all factors that are thought to matter (Bressers and OâToole 2005). Even in a rather simple model of ï¬fteen factors having each only two possible values there are some thirty thousand different combinations of circumstances that can be imagined. That is not only unworkable as an analytical tool (Goggin 1986), it is also overdone. There are no thirty thousand (or more) fundamentally different implementation settings. But since interaction processes are human activities, all influences flow via the key characteristics of the actors involved (Bressers and Klok 1988). Thus, it is possible to explain the course and effects of implementation processes with a set of three core factors per actor. Such explanatory model is far more parsimonious, at least to begin with. All other factors, including governance conditions, are regarded as belonging to the context that may influence this set of core factors. In Fig. 3.1 we include these factors: their motivations that may spur the actors into action, their cognitions, information held to be true, and their resources, providing them with capacity to act individually and power in relation to other actors. Among the actors involved in the process there need to be a sufï¬ciently"
78,202,0.983,The Onlife Manifesto : Being Human in a Hyperconnected Era,"supported by pragmatic processes of relevance (Sperber and Wilson 1995), which, again, rely on the sharing of attentional spaces. It is precisely this capacity for joint attention that causes us to feel lost in the inordinate flow of requests, messages, instructions and information so well described by Gergen (2000). Our capacity to join into othersâ attentional spaces, read intentions from minimal traces, attribute meaning and co-ordinate around presumed shared mental states, means that we are able to collaborate on the reduced fragments of data because we can fill in the gaps. Clearly, when the experience is impoverished or the intentions of the other are too opaque and it is difficult to assume that the system is actually functioning with a principle of relevance, the communicational process becomes extremely costly. This cost may be part of the subjective feeling of loss and fatigue. In this case, the issue of attentional strain is not one of overload or excess, but of impoverishment, unintelligibility and incompleteness."
297,714,0.983,The R Book,"The main distributions used in hypothesis testing are: chi-squared, for testing hypotheses involving count data; Fisherâs F, in analysis of variance (ANOVA) for comparing two variances; and Studentâs t, in smallsample work for comparing two parameter estimates. These distributions tell us the size of the test statistic that could be expected by chance alone when nothing was happening (i.e. when the null hypothesis was true). Given the rule that a big value of the test statistic tells us that something is happening, and hence that the null hypothesis is false, these distributions deï¬ne what constitutes a big value of the test statistic (its critical value). For instance, if we are doing a chi-squared test, and our test statistic is 14.3 on 9 degrees of freedom (d.f.), we need to know whether this is a large value (meaning the null hypothesis is probably false) or a small value (meaning that the null hypothesis cannot be rejected). In the old days we would have looked up the value in chi-squared tables. We would have looked in the row labelled 9 (the degrees of freedom row) and the column headed by Î± = 0.05. This is the conventional value for the acceptable probability of committing a Type I error: that is to say, we allow a 1 in 20 chance of rejecting the null hypothesis when it is actually true (see p. 358). Nowadays, we just type: 1-pchisq(14.3,9) [1] 0.1120467"
264,68,0.983,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"Examples I will now give some examples, some of which will relate to more than one principle. The examples will come from mathematics itself as a discipline and from mathematics education. Mathematics is, par excellence, an example of the Perspective Principle. It embodies this principle in how it worksâmuch of the mathematics of today could not have been imagined even two hundred years ago, it required shifts in conceptualisations of basic mathematical ideas. The very concept of a number has changed many times over mathematical history. This reminds us, of course, that today we cannot imagine aspects of the mathematics of the future. This has serious implications for university level mathematics education. But the principle works at another level. How do people outside the ï¬eld see mathematics? Do we have a good understanding of other ways of seeing our subject. This is critical for us as educators since many of our students come from other ï¬elds and are studying mathematics for its relationship to those ï¬elds. We are the poorer for not understanding their perspectives properly. The consequences of developing any particular mathematical idea are also more complex than we can imagine. This highlights the responsibility for might happen in society as a result of a mathematical idea. What responsibility does a mathematician, or mathematicians as a group, have when their mathematics gets misused, or deliberately used for destructive ends?"
78,72,0.983,The Onlife Manifesto : Being Human in a Hyperconnected Era,"The common sense vision on knowledge and information is underlined by the omniscience/omnipotence utopia. The assumption is that, if only we knew everything that there is to know, we would act perfectly, or, alternatively, that mistakes and wrong doings could be attributed to a lack of knowledge. This, again, has been challenged by some schools of thought for some time, but is now becoming commonplace. Indeed, we are orphans of the encyclopaedic ideal and subject to the new experience that the binding constraint is not our knowledge, but instead our attention capacity. Information, and even knowledge, is like what used to be a natural resource: plentiful. We have shifted our sense of boundlessness from natural resources (now recognized as finite quantities) to information and knowledge. Indeed, with the digital transition, there are fewer and fewer activities that do not produce a âdigital shadowâ. All the electronic devices we engage with (portable or not) leave a recorded trace: where we are, what we read, what we buy, not to mention the information we post about ourselves on social networks or blogs. Information is akin to natural resources of a third kind, besides the non-renewable and the renewable, we have the exponential. Instead of aiming at a global or encyclopaedic overview, we need to learn to navigate through information-saturated waters, and make sense of and value the abundance of information through datamining and other filtering activities. This radical mental shift has consequences on our behaviours as knowers, in our collective representation of what knowledge and information are, on the link between knowledge and action (consider the veil of ignorance) and also, more concretely, on the framing of the fundamental right to privacy, as the current principles of control and data minimisation on which the privacy framework is built fail to grasp optimally the new societal concerns regarding privacy, reputation and image."
297,1664,0.983,The R Book,"It is common to have repeated measures on subjects in observational studies, where we would expect that the observation on an individual at time t + 1 would be quite strongly correlated with the observation on the same individual at time t. This contravenes one of the central assumptions of linear models (p. 503), that the withingroup errors are independent. However, we often observe signiï¬cant serial correlation in data such as these. The following example comes from Pinheiro and Bates (2000) and forms part of the nlme library. The data refer to the numbers of ovaries observed in repeated measures on 11 mares (their oestrus cycles have been scaled such that ovulation occurred at time 0 and at time 1). The issue is how best to model the correlation structure of the data. We know from previous work that the ï¬xed effect can be modelled as a three-parameter sineâcosine function of time x: y = a + b sin (2Ï x) + d cos (2Ï x) + Îµi j , and we want to assess different structures for modelling the within-class correlation. The dataframe is of class groupedData which makes the plotting and error checking much simpler. data(Ovary) attach(Ovary) names(Ovary)"
380,26,0.983,Grassroots Politics and Oil Culture in Venezuela : The Revolutionary Petro-State,"Venezuelan oil state that Coronil analyzed. At the same time, it extends this line of thought through inquiring into how the imagery and sociopolitical architecture of the Venezuelan oil state shaped the Bolivarian processes in the ChÃ¡vez era. Thus, this book is inspired by Coronilâs deep insights into Venezuelan society, while at the same time I locate my analysis in a different ethnographic space, context and temporality. There is a growing body of literature on the cultural, epistemic and material dimensions of oil, seemingly inspired not only by an acute awareness of how our lives are both materially and viscerally saturated and shaped by oil in the age of capitalist modernity, but also of the looming threat, or even mass-destruction, that oilâs omnipresence appears to harbor. The current renewed interest in oil is considered a âthirdâ wave of oil- and energy scholarship within anthropology (Rogers 2015:366). Challenging the political-science-dominated concept of âthe resource curseâ (Sachs and Warner 1995, 2001; Collier and Hoeffler 1998, 2000, 2005; Rosser 2006), anthropological approaches favor to explore and theorize how oil itself as well as the industries that extract it and put it into circulation âenter social, cultural, political, and economic relationshipsâ (Rogers 2015:370â371). This book aligns itself with this line of thinking, through adapting a perspective that sees oil wealth as a formative agent of multiple and interlinked aspects of Venezuelan society (Coronil 1997; Tinker Salas 2009). However, my analytical scope is conditioned by the ethnographic field that I am concerned with. Thus, I am not striving for an analysis of the oil economy per se. Rather, I am focusing on how Venezuelan social, cultural and political formationsâsuch as its political system, state apparatus, class relations, urbanization processes and national identitiesâwere shaped by the countryâs history as a petro-state, and how these formations were challenged, transformed and fought over during the ChÃ¡vez era. Moreover, I do not seek to establish a stringent analysis of how exactly oil wealth entered and circulated in the political and social system during the ChÃ¡vez era, but to better understand the multi-faceted ways in which oil wealth gained salience as a material resource, a social property and a cultural imagery. My analytical purpose for coining the concept âlens of oilâ is therefore to tease out oil wealthâs multiple properties as it enters social relations, while also underlining how its social dimensions are intertwined with its structural embeddedness in local and global political economies. Logan and McNeish (2012) argue that we need to do a âqualitative"
65,307,0.983,Handbook of Ocean Wave Energy,"period. Because ocean waves come with varying wave frequency, this is an important property for a wave energy converter. We will return to this subject later. A useful approximation for the heave resonance period of a freely floating body may be derived if we assume that the cross-section of the buoy is fairly round and relatively constant with depth, such that the heave added mass mr may be estimated from the width d of the body: T0;3  2p"
113,401,0.983,Pentecostalism and Witchcraft : Spiritual Warfare in Africa and Melanesia,"NOTES 1. See for a recent and spirited defense of anthropologyâs comparative method (van der Veer 2016). 2. In my view such a double-sided articulationâthe impact of global economic changes being profoundly affected by peopleâs imaginaries of occult forces, but the latter in turn provoked by such changesâwas exactly what Jean and John Comaroff had in mind with their notion of âoccult economies,â Myhreâs main target. But the plea for a view in terms of âan action upon an actionâ is of course most welcome. 3. Seagull Press is now preparing a translation of Tondaâs Le Souverain moderne. 4. See for a similar view from Melanesia, Robbins (2007) 5. Cf. Geschiere (1997); see also Lattas (2010) for New Britain."
49,371,0.983,Artificial Intelligence and Cognitive Science IV,"4 Conclusions and Future Work We have showed that the MTRNN model was able to learn eight different behavioral sequences. These constitute the motor primitive for ongoing experiments for the learning of action and language compositionality. Current developments involve the use of three additional self-organizing maps, linked to the MTRNN, and trained to represent simple linguistic inputs, as well as the object shapes and color features, obtained from images fed through logpolar transform inspired by human visual processing. This extension will facilitate our investigation of language learning. In particular, our next experiments will be addressing a specific linguistic hypothesis first proposed by the cognitive psychologist and linguist Michael Tomasello. The hypothesis, which is also known as the verb island theory, predicts that verbal argument structures are learned on a purely itemspecific basis [30]. In other words, children do not learn that adult-like verb constructions can be combined with certain types of nominals and clauses, e.g. to get a transitive directobject structure. Rather, children acquire verb concepts by developing this knowledge on an item-by-item basis where the understanding of verbs is at first limited to the context where these verbs appeared. Consequently, the general notion of transitive construction and direct object is an abstraction that occurs only during later developmental stages when a critical mass of these verb islands have been attained and thus recognized as the instances of the same general underlying (sensorimotor) structure through the process of semantic analogy [31]. During the later developmental stages children are exposed to more and more construction types where different semantic roles are linked in a similar way and as a result the involved syntactic categories will be abstracted into subjects and objects [32]. The first planned experiment will investigate the role of semantic similarities between different words during early language acquisition. In particular, the hypothesis addressed by this experiment is whether a generalization to unheard sentences is easier in condition where all learned events are of the same semantic type. Though conceptually simple, this experiment will constitute the first viable extension of the already conducted research within the iTalk Project. In addition, these problems are also discussed in child development research and therefore this work could provide useful insights."
51,179,0.983,How Generations Remember,"In this excerpt, the lecturer reinforces his disapproval of nationally divided universities. He claims that even so-called national subjects do not have to be taught separately so long as they are based on objective science. In the case of history, the lecturer argues that objective history is possible only if one excludes (national) emotions. If history is based on facts and excludes emotions, it can be objective and therefore attractive to all students regardless of their nationality. However, when we examine the last paragraph of the interview cited above, it becomes evident that the decision of what is defined as an objective fact lies solely with the respective historian. It is the authority his words are granted that enables him to claim objective history for himself while denying it to a national counterpart. As will be explored further in the remainder of the chapter, the victimisation of the Bosniak people is at the centre of the Bosniak local history representation. This is also the point from which the lecturer begins his narration, as indicated in his last paragraph cited above, in which he legitimises what happened to the Bosniaks as objective historical fact by using phrases such as âhistorical tragedyâ, a âtrue tragedyâ and âobjectively one true tragedyâ in the same sentence. As I will explore in the next section, the Bosniaks, unlike the Croats, do not draw the same conclusion about their perceived historical role as victims."
294,382,0.983,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"Handling of the potential division by zero is done by a try-except construction. Python tries to run the code in the try block. If anything goes wrong here, or more precisely, if Python raises an exception caused by a problem (such as division by zero, array index out of bounds, use of undefined variable, etc.), the execution jumps immediately to the except block. Here, the programmer can take appropriate actions. In the present case, we simply stop the program. (Professional programmers would avoid calling sys.exit inside a function. Instead, they would raise a new exception with an informative error message, and let the calling code have another try-except construction to stop the program.) The division by zero will always be detected and the program will be stopped. The main purpose of our way of treating the division by zero is to give the user a more informative error message and stop the program in a gentler way. Calling sys.exit with an argument different from zero (here 1) signifies that the program stopped because of an error. It is a good habit to supply the value 1, because tools in the operating system can then be used by other programs to detect that our program failed. To prevent an infinite loop because of divergent iterations, we have introduced the integer variable iteration_counter to count the number of iterations in Newtonâs method. With iteration_counter we can easily extend the condition in the while such that no more iterations take place when the number of iterations reaches 100. We could easily let this limit be an argument to the function rather than a fixed constant. The Newton function returns the approximate solution and the number of iterations. The latter equals 1 if the convergence criterion jf .x/j <  was not reached within the maximum number of iterations. In the calling code, we print out the"
308,134,0.983,Contemporary Bioethics : Islamic Perspective,"(Orthodox Caliphs) consulted the Sahaba on whatever a novel issue that arose. If they all agree to one opinion, then it is considered ijma that has to be accepted by all coming generations of jurists. However, if ijma is on an issue of war and peace, or of a political situation that is going to change, it is not considered binding for future generations of jurists. IV. Qiyas (Analogy, Syllogism) Dr. Isam Ghanem in his concise book âOutlines of Islamic Jurisprudenceâ [39, 40] discussed the Qiyas lucidly and succinctly. I will quote him here: In its widest sense, the use of human reason in the elaboration of the law was termed ijtihad (âeffortâ or âexerciseâ of oneâs own judgment) and covered a variety of mental processes, ranging from the interpretation of texts to the assessment of the authenticity of Traditions. Qiyas or analogical reasoning, then, is a particular form of ijtihad, the method by which the principles established by the Qurâan, Sunna, and ijmaâ are to be extended and applied to the solution of problems not expressly regulated therein. Qiyas (analogical deduction) must have its starting point in a principle of the Qurâan, Sunna, or ijmaâ and cannot be used to achieve a result which contradicts a rule established by any of these three primary material sources. When a new case or issue presents itself, reasoning by analogy with an original case covered by the Qurâan, the Sunna or ijma; is possible provided the effective cause of âilla is common to both cases, e.g., wine is prohibited by the texts, and the âillaâ (cause) is intoxication. Therefore, spirits are prohibited by qiyas because they also cause drunkeness. So the prohibition is extended by analogy. The majority of Muslims, including the four major Sunni schools, accept qiyas as a legitimate method of deducing rules of law. Indeed Caliph âUmar in his famous letter to his Judge? Qadi in Kufa (Iraq), Abu Musa al-Ashâari, wrote âStudy similar cases and evaluate the situation by analogy,â which is a clear direction to judges to reason by analogy where applicable. The Prophet approved Muâadh bin Jabalâs use of judicial opinion where no text in the Qurâan or the Sunna covered the point in issue. V. 59, S. al-Nisaâ (WomenâChap. 4) reads: âObey God and obey the Messenger and those in authority among you. If you should quarrel on anything refer it to God and the Messengerâ. The istidlal or guidance provided is that if no rule obtains, then go back to the texts to deduce a rule. V. 43, S. al-âAnkabut (The SpiderâChap. 29) reads: âAnd these similitudes We put forward for mankind, but none will understand them except those who have knowledge (of Allah and His Signs, etc.). (Q. 29:43) Thus the four pillars of qiyas are:(1)"
69,615,0.983,"Fundamental Approaches to Software Engineering : 21st International Conference, FASE 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","the feature execution, but only to states before or after running the feature, effectively treating features as atomic. In this paper, we use this notion of atomicity to formalize commutativity. The foundation of our technique is the separation between feature behavior and feature composition and efficiently checking whether different feature compositions orders leave the system in the same internal state. Otherwise, a property distinguishing between the orders can be found, and thus they do not commute. We call the technique and the accompanying tool Mr. Feature Potato Head (FPH ), named after the kidsâ toy which can be composed from interchangeable parts. In this paper, we show that FPH can perform commutativity analysis in an efficient and precise manner. It performs a modular checking of pairs of features [17], which makes the analysis very scalable: when a feature is modified, the analysis can focus only on the interactions related to that feature, without needing to consider the entire family. That is, once the initial analysis is completed, a partial order between the features of the given system can be created and used for detecting other types of interactions. Any feature added in the future will be checked against all other features for non-commutativity-related interactions to define its order among the rest of the features, but the existing order would not be affected. In this paper, we only focus on the non-commutativity analysis and consider interaction resolution as being out of scope. Contributions. This paper makes the following contributions: (1) It defines commutativity for features expressed in imperative programming languages and composed via superimposition. (2) It proposes a novel modular representation for features that distinguishes between feature composition and behavior. (3) It defines and implements a modular specification-free feature commutativity analysis that focuses on pairs of features rather than on complete products or product families. (4) It instantiates this analysis on features expressed in Java. (5) It shows that the implemented analysis is effective for detecting instances of non-commutativity as well as proving their absence. (6) It evaluates the efficiency and scalability of the approach. The rest of the paper is organized as follows. We provide the necessary background, fix the notation and define the notion of commutativity in Sect. 2. In Sect. 3, we describe our iterative tool-supported methodology for detecting feature non-commutativity for systems expressed in Java. We evaluate the effectiveness and scalability of our approach in Sect. 4, compare our approach to related work in Sect. 5 and conclude in Sect. 61 ."
315,255,0.983,Situating Children of Migrants Across Borders and origins : a Methodological Overview,"With the help of these three strategies, we minimized the effects of the type of information requested by the fulfillment of the LIVES history calendar to develop the life story. On the contrary, we maximized our chances to obtain the life story as well as a completed calendar within the short duration of the participation. The implementation of the âcalendar then interview in one meetingâ procedure was satisfying for the study of our specific population, provided that we paid attention to the willingness of participants to develop a narrative. Thus, in this case, adapting the method to each participantâs characteristics (if he or she spoke easily or not at all) using the three strategies introduced before was fundamental to obtaining satisfying data quality and quantity."
134,143,0.983,Curriculum Reform in The European Schools : Towards a 21St Century Vision,"Internationalization of Assessment An extremely important aspect of assessment is its increasing internationalization, exemplified by large-scale cross-national assessment studies, such as the Programme for International Student Assessment (PISA). Andreas Schleicher (2013), from the Organization for Economic Cooperation and Development (OECD), uses a methodology that involves the ranking of a variety of countries in relation to their performance on a series of tests, and then the identification of those systemic elements that are present in high performing countries and not present in low-performing countries. From this he concludes that it is possible to identify the optimum conditions for a systemâs effectiveness. He is therefore able to suggest that: children from similar social backgrounds can show very different performance levels, depending on the school they go to or the country they live in; there is no relationship between the share of students with an immigrant background in a country and the overall performance of students in that country; there is no relation between class size and learning outcomes within or across countries (the conceptual framework he works to here makes the unjustified assumption that all the different types of learning activities are optimally performed with the same class size); there is no incompatibility between the quality of"
80,39,0.983,Innovations in Quantitative Risk Management (Volume 99.0),"random holding period with power tails adds positive tail dependence but not finite dependence. In fact, one can prove a more general result easily by resorting to the tower property of conditional expectation and from the definition of tau based on independent copies of the bivariate random vector whose dependence is being measured. One has the following âno goâ theorem for increasing Kendallâs tau of jointly Gaussian returns through common random holding periods, regardless of the tailâs power. Proposition 4 (A common random holding period does not alter Kendallâs tau for jointly Gaussian returns) Assumptions as in Proposition 3 above. Then adding a common nonnegative random holding period H0 independent of W âs leads to the same Kendallâs tau for W H1 0 , W H2 0 as for the two returns Wt1 , Wt2 for a given deterministic time horizon t. Summing up, this result points out that adding further finite dependence through common SHPs, at least as measured by Kendallâs tau, can be impossible if we start from Gaussian returns. A different popular rank correlation measure, Spearmanâs rho, does not coincide for the bivariate t and Gaussian cases though, so that it is not excluded that dependence could be added in principle though dependent holding periods, at least if we measured dependence with Spearmanâs Ï. This is under investigation. More generally, at least from a theoretical point of view, it could be interesting to model other kinds of dependence than the one stemming purely from a common holding period (with power tails). In the bivariate case, for example, one could have two different holding periods that are themselves dependent on each other in a less simplistic way, for example through a common factor structure, rather than being just identical. In this case it would be interesting to study the tail dependence implications and also finite dependence as measured by Spearmanâs rho. We will investigate this aspect in further research, but increasing dependence may require, besides the adoption of power tail laws for the random holding periods, abandoning the Gaussian distribution for the basic assets under deterministic calendar time. A further aspect worth investigating is the possibility to calculate semi-closed form risk contributions to VaR and ES under SHP along the lines suggested in [26], and to investigate the Euler principle as in [27, 28]."
238,268,0.983,Nanoinformatics,"APT data is a point cloud data and in order to study hidden features like precipitates or grain boundaries, isosurfaces are often used. These isosurfaces are drawn at a particular concentration threshold. We calculate the uncertainty in spatial location of isosurfaces here and use visualization techniques that lead to the incorporation of uncertainty information in the ï¬nal image (Fig. 7.13). Isosurfaces were drawn by joining voxels which have the same value of density or concentration, as deï¬ned in Sect. 7.4. For uncertainty calculations in the APT data, we followed the approach described in Sect. 7.3 for calculating the error and difference from ideal density. Consider xi as the atom in a voxel with coordinates (xxi , xyi , and xzi ), the mean Âµx and standard deviation Ïx along the x-axis will be given as follows:"
82,185,0.983,Fading Foundations : Probability and The Regress Problem,"by which we got rid of P(A2 ). And so on. After a finite number m of steps we obtain the following formula: P(q) = Î² + Î² (Î± â Î² )+ Î² (Î± â Î² )2 +. . .+ Î² (Î± â Î² )m +(Î± â Î² )m+1 P(Am+1 ) . (3.15) Eq.(3.15) is the beginning of the âregress of probability-valuesâ that Lewis is talking about. His argument is that, if this series is continued ad infinitum, P(q) will always tend to zero, notwithstanding the fact that Reichenbachâs correction has been taken into account. This is presumably why Lewis comments: âI disbelieve that it [the addition of the second term] will save his point.â Let us see whether Lewisâs disbelief is justified. There are two things that should be noted about (3.15). The first is that it contains only one factor of which the value is unknown. This is P(Am+1 ), i.e. the probability of the first proposition, Am+1 , in this finite series. Since all the probabilities in the series are ultimately computed on the basis of this unconditional probability, it seems that we must know its value in order to be able to calculate P(q). The second thing is that, as m gets bigger and bigger, so that the justificatory chain becomes longer and longer, (Î± â Î² )m+1 gets smaller and smaller without limit, finally converging to zero. But of course, if (Î± â Î² )m+1 converges to zero, then (Î± â Î² )m+1 P(An+1 ) dwindles away"
372,496,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"a serious problem in imaging at these low frequencies, but it is possible to calibrate the ionosphere over a wide angular range by forming beams in the directions of calibration sources for which the positions are accurately known. LOFAR and the Murchison Widefield Array (Lonsdale et al. 2009) and the Allen Telescope Array (Welch et al. 2009) are examples of this type. Ellingson (2005) describes a system using dipoles below 100 MHz. To achieve the maximum sensitivity, it is necessary only to match the antennas to the receivers sufficiently well that the total noise is dominated by the background component received by the antennas. This is an advantageous situation since it allows the dipoles to be used over a much wider frequency range than is possible when the impedance must be well matched. To investigate the performance of an invertedV dipole under these conditions, let , be the power ratio of the background noise received from the sky to the noise contributed by the receiver. Then we have , ' er"
95,226,0.983,Elements of Robotics,"In the presence of friction or a moving object, the error will be integrated and cause a higher motor power to be set; this will cause the robot to converge to the reference distance. A problem with a PI controller is that the integration of the error starts from the initial state when robot is far from the object. As the robot approaches the reference distance, the integral term of the controller will have already a large value; to decrease this value the robot must move past the reference distance so that there are errors of opposite sign. This can generate oscillations (Fig. 6.5)."
223,193,0.983,Knowledge and Action (Volume 9.0),"Knowledge exercises an active function in the societal sequence of actions only when action is not carried out in essentially stereotyped habitual (effortless) patterns or is otherwise largely regulated, that is, where there is leeway and the need for decisions and where this situation necessitates mental exertion.5,6 The societal practices in which decisions are possible and necessary represent the ecology of knowledge or, more exactly, of its application. Every implementation of knowledge, not only of great scientific experiments, requires control of the circumstances of action (the initial conditions) through active agents, who, for example, want to translate laboratory successes (or a thought experiment) into practice. In other words, when âscientific knowledge is to be âappliedâ in society, adaptation to the initial conditions prevailing there has to be made, or societal practice has to be remodeled according to the standards set by scienceâ (Krohn & Weyer, 1989, p. 354).7"
335,300,0.983,"Open Source Systems : Towards Robust Practices 13Th Ifip Wg 2.13 international Conference, Oss 2017, Buenos Aires, Argentina, May 22-23, 2017, Proceedings","It is interesting to note that file authorship follows a pyramid-like shape of increasing authority; at the top, Linus Torvalds acts as a âdictatorâ, centralizing authorship of most of the files (after all, he did create the kernel!). Bellow him lies his hand-picked âlieutenantsâ, often chosen on the basis of merit. Such organization directly reflects the Linux kernel contribution dynamics, which is itself a pyramid [4]. However, as the kernel evolves, we see that Torvalds is becoming more âbenevolentâ. As Fig. 2 shows, the percentage of files authored by him has reduced from 45% (first release) to 10% in v4.7. Currently, he spends more time verifying and integrating patches than writing code [7]. Similar behavior is observed downwards the authorship pyramid. The percentage of files in the hand of the next top-9 Linux kernel authors (bars) is consistently decreasing. This suggests that authorship is increasing at lower levels of the pyramid, becoming more decentralized. This is indeed expected and to an extent required to allow the Linux kernel evolves at the pace it does."
281,284,0.983,Stochastics of Environmental and Financial Economics (Volume 138.0),"where in the last inequality we have used the upper bound stated in [20, Lemma 2.5]. It is very easy to adapt the proof of this lemma to the context of this section. Note that the constant Cn in the previous equation does not depend on Î± because 1âÎ±/2 1âÎ±/2  E |Ï (u(t â Îµ, 0))|2n/(2âÎ±) â¤ E (|Ï (u(t â Îµ, 0))| â¨ 1)2n â¤ E |Ï (u(t â Îµ, 0))|2n â¨ 1 ."
175,503,0.983,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","computers to program themselves. That goal has, for many years, proven very difï¬cult. As a consequence, computer scientists have pursued more modest goals. A good present-day deï¬nition of machine learning is given by Mitchell (1997), who identiï¬es machine learning as the study of computer algorithms that improve automatically through experience. Genetic programming (GP) aspires to do just that: to induce a population of computer programs or models (objects that turn inputs to outputs) that improve automatically as they experience the data on which they are trained (Banzhaf et al. 1998). Genetic programming is one of the many machine-learning methods. Within the machine-learning community, it is common to use âgenetic programmingâ as shorthand for any machine-learning system that evolves tree structures (Koza 1992). While there is no GP today that will automatically generate a model to solve any problem, there are some examples where GP has evolved programs that are better than the best programs written by people to solve a number of difï¬cult engineering problems. Some examples of these human-competitive GP achievements can be seen in Koza et al. (1999), as well as in a longer list on the Internet (www.genetic-programming.com/ humancompetitive.html). Since Babovic (1996) introduced the GP paradigm in the ï¬eld of water engineering, a number of researchers have used the technique to analyze a variety of water management problems. The main distinctive feature of GP is that it conducts its search for a solution to a given problem by changing model structure rather than by ï¬nding better values of model parameters or variables. There is no guarantee, however, that the resulting structure (which could be as simple as regression Eqs. 5.1, 5.2, or 5.3) will give us any insight into the actual workings of the system. The task of genetic programming is to ï¬nd at the same time both a suitable functional form of a model and the numerical values of its parameters. To implement GP, the user must deï¬ne the basic building blocks (mathematical operations and variables) that may be used; the algorithm then"
84,553,0.983,Eye Tracking Methodology,"particularly exciting aspect of the methodology is that it can be naturally extended to issues of segmentation and lexical access in continuous speech under relatively natural conditions. Mathematics, Numerical Reading, and Problem Solving In this area of investigation, eye movements are recorded as participants solve math and physics problems, as well as analogies. Not surprisingly, more complicated aspects of the problems typically lead to more and longer fixations. Eye Movements and Dual Tasks This methodology involves examination of eye movements when viewers are engaged in a dual-task situation; for example, a speeded manual choice response to a tone is made in close proximity to an eye movement. Although there is some slowing of the eye movement, the dual-task situation does not yield the dual-task interference effect typically found. Face Perception When examining faces, people tend to fixate on the eyes, nose, mouth, and ears. Fixations tend to be longer when comparisons have to be made between two faces rather than when a single face is examined. Illusions and Imagery These studies deal with illusions, such as the Necker cube or ambiguous figures. Brain Damage Brain damage studies have examined eye movements of patients with scotomas and visual neglect as they engage in reading, visual search, and scene perception. Dynamic Situations Eye movements have been examined in a host of dynamic situations such as driving, basketball foul shooting, golf putting, table tennis, baseball, gymnastics, walking in uneven terrain, mental rotation, and interacting with computers. Some of these applications are covered in the next chapter. Studies in which eyeâhand coordination is important, such as playing video games, have revealed orderly sequences in which people coordinate looking and action."
289,85,0.983,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","a sophisticated type system [7]. In the backwards direction the exit condition is used for deciding branching: the branch with its exit condition satisfied by the updated view (when more than one match, the original branch used in the forwards direction has higher priority) will be picked for execution. The idea is that due to the update in the view, the branch taken in the backwards direction may be different from the one taken in the original forwards execution, a feature that is commonly supported by lens languages [7] which we call branch switching. Branch switching is crucial to putâs robustness, i.e., the ability to handle a wide range of view updates (including those affect the branching decisions) without failing. We explain its working in details in the following. Branch Switching. Being able to choose a different branch in the backwards direction only solves part of the problem. Let us consider the case where a forward execution chooses the nth branch, and the backwards execution, based on the updated view, chooses the mth (m = n) branch. In this case, the original value of the pattern-matched expression e, which is the reason for the nth branch being chosen, is not compatible with the put of the mth branch. As an example, let us consider a simple function that pattern-matches on an Either structure and returns an list. Note that we have purposely omitted the reconciliation functions. f :: B(Either [A] (A, [A])) â B[A] f x = case x of Left ys â ys with Î» .True {- no by here -} Right (y, ys) â y : ys with not â¦ null"
117,53,0.983,Care in Healthcare : Reflections On Theory and Practice,"The General Phenomenology of Care Care is always initiated by an appeal. Something appeals to usâperhaps purely coincidentally. I then allow this appeal to become a matter of my concern. In accordance with this concern, I develop a volition: I want that which promotes the thrivingâeven to the smallest extentâof that which has appealed to us, that which concerns us, regardless of how I may establish what that entails. Eventually I take practical action. This connection is what I refer to as care.2 Perhaps I hold on to the object of my care in the future and the care becomes love (Freter 2016, pp. 351â363), or perhaps, as the case may also be, I immediately release the source of the appeal from my care again and send it on its way. In the following section, I will attempt to show how this basic phenomenological structure can indeed be derived from a famous literary account, namely the âGospel According to Lukeâ, in the so-called Parable of the Good Samaritan."
26,45,0.983,Cognitive Supervision for Robot-Assisted Minimally Invasive Laser Surgery,"how matter affects the propagation of a laser beam. When laser light is incident on a material, three basic effects can be observed, as illustrated in Fig. 2.7: â¢ Transmission refers to the undisturbed propagation of light through the material. The material is said to be transparent: light travels through it without any attenuation, maintaining the same direction of propagation. The energy density entering the medium equals the one that escapes it. â¢ Attenuation occurs when the energy associated with the laser beam is lost within the material volume. The propagation of the beam through the material results in an attenuation of its energy density, either because energy is absorbed by the material or it is dispersed into it. Basic attenuation mechanisms are absorption and scattering [5]. â¢ Reflection and Refraction occur when light crosses the boundary between materials with different optical properties. A fraction of the incident wave is returned from the surface of the material (reflection), while the remaining part propagates into it (refraction). Refraction is usually associated with a change in the speed and direction of propagation. Reflection and refraction are strongly related to each other by the Fresnelâs Equations [8]. In general, these effects might occur simultaneously in a combined fashion, i.e. when light impinges on the surface of a material part of it is reflected, part is transmitted and part is either absorbed or dispersed into the material. The proportions of light which is transmitted, returned or dispersed are dictated by the incident wavelength and the optical properties of the material. The principle of energy conservation holds: the sum of the energies associated with each of these interactions gives the total amount of incident energy. In the scope of this doctoral thesis, we shall focus on a specific set of laser-matter interactions, in which the target material consists of biological tissue. Most relevant laser-tissue interactions are induced by the absorption of laser energy within the tissue volume. Among these, thermal interactions play a crucial role in laser surgery, as we will find later on. In the next section, we present the fundamentals of lasertissue interaction mechanisms, with particular focus on the effects that laser radiation can produce on tissues."
298,59,0.983,The Globalization of Science Curricula,"Having outlined our statistical approach and the rationale behind it we now give a more thorough account of the statistical techniques used in this investigation. Cluster analysis is a statistical technique that allows the grouping of a set of observations according to certain characteristics, in such a way that observations in the same cluster are more similar to each other than to observations in other groups. In the context of our investigation, the observations that are clustered are countries, and the variables used to group them are the responses given to the TIMSS curriculum questionnaire items on the science topics that are included in countriesâ intended science curricula. Our analysis aims to investigate the existence of convergence in science curricula, with convergence signaled by the tendency of a group to expand at the expense of others. When running the analysis on the TIMSS dataset we chose to implement a two-step cluster analysis. The ï¬rst step of this procedure consists of pre-clustering the records into small sub-clusters that are then consolidated into a smaller number of groups in the second step. The advantage of this methodology over other clustering algorithms is that, when the number of groups is unknown, the algorithm automatically returns the optimal number of clusters based on the Bayesian information criterion (BIC).2 Observations are grouped together based on a distance measure that, in this case, reflects how different countries are from each other in terms of science topics covered in their curricula. In order to cluster the observations, we adopt the log-likelihood criterion that is appropriate when grouping observations using continuous as well as categorical variables. This distance measure works best when all variables are independent and categorical variables have a multinomial distribution.3 The log-likelihood is a probability-based distance. The distance between two clusters is related to the decrease in log-likelihood as they are combined.4 Our original intention was to perform a latent class analysis on the data. We performed a cluster analysis followed by a discriminant analysis due to the reduced size of the sample. Given the large number of parameters estimated, latent class analysis requires a considerable number of observations. (In statistics, a model cannot be identiï¬ed unless the sample size is one more than the number of predictors and, as a rule of thumb, we need around ten observations per parameter to estimate a model with reasonable precision.)"
241,832,0.983,Second Assessment of Climate Change for the Baltic Sea Basin,"In relation to the Baltic Sea region, this chapter attempts to answer the overriding questions: What are the main atmospheric pollutants and where do they originate? How has D. Simpson (&)  J. Bartnicki Research Department, Norwegian Meteorological Institute, Oslo, Norway e-mail: david.simpson@chalmers.se J.-P. Jalkanen Atmospheric Composition Research, Finnish Meteorological Institute, Helsinki, Finland H.-C. Hansson Department of Applied Environmental Science, Stockholm University, Stockholm, Sweden O. Hertel Department of Environmental Science, Aarhus University, Roskilde, Denmark J. Langner Research Department, Swedish Meteorological and Hydrological Institute, NorrkÃ¶ping, Sweden"
187,320,0.983,Managing The Complexity of Critical infrastructures : a Modelling and Simulation Approach (Volume 90.0),"The above-described structured approach to doing V&V has a number of advantages that make it more effective and more efï¬cient than doing V&V in a less structured way. Below some of the key advantages are discussed. The right starting point for the V&V effort leads to more effective results The V&V effort should start from the perspective of risk. Who runs the real risk in an M&S endeavour? It is not the modeller, not the implementer (maybe there is a risk of repetitive strain injury) and not the person who executes the simulation (maybe if there is a moving base simulator). In general the real M&S use risk is found when the M&S based results are applied in the real word. Therefore V&V processes that are developer oriented might miss the real risk. Also, when studying the 4-world view in Fig. 5 it may become clear that possibly many more aspects may need to be considered than just the domain knowledge as coded in a simulation. Thus organizational aspects that may make or break the use of simulation, the level of proï¬ciency of all people involved, the processes used to derive the products such as the Operational Needs, etc. may all play a signiï¬cant role and may need to be included in determining the overall utility and thus in the V&V approach. If such a very broad scope is used it becomes clear that a domain oriented V&V process may also miss some aspects. Therefore a general methodology that starts at the true M&S use risk and that can incorporate domain speciï¬c elements as well as other aspects will result in a more effective V&V result because the right starting point can be chosen and all relevant aspects included. Balancing resources with needs leads to efï¬ciency and effectiveness A structured decomposition of the Acceptance Goal into all aspects that are relevant and on top of that a decomposition of the contribution of the M&S use risk attached to the Acceptance Goal leads to the possibility to spend the available resources for the V&V effort wisely. Based on priorities related to the contribution to the overall M&S use risk it can be decided which parts of the decomposition requires more or less effort. When available resources do not allow testing all aspects to their maximum, i.e. in all practical situations, it can be decided to let the goals with low contribution to risk remain undeveloped. In that case it should be explicitly recorded that that goal is not used in the rest of the V&V work, see âKnowledge of the completeness of the V&V effort leads to effectivenessâ below. If nodes are developed to the point where tests can be deï¬ned, the contribution to the M&S use risk can be used to make choices for tests. Low contribution to the risk allow for cheaper tests"
378,83,0.983,The Great Mindshift : How a New Economic Paradigm and Sustainability Transformations Go Hand in Hand,"thinking, feeling, and acting. SETSâs are created, ordered and stabilized through human decision-making and (often) conscious creation of regime structures. Searching for more efï¬cient technologies and more effective economic incentives is not enough when looking for sustainability solutions. It is the institutional setups and sociocultural frameworks that deï¬ne the purpose for which technologies and economic instruments are used. Here is where we ï¬nd the root causes of trends. Incentives and technologies mostly function as accelerating or balancing feedback, but not in themselves as game changers. This is why the multi-phase concept as I posit it here gives the sociocultural anchoring of alternative proposals and pioneering solutions a crucial role in all phases of transformation. In the amended MLP it is the purple and blue arrows that make the link. They indicate how mind-sets mediate between agents and structures and how the dominant paradigm functions as a reference framework for justiï¬cations and narratives of change. The big arrow on the right hand side of the graph also shows, however, that each individual is constantly involved in shaping the future paradigm. By providing reason, opinions, arguments and experiences as well as non-verbal reactions and behavior we can all participate in paradigm shifts and thus in changing reality. Polanyi demonstrated this link in his account of the Great Transformation. The classical economic paradigm played a crucial role in making todayâs default solution the growth-ï¬xated development path. This paradigm survived over two centuries of criticism by amending itself into a neoclassical version. But today its basic assumptions are challenged from so many angles and the institutional solutions and processes based on it deliver so many crises that the time is ripe to shift from diversiï¬ed irritation to unifying consolidation: which insights on human needs and natural resource reproduction in todayâs scientiï¬c debates could become the foundational ideas of a new development paradigm? Open Access This chapter is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, duplication, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made. The images or other third party material in this chapter are included in the workâs Creative Commons license, unless indicated otherwise in the credit line; if such material is not included in the workâs Creative Commons license and the respective action is not permitted by statutory regulation, users will need to obtain permission from the license holder to duplicate, adapt or reproduce the material."
170,227,0.983,Impact of Information Society Research in the Global South,"We think that not only assets accumulate, but also disadvantages or liabilities. As Fig. 2 shows, the gaps in lower levels of the divide have consequences to the whole chain of digital achievements and, thus, to the chances of ICT having a positive impact on wellbeing. As obvious as it may seem at first glance, this disadvantage accumulation is a key point of the framework: ICTâs effect is non-assured; it could be null, positive or even some combinations of assets, i.e. of low access and poor usage; and it may have negative effects on several aspects of wellbeing. We think it is useful to remember that we are assessing ICTâs effect and not blindly preaching about its potential; it is not only logical but scientifically desirable, we think, to find empirical evidence of situations in which ICTâs effect is not clearly positive."
311,1388,0.983,The Physics of the B Factories,"This formula separates out the process independent nonperturbative quantities into F BâVâ¥ , a form factor evaluated at maximum recoil (q 2 = 0), and the light-cone distribution amplitudes (LCDA), ÏB and ÏVâ¥ , for the heavy and light mesons. This leaves the quantities T I and T II , known as hard-scattering kernels, which can be calculated perturbatively. These correspond to vertex and spectator corrections, respectively, and have been calculated to O(Î±S1 ) (Ali and Parkhomenko, 2002; Beneke, Feldmann, and Seidel, 2001; Bosch and Buchalla, 2002b; DescotesGenon and Sachrajda, 2004), and recently in some cases to O(Î±S2 ) (Ali, Pecjak, and Greub, 2008). The LCDA of light pseudoscalar and vector mesons that enter the factorization formula have been studied in detail through the use of light-cone QCD sum rules (Ball and Braun, 1999; Ball, Braun, Koike, and Tanaka, 1998; Braun and Filyanov, 1989, 1990). However, not much is known about the B meson LCDA, whose first moment enters the factorized amplitude at O(Î±S ). Because this moment also enters the factorized expression for the B â Î³ form factor, it might be possible to extract its value from measurements of decays such as B â Î³eÎ½, if the power corrections are under control. The QCDF formula introduces an important simplification in the form factor description. The B â Vâ¥ form factors at large recoil have been analyzed in SCET and are independent of the Dirac structure of the current in the heavy quark limit (Charles, Le Yaouanc, Oliver, PeÌne, and Raynal, 1999). As a consequence of this, all the form factors reduce to a single form factor up to factorizable corrections in the heavy quark and large energy limits. Field-theoretical methods such as SCET make it possible to reach a deeper understanding of the QCDF approach. The various momentum regions are represented by diï¬erent fields, and the hard-scattering kernels T I and T II can be shown to be Wilson coeï¬cients of eï¬ective field operators. Using SCET one can prove the factorization formula to all orders in Î±S and to leading order in Î/mb (Becher, Hill, and Neubert, 2005). QCD is matched on SCET in a two-step procedure that separates the âhard scale Î¼ â¼ mb and then the hard-collinear scale Î¼ â¼ Îmb from the hadronic scale Î. The vertex correction term T I involves the hard scales, whereas the spectator scattering term T II involves both the hard and the hard-collinear scales. This is why large logarithms have to be resummed, which can be done most eï¬ciently in SCET. In principle, the field-theoretical framework of SCET allows one to go beyond the leading-order result in Î/mb . However, a breakdown of factorization is expected at that order. For example, in the analysis of B â K â Î³ decays at sub-leading order, an infrared divergence is encountered in the matrix element of O8 (Kagan and Neubert, 2002). In general, power corrections involve convolutions, which turn out to be divergent. Currently, no solution to this well-analyzed problem of end-point divergences within power corrections is available (Arnesen, Ligeti, Rothstein, and Stewart, 2008; Becher, Hill, and Neubert, 2004; Beneke and Feldmann, 2004). Thus, within the QCDF/SCET approach, a general, quantita-"
355,352,0.983,"Agile Processes in Software Engineering and Extreme Programming: 19th International Conference, XP 2018, Porto, Portugal, May 21â25, 2018, Proceedings (Volume 314.0)","reasonable to cluster the projects as âagileâ if the response was 1 or 2, âpartly agileâ if the response was 3, and ânot agileâ if the response was 4 or 5. There were, however, no simple connection between the self-assessed degree of agility (using the scale from 1 to 5) and the implemented agile practices. This makes the development category boundaries, especially the boundary between agile and partly agile, to some extent fuzzy and subjective. While this may limit the strength of the analysis, it is clear from the analysis that those categorized as agile on average have more agile practices than those categorized as partly agile. While we believe that this is sufï¬cient for meaningful analyses, it is important to be aware of that degree of agility in our study is based on the respondents subjective assessment.6 Our measure of a projectâs level of success used a combination of three success dimensions: client beneï¬ts, cost control, and time control. To be categorized as âacceptableâ, we require a score of at least âacceptableâ on all three dimensions. Fifty-four percent of the projects were categorized as acceptable using this deï¬nition. Notice that the inverse of âacceptableâ (46% = 100% â 54%) is the set of projects assessed to have a non-acceptable outcome on at least one of the success dimensions, i.e., the set of âproblematicâ projects. To be categorized as âsuccessful,â we require that all three dimensions should be assessed as âsuccessful.â Only 12% of the projects belonged to that category."
264,1262,0.983,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"Reaction Time Test From the given materials, participants studied the nature of a free-falling object by measuring reaction time. The plot between reaction time and the number of participants with that reaction time (frequency) was studied. The distribution plot tends to be a normal distribution."
247,187,0.983,Humanities World Report 2015,"1. It may lack sufficient rigour. For several respondents, scholars need to be trained within a well-contained field with its own standards and methodologies. Put simply, their concern was that too much pressure for interdisciplinary research causes scholars to become amateurs. In some responses the solution seemed to be to let scholars master their discipline before they reach out to others. This point was made by 19 respondents, As6, As10, Au4, E2, E9, E14, LA2, LA4, ME3, ME4, NA1, NA3, NA4, NA14, NA16, R5 and: R1: I think that the most important disadvantage of the interdisciplinary research is that there is a risk of remaining an amateur and missing some important nuances of some of the disciplines involved. E8: There is an increasing threat to monodisciplinary research. People need to be deeply trained within a single discipline; there is a need for rigour. In the past rigour was exclusive, e.g. in Celtic studies medievalism was considered top of the tree. But there is a need to have intensive monodisciplinary training and then branch out. 2. Interdisciplinary work can be extremely time-consuming, starting from the basic information exchange needed at the outset, and then involving possibly years of hard work, often for an uncertain outcome. This point was made by 7 respondents, As8, E1, E10, E12, E14, NA8, NA15, R9 and: Af3: [interdisciplinary work] is difficult to carry out because it brings much greater complexity. It also requires that people from different disciplines work together and this isnât always easy. The advantage of monodisciplinary work by contrast is that it is much easier to carry out, involving a single perspective on an issue. It is important to consider these objections in the light of distinctions between different ways of being interdisciplinary. If what is at issue is interdisciplinarity in the strong sense of A3 above, the first criticism has considerable force. But it has much less force against A1 and A2. Multidisciplinarity seems to get round the objection that the whole point is to allow scholars to remain masters in their field while still working with other experts from other fields on a collaborative project. However,"
119,193,0.983,Contemporary Issues in Human Rights Law : Europe and Asia,"S.-P. Hwang for the other. The possibilities of interpretation in a manner open to the Convention end where it no longer appears justiï¬able according to the recognized methods of interpretation of statutes and of the constitution. Furthermore, even where the Basic Law is interpreted in a manner open to the Conventionâjust as when the case-law of the European Court of Human Rights is taken into account on the level of ordinary lawâthe case-law of the European Court of Human Rights must be integrated as carefully as possible into the existing, dogmatically differentiated national legal system, and therefore an unreflected adaptation of international-law concepts must be ruled out.9"
375,70,0.983,Musical Haptics,"also very present in mucoscal membranes. In the glabrous skin, they have up to 50 terminations for a single main axon [30]. The physiology of Merkel cells is not well understood [54]. They would participate in mechanotransduction together with the afferent terminals to provide these with a unique firing pattern. In any case, Merkel complexes are associated with slowly adaptive responses, but their functional significance is still obscure since some studies show that they can provide a Paciniantype synchronised response up to 1500 Hz [27]. The Ruffini corpuscle, which we already encountered while commenting on joint capsules, has the propensity to associate itself with connective tissues. Recently, it has been suggested that its role in skin-mediated touch is minor, if not inexistent, since glabrous skin seems to contain very few of them [58]. This finding was indirectly supported by a recent study implicating the Ruffini corpuscle not in mechanical stimulation due to direct contact with the skin, but rather in the connective tissues around the nail [5]. Generally speaking, the Ruffini corpuscle is very hard to identify and direct observations are rare, even in glabrous skin [12, 31]. Finally the so-called C fibres, without any apparent structure, innervate not only the skin, but also all the organs in the body and are associated with pain, irritation and also tickling. These non-myelinated, slow fibres (about 1 m/s) are also implicated in conscious and unconscious touch [76]. It is however doubtful that the information that they provide participates in the conscious perception of objects and surfaces (shape, size, or weight for instance). This properties invite the conclusion that the information of the slow fibres participates in affective touch and to the development of conscious self-awareness [56]. From this brief description of the peripheral equipment, we can now consider the receptors that are susceptible to play a role in the perception of external mechanical loading. As far as the Ruffini corpuscles are concerned, several studies have shown that the joints, and hence the receptor located there, provide proprioceptive information, that is estimation of the mechanical state of the body (relative limb position, speed, loading). It is also possible that they are implicated in the perception of the deformation of deep tissues which occurs when manipulating a heavy object. It might be surprising, but the central nervous system becomes aware of limb movements not only by the musculoskeletal system and the joints, but also by the skin and subcutaneous tissues [22]. It is clear that the receptors that innerve the muscles also have a contribution to make, since at the very least the nervous system must either control velocity to zero, or else estimate it during oscillatory movements. Muscles must transmit an effort able to oppose the effects of both gravity and acceleration in the inertial frame. Certainly, Golgi organsâwhich are located precisely on the load pathâwould provide information, but only if the load to be gauged is significantly larger than that of the moving limb. Lastly, the gauged object in contact with the hand would deform the skin. From this deformation, hundreds of mechanoreceptors would discharge, some transitorily when contact is made, some in a persisting fashion. At this point, it should be clear that the experience of the properties of an object, such as its lack of mobility, is really a âperceptual outcomeâ arising from complex processing in the nervous system and relying on many different cues, none of which"
192,252,0.983,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"Prof. Isidore Cantor is a biochemist who became a cell biologist and works at a small university on tumorigenesis research. During a nightly visit to the toilet, he has a eureka-experience. His idea is that, because of some mutation affecting the production of arginine (an amino acid named after its bright, silvery-white colouring) certain proteins are suddenly able to move freely in and out of cells (cell membranes normally permit translocation only in one direction). To test the validity of his brain wave, he designs an innovative experiment with tagged proteins as radioactive labels and orders his post-doc Jeremiah (Jerry) Stafford to perform it. Cantor insists on Jerryâs complete availability for this research, for he believes it may bring them the Nobel Prize, but this commanding assignment puts substantial pressures on the latterâs relationship with girl-friend Celestine Price, a promising biologist, but also a muscular campus athlete who shares an apartment with Leah, a humanities scholar specialised in Bakhtin and dialogism. According to Cantor, to unravel the enigma of tumorigenesis would certainly be a Nobel-prize winning achievement, comparable to climbing Mount Everest or K-2 (p. 37). The analogy between scientific research and mountain climbing occurs several times in the novel and is a well-known trope (Collins 2011; Zwart 2011). Cantor sees his research field as a scientific Himalaya (83) and his project as a scientific Everest (p. 82), while Stafford is referred to as Cantorâs Sherpa (p. 37, p. 83). The Himalaya metaphor (with the Nobel Prize as the summit) reflects the dimension of verticality in academic research (Zwart 2014c). When Stafford finishes the experiment (allegedly successfully), Cantor sends a manuscript to John Maddox, editor of Nature, who agrees to bypass the usual refereeing process because it is such a hot topic. No experimental details are given. Their article appears in print within 10 days of the manuscriptâs arrival, and Stafford learns from Cantor how scientists may tilt the choice of referees in their favour. Adding citations of someoneâs work, for instance, is likely to lead the journal editor Â© The Author(s) 2017 H. Zwart, Tales of Research Misconduct, Library of Ethics and Applied Philosophy 36, DOI 10.1007/978-3-319-65554-3_7"
311,1817,0.983,The Physics of the B Factories,"where Mrecoil is defined as the ârecoil mass.â This expression is valid in any reference frame. It is usually convenient to then choose a reference frame that simplifies the computation while still giving access to the invariant quantity in question. The most common use of this technique is to determine the mass of a daughter resonance using the fourmomentum of the reconstructed transition particles and that of the parent resonance, in the rest frame of the parent resonance. In this case, the equation simplifies to: Mrecoil = M12 + m2tr â 2M1 Etr"
360,384,0.983,Compositionality and Concepts in Linguistics and Psychology,"As Schema 1 shows, the ambiguity resolution involves deciding between the dimensions along which the structured individual associated with the complement denotation will be construed. Crucially, one of these dimensions can be eventive, leading to the standard coercion reading. As Frisson and McElree (2008) have shown, the kind of ambiguity between, say, reading the anthology versus writing the anthology, is not expected to contribute to cost. The lexical functions and the ontological dimensions listed here are not exhaustive. PiÃ±ango and Deo (2015) indicate that the dimensions can be âmore abstract than that of space, time, or pieces of text.â Consider their examples (web-attested): (i) Black starts and ends the visible spectrum. (ii) â¦. the 6 neighborhoods based on social status (on a scale of 1 to 6): 1 is the poorest and the most dangerous of the neighborhood, 4 begins the middle classâ¦. Sentence (i) makes reference to âan ordering of electromagnetic radiation corresponding to the visible spectrum by wavelengthâ and (ii), to social status. This study shows that verbs with more possible argument structure arrangements (dative verbs) increased response time to the secondary lexical decision task than those with fewer possible arrangements (transitive verbs), regardless of the context. The authors therefore suggest that all the argument structure information of a verb must be momentarily activated during real-time processing. They found that delaying the disambiguation information for sentences that contain a word with multiple meanings increased processing cost."
101,81,0.983,Finite Difference Computing with Exponential Decay Models (Volume 110.0),"The purpose of verifying a program is to bring evidence for the property that there are no errors in the implementation. A related term, validate (and validation), addresses the question if the ODE model is a good representation of the phenomena we want to simulate. To remember the difference between verification and validation, verification is about solving the equations right, while validation is about solving the right equations. We must always perform a verification before it is meaningful to believe in the computations and perform validation (which compares the program results with physical experiments or observations). The most obvious idea for verification in our case is to compare the numerical solution with the exact solution, when that exists. This is, however, not a particularly good method. The reason is that there will always be a discrepancy between these two solutions, due to numerical approximations, and we cannot precisely quantify the approximation errors. The open question is therefore whether we have the mathematically correct discrepancy or if we have another, maybe small, discrepancy due to both an approximation error and an error in the implementation. It is thus impossible to judge whether the program is correct or not by just looking at the graphs in Fig. 1.6. To avoid mixing the unavoidable numerical approximation errors and the undesired implementation errors, we should try to make tests where we have some exact computation of the discrete solution or at least parts of it. Examples will show how this can be done. Running a few algorithmic steps by hand The simplest approach to produce a correct non-trivial reference solution for the discrete solution u, is to compute a few steps of the algorithm by hand. Then we can compare the hand calculations with numbers produced by the program."
186,52,0.983,Dignity in The 21St Century : Middle East and West,"is indeed intrinsic to human beings, or whether one has to strive to achieve it. The next chapter will provide a brisk walk through philosophical history related to the concept of dignity and thereby also illuminate the origins of the two riddles."
139,190,0.983,Programming for Computations - MATLAB/Octave (Volume 14.0),"If we call the above test_midpoint_double function and nothing happens, our implementations are correct. However, it is somewhat annoying to have a function that is completely silent when it works â are we sure all things are properly computed? During development it is therefore highly recommended to insert a print statement such that we can monitor the calculations and be convinced that the test function does what we want. Since a test function should not have any print statement, we simply comment it out as we have done in the function listed above. The trapezoidal method can be used as alternative for the midpoint method. The derivation of a formula for the double integral and the implementations follow exactly the same ideas as we explained with the midpoint method, but there are more terms to write in the formulas. Exercise 3.13 asks you to carry out the details. That exercise is a very good test on your understanding of the mathematical and programming ideas in the present section."
192,413,0.983,Tales of Research Misconduct : a Lacanian Diagnostics of integrity Challenges in Science Novels,"that there is no core Self and that the subject, as Lacan already argued (Chap. 2) is structured like a MÃ¶bius ring. Stapelâs misdemeanour concurs with the problem of the plagiarist as described by Kris: ironically, only fabricated data are publishable or worth publishing, meaningful, significant. This is the negation of social psychology as the negation of absurdism (the negation of the negation) which allows Stapel to develop a more sincere and revelatory style of writing, oriented towards á¼Î»Î®Î¸ÎµÎ¹Î± rather than adequatio. In his most recent book, Stapel (now using the signature âd.â) compares two types of discourse on cinema, namely the scholarly writings of David Bordwell (representing contemporary university discourse in film studies) and those of his flamboyant antagonist Slavoj Å½iÅ¾ek (wavering between the discourse of the analyst and the discourse of the hysteric). These types of discourse represent a âparallaxâ (p. 313), Stapel argues (borrowing the term from Å½iÅ¾ek), for both open up important perspectives, although both perspectives remain impossible to converge into a comprehensive, overarching view. The pre-traumatic and post-traumatic parts of Stapelâs oeuvre likewise represent reverse sides of a Moebius ring. From his position as a social psychologist, Stapel spiralled back into a more literary form of authorship, and the cover of his latest book announces that he is currently working on âthree playsâ."
186,47,0.983,Dignity in The 21St Century : Middle East and West,"something one can aspire to that could lead to a laurel, as in Goetheâs poem, if achieved. It is not something intrinsic in humankind, as the German constitution, via the earlier riddle, assumes. If somebody from a culture that does not have a word for dignity is learning English, would the above help her understand the concept? In some respects yes. She would probably come to the conclusion that, in most cases, authors use the term as a descriptive property. Among other things, characters can be humble, modest, pleasant, charming, plain, lazy, mean, vicious, aggressive, serene, andâdigniï¬ed. They can demonstrate intelligence, beauty, arrogance, pride, languor, humility andâdignity. In almost all of the quotes above, one learns something about particular human beings that sets them apart from others; some show dignity, others do not. If dignity were an inherent property of all human beings, it would have no useful meaning in ï¬ction. No author would use it to describe their characters, as it would not set them apart from others. The phrase âa digniï¬ed old ladyâ would be identical in meaning to the phrase âan old ladyâ. This is clearly not the case. Bertolt Brechtâs short story âDie unwuerdige Greisinâ,5 about an allegedly undigniï¬ed old lady, would not make sense. Hence, in ï¬ction and poetry, dignity is a useful descriptor precisely because some people display it and others do not. Judged from its use in the above quotes alone, dignity would seem to be a property that is not inherent in all human beings; it can be seen and recognised, but not all humans possess it. Therefore dignity does not seem to be inviolable and intrinsic. Its presentation in the quotations above comes down ï¬rmly on the side of the ï¬rst riddle. Of course, the selection of excerpts might have been highly selective, designed to give that impressionâand besides, these quotations are too few to represent the breadth required for the blanket conclusions to be drawn. But one can say that in ï¬ction, at least sometimes, dignity is used as a descriptive property which sets people apart, as opposed to a property that applies universally to humankind. The following two excerpts show the use of dignity in ï¬ction in a much broader way than in the earlier examples. I found a run-down cafÃ©. â¦ I sat there for an hour. I thought that somewhere in the universe must lie the other world â¦ a sun-golden world, a digniï¬ed world. Where every human found the one meant for them, where every love was true love and where one lived eternally. And, of course, I immediately thought of those who could not live even there; who were not suited for such generous, sumptuous grace. The damned, who would take their own lives even there. (Grossman 2003: 250, DS translation)."
285,837,0.983,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","Most people easily recognise well known melodies even when they are transposed to a different key. The invariant property of transposed melodies is the preserved pitch ratio relationship between notes of the melody; i.e. pitch intervals of the melody remain the same despite changes in absolute pitch. For this reason, it is assumed that the ability to recognise pitch relationships (relative pitch perception) is rather robust and commonly found in the population. Recognition of preserved pitch interval patterns irrespective of absolute pitch is an auditory example of translationinvariant object perception (Kubovy and Van Valkenburg 2001; Griffiths and Warren 2004; Winkler et al. 2009). The robustness of the ability to recognise tone patterns has been supported by recent findings showing that listeners can detect random tone patterns very quickly (after ca. 1.5 repetitions) within rapidly presented tone sequences, even if the patterns are quite long (up to 20 tones in a pattern) (Barascud 2014). The human brain is also sensitive to pattern violations, with regular to random transitions (Chait et al. 2007) being detected within about 150 ms (~ 3 tones) from deviation onset (Barascud 2014). However, in these examples tone patterns were always repeated exactly, i.e. without transposition, so it is not clear whether listeners were remembering absolute pitch sequences or relative pitch relationships. In support of the assumed generality of relative pitch perception, it has been shown that violations of transposed pitch patterns elicit discriminative brain responses in neonates (Stefanics et al. 2009) and young infants (Tew et al. 2009). So it is surprising that relative pitch perception can be rather poor (e.g. see (Foster and Zatorre 2010; McDermott et al. 2010)), especially if contour violations and tonal melodies are excluded (Dowling 1986). McDermott et al. (2010), commenting on the poor pitch interval discrimination threshold they found, suggested that the importance of pitch as an expressive musical feature may rest more on an ability to detect pitch differences between tones, rather than an ability to recognise complex patterns of pitch intervals. Some years ago, in a pilot experiment we noticed that an oddball interval (e.g. a tone pair separated by 7 semitones) did not pop out as expected within a randomly transposed series of standard intervals (e.g. 3 semitones). We subsequently ran a series of experiments in which we maintained a standard pitch contour, but varied the"
194,35,0.983,Dynamic Dispatch for Method Contracts Through Abstract Predicates,"than in our presentation. In particular, the implementation also accounts for the wellformedness of heap expressions, possible null references, exceptions, object creation, etc. A lot of the effort has gone into extending the JMLâ parser to accept the new syntax. On the level of the prover engine, the previously existing support for model fields and parameter-less observer symbols was extended to support model methods. In particular, the KeY data structures were changed to allow for the observer symbols to take additional heap arguments as well as formal parameter arguments. Consequently, the generation of the corresponding proof rules and proof obligations changed accordingly. The only really new thing in terms of the implementation are the contract rules that allow the use of model method specifications as lemmas. That is, we had to implement generation of proof rules representing formula (2) for every model method specified with a contract rule. The implementation of all the other formulas was an extension and adoption of existing rules for model fields. The Cell example that we have used in this paper is part of the KeY distribution, along with other small examples. All of these examples are proven correct fully automatically by KeY. One particular example that we used as a test bed when implementing model methods is a binary tree deletion of minimal element challenge from the VerifyThis 2012 verification competition. Although this challenge does not, e.g., require the use of two-state model methods or in fact even inheritance, the solution that uses model methods is far more elegant than all the other solutions we have (unsuccessfully) tried previously. The challenge and our solution are described in full in [9], here we only present the essence of our solution. The competition challenge is about verifying an iterative procedure for removing the minimal element from a binary search tree with an obvious recursive linked data structure representation. That is, the class Tree declares field val for the node value, and two fields, left and right, linking the left and the right subtrees, with null denoting the leaves of the tree. The essence of our solution to the challenge is the leftSubTree(Tree t) model method, which by recursion checks whether the given tree t is one of the sub-tree nodes reachable through following the left links from the current node. Through specifying an appropriate method contract for leftSubTree, this method is delegated to maintain a set of crucial facts about the binary tree structure. For example, if the tree t is in fact a node somewhere in the path of left links, then we know that so is t.left (if not null) or that we can partition the current tree"
228,277,0.983,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"7.3.6 Applications of Metasets The conditional membership reflects the idea that a metaset Âµ belongs to a metaset Ï whenever some conditions are fulfilled. The conditions are represented by nodes of T but they relate to elements of algebra B. In applications they refer to a modeled reality and denote some real conditions that justify the statement. Let Âµ be some individual and let Ï be the family of those individuals who are nice: they satisfy the property of being nice. The sentence Âµ Îµp Ï says that Âµ is nice under the condition p, or, in other words, to the degree p. The condition p itself might be expressed using human language terms, for example: pretty face (thus Ï is nice because of pretty face). Labeling conditions with human language terms requires imposing partial ordering on these terms, which is generally rather subjective and not straightforward. We investigated such orderings in a series of papers discussing a new decision-support system based on this idea (see [31â33])."
48,5,0.983,Time Predictions : Understanding and Avoiding Unrealism in Project Planning and Everyday Life (Volume 5.0),"systematic review of relevant research,1 it is likely to have been biased towards our own research on time predictions. That is the privilege of authoring a book. On some of the topics we address, only a few research studies have been published, sometimes only one. The results and recommendations based on such limited evidence should be taken with a grain of salt, so please use your common sense if advice or a result sounds unreasonable in your context. The terminology used when talking about time predictions is far from standardized. Other terms are effort estimation, time forecasts, and performance time predictions. We will use the term time usage prediction or just time prediction in this book. Although the bookâs focus is on how much work a task or project requires, we also add results on the prediction of the point in time (completion time) and the number of days (duration) in which we predict the work to be completed. The book starts with a brief chapter on prediction successes and disasters, illustrating how poorly and how well people can be at predicting time (Chap. 1). We then reflect on how judgement-based time predictions are made and how central they are to our lives (Chap. 2). If you are mainly interested in the practical aspects of time predictions, you may want to skip these chapters and go directly to Chap. 3. That chapter describes the basis for time predictions, with an emphasis on the importance of probability-based thinking. A good grasp of this chapter is important for an understanding of what comes later. The chapter on overoptimism (Chap. 4) gives several possible explanations of why time predictions are often overoptimistic. As you will learn here, a tendency towards overoptimistic time predictions is not necessarily caused by a disposition towards overoptimism and may have many other explanations. Chapter 5 reviews common time prediction biases, their importance, and when they are likely to occur. An awareness of these biases is important to improve time prediction processes. In many situations, a simple number stating how many work hours are required is not enough. One would also like to know something about the conï¬dence in and the uncertainty of the time prediction. We have, therefore, included a chapter reporting what is known about our tendency to be overconï¬dent (Chap. 6), that is, the tendency to think that our time predictions are more accurate than they are. In Chap. 7, we describe how to improve the accuracy of time usage predictions through the use of evidence-based techniques, methods, and principles. The two latter chapters are the most practically oriented. If you just want advice on how to improve time prediction and uncertainty assessments, these are the main parts to read. Finally, we include a guide to selecting time prediction methods (Chap. 8) and a chapter describing how easy it is to influence people into making overoptimistic time predictions (Chap. 9). Besides the authors (the sequence of authorship was determined based on coin flips), a number of people have contributed to this book. In particular, we would like to thank Karl Halvor Teigen, Scott Armstrong, and Jostein Rise for valuable"
49,312,0.983,Artificial Intelligence and Cognitive Science IV,"The goal of RL is to choose actions in response to states so that the reinforcement is maximized, this means that an agent is learning policy: a mapping from states to actions. There are several possible ways to implement a learning process; here was chosen a Q-learning. In this form of RL an agent learns to assign values to state-action pairs a Q-value function, the value of this function is sum of all future events. While immediate rewards are more important, hare is used discounted cumulative reinforcement, where future . The equation (1) represents reinforcements are weighted by value the optimal Q-value function. At each step, the agent executes one action (selected based on the discounted Qvalue function) receives reinforcement and updates Q-value of a given stateaction pair in the table according to the off policy Temporal Difference (TD) control - equation (2), where is the learning rate. In order to get a good trade-off between exploration and exploitation, an action selection mechanism uses some kind of randomization, instead of pure greedy"
315,421,0.983,Situating Children of Migrants Across Borders and origins : a Methodological Overview,"transcultural belonging is rooted in the familial origin, eventual networks are constituted later and outside the family; (4) belonging beyond transnational tiesâthe type called in the earlier typology âbeing Spanish in Switzerlandâârepresents an interesting case of expressing a feeling of belonging to the Spanish community specifically in Switzerland but not to Spain or to family and friends in Spain; and (5) rupture of networks and detachment from belongingâthe last type, then, breaks with the networks as well as with the feelings of belonging. These interviewees clearly stated that they defined themselves as Swiss and not as belonging to both countries at once. The typology shows that with respect to transnationalism, the ways of being and of belonging are linked, but not in a simple way. For the second generation it is again the inheritance of their parentsâ networks and the way they are able to appropriate and later maintain the social ties that are crucial for their ways of belonging transculturally. The connection between both forms of transnationalism cannot be expressed in a direct correlation; there are cases where the transnational links are missing, but a transcultural belonging (based not on family links, but related, for instance, to culture in the sense of arts) is still expressed. Other cases connect both or are missing both. We could not find any case of existing transnational links without transcultural belonging. We think this is because of the case we examined: the second generation bases its networks and feelings of belonging, at least at the beginning, on the sense of origin inherited from their parents. Other transnational actors that build their social capital on a shared interest and not on a belief of descent and national/local belonging, such as business people or lifestyle groups, might maintain networks without a sense of belonging. Third, we relied on the accounts, but also, in particular, on the drawings done by the interviewees to apprehend their notion of the spatial and social formation that spans their places of living and working and their familyâ and friendsâ places in Spain. A first point to note is that the spatial accounts evoked very concrete places in Spain, such as the street where the grandparents live, or the little stream where they used to play with other kids from the village when they were younger. Although we started the interviews by asking them about their relationship with Spain, it became clear that the places they always talked about were connected to stories in their biography. This might in itself not be a surprise, but it is an important point to consider when planning the data collection and when designing the instruments. Talking about Spain usually evoked a very abstract account whereas the concrete places were tied to emotional moments and memories along their biographies. Finally, the research also underlines the importance of researching a social phenomenon that is based on movement by âmoving methodsâ (Tarrius 2001; Watts and Urry 2008). Even though the research question did not focus on movement, travelling between Switzerland and Spain plays an important part in maintaining the transnational ties. Therefore, movement lies in an implicit way at the very heart of the study, though it remains restricted to a few times per year and to roughly the same trajectories every time. The notion of the moving researcher seemed particu-"
232,562,0.983,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"presentation). To have achieved this, the value-laden issue needs to be deï¬ned, then the context of the technological problem conveyed, then the problem consulted. The results of the consultation can then be analyzed by the researchers and translated back into its implications for discussions of technology use. This process needs to sufï¬ciently convey the context of the discussion, and be appropriately deï¬ned (if the issue is too abstract in terms of value, for example if people were asked to discuss political values each endorses, this would likely not work any better than asking them to talk about the appropriate thickness of waste containers). The method of consultation devised here takes the form of a group interview. During August 2014 to February 2015, 18 people were interviewed in various groups, each session lasting around 90 min. Participants were asked to relax and to discuss freely âwhat you think is a desirable means of âdisposalâ of high level radioactive wasteâ without worrying about present technical limitations. A less than 5 min explanation of what high level radioactive waste is, what it looks like, how much radioactivity it holds and how long that lasts, how much of it Japan has, and its present designation was given at certain timings using the ï¬gures shown (Fig. 1). One important characteristic of these interviews was that the targeted age group of the participants was limited to âyoungâ people, set as being between 16 (a high school freshman) to 34â35. This target was meant as a current attempt of listening to the âfuture generationâ."
380,511,0.983,Grassroots Politics and Oil Culture in Venezuela : The Revolutionary Petro-State,"Cabrujasâs speech has more historical un-packing and sophisticated nuances than I can cover here. It is also so culturally rich and subtle that numerous interpretations can be drawn from it. However, I wanted to quote him here because he manages to indicate the deep-seated and complex template of viveza, or what is called la viveza criolla in several Latin American countries, or in the case of Brazil, jeitinho. The concept denotes a form of cunningness and calculated astuteness that forms part of the Latin American imagery of collective personhoods and social arrangements. âFirst we invent the trampa (the trick, trap) and then the law, with enough cracks to allow us to go through with la trampa,â said Ezequiel MartÃ­nez Estrada, Argentinian intellectual and author (La Nueva 2010, authorâs translation from Spanish). It describes the socialization of an individualized and shameless anything-goes behavior, whereby only a fool obeys the rules. In Venezuela, the concept encapsulates everything from corruption to shady or semi-legal rebusques (inventive ways of making quick money or earn an extra income), jumping ahead in the bus line, pushing oneâs way through by force in a traffic jam, refusing to offer prioritized seating to elderly and pregnant people on the metro, a general lack of respect for others and any kind of self-seeking behavior. It could also be used to denounce politicians or career climbers, or anyone who seek to get ahead through cunningness and foul play. In colloquial speech it is often used when someone tries to be clever or trick themselves out of (or into) something, as in no seas vivo/a (donât be inventive/donât be mischievous). While it is commonly referred to as a negative trait, it is also at times ambiguously interpreted as a positive trait symbolizing the ability to survive in a difficult environment, a âtalent for life,â and, to a certain extent, social success. Sometimes, you would hear people practically boast about their capacity to enrich themselves through petty corruption or favorable rebusques. Among some, it seemed to be considered a sign of masculinity."
208,69,0.983,Actors and the Art of Performance,"whose fateful threads become visible to the audience as they are played. No, it is the actor himself in the act of acting whose body becomes the demonstrative site of the vulnerability of exposition. He is its mask, its face, its name â as if to remind us, as if to remind us of the specter, the spectacle of our being, before the eyes and ears of others. The actorâs demonic power is perhaps the way in which he brings together the canny and the uncanny. Perhaps he is an erote, a messenger of the fatality of our existence. Through his act of aisthesis, in his possession of the knowledge of pathos â laughing, cursing, murdering, whoring, stammering, praying, loving, truth saying, soothsaying â he reminds us that we cannot escape this facticity nor manage without it. That we can only, by playing masterfully, re-sign ourselves to it."
217,541,0.983,Finite Difference Computing With Pdes : a Modern Software Approach,"The famous diffusion equation, also known as the heat equation, reads @2 u D Ë 2; where u.x; t/ is the unknown function to be solved for, x is a coordinate in space, and t is time. The coefficient Ë is the diffusion coefficient and determines how fast u changes in time. A quick short form for the diffusion equation is u t D Ëuxx . Compared to the wave equation, u t t D c 2 uxx , which looks very similar, the diffusion equation features solutions that are very different from those of the wave equation. Also, the diffusion equation makes quite different demands to the numerical methods. Typical diffusion problems may experience rapid change in the very beginning, but then the evolution of u becomes slower and slower. The solution is usually very smooth, and after some time, one cannot recognize the initial shape of u. This is in sharp contrast to solutions of the wave equation where the initial shape is preserved in homogeneous media â the solution is then basically a moving initial condition. The standard wave equation u t t D c 2 uxx has solutions that propagate with speed c forever, without changing shape, while the diffusion equation converges to a stationary solution u.x/ as t ! 1. In this limit, u t D 0, and uN is governed by uN 00 .x/ D 0. This stationary limit of the diffusion equation is called the Laplace equation and arises in a very wide range of applications throughout the sciences. It is possible to solve for u.x; t/ using an explicit scheme, as we do in Sect. 3.1, but the time step restrictions soon become much less favorable than for an explicit scheme applied to the wave equation. And of more importance, since the solution u of the diffusion equation is very smooth and changes slowly, small time steps are not convenient and not required by accuracy as the diffusion process converges to a stationary state. Therefore, implicit schemes (as described in Sect. 3.2) are popular, but these require solutions of systems of algebraic equations. We shall use readymade software for this purpose, but also program some simple iterative methods. The exposition is, as usual in this book, very basic and focuses on the basic ideas and how to implement. More comprehensive mathematical treatments and classical analysis of the methods are found in lots of textbooks. A favorite of ours in this respect is the one by LeVeque [13]. The books by Strikwerda [17] and by Lapidus and Pinder [12] are also highly recommended as additional material on the topic. Â© The Author(s) 2017 H.P. Langtangen, S. Linge, Finite Difference Computing with PDEs, Texts in Computational Science and Engineering 16, DOI 10.1007/978-3-319-55456-3_3"
72,17,0.983,New Frontiers in Social Innovation Research,"Social innovation and the pursuit of human potential Finally, if the deep underlying idea of social innovation is an optimistic one about human potential to govern individual lives and design the world, then there is a need to understand how societies can make the most of that potential."
23,26,0.983,Anti-Vivisection and The Profession of Medicine in Britain : a Social History,"The history of vivisection is inseparable from that of medical science. Without animal experimentation, the course of medicine would have been radically different (one can admit as much without making any presumption about the validity of animal models). Since the nineteenth century, laboratory experimentation has become the gold standard of academic medicine, shaping not only its approach to solving problems, but also the moral conduct and education of doctors. To experimentalists, it was axiomatic that medical science must be objective, rational, and dispassionate: if its advancement required the infliction of pain on laboratory animals, then it was unprofessional, even unethical, to allow squeamishness or sentiment to get in the way. Thus there arose a tension in medicine between the scientific spirit of cool indifference to suffering and the clinical tradition of compassion and caring. When the Continental fashion for vivisection first touched Britain in the 1820s, many doctors chose to distance themselves from it for the sake of their reputation, and the few who did undertake it felt the need to defend a choice that seemed at odds with the ethos of their profession. Though âanti-vivisectionâ became, in the course of the nineteenth century, so familiar a term of self-description that it would be obtuse to call organized opposition to animal experimentation by any other Â© The Author(s) 2017 A.W.H. Bates, Anti-Vivisection and the Profession of Medicine in Britain, The Palgrave Macmillan Animal Ethics Series, DOI 10.1057/978-1-137-55697-4_2"
315,27,0.983,Situating Children of Migrants Across Borders and origins : a Methodological Overview,"not only is his or her life development recorded but also his or her values, attitudes, and expectations about the future. In this way, without that having to be a strong rule, a retrospective design is more appropriate when the research focuses on event sequences (more âobjectiveâ), while a prospective design focuses on the accumulation of experiences (with consideration of the subjective dimensions of the life course as well). This distinction applies only partially to qualitative approaches, since in both retrospective and prospective designs, the aim is to collect information on respondentsâ evaluations and interpretations of their own life and experiences."
244,108,0.983,"Advancing Human Assessment : The Methodological, Psychological and Policy Contributions of Ets","Classical test theory traditionally deals with the hypothetical scenario where examinee p takes an infinite number of parallel test forms (i.e., forms composed of different items but constructed to have identical measurement properties, Xâ², Xâ³, Xâ´, â¦ ). As the examinee takes the infinite number of test administrations, the examinee is assumed to never tire from the repeated testing, does not remember any of the content in the test forms, and does not remember prior performances on the hypothetical test administrations. Under this scenario, classical test theory asserts that means of observed scores and errors for examinee p across all the Xâ², Xâ³, Xâ´â¦ forms are"
132,65,0.983,AiREAS: Sustainocracy for a Healthy City: Phase 3: Civilian Participation â Including the Global Health Deal Proposition,"1.22.3 Reflection: Marjan Hagelaars I chose the Human in Technology learning line because it was likely the learning line most linked to my bachelor in Biomedical Engineering and it ï¬t perfectly into my schedule. However, my perception was a bit wrong, as I learned more about the psychological background than anything else. Fortunately, psychology does interest me, mostly as to how people think and their perception of products and their surroundings. In my opinion, however, the learning line is a bit too much. After one course of 5 ECTS, I had learned enough about psychology, with the two other courses of Human in Technology merely acting as repetitions of the ï¬rst course. My interest faded because of these repetitions and my motivation began to decrease. The project, on the other hand, was a lot of fun. We used the information we learned in a practical sense. My conclusion therefore is that one course with lectures and one project is sufï¬cient; in this way, you keep students motivated. In this project, I learned about the different persuasive techniques, the usability and how all these features are important for achieving an application with a goal of changing the way people think. The USE learning line could stand to become more useful for students, however the concept is clear and USEable!"
386,32,0.983,Socioeconomics of Agriculture,"2.1.1 Weberâs Iron Cage In any writing on the rationale of the public administration, Max Weber should play a prominent role, partly because the German sociologist (1864â1920) was among the first to give the public administration a central position in sociological theories. Weberâs general focus was on two concepts: one was rationalisation, which he considered as the most general element in our historic development; the other was domination, an apparently legitimate exercise of power. It is obvious that both concepts are easily traceable in the apparatus of public administration. His famous terming of the administration as an âiron cageâ to describe both principles can be found in his seminal work The Protestant Ethic and the Spirit of Capitalism: The Puritan wanted to work in a calling; we are forced to do so. For when asceticism was carried out of monastic cells into everyday life, and began to dominate worldly morality, it did its part in building the tremendous cosmos of the modern economic order. This order is now bound to the technical and economic conditions of machine production which today determine the lives of all the individuals who are born into this mechanism, not only those directly concerned with economic acquisition, with irresistible force. Perhaps it will so determine them until the last ton of fossilized coal is burnt. In Baxterâs view the care for external goods should only lie on the shoulders of the âsaint like a light cloak, which can be thrown aside at any momentâ. But fate decreed that the cloak should become an iron cage. (Weber 1905: 28)"
249,35,0.983,Advances in Proof-Theoretic Semantics (Volume 43.0),"5.3 Validity of Arguments We can now define what it is for an argument to be valid by adopting three principles analogous to the ones stated for valid deductions: I. A closed canonical argument (A , J ) is valid, if for each immediate sub-argument structure A â of A , it holds that (A â , J ) is valid. II. A closed non-canonical argument (A , J ) is valid, if A reduces relative to J to an argument structure A â such that (A â, J ) is valid. III. An open argument (A , J ) depending on the assumptions A1 , A2 , . . . , An is valid, if all its substitution instances (A â , J â ) are valid, where A â is obtained by first substituting any closed terms for free variables in sentences of A , resulting in an argument structure A â¦ depending on the assumptions Aâ¦1 , Aâ¦2 , . . . , Aâ¦n , and then for any valid closed argument structures (A i , Ji ) for Aiâ¦ , i â¤ n, substituting A i for Aiâ¦ in A â¦ , and where J â = iâ¤n Ji âª J . Because of the assumed condition on the relative complexity of the ingredients of an introduction inference, the principles I-III can again be taken as clauses of a generalized inductive definition of the notion of valid argument relative to a base B , which is to consist of a set of closed argument structures containing only atomic sentences. If A is an argument structure of B , the argument (A , â), where â is the empty justification, is counted as canonical and outright as valid relative to B . A base is seen as determining the meanings of the atomic sentences. An argument that is valid relative to any base can be said to be logically valid. If A is an argument structure representing mathematical induction as exhibited in Sect. 5.1, J is the justification associated with A as described in Sect. 5.2, and B is a base for arithmetic, say corresponding to Peanoâs first four axioms and the recursion schemata for addition and multiplication, then the argument (A , J ) is valid relative to B (as was in effect first noted in a different conceptual framework by Martin-LÃ¶f (1971) [14]. This is an example of a valid argument that is not logically valid but whose validity depends on the chosen base. However, I shall often leave implicit the relativization of validity to a base. Instead of saying that the argument (A , J ) is valid it is sometimes convenient to say that the argument structure A is valid with respect to the justification J . But it is argument structures paired with justifications that correspond to proofs and that will be compared to BHK-proofs."
289,48,0.983,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","Matching. The matching judgment of Siek et al. [25] can be extended to polymorphic types naturally, resulting in Î¨ â¢ A â² A1 â A2 . In M-Forall, a monotype Ï is guessed to instantiate the universal quantifier a. This rule is inspired by the application judgment Î¦ â¢ A â¢ e â C [11], which says that if we apply a term of type A to an argument e, we get something of type C. If A is a polymorphic type, the judgment works by guessing instantiations until it reaches an arrow type. Matching further simplifies the application judgment, since it is independent of typing. Rule M-Arr and M-Unknown are the same as Siek et al. [25]. M-Arr returns the domain type A1 and range type A2 as expected. If the input is â, then M-Unknown returns â as both the type for the domain and the range. Note that matching saves us from having a subsumption rule (Sub in Fig. 2). the subsumption rule is incompatible with consistent subtyping, since the latter is not transitive. A discussion of a subsumption rule based on normal subtyping can be found in the appendix."
360,568,0.983,Compositionality and Concepts in Linguistics and Psychology,"the literature typically includes nouns such as idiot, nerd, soccer fan, airhead, goat cheese enthusiast, simpleton, Barbie doll lover, loser, and weirdo (de Vries 2010). It is easy to observe that all of them belong to the social domain. Thus, their dimensions are expected to be relatively accessible for grammatical operations to quantify over as in idiot with respect to his political views and complete/total idiot (de Vries 2010). Moreover, these nouns seem to have morphologically gradable dimensions (e.g., stupid, intelligent, admiring) accessible for size adjectives to select and use for their interpretation. Dimension accessibility for quantiï¬er binding seems to affect additional gradable constructions. For example, in various languages human categories such as boy and girl directly combine with the modiï¬er very, as in the Hebrew meod yalada (âvery girlâ, âvery girlish/immatureâ) and Spanish Es muy hombre (âis very manâ, âHe is very much a manâ; Espinal 2013). Additive dimension binding for the noun girl makes its dimensions more accessible than those of most other nouns. The importance of gradable traits in the stereotype of girls (e.g., loving pink) can be stretched to the point that girl is interpreted as equivalent to girlish, thereby licensing very. In sum, nouns carrying expressive or evaluative components such as idiot, coward, hero or child in its metaphoric sense, get as close to gradable adjectives as nouns can (Constantinescu 2011: 49â96), perhaps because they have adjectival dimensions (another point to consider in the future). To conclude, the main result of the reported study is a connection between the acceptability of a given noun or adjective in comparison constructions and its type of characteristic categorization criterion (i.e., whether, as a default, its dimensions combine into a single criterion via quantiï¬ers or other operations). This result highlights connections between cognitive psychological ï¬ndings and linguistic phenomena, thus potentially contributing to the study of morphological gradability within linguistics and to an improved understanding of certain experimental results in cognitive psychology, where the potential role of the noun-adjective distinction was overlooked (Wattenmaker 1995). By raising awareness both to grammatical and conceptual distinctions, and by pointing out directions for future research, this paper has aimed to deepen our understanding of the relations between the formal and conceptual components of natural languages. Acknowledgements Many thanks for the ï¬nancial support by a Grant from the GIF, the German-Israeli Foundation for Scientiï¬c Research and Development. Special thanks also to the editors, James Hampton and Yoad Winter, for the inspiring workshop and very useful comments."
173,45,0.983,"intelligent Human Computer interaction : 9Th international Conference, Ihci 2017, Evry, France, December 11-13, 2017, Proceedings","This article explored several haptic rendering methods to present geometrical shapes through the touch using several ï¬ngers on a large physical surface: the dot-matrix display. The presented study allowed us to collect 12960 recognition times and 12960 recognition scores (324 shapes Ã 40 participants). Results show that the best rendering method is the one that combines static outline with empty inside and that squares, right triangles, and crosses are more quickly recognized than circles, diamonds, and simple triangles. These results are interesting for our project concerning spatial access to docuâ ments by the blind. The protocol of the presented study was inspired by a similar study conducted by Levesque and Hayward on a smaller device that allows exploring a virtual surface using only one ï¬nger: the STReSS2 device. The comparison of results shows that the recogâ nition rates and times on a dot-matrix display are better in all cases. However, further investigations are needed to determine if this is due to mono-ï¬nger vs multi-ï¬nger exploration or for other reasons. Next step of this work will be to reproduce the same experiment with visually impaired people. It would be also interesting to study the eï¬ects of diï¬erent vibration frequencies and diï¬erent outline widths as well to compare the performances of the dotmatrix display with those of a vibrotactile device such as in [13]."
78,207,0.983,The Onlife Manifesto : Being Human in a Hyperconnected Era,"common is what is being described as âburn outâ. This is characterized by the paradoxical feelings of being permanently exhausted, overloaded, under pressure, and yet not being able to achieve what is expected and losing productivity. While not new as a set of symptoms, the expectations of permanent availability and self promotion associated with the professional model of the âentrepreneur of the selfâ has heightened the sense of disorientation. Controlling the attention of others, and dealing with the constant solicitation of others, is accompanied by a dramatic sense of loss of self-direction, intentionality and planning. The French expression of for intÃ©rieur can help us understand the human issues at stake here. In Latin, âforâ means jurisdiction. The common understanding (not the ecclesiastic one) of the for intÃ©rieur is the jurisdiction that each person applies to her/himself; it corresponds to what in social sciences is called a sense of agency. Managers and employees in organizations that are heavily reliant on digital environments, such as banks, public administrations, large corporations, describe a sort of permanent blurring between their interior life and their life online. They describe the difficulty of making their for intÃ©rieur exist vividly in their daily lives. They talk of burning from the inside. This sense of disorientation is not unique to workplaces, and seems to be emerging in the home. The feeling of losing a sense of control when engaged with digital devices is described equally by gamers, online shoppers, video consumers or social media participants. Invariably, users talk of their devices as âtime sucksâ, as environments in which they lose their intentions and agency. Another facet of the same problem is what R. Sennett (1977) describes as the current tyranny of intimacyâthat is, the central position of intimate relations in the perception of self-realization. In contrast to traditional patterns of social interactions, organized through distinct roles where individuals were more easily categorized as workers, lovers, parents, citizens, we now observe a greater fluidity and confusion of boundaries. Nowadays, observes Sennett, the king is naked. Social distances, masks and shelters have disappeared. Individuals have no sanctuaries to retreat to and hide from the scrutiny of others, but feel always visible and transparentâ¦ raising obvious questions for the plurality of social identities. To some extent, this explains the increasing position of the home and of the inner circle of the family as a protective cocoon and the growing success of activities such as cooking and gardening, which restore the sense of duration, agency and privacy. On the digital side, we also have evidence of a retreat into the private, intimate and controllable. There is ample evidence showing that all new digital communication channels, from texting to Skype, from Facebook to instant messaging, are being used to strengthen peopleâs closest and most intimate relations (Baym 2010; Broadbent 2011; Madianou and Miller 2012). Contrary to common public discourse, people have not hugely extended their social network nor do they spend much time communicating with unknown digital acquaintances. Close scrutiny of what people actually do, with all the channels they have at their disposal, shows an intensification of exchanges with a few close ties, often less than five, leading to the strengthening of these relationships. A recent survey of 3,000 teenagers in Belgium (Gallez and Lobet-Maris 2011) confirmed the results of similar studies in the US (Ito 2010), showing that most of the participants had an âbetween usâ connectivity"
277,107,0.983,integrating Immigrants in Europe : Research-Policy Dialogues,"may well be needed. In the final analysis, however, a flexible use of indicators is required rather than a fixation on particular categories identifying particular segments of the population. While perhaps cumbersome, the allusions of sameness and difference conveyed by statistical indicators and discussed in the introductory section of this chapter need to be regarded as an obstacle to knowledge production on migration, rather than as a facilitator."
148,48,0.983,Anti-fragile ICT Systems (Volume 1.0),"is the âunknown unknown,â a rare bombshell event that none of the stakeholders have considered. Two important observations can be made about black swans. First, a black swan cannot be described by any of the stakeholders because the event is completely unknown to all of them. Second, while a black swan is a total surprise to all the stakeholders considered, there may be other individuals outside the group of stakeholders for which the event is not a big surprise. As an example, while the economic crisis of 2007/2008 came as a huge surprise to most people, a few individuals, including Taleb [9], foresaw the crisis, even though they could not say when the crisis would occur or exactly how serious the consequences would be. A gray swan is a metaphor for rare global behavior with a large negative impact that is somewhat predictable but typically overlooked by most of the stakeholders considered. It is the âknown unknown,â a rare event that some know is possible but no one knows when or whether it will occur. Because a gray swan is not a complete surprise to all stakeholders, it tends to have less impact than a black swan. However, its impact is still huge. For simplicity, we often neglect to define a set of stakeholders when we discuss gray and black swans. However, the reader should assume that users, owners, software developers, operators, and regulatory government agencies are always among the stakeholders."
283,321,0.983,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","The probability of decoder error pC is simply found from e(z) by summing all coefficients of z âi where i is greater than n â k. This is very straightforward with a symbolic mathematics program such as Mathematica. The results for the RS (256, 234, 23) code are shown in Fig. 7.1. It can be seen that there is an improvement over the hard decision case but it is rather marginal. A rather more convincing case is shown in Fig. 7.2 for the RS (256, 250, 7) code where the performance is shown down to frame error rates of 1 Ã 10â20 . In this case, there is an improvement of approximately 0.4 dB."
264,1177,0.983,Proceedings of The 13Th international Congress On Mathematical Education : Icme-13,"We then introduced the formal counterpart of the intuitive process: Dividing the braid into levels and assigning a symbol to each of them corresponds to the theoretical process of ï¬nding generators for the braid group and building braids via an operation called composition. We noted the power of this speciï¬c example, where students naturally construct a correct model. This is just a small example, but it is representative of the general mathematical activity. Relating the informal description to a formal one can help the students appreciate the importance of formalism. Finally, we explored the axioms of groups using the speciï¬c cases of braids under the operation of composition and integer numbers under addition, drawing a parallel between the two examples and ï¬nding one difference. We discussed the importance of the idea of structure in mathematics, which is often not recognized at all. The idea that the subject of mathematics is computations has been widespread, but the justiï¬cation as to why computations are possible has been overlooked: Operations, and ultimately structure, make computations and their rules correct and applicable. Braids are a simple example where this can be demonstrated: Some properties hold which are similar to those for numbers, yet there are some differences. Finding rules that hold for braids is an appealing task, because it is in the zone of proximal development for high school students. Participants in the workshop had different backgrounds: Some were high school teachers, some researchers in mathematics education, and some scholars doing research in mathematics and teaching. The discussion was enriched by the differences in background, teaching experiences, and goals. The contributions of the participants also included considerations about teaching group theory at the college level and about braids in ethnomathematics."
141,161,0.983,Cyber-Physical Systems of Systems: Foundations â A Conceptual Model and Some Derivations: The AMADEOS Legacy,"Clearly a conscious and aware design discipline aims to move, as knowledge progresses, more and more emergent phenomena from quadrant 4 to quadrant 2, in which provisions can be taken to mitigate, eliminate or prevent detrimental emergence. To exemplify just observe that while at its ï¬rst manifestation deadlock was a problematic issue in distributed systems, today every computer student is though many of the different ways we have developed to properly address it. Still our knowledge regarding CPSoS may remain limited and our ignorance about them can hardly be sufï¬ciently reduced especially when we consider COTS components and legacy constituent systems. In fact, most CPSoS are built incorporating such LEGACY and COTS on which very little is known and where the information flow is often quite hidden. In the remainder of this section we will focus on quadrant 4, the problematic case of detrimental unexpected emergent with special regards to undiscovered emergent phenomena never seen before."
242,1131,0.983,"Migration, Gender and Social Justice : Perspectives on Human Insecurity","In a context of anti-immigrant hostility, one of the challenges facing academic research and social organizations is how to respond to it, both in terms of content and in the cultural forms and means used. This section examines some of the arguments with which an attempt is made to respond to anti-immigrant hostility. A first argument is the ânumbers game,â where it is often suggested that if the number of immigrants drops, the hostility will decrease as well. A second repertoire is instrumental: we must accept the immigrants given that they undertake work the local population does not do. A third way of responding, common among the critical social sciences, is to assume that the hostility is a product of unfounded images about the relationship between the presence of immigrants and insecurity or lack of facilities in the institutions. A fourth way of responding refers to the application of values such as hospitality and solidarity to try and transcend the hostility and exclusion associated with migratory legislation. The ânumbers gameâ (Hall 1981) is usually used from conservative positions, not so much to refute the hostility as to justify it. Thus the main thesis is that if there werenât âso many migrantsâ there would be no hostility. That makes the immigrants responsible for their own discrimination. Ghassan Hage (1998: 92) notes something for the case of Australia that could also be said in the Costa Rican case: when it is considered that there are âmany immigrantsâ, it ârepresentsâ¦the possibility of becoming beyond control and"
167,438,0.983,The Interconnected Arctic â UArctic Congress 2016,The trial described here was the first of its kind to be carried-out in the field since publication of the IMO Polar Code. Results suggest that there are gaps in performance for survival equipment currently approved by SOLAS compared to what is required by the Polar Code. It is clear that individual motivation and knowledge play an important role in a survival scenario. Conducting simple tasks like unzipping the survival suits at regular intervals for ventilation and drying out the insulating layer can greatly influence the outcome for that individual. The Polar Code
307,38,0.983,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"1.3.1 Linear Algebra Approach to Finding the Equilibrium Solution Calculations to find the equilibrium solution will be done repeatedly in these notes. We will always use the special structure of the Markov model to derive the equilibrium solution, but it also worth noting that this can be done by solving a linear system. The master equation associated with a Markov model of the form (1.14) or of the form given in Fig. 1.4 can always be written in the form p0 D Ap; where p is a vector containing the probabilities of occupying the different states of the Markov model. Since the sum of the probabilities adds up to one, the number of unknowns can be reduced by one and the system takes the form q0 D b"
271,150,0.983,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"CafÃ©s can register and become visible through appearing on a map and in a calendar announcing events.2 While repairing is an old practice, what is new is that the act of repairing becomes public in Repair CafÃ©s, and the actual repairing as well as the repair events are staged as political actions which strive for cultural transformation aiming at sustainability. In this chapter, results of a qualitative study are presented in which Repair CafÃ©s in Germany have been analyzed from the perspective of media and communication studies. Choosing this approach, the focus of the study was on the people repairing media technologies as well as the organizers of the events. Why do people participate in Repair CafÃ©s and repair media technologies? What do Repair CafÃ©s and the practice of repairing mean to the participants? And what relevance do the participants see in the Repair CafÃ©s for a (mediatized) society? A figurational perspective (see Hepp and Hasebrink in this volume) is helpful to structure the findings, to further analyzed Repair CafÃ©s and to answer the research questions. When analyzing Repair CafÃ©s from a perspective of media and communication studies, the transformation of media and communicative practices becomes visible as do media practices aiming at cultural change. Therefore, on the basis of the study conducted, it can be discussed how media are and can be used for cultural transformation; here, with a view to sustainability. Defining the repairing of media technologies as media practice in this chapter, it is argued that the term media practice has to be understood in a broad sense in media and communication studies, not only taking into account what people do with media content but also what they do with media technologies."
95,309,0.983,Elements of Robotics,"9.4 Mapping Using Knowledge of the Environment Now that we know how to explore an environment, let us consider how to build a map during the exploration. In Chap. 8 we saw that a robot can localize itself with the help of external landmarks and their representation in a map. Without such external landmarks, the robot can only rely on odometry or inertial measurement, which are subject to errors that increase with time (Fig. 5.6). How is it possible to make a map when localization is subject to large errors? Even with bad odometry, the robot can construct a better map if it has some information on the structure of the environment. Suppose that the robot tries to construct the plan of a room by following its walls. Differences in the real speeds of the left and right wheels will lead the robot to conclude that the walls are not straight (Fig. 9.11a), but if the robot knows in advance that the walls are straight and perpendicular to each other, the robot can construct the map shown in Fig. 9.11b. When it encounters a sharp turn, it understands that the turn is a 90â¦ corner where two walls meet, so its mapping of the angles will be correct. There will also be an error when measuring the lengths of the walls and this can lead to the gap shown in the figure between the first and last walls. The figure shows a small gap which would not be important, but if the robot is mapping a large area, the problem of closing a loop in a map is hard to solve because the robot has only a local view of the environment. Consider a robotic lawnmower given the task of mowing a lawn by moving back and forth; it has to close the loop by returning to its charging station (Fig. 9.12). It is not possible to implement this behavior using odometry alone, since small errors in velocity and heading lead to large errors in the position of the robot. It is highly unlikely that through odometry alone the robot will mow the entire surface of the lawn and return to its charging station. Landmarks such as signaling cables in the ground need to be used to close the loop."
214,486,0.983,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"One way of describing the philosophy of a climate model is that a global climate model bounds each and every process by physical laws, starting from the conservation of energy and mass. From this constrained set of budget equations, combined with different representations of the processes (like condensation in clouds), complex results emerge. But these results have to be compatible with the physical laws (like conservation of mass, or the equations governing fluid flow on a rotating sphere). The emergent complexity is a reflection of reality. The physical laws behind climate models are well known and observed. The most recent ânewâ theories are well over 100 years old. They are also the same physical laws that govern many other ï¬elds of science and engineering. The description of the motion of fluids in the atmosphere and ocean are the same equations used to build numerical models of how an airplane will perform. The equations that govern the flow of energy in the climate system from the sun, through the atmosphere to the earth, and then back are the same equations describing how cellular phones and radios work."
58,383,0.983,Enabling Things to Talk,"It is important to understand that the IoT Domain Model is not attempting to be a domain model for all types of ICT systems. Rather, it focuses on the IoT-specific parts. When modelling a complete system, many of the aspects to be covered are not IoT-specific. For these aspects, the IoT Domain Model will provide only little help. For example, the Service concept in the Domain Model is primarily focused on modelling IoT Services that directly or indirectly expose Resources; however, the Service concept also can be used to provide a link to general services in the ICT domain."
244,724,0.983,"Advancing Human Assessment : The Methodological, Psychological and Policy Contributions of Ets","Within a predictor-criterion framework, the previous two sections have focused on the formerâindividual differences in creative ability as reflected in performance on tests purportedly related to creativity on analogical or theoretical grounds. In some cases, various creativity criteria were available, making it possible to examine the concurrent or predictive validity of the creativity tests. Such research is informative about whether the creativity or scientific thinking label applied to the test is in fact warranted. In the present section, the focus is on the creative product itself. In some cases, investigators seek possible associations between the judged creativity of the product and the demographic or psychological characteristics of the individual who produced the product. In such instances, the predictor-to-criterion sequence is actually reversed. Study of creative products can take two forms. The most direct form involves the evaluation of a concrete product for its creativity. A second and somewhat less direct form relies on test-takersâ self-reports. The test taker is asked to describe his or her activities and accomplishments that may reflect different kinds of creative production, concrete or abstract, in such domains as science, literature, visual arts, and"
25,23,0.983,Teaching and Learning About Whole Numbers in Primary School,"(3) Appropriateness of cognitive elements that sustain the solution process: Researchers who adopt this approach do not share the idea of choice between different solution paths (see, e.g., Threlfall 2002, 2009; Rathgeb-Schnierer 2010; Rathgeb-Schnierer and Green 2013, 2015). Rather than the choice of the most suitable solution or the quickest way of obtaining a solution, appropriateness is conceived as a match between the combination of strategic means and the recognition of number patterns and relations of a given problem during the computation process. The recognition of number patterns and relations and their use in solving a computation depends on a studentâs knowledge of numbers and operations. In this sense, flexibility in mental calculation can be considered as an âinteraction between noticing and knowledgeâ (Threlfall 2002, p. 29). All three approaches to mental flexibility can be linked to the model proposed for the process of calculation presented in Fig. 1. The ï¬rst two approaches focus predominantly on a single domain of the calculation process: either the domain methods of calculation or the domain tools for solution. Within the third approach, researchers also take the cognitive elements into account and therefore focus on two different domains to identify the degree of flexibility in studentsâ mental arithmetic: solution methods and the cognitive elements that sustain the solution processes. Consequently, evidence of flexibility in mental calculation can exist only if the tools for solution are linked in a dynamic way to problem characteristics, number patterns, and relationships. Different methods have been used in the study of flexibility in mental calculation, some of which, such as the choice/no-choice method, borrowed from other domains of research where decision making is involved. We single out the choice/no-choice method for description here, as it may lead to other uses of the method in different contexts in mathematics education. The paradigm is simple: in the choice condition, the participants are allowed to solve the task in whichever way they desire. In the no-choice condition, participants are shown a demonstration of the method they should use in the task. Using this methodology, Verschaffel and his colleagues have been able to demonstrate that some of the assumptions made about what is the best strategy for some tasks might be questionable. When students are taught different ways of approaching subtraction, for example, they are encouraged to use subtraction by complementary addition in tasks where the difference between the minuend and the subtrahend is small; it is assumed that in tasks where the difference between the minuend and the subtrahend is large, direct subtraction is more efï¬cient. However, using the choice/no-choice method, they observed that children (Verschaffel et al. 2016) as well as adults (Torbeyns et al. 2009c, 2011) are faster and more accurate when they use complementary addition as the route to solving subtraction problems, irrespective of whether the difference between the minuend and the subtrahend is large or small. Based on different deï¬nitions of flexibility and on different methods, researchers have examined different elements of flexibility in mental addition and subtraction and reported a variety of results:"
128,337,0.983,Solidarity in Europe : Citizens' Responses in Times of Crisis,"characterised by its own, typical repertoire of solidarity. Individual forms of action prevail in the field of disability and, in relative terms, also in the field of refugees (in the latter case, in an overall context of much lower solidarity). By contrast, collective forms of action prevail in the field of unemployment. These first results concerning the amount of solidarity expressed and its repertoire certainly go some way to confirm our expectations. Solidarity expresses itself most strongly in the field of disability, since it brings together both of the trajectories that connect French citizens to the disabled. On the one hand, emphatic feelings of self-identification and proximityâwhich express themselves most strongly in the case of people who do not enjoy the same good health as the majority of the populationâgo hand in hand with a high prevalence of individual forms of solidarity actions. On the other hand, since the disabled are themselves citizens, it is logical that their problems should also be seen as issues that concern the whole republican community, hence the high amount of collective forms of solidarity action. Solidarity actions are less common in the field of unemployment. This is also in line with our theoretical framework, since our expectation was that feelings of empathy would not play the same role as in the case of the disabled. Just as expected, the high prevalence of collective types of solidarity actions indicates that unemployment in France tends to be seen as a political matter rather than as an individual one and that the whole republican community is held accountable for the problem. Lastly, solidarity is at its weakest in the case of refugees, which further serves to reinforce our argument. Since their status means that they stand outside the republican community, refugees only benefit from the type of solidarity that springs from a general empathy with human suffering and an ability to see all people as fellow human beings (yet, as said, a much more distant community of equals than fellow citizens). We can now move on to consider the respective impacts of the individual trajectory and of the political one. Indeed, the results in this section suggest that the links between these trajectories and solidarity actions deserve additional scrutiny. We must therefore pay closer attention to the specific assumptions underpinning these expectations, namely, that each of these two trajectories represents significant predictors of solidarity action in the field of disability, while only one of the two functions as a significant predictor in the other two fields (the political trajectory in the case of the unemployed and the individual trajectory for the disabled)."
244,359,0.983,"Advancing Human Assessment : The Methodological, Psychological and Policy Contributions of Ets","summarized in four pages by Linn and Werts (1971). One of the quandaries faced by researchers that was not noted in this 1971 study is that some of the variables that contribute to prediction are variables over which a test taker has little control, such as gender, race, parentâs level of education and income, and even zip code. Use of variables such as zip code to predict grades in an attempt to eliminate differential prediction would be unfair. Linn (1975) later noted that differential prediction analyses should be preferred to differential validity studies because differences in predictor or criterion variability can produce differential validity even when the prediction model is fair. Differential prediction analyses examine whether the same prediction models hold across different groups. Fair prediction or selection requires invariance of prediction equations across groups, R (Y |X,G = 1) = R (Y |X,G = 2 ) = â¦ = R (Y |X,G = g ) , where R is the symbol for the function used to predict Y, the criterion score, from X, the predictor. G is a variable indicating subgroup membership. Petersen and Novick (1976) compared several models for assessing fair selection, including the regression model (Cleary 1968), the constant ratio model (Thorndike 1971), the conditional probability model (Cole 1973), and the constant probability model (Linn 1973) in the lead article in a special issue of the Journal of Educational Measurement dedicated to the topic of fair selection. They demonstrated that the regression, or Cleary, model, which is a differential prediction model, was a preferred model from a logical perspective in that it was consistent with its converse (i.e., fair selection of applicants was consistent with fair rejection of applicants). In essence, the Cleary model examines whether the regression of the criterion onto the predictor space is invariant across subpopulations. Linn (1976) in his discussion of the Petersen and Novick (1976) analyses noted that the quest to achieve fair prediction is hampered by the fact that the criterion in many studies may itself be unfairly measured. Even when the correct equation is correctly specified and the criterion is measured well in the full population, invariance may not hold in subpopulations because of selection effects. Linn (1983) described how predictive bias may be an artifact of selection procedures. Linn used a simple case to illustrate his point. He posited that a single predictor X and linear model were needed to predict Y in the full population P. To paraphrase his argument, assume that a very large sample is drawn from P based on a selection variable U that might depend on X in a linear way. Errors in the prediction of Y from X and U from X are thus also linearly related because of their mutual dependence on X. Linn showed that the sample regression for the selected sample, R (Y|X, G) equals the regression in the full unselected population if the correlation between X and U is zero, or if errors in prediction of Y from X and U from X are uncorrelated. Myers (1975) criticized the regression model because regression effects can produce differences in intercepts when two groups differ on X and Y and the predictor is unreliable, a point noted by Linn and Werts (1971). Myers argued for a linking or scaling model for assessing fairness. He noted that his approach made sense when"
217,822,0.983,Finite Difference Computing With Pdes : a Modern Software Approach,"4.1.1 Simplest Scheme: Forward in Time, Centered in Space Method A first attempt to solve a PDE like (4.1) will normally be to look for a time-discretization scheme that is explicit so we avoid solving systems of linear equations. In space, we anticipate that centered differences are most accurate and therefore best. These two arguments lead us to a Forward Euler scheme in time and centered differences in space: ÅD tC u C vD2x u D 0Âni : Written out, we see that this expression implies that unC1 D un  C.uniC1  uni1 /; with C as the Courant number C D"
153,212,0.983,Solving Pdes in Python : The Fenics Tutorial I,"Note that u[0] is not really a Function object, but merely a symbolic expression, just like grad(u) in FEniCS is a symbolic expression and not a Function representing the gradient. This means that u_1, u_2, u_3 can be used in a variational problem, but cannot be used for plotting or postprocessing. To access the components of u for plotting and saving the solution to file, we need to use a different variant of the split function: u_1_, u_2_, u_3_ = u.split()"
375,47,0.983,Musical Haptics,"a control engineering perspective, the body-/instrument-coupled dynamics form an inner feedback loop; the dynamics of this inner loop are to be driven by an outer loop encompassing the playerâs central nervous system. This new perspective becomes a call to arms for the design of digital musical instruments. It places a focus on the haptic feedback available from an instrument, the role of energy storage and return in the mechanical dynamics of the instrument interface, and the possibilities for control of fast dynamic processes otherwise precluded by the use of feedback with loop delay. This perspective also provides a new scaffold for thought on learning and skill acquisition, as we have only briefly explored. When approached from this perspective, skill acquisition is about refining control of oneâs own body, as extended by the musical instrument through dynamic coupling. Increasing skill becomes a question of refining control or generalizing previously acquired skills. Thus, soft-assembly of skill can contribute to the understanding of learning to play instruments that express musical ideas. The open question remains: what role does the playerâs perception of the coupled dynamics play in the process of becoming a skilled performer? Answering this question will require us to step inside the coupled dynamics of the player/instrument system. With the advent of new methods for on-body sensing of fine motor actions and new methods for embedding sensors in smart materials, the capacity to perform such observations is now within reach."
166,38,0.983,Surveying Human Vulnerabilities across the Life Course (Volume 3.0),"4.2 Interdisciplinarity and the Need for Transparency At the start of this chapter we noted two defining features of the LIVES research programme besides the shared substantive goal of investigating the experience of vulnerability at different stages and key transitions of the life course. These features were its interdisciplinarity â both within and across the various individual projects involved â and a predominant emphasis on quantitative methods. Yet while the overriding methodological approach of the LIVES projects relies on survey data collection, variations in implementation can be observed as a result of deeply entrenched, unique methodological traditions of the different disciplines involved. Each discipline has its own toolbox, which is not purely technical but also acts as a lens through which to look at social phenomena, and to produce, manipulate and interpret data. Such âtraditionsâ are apparent when reading different journals,3 but most of the time they manifest themselves as strong but implicit rules governing the conduct of research within a given scientific field. In this book, as in the LIVES project more generally, there has been no attempt to obviate this reality. As a result of varied disciplinary priorities, the various projects described have inevitably paid more or less attention to different sources of error in their data, or emphasised different notions of quality over others. One example of this concerns measurement, where it is not just that the questions asked of respondents in different disciplines can differ, but also that the priorities, in terms of design, can vary: for example, psychologists tend to privilege âvalidatedâ scales, often long, while sociologists tend to favour shorter multi-item measures. Similarly, the way in which household income is considered will often be different in economic models where exact values are seen as important, while in other"
164,53,0.983,"Marginality : Addressing the Nexus of Poverty, Exclusion and Ecology","factors underlying poverty (Meinzen-Dick et al. 2004; von Braun et al. 2009, 44). These shortcomings led us to the development of a more inclusive and interdisciplinary research framework, that of marginality. Kant (1819) noted that a concept is a general representation that is common to several specific objects. Accordingly, the concept of marginality is an abstraction of the idea that the causal complexities underlying peopleâs living conditions interact in ways that are at systemic margins. These conditions are far from what would be considered optimal, in balance, just, equal, sufficient, good, or fairâattributes that describe conditions and positions in human life that are enabling and supportive, and that are used to define poverty. Despite the critiques of marginality in social science (Cullen and Pretes 2000; Del Pilar and Udasco 2004), the persistence of marginality as a concept (Dickie-Clark 1966) should be regarded as an indication of the demand to express observations of a similar kind across different epistemic cultures (Knorr Cetina 1999), and to find solutions, here, to the phenomena of poverty. The problem of measuring the degree of marginalization is that the reference is not fixed or unknown, and therefore when used as a theory marginality has been criticized for the lack of construct validity (Del Pilar and Udasco 2004, 11). In their critique, however, those authors reviewed the use of marginality as a theory and came to the conclusion that âmarginality cannot work [as a theory] if it has multiple levels of meaning.â This critique of the marginality concept as a theory also rests on the belief that a concept must be a âuniform kind of mental representationâ (Weiskopf 2009, 145). Weiskopf rejects this assumption in psychology and outlines a pluralist theory of concepts in which they are constituted by multiple representational kinds. In the following we present a framework for the investigation of marginalityânot a theory of marginality. Although poverty can be observed in many different forms and is caused by many different factors, all forms of poverty can be described through the concept of marginality. Someone who is poor will always be marginalized in one or more dimensions, whereas the socio-cultural context and individual perception will define in which and in how many dimensions someone needs to be marginalized in order to be considered poor. The aim of establishing a concept of marginality is therefore to better understand the various causal complexities of poverty by deepening and broadening the scope of scientific investigation through: 1. identifying common causalities of poverty across scientific disciplines, and 2. including phenomena that are not typically considered as poverty or contributing to poverty alone (e.g., living in harsh or resource scarce environments). Deepening and broadening the scope of investigation thereby also includes incorporating theories and models from other (non-social science) epistemic cultures and scientific disciplines. In that sense marginality is not only a concept, but also a conceptual framework. It is a framework for different theories of poverty within which various models can be tested. Frameworks. Theories and models are understood as ânested set[s] of theoretical concepts, which range from the most general to the most detailed types of"
232,43,0.983,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"2 Industrial Degradation, Causal Links and Preventive Maintenance The degradation of an environment can be seen from outside the subject, i.e. as an object. In this case, the issue of loss has no emotional connotations for the person who observes it. If we consider the consequences of the nuclear accident at Fukushima Daiichi, there are many reasons for this indifference: lack of information, mistrust regarding its content or source, geographical remoteness, few signs of"
321,61,0.983,Beyond the Limits to Growth : New Ideas for Sustainability from Japan,"Haber and Carl Bosch received Nobel Prizes. Because it became possible to synthesize ammonia using an iron catalyst, the creative power of catalysts to drive chemical reactions was born. Until that time, production of nitrogen fertilizers had relied on human and animal excreta and other sources. Thanks to the synthesis of ammonia, it became possible to synthesize nitrogen fertilizer, which created the public value of a leap in agricultural production. It also created economic value in the sense that the chemical industry emerged to produce ammonia and yielded huge profits. The synthesis of ammonia was linked to intellectual, public, and economic values in a simple manner. In 1928, Alexander Fleming discovered penicillin from Penicillium notatum (Fleming 1945). It was put to practical use through the ârediscovery of penicillinâ by Howard Florey and E.B. Chain, and radically changed the clinical treatment of infectious diseases. For this result, those threeâFleming, Florey, and Chainâwere awarded Nobel Prizes. The discovery of penicillin created the intellectual value of antibiotics, the public value of saving humanity from suppuration, and the economic value of the development of the pharmaceutical industry. Similarly, the synthesis of nylon by Wallace Carothers at Dupont in the U.S., the invention of the three-terminal transistor by William Shockley, John Bardeen, and Walter Brattain at AT&Tâs Bell Laboratories, and the invention of the integrated circuit (IC) by Jack Kilby at Texas Instruments (TI) created new values in a simple and clear manner. As science developed and became compartmentalized into numerous specialized areas, the distance between human values and science gradually increased. This is easier to understand if you compare two Nobel Prize winners: Hideki Yukawa and Masatoshi Koshiba. Hideki Yukawa clarified the mechanism of the nuclear force (Yukawa 1949). An atomic nucleus, which is at the center of an atom, consists of protons, which have a positive electric charge, and neutrons, which possess no charge. It was a mystery why many protons can remain bound. This was Yukawaâs question. Because they have the same positive charge, they should repel each other and scatter, yet they are bound as a nucleus. Yukawa predicted by using a mathematical model that, if there are entities called âmesons,â positively charged protons can stay bound through these intermediaries. This was subsequently proved by an experiment, and he received the Nobel Prize. Yukawa can even be said to have created a pattern of research for elementary particle theory, and his work ranks at the highest level among those of Nobel Prize winners. On the other hand, Masatoshi Koshiba received the Nobel Prize for his work on neutrinos (Koshiba 2002). Studies of neutrinos have recently been at the center of physics, and I think his work is also at the highest level. It is not as easy to understand why neutrino research has become a central area in physics as it is to understand the story of the nuclear force. I suspect that most people cannot understand the former. Although both Nobel Prize-winning works were similarly at the highest level, they were very different in terms of their distance from everyday human intuition. The creation of penicillin made it possible to cure many infectious diseases"
124,200,0.983,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"There remains the possibility that for some examples, as with external counterfactuals, the relevant degree of similarity between histories within a tree might take some other form than simple distance in time back to their nearest common moment. One notorious example which appears to be of this type arises if I accidentally leave my coat behind in the cloakroom at the close of a conference session, and return the next day to find it still there.14 Knowing that there were dubious characters in the neighborhood when I left, and that many individuals had access to that cloakroom during my absence, I would reject as false the sentence: If my coat had been stolen, it would have been the most recent person to visit the cloakroom who would have stolen it. No doubt the first really dubious character who came by after I left was likely to take my coat, pre-empting any opportunity that the most recent nefarious visitor might have had. For such an example, it is difficult to say what the relevant sense of similarity between histories might be, but it seems clear nonetheless that no comparison with histories from other worlds is particularly apt. For external counterfactuals, we have a choice: we could base our account on a similarity relation between worlds or on a similarity relation between points of evaluation. For a sentence such as If the match were struck, it would light it might be most appropriate to compare moment/history pairs (without regard to what world they were in) and focus on the ones whose factual conditions were most similar to those at the point of evaluation. On the other hand, for Lewisâs example If kangaroos didnâ² t have tails, they would fall over backwards it might be best to refer to similar worlds, particularly since there are probably no suitably similar moment/history pairs in our world at which kangaroos lack tails, and it would take a very different world to include such situations in a coherent way. The moral of all this rumination about supermodels is that they open up considerable new prospects for exploration and exploitation. We now begin to glimpse the plausibility of supposing, for example, that different kinds of counterfactuals call for different accounts, and we see that supermodels might provide an environment friendly to such fine-grained discriminations. Moreover, it is not unreasonable to suppose something similar might be the case with accounts of causation, particularly if we suppose that counterfactuals play a major role in, or are in some other way closely connected with, an account of causation."
294,23,0.983,Programming For Computations - Python : a Gentle introduction To Numerical Simulations With Python (Volume 15.0),"Although Python is responsible for reading and understanding your program, it is of fundamental importance that you fully understand the program yourself. You have to know the implication of every instruction in the program and be able to figure out the consequences of the instructions. In other words, you must be able to play the role of a computer. The reason for this strong demand of knowledge is that errors unavoidably, and quite often, will be committed in the program text, and to track down these errors, you have to simulate what the computer does with the program. Next, we shall explain all the text in ball.py in full detail. When you run your program in Python, it will interpret the text in your file line by line, from the top, reading each line from left to right. The first line it reads is # Program for computing the height of a ball in vertical motion."
163,43,0.983,a Fair Share of Tax : a Fiscal Anthropology of Contemporary Sweden,"Moving our scrutiny to contemporary Italy, more specifically amongst the popolino in Naples,10 the recipient of many gifts is considered privileged because status in society increases with gifts (Pardo 1996: 154). Many gifts imply that the donor has resources that has been or will be used. There is a fine line between this position and receiving too much without reciprocating: if society considers the reciprocal action to be too slow or not reaching expected levels, esteem will turn into contempt. The recipientâs superiority as net provider is no more; instead the reputation of a net receiver is acquired and a status of dependence manifests itself (Cuco i Giner 2000: 315). Pure charity hurts a recipientâs pride and his/her status in society diminishes if there is no possibility of reciprocating the gift (Mauss 2002 [1990]: 83). Politics of welfare can simultaneously foster solidarity between societyâs members but also exclude net receivers, in their own and otherâs view, as they have no means of reciprocating (Komter 1996: 7). But what is it that creates the relationship? Is it what is given or who the giver is? Mauss described the reciprocal relation created by the gift as a spirit, the hau, which he borrows from the Maori in New Zealand. This spirit of the gift-giver resides within the thing given and stays with it until reciprocated; âto accept something from someone is to accept something of his spiritual essence, of his soulâ (Mauss 2016: 73). This was criticized by Raymond Firth, who instead showed that this spirit was within the gift given, not with the person who gave it (Firth 1959). There are thus various views of who carries the spirit; whether it is the something that is given which is of importance, or the person who gives it. The most common exchange in modern society is the market exchange.11 Both Mauss and Marshall Sahlins in his study of stone-age economics (see below) left this out of their analysis, and the existence of a market transaction in primitive societies is contested. A market transaction was originally defined as being exempt from creating reciprocal relations; it was supposed to be a spot exchange performed through profit maximizing and by unsocial actors (Swedberg 2003), but has in later studies been shown to result in different types of reciprocal relations as well (e.g. Befu 1977; Davis 1992; Offer 1997). I will delve into the market transaction in what follows as it plays a fundamental role in taxation. It is market exchanges that first comes to mind when we think about exchanges subject to tax. The modern market trade has money as a means of settlement in return for a product. The exchange is usually based on a predefined price. The"
269,47,0.983,Rethinking Interdisciplinarity across the Social Sciences and Neurosciences,"wanted to have any such impact.) The advice? We should âbe much more direct about what it is that [we] mean to sayâ. Neuroscientists and experimentalists would, in fact, warm to âa list of facts or statements separated by periods with the occasional transition term to hold it all togetherâ; at the moment, it was hard to â[extract] the message out of what seemed like an artistically-written piece that made use of many unconventional phrases and secondary definitions of otherwise common wordsâ. The second reviewer came from quite a different angle â arguing that there was âsomething peculiarly paradoxical in [the paperâs] very conceptâ. We, the authors, were trying to set ourselves up as âexperimentally, experientially, and intellectually outside of the âblack boxâ â, but in actual fact, we were held captive by âthe tools of cognitive neuroscience, i.e., on a scientific apparatusâ. No matter the âdepth and authenticity of the joint collaborative exerciseâ for which we were advocating, we were pinioned by the inescapability and irreducibility of the scientific laboratory. For this reviewer, the point of âthis âfourth wayâ of neuro-engagementâ was far from clear. For the scientist, the rhetorical style in which the âartistically-writtenâ paper was written obviated the chance that its arguments would be received by those coming from different, scientific, traditions of research writing. For the second reviewer, whom we assume to be a social scientist or humanities scholar, it conceded far too much to the sciences. This is a fairly consistent refrain. And yet, in general the two of us, as well as many of our collaborators, have been quite successful at publishing significantly interdisciplinary papers in relatively high impact journals â a success that we attribute at least in part to such papers having a certain currency among editorial board members at some journals. We do not want to prejudge whether this enthusiasm is necessarily shared by the editorial board members of many mainstream neuroscience journals. And for the collaborating neuroscientist, being published in a high impact social science journal, which may have both a relatively long time from submission to date of publication, and a relatively low âimpact factorâ compared to even mid-ranking journals in some biological and medical fields, might represent poor return for her labour. (Letâs note at this point that while the 2014 impact factor of the leading journal Nature Reviews Neuroscience was 31.427, that of Social Science & Medicine (a very highly regarded social science journal) was 2.890.) There does appear to be a growing appetite among clinically-oriented journals for interdisciplinary research that crosses the life sciences, social sciences, and humanities. (e.g. The Hearing the Voice research team, of"
307,51,0.983,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"which clearly increases as a function of the mutation severity index . It is also interesting to observe that, for this mutation, the mean open time is unchanged. We will refer to a mutation of this form as a CO-mutation and we will show repeatedly that, for CO-mutations, closed state blockers are theoretically optimal."
217,812,0.983,Finite Difference Computing With Pdes : a Modern Software Approach,"Discuss when the scalings in a) and b) are appropriate. c) One aim with scaling is to get a solution that lies in the interval Å1; 1Â. This is not always the case when uc is based on a scale involving a source term, as we do in a) and b). However, from the scaled PDE we realize that if we replace fN with Ä± fN, where Ä± is a dimensionless factor, this corresponds to replacing uc by uc =Ä±. So, if we observe that uN  1=Ä± in simulations, we can just replace fN by Ä± fN in the scaled PDE. Use this trick and implement the two scaled models. Reuse software for the diffusion equation (e.g., the solver function in diffu1D_vc.py). Make a function run(gamma, beta=10, delta=40, scaling=1, animate=False)"
271,492,0.983,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"whole forms part of a larger figuration. For figurations of figurations to come into existence and function in a meaningful way, mediated communication and a shared media ensemble are crucial (Couldry and Hepp 2017: 73). A second principle that allows figurations of figurations to exist is a meaningful arrangement in the Weberian sense. More specifically, as Couldry and Hepp (2017: 74) explain, figurations of figurations are based upon and within âcertain discourses that connect these figurations and their meaning in the social world, and certain larger scale relations of interdependency between domains of action [â¦] that come to be associated with assumed relations of meaningâ (emphasis in the original). Going back to the example of the transnational company with offices in Berlin, Amsterdam and Singapore, it might be useful to conceptualize this type of organization as a figuration of figurations. In ethnographic studies, the researched is commonly referred to as âthe fieldâ (NÃ¦ss 2016). Several scholars have pointed towards the difficulties of defining where the field begins and where it endsâespecially in an increasingly complex social and largely mediated world (for example Lohmeier 2014; Lohmeier in press; Mitchell 2012). Hans Erik NÃ¦ss (2016: para. 2) emphasizes this point when he writes: In contrast to the conventional view on the field as a territorial unit, [I argue that it] should be seen as composed of several sites, processes and relations â sometimes far from each other geographically and connected with each other in different ways, on different scales and with different intensity. A field consequently, is where the phenomenon can be said to exist. Sites are localities where you can investigate the processes, actions and relations within this phenomenon ethnographically."
87,151,0.983,"Bioeconomy : Shaping The Transition To a Sustainable, Biobased Economy","symbols together form the cultural system. Thus, values, beliefs and symbols must be considered in the analysis of social action situations. Referring to our former discussion, one could say that the cultural system is the basis for information flows and communication process in social systems. Like the social system, the cultural system provides comparatively abstract structures that from the perspective of the individual may appear as given. While social structures provide institutions, Parsons calls cultural structures of symbolic signification generalised media of interaction. The prototype and most highly developed example of generalised media of social interaction is language. Parsons argues that social action situations can be seen as (action) systems, in which the personal, the social and the cultural systems are tied together and interpenetrate each other. At a later stage, he added the biological organism as a fourth system. All systems shape action situations by providing orientations (motivations, normative expectations, values, instincts) as well as structures (abilities/resources, rules, media, physical conditions). Social Systems as Communication Situations While Parsons developed his systems theory starting from the analysis of social action situations, the German sociologist and systems thinker Niklas Luhmann (1927â1998) has shifted the perspective to the analysis of the reproduction of social systems (Luhmann 2013). One could say, while Parsons is focussing on the single acts and social organisations at a given point in time, Luhmann is interested in the perpetuation and continuation of social processes in the flow of time. Central to his analysis is the connectivity of events. Rather than to ask how systems shape actions, he asks how systems emerge out of individual acts. Thus, his concern is less about the person that acts but more about the other actors that observe, interpret the act and may react or do not react. Accordingly, the central element of systems is not action but communication."
49,236,0.983,Artificial Intelligence and Cognitive Science IV,"Figure 22. A diagrammatic outline of relationship between connectionism and symbolism in interpretations of cognitive activities of human brain. An interpretative efficiency of this diagram consists in theorems proved by S. Kleene and N. Chomsky [2,8], which state that each neural network is equivalent to a finite state machine, and vice versa. It means that going from bottom to up, we look for symbolic correlates of neural activities. Reversely, going from up to bottom, we look for connectionist correlates for symbolic notions. Henceforth, we may say that between connectionist and symbolic approaches for a study of cognitive activities there doesnât exist an exclusive disjunction. The main criterion of inclusion of the first or second approach consists in an effectiveness and easiness of study of cognitive activities. Recently, there is used a compromise solution that higher level activities are considered on symbolic level (though there exist good connectionist models), whereas low level cognitive activities are considered on connectionist level. For completeness, we mention that D. Gabbay published a seminal book Neural-Symbolic Cognitive Reasoning (Springer, 20008) in which he and his coworkers demonstrated connectionist approaches based on neural networks for a study of logical reasoning."
186,186,0.983,Dignity in The 21St Century : Middle East and West,"However, if we see freedom and rights based on the principles of tawheed, we can sense that freedom; it is a freedom that the intellect feels at the moment of creativity, at the moment we become one with intelligent life. This is why any ideas which are devoid of force become âfreeâ. Thus, not only can freedom not be limited; it furthermore removes limits and boundaries; it is not the limit of anotherâs freedom, but its extension. What are the freedoms that Godâs generosity has given us?"
214,501,0.983,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"conserves energy and mass, then the energy from the sun put into the system has to go somewhere. Most of the energy escapes again, but if mass is conserved, then the difference between the energy into the system and out represents energy available in the system. Figuring out where the energy goes is complex, but it is necessary to make sure energy is conserved. This also allows us to move âoff scaleâ of current energy inputs and have some conï¬dence that we are not accidentally gaining or losing energy in the simulated climate system. The concept of evaluation and the energy and mass constraints can also be used to describe how a climate model is able to represent the complex earth system with complex interactions of processes occurring on many scales. If each process or parameterization or set of processes (such as a cloud model, or a biogeophysical model of how plants move water and carbon) can be evaluated against observations, and also is bounded by physical constraints, then the resulting combination of these processes should be able to represent important features of the climate system. What does this basic physical constraint mean? For a cloud model (or cloud parameterization in an atmosphere component of a climate model), there are a series of descriptions of evaporation, formation of cloud drops, how rain begins to fall, freezing, and the like. But the overall cloud can have only as much water as is available to condense, and the energy of that condensation and/or evaporation has to go somewhere. These constraints act at every point in space and time in a model, and require all clouds in a model to meet these constraints and be physically realistic. Add up many processes pushing and pulling on the system, and climate models actually do a pretty good job of getting a decent climate for the present based on detailed comparisons to observations. The constraints of energy and mass also allow for some conï¬dence in prediction. Another method of evaluation is to use a climate model with appropriate initial conditions to simulate individual weather events. Many models are moving to âuniï¬edâ weather and climate models for this reason (see below)."
234,226,0.983,Mobile Professional Voluntarism and international Development : Killing Me Softly?,"of tacit knowledge leads to the identiï¬cation of a âknowledge deï¬citâ encouraging uni-directional ï¬ows (from the host to the LMIC or from doctors to midwives). We are not adding skills to context (as the COM-B model implies): context lies at the heart of complex ï¬uid and, oftentimes confounding knowledge combinations. Or, as McCormack puts it: ânothing exists and can be understood in isolation from its contextâ (2015: 3009). On the basis of our research and learning, we would argue that far from a skills deï¬cit we are in a situation of knowledge saturation; clinical skills/ information are not the primary problem and using clinical âexpertsâ as conduits for yet more skills-forcing interventions is neither efï¬cient nor effective. To the contrary, it is both arrogant and wasteful. This arrogance does not derive solely (although this certainly does contribute) to the imposition of âexpert informationâ from the global North: from wellmeaning but narrow thinking foreign clinical experts and âdonorsâ. It is the consequence of a failure to see the bigger picture â of narrowing disciplinary knowledge paradigms. Although the lack of patient management is recognised as an issue by all UK clinicians working in Uganda: it was a visiting Ugandan obstetrician who proposed the initiative to introduce the African Maternal Early Warning Score (AMEWS) into Mulago Hospital. Enthralled by its apparent success in the UK (visiting foreigners are often not exposed to or fail to see the fundamental weaknesses of UK systems) she quite understandably attempted a âpolicy transferâ approach: she âsawâ a clinical solution to a clinical problem and tried to transfer it. And as a project we sought to support her in this without comprehending the localised knowledge that would ultimately render it unsuccessful. Knowledge can be both enabling and disabling; it is not only a deï¬cit of skills/knowledge that hampers progress; narrow siloed knowledge (focused on harvesting facts) can limit our ability to see the social world and the truth that lies behind interventions. Resistance to change may be fuelled by knowledge and often quite entrepreneurial tacit localised knowledge. From an individual perspective â how to make ends meet and sustain your family â or, from a systems perspective â how to organise and funnel the rewards from systemic forms of corruption â requires a level of localised tacit knowledge and rational/entrepreneurial decision making that foreign volunteers may fail to âseeâ. The overall conclusion of this book, based on intense multi-method action-oriented research over a period of 8 years, is that international development initiatives in the form of Health Partnerships and through"
360,15,0.983,Compositionality and Concepts in Linguistics and Psychology,"how this part of the cortex responds to conceptual combination. The greater the degree to which the modiï¬er leads to specialization of the head noun, the more powerful is the response in the LATL. Westerlund and PylkkÃ¤nen discuss different accounts of conceptual combinationâin particular schema based models that lead to modiï¬cation of the head noun schema (e.g. Smith et al. 1988), and relation based models that propose that meaning of a compound is generated by seeking an appropriate semantic relation to link the two nouns (e.g. a dog house is a house built for a dog). This work is connected with the chapters by Hampton, and by GagnÃ© et al., and the neurological data adds valuable criteria for how different accounts may be evaluated. Sassoon addresses a systematic difference between the way concepts are expressed by adjectives and nouns, which surfaces in the acceptability of their composition with comparative expressions. Many adjectives show their gradability with comparative forms like richer, more expensive or more American. By contrast, nouns do not easily allow constructions like more a millionaire, more an American, or more a duck. In agreement with the prototype theory of concepts, Sassoon proposes that both noun concepts and adjective concepts involve gradability. However, according to her proposal, only adjectives have their gradable aspects open for modiï¬cation by comparative operations (e.g. more or -er). Sassoon describes some empirical and preliminary experimental results supporting her views on the question of gradability and concept composition."
26,21,0.983,Cognitive Supervision for Robot-Assisted Minimally Invasive Laser Surgery,"The onset of carbonization is not easy to control, as it offers limited visual cues: the tissue blackening associated with it (Fig. 1.2) appears once damage has already occurred. To optimize medical outcomes, carbonization should be avoided [15], as it leads to a longer patient healing time and may leave scars, diminishing the quality of surgery [27]. From the simple scenario described above, it is apparent that the control of the incision process relies entirely on the experience and dexterity of the surgeon, who intrinsically establishes the state of the cutting and thereby decides the laser actions to be perform. The interaction between the laser and the tissue is the elemental building block at the core of laser-based surgery: it is through this interaction that incisions are performed. Unfortunately, nowadays there are no technical solutions for the automatic supervision of this process. To this end, a predictive model would be necessary, i.e. a model that allows to predict the outcome of the laser-tissue interaction process is needed. Analytical models of laser-tissue interactions (LTI) are well known [15]: these seem impractical to use in our scenario, as they admit a solution only under very strict assumptions. Furthermore, these models depend on a considerable number of variables representing properties of both the laser and the tissue (e.g. laser wavelength, absorption and scattering coefficients of tissue, tissue thermal conductivity, etc.), that are not straightforward to measure in a surgical setup. If a system is supposed to supervise the state of the LTI, it should rely on inputs similar to those used by surgeons and not on analytical models based on tissue properties. Accordingly, we propose to use models motivated by the capacity of humans to map and fuse diverse sets of information and infer the future state of events."
132,254,0.983,AiREAS: Sustainocracy for a Healthy City: Phase 3: Civilian Participation â Including the Global Health Deal Proposition,"But this collapse has only to do with our materialism, not our original free human spirit. As this happens, our wit and creativity in regard to life is once again stimulated, developing a movement for change that has already crossed over the chaos the materialistic focus, allowing us to let go of that while concentrating on life. The human spirit follows natural patterns between relaxation and stress, as we have seen in our medical research into heart rate variability. But our economy only follows the single pattern of relaxation and has no mechanism for stress other than its own collapse. To compensate for this, a dual economic system needs to be put in place, one of both transaction (relaxation) and transformation (stress), representing continuous interaction within a community that can keep it alive and progressive. The next comparison shows how the human body works and how the economy could work in the same manner."
166,208,0.983,Surveying Human Vulnerabilities across the Life Course (Volume 3.0),"high expressed emotions pattern with several criticisms toward his partner; the monologue began with the sentence: âShe was a girl I knew among others. I was taken by surprise, so my father told me that I made a mistake that I will pay to the bitter end. I didnât know her. I quickly saw that I could manipulate her, that I could rule her with a rod of iron.â The husband gave next some criticisms: âShe is quite impulsive. Her brother says she is unreliable. She sometimes deserves : : : .[silent, sigh], she freaks out.â This example illustrates the necessity to assess this construct with several methods as the results are not the same depending on the method that was used: according to the questionnaire, the woman was high on expressed emotion while the man was low; according to the situation of observation, the woman was low on expressed emotions and the man was high. This certainly refers to a certain degree of ambivalence in their relationship; the mixed-method approach allowed us to note that both partners are ambivalent while a single method would have led us to conclude that one partner only is high on emotion and thus âresponsibleâ for the negativity in the relation. Moreover, we would not have identified the same partner as the ânegative oneâ depending on the chosen method. The mixed method also allows a more complex and comprehensive evaluation of a situation. In fact, each method measures a different aspect of this situation and grasps its nuances and subtleties."
80,36,0.983,Innovations in Quantitative Risk Management (Volume 99.0),"In particular, in analogy with the unidimensional case, the mixture may potentially generate skewed and fat-tailed distributions, but when working with more than one asset this has the further implication that VaR is not guaranteed to be subadditive on the portfolio. Then the risk manager who wants to take into account SHP in such a setting should adopt a coherent measure like Expected Shortfall. A natural question at this stage is whether the adoption of a common SHP can add dependence to returns that are jointly Gaussian under deterministic calendar time, perhaps to the point of making extreme scenarios on the joint values of the random variables possible. Before answering this question, one needs to distinguish extreme behavior in the single variables and in their joint action in a multivariate setting. Extreme behavior on the single variables is modeled, for example, by heavy tails in the marginal distributions of the single variables. Extreme behavior in the dependence structure of, say, two random variables is achieved when the two random variables tend to take extreme values in the same direction together. This is called tail dependence, and one can have both upper tail dependence and lower tail dependence. More precisely, but still loosely speaking, tail dependence expresses the limiting proportion according to which the first variable exceeds a certain level given that the second variable has already exceeded that level. Tail dependence is technically defined through a limit, so that it is an asymptotic notion of dependence. For a formal definition we refer, for example, to [19]. âFiniteâ dependence, as opposed to tail, between two random variables is best expressed by rank correlation measures such as Kendallâs tau or Spearmanâs rho. We discuss tail dependence first. In case the returns of the portfolio assets are jointly Gaussian with correlations smaller than one, the adoption of a common random holding period for all assets does not add tail dependence, unless the commonly adopted random holding period has a distribution with power tails. Hence, if we want to rely on one of the random holding period distributions in our examples above to introduce upper and lower tail dependence in a multivariate distribution for the assets returns, we need to adopt a common random holding period for all assets that is Pareto or Inverse Gamma distributed. Exponentials, Lognormals, or discrete Bernoulli distributions would not work. This can be seen to follow from properties of the normal variance-mixture model, see for example [19], p. 212 and also Sect. 7.3.3. A more specific theorem that fits our setup is Theorem 5.3.8 in [23]. We can write it as follows with our notation. Proposition 3 (A common random holding period with less than power tails does not add tail dependence to jointly Gaussian returns) Assume the log returns to be Wti = ln Vti , with Vti the value at time t of the ith asset, i = 1, 2, where â Wt1 , Wt+h â Wt2"
232,332,0.983,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"2 The Management of Extreme Situations, Multiple Dilemmas Broadly speaking, the application of ethics to major accidents and the management of extreme situations leads to a focus on the relationship between people (both individuals and collectives) and these events. During the workshop, the concept of the victim (which makes the relationship between people and the accident explicit) emerged as a heuristic that united the various ethical questions that were raised. For instance, how should the public (who are all potential victims of major accidents) be taken into account, and involved in decisions that may impact them at some point? As major accidents unfold, how do we determine what truly helps victims, and what is in their best interest? Should such a âbest interestâ be acted upon, even if victims do not give their consent? Finally after an accident has occurred, who should be considered as a victim? How can victims be recognized as suchâand ultimately, be compensated for their loss, assuming that such compensation is possible? One important issue that was raised is that of a mediator who arbitrates between victims and the accident. Such a mediator is necessary to establish a relationship between people and what has happened to them. This relationship is a prerequisite for the evaluation of the post-accident situation and attempts to restore harmony, from which a new cycle can begin. The question becomes even more difï¬cult when one looks at non-human victims (i.e. nature), that have no voice to express the damage it has suffered and where it may not be possible to restore harmony. During the workshop, the Sorites paradox1 was used to illustrate the immense difï¬culty of giving an identity to victims of major accidents. This is not only a question of the number of victims: tens of thousands could be named. But, for example, who should be considered as a victim of the Chernobyl accident, where millions were exposed to very small doses of radiation? If someone lives in an area that was affected by radiation from Chernobyl and develops cancer, how can we determine whether s/he is a victim of the Chernobyl accident? This philosophical argument is supported by epidemiology, which highlights the difï¬culty of correlating radiation maps with actual damage to human health. Together, these issues question the ability of traditional models to shed heuristic and instrumental light on phenomena that are as complex as major accidents. The issue of traditional models was only one way in which the relevance of current scientiï¬c approaches to major accidents was questioned. Other issues concerned how to establish a relationship between scientists and the public, and how to make scientiï¬c knowledge available to less-expert audiences and include them in decisions that may ultimately disrupt their life. Consequently, public"
116,127,0.983,Moral Reasoning At Work : Rethinking Ethics in organizations,"workplace beyond ethics. Confirmation bias is the tendency we have to notice and seek information that confirms our beliefs, and to be inattentive to information that provides us with reasons to change our beliefs. The phenomenon is well documented in research (Nickerson, 1998), and produces formidable challenges in many professions. Police investigators can make up their minds about which person has committed a crime, and only pursue and notice information that confirms that conclusion. Teachers can have preconceptions about the intelligence and abilities of their pupils, and fail to see upward and downward spirals in their developments. Researchers can be so satisfied with their hypotheses and explanations of phenomena that they become blind to glaring counterevidence and reasons to revise them. In these and other professions, knowledge about confirmation bias is part of the professional training. This is nevertheless a pervasive decision-making trap, and one that emphasizes the need to have communication climates where colleagues look out for each other and intervene when someone at work stubbornly holds on to one belief or viewpoint rather than revises it in the light of new and relevant information. The other psychological phenomenon that can slow down a process of identifying and addressing morally relevant aspects of behaviour in an organization is the bystander effect. Research on human behaviour in real situations and in experiments show that the greater the number of bystanders to an event where somebody needs help, the less likely is it that any one of them will actually help (Darley and LatanÃ©, 1968; Hudson and Bruckman, 2004). One cause for this effect seems to be that we consider responsibility to help in a situation to be one unit that we share evenly with the other people at the scene. If we are one hundred bystanders to a critical situation, we seem unconsciously to split responsibility into one hundred tiny pieces, leaving each of us with one hundredth of a responsibility to intervene and help. That is a very small piece of responsibility. If we instead are fifty bystanders, the responsibility is double that of in the previous situation, but one fiftieth of a responsibility is still very little. This way of thinking is what Derek Parfit labels mistakes in moral mathematics. We do have individual responsibilities to help, no matter how many others are present. It is unreasonable to consider responsibility to be one cake we share evenly into thin slices. Each has his or her own cake of responsibility. Another cause for the bystander effect is that each of us tend to interpret the inactivity of the others as a sign that nothing serious is going on, DOI: 10.1057/9781137532619.0014"
346,296,0.983,"History Education and Conflict Transformation : Social Psychological Theories, History Teaching and Reconciliation","CONCLUSION: DILEMMATA OF âENLIGHTENEDâ TEACHING we started this chapter with the observation that societies that experienced a recent transition from a Soviet style to a western democratic style government provide a fruitful ground for observing the dilemmata of history teaching. Every new country and its government needs to justify and emphasise its newly found political orientation and foundational myth (Liu and Hilton 2005; wagner et al., forthcoming) as well as observe the tolerant âenlightenedâ perspective that accepts that other regions in the world have a right to their own evaluation of historical events, persons and notions in inter-generational transmission of identity and loyalty. This is particularly dilemmatic if, as in the Baltic states, there exists a considerable minority of Russian pre-transition immigrants who have their own historical values and perspectives. This institutional frame, together with the teachersâ interests, motivations and memories, makes navigating the âsea of historyâ in teaching fraught with risky cliffs. Under any preconditions, a really neutral dissemination of a relevant variety of facts and perspectives can only be achieved for a limited number of issues. Even if being modest and attempting to present some more relevant and well-known perspectives to the students, the twin problems of âlocationâ and âsufficiencyâ remain: finding a tentative balance and âlocation of opennessâ between the positions presented. Each studentâs perception of what is taught is idiosyncratic to a certain extent. A point at which a class has dealt âsufficientlyâ with a topic, that is at which point"
363,10,0.983,History and Cultural Memory in Neo-Victorian Fiction,"nineteenth century as the place to get off the train? What is it about the look of this past that appeals to the late-twentieth-century passenger?â (Green-Lewis, 2000: 30).3 Neo-Victorian fiction ensures that the Victorian period continues to exist as a series of afterimages, still visible, in altered forms, despite its irrevocable past-ness, its disappearance. They couple a contemporary scepticism about our ability to know the past with a strong sense of the pastâs inherence in the present, often in non-textual forms and repetitions. The neo-Victorian novels examined here expand âhistoryâ beyond textual, representational apparatuses, to include other, non-textual modes of memory and retrieval. These include oral histories, geographies, cartographies, paintings, photographs and bodies, all of which join diaries, letters, poems, novels and historical archives as means through which aspects of the past can be remembered and, often, repeated. Thus, while Frederick Holmes suggests that âthe [historical] novel emphasizes the efficacy of the imagination in providing us with provisional structures with which to make sense of the pastâ (Holmes, 1994: 331), I argue that, in many ways, these fictions are less concerned with making sense of the Victorian past, than with offering it as a cultural memory, to be re-membered, and imaginatively re-created, not revised or understood. They remember the period not only in the usual sense, of recollecting it, but also in the sense that they re-embody, that is, re-member, or reconstruct it. As we shall see, the dis(re)membered pieces of the past are reconstituted in and by the text, and also in the readerâs imagination. The reader thus literally embodies (re-members) the reimagined past. In History as an Art of Memory (1993), Patrick Hutton contends that today historians âspeak less of invoking the past, and more of using itâ (Hutton, 1993: xxii). I suggest that for contemporary novelists this is not true; novelists today are still interested in invoking the Victorian past. My analysis of the significance of the Victorian era for contemporary novelists and their readers begins by examining what it means to rework the past in fiction. Chapter 1 discusses the protean forms of history, fiction and historical fiction as tools for historical knowledge. It traces the reception of the historical novel from the late-eighteenth century to the present, exploring how the relationship between history and fiction has been constructed, and how this has impacted upon critical approaches to the historical novel. Its title, âMemory Textsâ, is taken from Gail Jonesâs reference to her novel Sixty Lights as a âmemory textâ ( Jones, 2005). It then turns to the recent critical interest in a more broadly conceived âhistorical imaginaryâ, which attempts to account for the multitude and variety of ways in which we think historically"
211,218,0.983,Entrepreneurial Cognition : Exploring The Mindset of Entrepreneurs,"Discipline folloWing open iDentity play The association between play and the formation of a new positive work identity is likely shaped by the degree to which the cognitive process includes disciplined imagination. Disciplined imagination denotes an evaluation and selection process in which individuals introduce discipline through the âconsistent application of selection criteria to trial-and-error thinkingâ and in which they trigger imagination through the âdeliberate diversity introduced into the problem statements, thought trials, and selection criteria that comprise that thinkingâ (Weick 1989: 516; see also Shepherd and Williams 2018). The construction of these aspects of disciplined imaginationâ namely, the problem descriptions, thought experiments, and criteria applied for evaluation and selectionâlikely influences a personâs ability to form conceivable outcomes. The outcome is a plausible new identity that is worth additional identity refinement and validation. Without forming a suitably plausible new identity, the individual is unlikely to engage in identity"
223,32,0.983,Knowledge and Action (Volume 9.0),"the key criterion for distinguishing between direct and mediated experiences and between face-to-face and mediated communication, the three main foci of this bookâaction, knowledge, and spaceâare conceptualized in a new framework, the socially constructed relations of space. The geographer Huib Ernste illustrates in his chapter that the divorce of rationality and reason during the philosophical development of modernity led to recognition of different types of rationality, each with its own logics of deliberation and argumentation. Poststructuralists emphasize that each rationality contains multiple paradigms, each establishing its own set of principles, institutions, and lines of conflict that need to be taken into account. He demonstrates how these views are intricately involved in late-modern geographical theories of action and in language-pragmatic approaches25 in geography. Proponents of poststructuralist approaches emphasize the structural aspects of discourse, especially power structures. Laclau and Mouffe (1985), by contrast, try to retain and restore the possibility of deliberative interventions in these discursive structures by inverting Foucaultâs power/knowledge equation. Ernste explores the extent to which this inversion reinstates responsible and rational spatial decisions and actions as a focus of research in human geography. In his view rationality could be reconstituted as a culturally contingent phenomenon, and critical geographical analysis could again contribute to concrete problem-solving, albeit in a culturally much more informed and embedded way than hitherto. Ernste also discusses geographical action theory as put forward by Werlen (1987, 1993a, 1993b, 1995, 1997, 2010a, 2010b, 2013, 2015; see also Werlenâs Chap. 2 in this volume) in the phenomenological tradition of SchÃ¼tz (1932). According to that school of thought, the internal mental intentionality directed to outer objects is what ascribes meanings to these objects, as people do through their everyday place-making and everyday spatially differentiated actions. Ernste interprets this geographic action theory as the subjectivist version of what Schatzki, Knorr-Cetina, and Savigny (2001) and Reckwitz (2002) designated as the mentalist paradigm in social theory. This approach contrasts with the objectivist version of mentalism, which stems from classical structuralism. Ernste shows that the advent of poststructuralist thinking ushered in a great reluctance to conceptualize human behavior as conscious rational actions and that the term action is generally avoided in most poststructuralist literature. Talking about practice instead of action indeed amounts to a novel picture of human agency and rationality (Reckwitz, 2008, p. 98). In contrast to Benno Werlen, with his subjective, meaning-oriented approach to geographical action theory, and unlike Zierhofer (2002), who advocated the language-pragmatic approach in geography, poststructuralist thinkers do not tend to place structures inside the mind or in pragmatic procedures of interaction but rather âoutsideâ bothâin chains of signs, in symbols, discourse, or text. Pragmatics is âa branch of linguistics dealing with language in its situational context, including the knowledge and beliefs of the speaker and the relationship and interaction between speaker and listenerâ (âPragmatics,â 2010)."
372,1416,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"and the phase goes through one sinusoidal oscillation in a sidereal day. Suppose that the source is circumpolar, i.e., above the horizon for 24 h. From continuous observation of ! over a full day, the 2"" crossings of ! can be tracked so that there are no phase ambiguities. The average value of the geometric term of Eq. (12.4) is zero, so that !in can be estimated and removed. When the source transits the local"
3,145,0.983,Instructional Scaffolding in STEM Education : Strategies and Efficacy Evidence,"the model (Belland et al., 2008; Land & Zembal-Saul, 2003; Quintana et al., 2004). Qualitative representations can then be iteratively improved. Modeling phenomena with mathematics includes setting up an equation that describes the phenomena. It is important to note that effective problem solvers do not solely model problems qualitatively or quantitatively; rather, they use both sorts of representation, as each informs the other and together can lead to a more effective solution and solution process (Chi et al., 1981; Jonassen, 2003; Van Heuvelen & Zou, 2001). For example, after creating a qualitative model, one may proceed to create a quantitative model. The finished qualitative model will influence how the quantitative model is set up. One should then see where the models are consistent, and where they contradict each other; in this way, the models can be progressively improved. By spending adequate time modeling, one can engage in more effective problem-solving, as it guides subsequent investigations, can activate solution schemas, and can provide the framework by which one can simulate what would happen when a variable is manipulated (Anzai & Yokoyama, 1984; Chi et al., 1981; Jonassen, 2003; Sins, Savelsbergh, & van Joolingen, 2005). Just as it is important to learn to create models, it is also important to be able to interpret the models created by others, especially in terms of what these diverse models say differently about the underlying problems (diSessa, 1988; Seufert, 2003; Wu, Krajcik, & Soloway, 2001). Doing so can lead to enhanced understanding of the problem (Seufert, 2003). This is particularly challenging for K-12 students (BrÃ¥ten et al., 2011; Seufert, 2003). Indeed, learners often simply adhere to the model that is closest to their own early experiences, or the simplest explanation of the underlying phenomenon, even when presented with a more accurate model (diSessa, 1988; Perkins & Grotzer, 2005). This may be explained in part by most K-12 studentsâ lack of familiarity with complex causal models, such as those that explain changes in a factor through indirect action from a combination of factors A and B (Perkins & Grotzer, 2005). While some evidence indicates that reluctance to consider an alternative model is widespread among learners of differing levels of prior knowledge and skill, other evidence indicates that it may be more prevalent among lower-achieving students (Seufert, 2003). Thus, it is especially important to endeavor to increase modeling skills from a social justice vantage point and to broaden participation in STEM (Lynch, 2001). 4.2.1.4.2.3 Argumentation Science is very much a social endeavor, as no scientist works in a vacuum (Ford, 2012). Rather, scientists work in a large community of practice in which they share and defend findings to one another, and build off of othersâ work. At the core of this is argumentation, defined as both backing claims with evidence and models, but also effectively evaluating claims on the basis of evidence and models (Ford, 2012; Osborne, 2010). The argumentation process allows scientific models and theories to be iteratively improved (Ford, 2012). To be able to engage in STEM effectively as citizens, individuals also need to be able to engage in clear argumentation (Aufschnaiter et al., 2008; Jonassen, 2011; Osborne, 2010; Perelman & Olbrechts-Tyteca, 1958). For example, when scientific issues are discussed, citizens need to be able to sort out well-founded claims from"
72,346,0.983,New Frontiers in Social Innovation Research,"The direction A consequence of the maximalist understanding of the movement is that its initiatives and experiments should exemplify and foreshadow a direction for society. The movement need not and should not commit itself to a single programme for any of the societies in which the innovators act, much less to a shared worldwide programme. The innovators must nevertheless have a direction. Their direction can result only from the path that they propose for society. Each group of participants in the movement must therefore struggle to see the social experiments that it tries to develop as the foreshadowing of such a direction. And each such direction must be defined, tentatively, by a dialectic between the innovations in practical arrangements"
213,200,0.983,Collider Physics Within The Standard Model : a Primer,"and mt measured separately (the top mass is measured from the invariant mass of the decay products), as can be seen from Fig. 2.28 [150]. The mass of the top (and the value of Ës ) can be determined from the crosssection, assuming that QCD is correct, and compared with the more precise value from the final decay state. The value of the top pole mass derived in [27] from the cross-section data, using the best available parton densities with the correlated value of Ës , is mt D 173:3Ë2:8 GeV. This is to be compared with the value measured at the Tevatron by the CDF and D0 collaborations, viz., mt D 173:2 Ë 0:9 GeV. This quoted error is clearly too optimistic, especially if one identifies this value with the pole mass which it resembles. This error is only adequate within the specific procedure used by the experimental collaborations to define their mass (including Montecarlo, with assumptions about higher order terms, non-perturbative effects, etc.). The problem is how to export this value to other processes. Leaving aside the thorny issue of the precise relation between mt with mt , it is clear that there is good overall consistency."
124,161,0.983,Nuel Belnap on Indeterminism and Free Action : Volume 2 of Outstanding Contributions to Logic (Volume 2),"there is a difference in the legal treatment of the different outcomes. The justification can be found by making clear the purpose of our legal systems. Before I give the argument, we will look at the formalisation of legal prohibition and obligation."
214,76,0.983,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"One of the major complexities of the climate system and climate change is that the âproblemâ or âpollutantâ we are discussing (CO2) is not something âforeignâ to the system. It is a part of the system, a critical part that is naturally all around us. We drink CO2 (in carbonated drinks and beer, for example), we exhale it, and are bodies are made up of carbon. Carbon is absorbed by plants with photosynthesis and used to build their tissues. This creates the natural annual cycle in Fig. 3.1 of carbon in air. So CO2 is not bad; it is a natural part of the system. The breathing is natural, but the increase is not. Thus we have direct records of CO2 in the atmosphere and evidence that recent changes are caused by humans. This is a strong forcing on the system. We also have evidence through proxy records that temperature has been correlated with CO2. So we know that in the past the earthâs climate has changed with CO2. To link the change in energy (forcing) with the changes in temperature, we need to understand where the energy goes in the climate system. Climate models are one tool for that, but we can discuss the energy flow in more detail."
334,248,0.983,Protest Movements in Asylum and Deportation,"events such as the World Refugee Day or those against restrictive measures decided by parliament, commonly referred to as âchange-oriented protestsâ (Ruedin et al. 2018). Instead, it examines case-specific protests that seek to protect an identifiable beneficiary, which can last several months, sometimes even years. The aim is to grasp how deportation decisions â as concrete applications of the law â are challenged. Based on five Swiss case studies, we strive to identify patterns of casespecific protests. We argue that the latter are neither singular contestations nor social movements, but something in between. We distinguish two ideal-types (in the sense of Weberâs understanding) of case-specific protests according to the strategies adopted and the role of the beneficiary in the protest. In the first type, the protection of the beneficiary is both the means and the end of the protest. The sole goal of the protest is that the beneficiary be not deported. In the second type, the protest against the deportation of the beneficiary is merely the means through which a broader message about policy change is communicated. The defense of the beneficiary serves to express overall criticism against deportation policies. This second type shows that case-specific protests and change-oriented ones can be intertwined. In other words, some anti-deportation protests are neither purely case-specific nor change-oriented, but rather a combination of both (case specific in the means and change-oriented in the purpose). Overall, the typology developed in this chapter allows a theoretical generalization of empirical observations that encompasses both the actor structure and the strategies underlying altruistic protests."
132,179,0.983,AiREAS: Sustainocracy for a Healthy City: Phase 3: Civilian Participation â Including the Global Health Deal Proposition,"One of the consequences of such evolution is that the old, fragmented interests suddenly start to ï¬nd each other in the center of that pyramid through the awareness-driven invitation. Entrepreneurship is no longer limited to money-driven business entities. Civil servants may also enter the same entrepreneurship of creating core values, not through regulation but co-creation. Civilians contribute through awareness-driven changes in their consumption patterns and productivity. The 1Ã proï¬t-based business practice is outdated and evolves into value-driven co-creation, affecting every participant. A product becomes an instrument, a user too, just like the ï¬nancial means, a policy or the application of knowledge. This is both a major breakthrough and a tremendous learning process for all involved. With this, we started to experiment in order to prove the evolution of entrepreneurship of which we ourselves were an example. We now needed to show how the To Be part became dominant over our To Do decisions and that the center of the pyramid was populated with multidisciplinary tables of co-creation efforts based on core PPP + P values. Core value-driven entrepreneurship was no longer conï¬ned to âbusiness peopleâ but expanded so as also to include civilians, civil servants, educators, executives, etc., all of whom contributed to progress through value-driven interaction. Entrepreneurship is no longer referred to as âmaking money through producing and sellingâ; it becomes âco-creating core values together through multidisciplinary interactionâ. With this basic understanding of the evolution of regional entrepreneurship, we could start ï¬nding our way in the complex duality of the existing reality, the old ï¬eld of speculative economics and the new ï¬eld of economic diversity through value-driven change and awareness-driven co-creation."
242,1133,0.983,"Migration, Gender and Social Justice : Perspectives on Human Insecurity","struction or paid domestic work, depend on the Nicaraguan labour force, and so they must be tolerated. A shortcoming of this concept of tolerance, however, is that those who supposedly propose it retain the right to decide who they exercise their tolerance on; they do not lose the power that allows them to be tolerant. As Hage (1998: 85â86) notes, âWhen those who are intolerant are asked to be tolerant, their power to be tolerant is not taken away from them. ... the advocacy of tolerance left people empowered to be intolerantâ. Recognition of the interdependence between the receiving society and the migrants could be a step forward from a temporary self-interested âtoleranceâ. Nonetheless, Costa Rican society is far from being in a condition to recognize how much it depends on those it does not accept. The recognition of interdependence is, in de Sousa Santosâ terms, a structuring gap in the Migration Law. In other words, that which is not there tends to shape the part that is. In this regard, the migratory legislation expresses values rooted in the social imaginary, such that trying to change it is not merely a legal issue, although that dimension is very important, but relates to a plane that is more ideological. The absence of the notion of interdependence also refers to the underdevelopment of the possible narrative or genres from which to represent interdependence. It is symptomatic that in Costa Rica, despite the frequency of references to immigration, only two novels have been published on this theme (Marcenaro 2007; Paniagua 2010), even though novels are possibly the cultural form that best reveals the social reality of a period (Bakhtin 1981; Williams 1977; Mora 2000). Nor is there major participation in the public debate about immigration by the first generation of the children of immigrants. They should be among those best prepared to be aware of the centrality of interdependence. In a context in which the ratio of paid jobs undertaken by women is higher than the proportion of reproductive work done by men at home, a good number of Costa Rican women recognize that without the paid domestic work frequently done by Nicaraguan women, their participation in the job market would be impossible. In a certain way, paid domestic work done by Nicaraguan women has made gender inequalities less explicit in Costa Rican society. However, such recognition does not translate into publicly shared narratives that can frame social imaginaries and policy-making on immigration in terms of interdependence. In such a context, the ways in which Nica-"
237,51,0.983,The Academic Book of the Future,"are challenging but increasingly exciting, and (3) new norms for what counts as knowledge are being generated more quickly than ever.â These features, along with the opportunities opened up by computational interrogation of big data, are intertwined and contribute to defining the boundaries around the ecosystem of any subject area. This has profound implications for publishing. âScaffoldingâ may not appear at first glance to be the right term to describe the support role that publishers provide in a very fluid ecology. However, given the rigidity of the legacy systems of supply and delivery, it may not be a bad metaphor. Physical books that have sustained us so well for centuries were (and are still) served by a host of intermediaries including bookshops, library suppliers, and aggregators. In other words a vast, established supply chain exists that is no longer best suited to deliver the new âbookâ. We are now experiencing a whole host of pressures that will require the dismantling and reconstruction of some kind of scaffolding. We are somewhere inside a fundamental transformation â in a âpupalâ stage of development. What will emerge is as yet unknown. Wherever and however we end up will be in response to changes to the way that academics conduct their work, how knowledge problems will be solved, and how traditional career paths might change. What does all this mean for the âbookâ of the future? Some of the challenges include: newly shaped ecologies of knowledge infrastructures demanding shared data; new forms of publication; interdisciplinarity; facilitated collaborative work; and fast turnaround. Features that are likely to remain are long-form publications, shorter narrative structures within a coherent whole that can stand alone (e.g. edited chapters) as well as collectively (edited volumes), alongside more sophisticated ways of presenting interpretation of data or sources in light of theory. Features of the âbookâ that are likely to be less prevalent are the physical object (which may not be printed unless requested) and therefore âwritingâ will become more influenced by the use and the embeddedness of multimedia. The rigidity of single disciplines will wane â though to what extent is still unknown. Digital affordances not only provide new answers to old questions â they encourage new questions to be asked. For years, there has been tension between subject depth vs subject breadth. Interdisciplinarity too has always been controversial. Now, with new digital affordances, we no longer have silos of discipline-limited knowledge infrastructures. Nevertheless, the publishing industry (admittedly of necessity) has lagged behind, following an age-old inclination DOI: 10.1057/9781137595775.0011"
49,409,0.983,Artificial Intelligence and Cognitive Science IV,"appropriate formal model of âifâ¦, thenâ¦â. C. I. Lewis (see Lewis [16], Lewis and Langford [17]), the key figure of its early modern history, thought that adding necessity is sufficient: âNecessarily (p â q)â was seen by him as the correct formal rendering of âp implies qâ. However, this formal model has its own discrepancies and soon more refined models were suggested, viz. the various relevance logics (Anderson and Belnap [2],[3] and Mares [19]). This ambition of modern logicians to âkeep upâ with the intuitions about actual reasoning and usage of the âlogical wordsâ has many examples, viz. conditional logics, non-monotonic logics, etc. To sum up, the relation of logic to actual reasoning is not as simple as it may seem. First, logic is not to be thought of as a single set of âcorrectâ rules of inference. A more appropriate view of logic is to see it as a discipline aiming at providing formal models of inference and inference-related concepts. Second, these models are strikingly diverse and most of them were born of the need to model actual reasoning more flexibly and appropriately. An important consequence of this viewpoint is that the relevance of logic to actual reasoning cannot be conclusively refuted by pointing out that a particular logical system does not fit in with intuitions or experimental data. There is always the possibility of providing a more appropriate system."
