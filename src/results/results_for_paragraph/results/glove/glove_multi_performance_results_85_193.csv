book_index,paragraph_index,similarity_score,recommended_book,recommended_text
145,230,0.992,"Evolution, Monitoring and Predicting Models of Rockburst : Precursor Information for Rock Failure","where a and b are the unknown constants at a certain monitoring area; Constant a reflect the overall level of MS activity in this area and is viewed as MS activity parameter, which is correlated with the total MS events above the threshold magnitude; Constant b describes the magnitude distribution of MS events, i.e., the relative number of small and large MS events in a certain time. Generally, b value is close to 1."
200,46,0.992,"Earthquakes, Tsunamis and Nuclear Risks","averaged seismicity from active faults. In addition, those areas where the density of the observed seismicity and distribution of active faults are both high show a relatively thin seismogenic layer and complex subsurface structure deduced from the Bouguer gravity anomaly. Thus, we consider that the first principal component might indicate the degree of tectonic activity. The results of the relations between the principal component loadings and four observed parameters of (b) southwestern Honshu and (c) Kyushu in Fig. 2.4 show a similar tendency. Thus, we expect that the spatial distribution of the first principal component loading relates to the seismotectonic provinces and adopted it as a parameter for the following cluster analysis. However, to the contrary, the first principal component loading (F1) in Fig. 2.4d of Hokkaido showing a 33 % proportion indicates that the lower limit of the seismogenic layer and the Bouguer gravity anomaly parameters are large positive values and that the remaining parameters are relatively small. Additionally, the first principal component loadings (F1) in Fig. 2.4e of Kanto indicate a different tendency from the other districts, showing that the first principal component loadings of the observed seismicity and Bouguer gravity anomaly parameters are largely negative. The reason for the different results for the Hokkaido and Kanto districts might relate to the distance between the district and plate boundary axis. The closer distance of the Hokkaido and Kanto districts results in complexity of the deep part of the tectonic structure, such as the depth and the shape of the subducting oceanic plate and the related seismicity and gravity anomaly. Hereafter, cluster analysis is applied to the first principal component scores of northeastern Honshu and southwestern Honshu, respectively, from the viewpoint of the tectonic meaning of intraplate shallow earthquakes and active faults as judged from the principal component loading results (Fig. 2.4). The statistical distances among grids in each district were measured by the group average method, and clusters were calculated in similarity order. The number of clusters is after that of Kakimi et al. [2], and four and six were set for northeastern Honshu and southwestern Honshu, respectively (Fig. 2.5). The result shows that each cluster in both northeastern and southwestern Honshu distributes with strong spatial relation, although no parameter regarding contiguity was involved in the principal component analysis. Thus, the similarity of the first principal component score based on the objective index of statistical distance is considered to have usefulness for considering the spatial correlation of tectonic provinces. Then, the tectonic province boundaries were set at the cluster boundaries between areas of clusters with the same category, as shown in Fig. 2.6a for northeastern Honshu and Fig. 2.6b for southwestern Honshu. The exception was the case of isolated cluster patches consisting of 10 or fewer grids, which were subjectively determined to be ignored or incorporated into a neighboring cluster by the shape and size of each patch. The following describes the distinctive features of our results (Fig. 2.6) and the tectonic province map of Kakimi et al. [2] (Fig. 2.1). Northeastern Honshu was divided into two large seismotectonic provinces by Kakimi et al. [2]. This boundary is located at the eastern foot of the Ou-Backbone Mountain Range and partly overlaps with west-dipping reverse fault systems, such"
311,586,0.991,The Physics of the B Factories,"where R is the radial parameter of the decaying meson and typically takes values between 1 and 5 (GeV) . In this prescription, F is normalized so that F = 1 for qr = qab . While the P - and D-waves of the decay amplitude are usually well described using a certain number of BW or GS propagators, the actual number depending on the speciï¬c decay, the S-wave typically contains a number of broad overlapping states, for which the isobar model gives a poor description. In that case, more complex alternatives have been adopted, which are reviewed in Section 13.2.2. Figure 13.2.1 illustrates how various intermediate twobody states appear in the Dalitz plot. Unlike the uniform distribution of the phase-space decay (Fig. 13.2.1(a)), scalar resonances appear as bands in the Dalitz plot, as shown in Fig. 13.2.1(b-d) for resonances in bc, ac, and ab channels, respectively. Angular distributions for vector and tensor intermediate states introduce characteristic nonuniformity of the event density along the resonance bands (Fig. 13.2.1(e,f)). Finally, the region where the amplitudes of two resonances overlap is sensitive to the phase diï¬erence between the two amplitudes (Fig. 13.2.1(g,h)). 13.2.2 K-matrix formalism The complex S-wave dynamics, which can also include the presence of several broad and overlapping scalar resonances, can alternatively be described through the use of a K-matrix formalism (Chung et al., 1995; Wigner, 1946) with the production vector (P-vector) approximation (Aitchison, 1972). Within this formalism, the production process described by the P-vector can be viewed as the initial formation of several states, which are then propagated by the K-matrix term into the ï¬nal state that is observed. This approach ensures that the two-body scattering matrix respects unitarity, which is not guaranteed in the case of the isobar model. At the B Factories this approach is most commonly used to describe the Ï + Ï â S-wave contribution to the Dalitz-plot amplitude, e.g. in the BABAR analyses (Aubert, 2008l, 2009h; del Amo Sanchez, 2010a,b) and Belle analysis (Abe, 2007b). In such cases the amplitude is given by Au (s) ="
372,1613,0.991,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where din and dout are called the inner and outer scales of turbulence, which may be less than a centimeter and a few kilometers, respectively. The parameter Cn2 characterizes the strength of the turbulence. Note that water vapor, which is the dominant cause of fluctuation in the index of refraction, is poorly mixed in the troposphere and therefore may be only an approximate tracer of the mechanical turbulence. The details of the derivation structure function of phase from the structure function of the index of refraction given in Eq. (13.97) are given in Appendix 13.2. The result is that D+ .d/ for a uniform layer of turbulence of thickness L has several important power-law segments: D+ .d/ & d5=3 ;"
307,287,0.991,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"which is a discrete version of (8.31) with  D oo C oc C co C cc . The simulation results are shown in Fig. 8.6 and summarized in Table 8.4. We observe that the transmembrane potential V significantly influences the probability density functions. In Table 8.4, we observe that the probability of the LCC being in the open state is highest for V D 0 mV and it is almost zero for V D 40 mV. In the computations, we use Ât D 0:001 ms, Âx D 1:02, 1:23, 1:54 and 1:95 M (the domain size varies with V), and Ây D 9:3 M. Note that the scale of the plots varies (see Fig. 8.6)."
311,812,0.991,The Physics of the B Factories,"The coeï¬cient C(Eâ , pâ X , pX , k) describes the quark decay b â uâÎ½ and can be computed in QCD perturbation theory. The âshape-functionâ F (k) is a non-perturbative function. It describes the momentum distribution of the b quark in the B meson (Bigi, Shifman, Uraltsev, and Vainshtein, 1994; Neubert, 1994a). For p+ X â¼ k â¼ ÎQCD , which includes a large portion of the small mX region, the full non-perturbative shape of F (k) is necessary to obtain an accurate description of the diï¬erential decay rate. On the other hand, in the limit p+ X â« k â¼ ÎQCD , only the ï¬rst few moments of F (k) are needed. Typically, the experimental measurements can lie anywhere between these two kinematic regimes. There are several sources of uncertainties in the theoretical predictions that must be considered. First, there are perturbative uncertainties in the calculation of C due to unknown higher-order corrections. Second, there are parametric uncertainties due to the imprecise knowledge of inputs, in particular the b-quark mass and F (k). The total decay rate scales like m5b , while partial rates restricted to the small mX region typically exhibit an even stronger dependence on mb . The ï¬rst few moments of F (k) are determined by mb and the expectation values of local operators that are constrained by ï¬ts to B â Xc âÎ½ moments. A substantial part of the mb dependence enters indirectly via the ï¬rst moment of F (k). Depending on the kinematic cuts, the shape of F (k) (beyond what is encoded in its ï¬rst few moments) can also have a signiï¬cant inï¬uence on the"
311,1041,0.991,The Physics of the B Factories,"semileptonic branching fractions for neutral and charged B mesons, and Ç«Â± ll are the eï¬ciencies for selecting dilepton events of unmixed and mixed origins. Belle determines the ratio Ç«+ ll : Ç«ll from MC simulation and fixes it in the fit to the data assuming detector eï¬ects that are not simulated correctly equally aï¬ect events with these origins. The Îz distributions are obtained for these distributions by conversion from Ît and convolution with the Îz resolution function. BABAR describes the Ît resolution function for dilepton events as the sum of three Gaussian distributions. The resolution function parameters are free parameters in the fit. Belle determines the signal Îz resolution function from J/Ï decays in data. For these decays the true Îz is equal to zero and the measured Îz distribution, after the contributions of backgrounds are subtracted, yields the Îz resolution function. A comparison between data and MC simulation shows that after convolving the MC Îz distribution of J/Ï decays with a Gaussian of width Ï = (50 Â± 18) Î¼m, the MC distribution agrees with data. The Ît and Îz distributions of background events are determined from MC-simulated events and data control samples. The large background from semileptonic B +B â events has the same resolution function as the signal events. The numbers of selected OS and SS dilepton pairs along with the corresponding mixing asymmetry as a function of Ît from the BABAR analysis are shown in Fig. 17.5.5. Due to the small mixing frequency, OS signal events are much more abundant than SS events. Most of the background events are also OS (for example from B +B â events). Therefore, even a small mistag probability will blur the characteristic features of the SS Ît distribution. This is particularly evident at Ît = 0 where the OS P+ distribution has its maximum and the SS Ît distribution Pâ is zero: the measured Ît distribution of selected SS events does not have a dip at zero. However, the mixing asymmetry"
233,472,0.991,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"(illustrated by the density distributions drawn around the medians in Fig. 4) reveals a very subtle trade-off: The basic core-area Zonation retains species representations more evenly, as it should by definition, whereas the phylogenetic diversity solution loses larger fractions of some speciesâ ranges earlier on in the cell removal process (longer downward tails in the density distributions at lowest 50 % fractions). In other words, with the phylogenetic diversity weighting the protection of some species is traded off against protection of locations with higher phylogenetic diversity. But as this tradeoff is minor and most visible at poorest fractions of the landscape, it is unlikely to be of concern for practical conservation."
285,220,0.991,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","Fig. 4 Results of the spectral modulation threshold task for the three strategies (SCE0, SCE2 and SCE4) given as just noticeable differences (jnd SMT) in decibels. The lower is the jnd the better is the result"
372,1943,0.991,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"in terms of models of the intensity distribution. Techniques for two-dimensional reconstruction without phase data [see, e.g., Bates (1984)] are also applicable. Optical interferometry is an active and growing field, and here we attempt only to give an overview of some basic principles. See Further Reading at the end of this chapter for a collection of important publications in optical interferometry. Before discussing instruments, we briefly review some relevant atmospheric parameters. The irregularities in the atmosphere give rise to random variations in the refractive index over a large range of linear scales. For any particular wavelength, there exists a scale size over which portions of a wavefront remain substantially plane compared with the wavelength, that is, atmospheric phase variations are small compared with 2#. This scale size is represented by a parameter, the Fried length df (Fried 1966); see the discussion following Eq. (13.102). The Fried length is equal to 3:2d0 , where d0 is the spacing between paths through the atmosphere for which the rms phase difference is one radian; see Eq. (13.102). Regions for which the uniformity of the phase path lies within this range are sometimes referred to as seeing cells. The scale size df and the height at which the dominant irregularities occur define an isoplanatic angle (or isoplanatic patch) size, that is, an angular range of the sky within which the incoming wavefronts from different points encounter similar phase shifts. Within an isoplanatic patch, the point-spread function remains"
238,75,0.991,Nanoinformatics,"all 955 points located in the shaded region must be sampled to select all positive points. The difference in the rank depends on whether structural optimization is performed, implying that the local structural relaxation around a proton in oxides is important. Although using the low-FN region as the region of interest improves the sampling performance, the performance still deviates from that of ideal sampling. Figure 2.10 shows the step numbers where each of the low-FN grid points (Nos. 1â15)"
280,232,0.99,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"Kristensen 2003). Similar size gradation has been found in the background scales in Junonia orithya, J. almana, Vanessa indica, and V. cardui (Kusaba and Otaki 2009; Dhungel and Otaki 2013; Iwata and Otaki 2016b). What about the size of scales that constitute elements? In J. orithya and J. almana, the scale size of an element is larger than that of its surrounding background (Kusaba and Otaki 2009; Iwata and Otaki 2016b) (Fig. 7.10). In this sense, scale color and size are reasonably correlated, which can be called the colorsize correlation rule for scales. This rule may sound trivial but is indeed important as a clue to understanding the possible nature of morphogenic signals for color patterns (see below). Furthermore, the largest scales in an element are found roughly at the center of an element (Kusaba and Otaki 2009; Iwata and Otaki 2016b). This may be called the central maxima rule for elemental scale size. It is important to recognize that scale size changes suddenly at the boundary between the inner black ring of an eyespot and a yellow ring. There are similar abrupt changes at the outer ring boundary and the PFE boundary. These abrupt size changes may reflect the independence of black areas (the binary rule and the uncoupling rule) rather than gradual changes of positional information. Additionally, scales of different colors differ in their structure, such as overall scale shape and scale ultrastructure (Gilbert et al. 1988; Nijhout 1991; Janssen et al. 2001). Our laboratory also obtained similar results using Junonia and other butterflies (Kusaba and Otaki 2009; Iwata and Otaki unpublished data; Kazama et al. 2017)."
311,940,0.99,The Physics of the B Factories,"(ML) fit. Events with |ÎE| < 300 MeV are typically accepted, although an asymmetric acceptance region is used if there is a chance of energy loss from photon emission or Ï 0 reconstruction. The minimum value of mES is set to allow a good fit to the mES background distribution and is rarely set less than 5.220 GeV/c2 (below this value, other selection criteria start to distort the selection eï¬ciency). The (mES , ÎE) plane is divided into regions to aid analysis. A signal region is defined around the point mES = mB , ÎE = 0 with a width roughly 3 times the resolution on mES (â¼ 3 MeV/c2 ) and ÎE (â¼ 20 â 50 MeV, depending on the number of neutral particles). Although the signal region is usually rectangular in shape, elliptical signal regions have been used e.g. Fig. 3 in (Garmash, 2005). Two sidebands are defined above and below the ÎE signal region, the upper region allowing for the study of two-body decays that have been combined with a random track and the lower region to study four-body decays that have lost a track. A further sideband below the signal region in mES can be used to study the continuum background, although care must be taken to account for any decays from B mesons. In the center-of-mass (CM) frame, the continuum background is characterized by a jet-like, back-to-back structure while the BB events have a more spherical distribution since they are produced close to rest (see Chapter 9 for details). Therefore, event shape variables are used to separate signal from this background. Many diï¬erent criteria have been used over the years including sphericity, spherocity, planarity, acoplanarity and thrust (see the Glossary and Chapter 9). In addition, angles are often measured between the direction of the B meson decay and a reference axis, such as the beam line or the direction of the rest of the event (ROE). An important example is the thrust angle in the CM frame, defined as the angle Î¸T between the thrust axis of the B meson candidate and that of the rest of the particles in an event. Signal events are uniformly distributed in cos Î¸T , while continuum events are peaked near cos Î¸T = Â±1. A requirement on cos Î¸T or | cos Î¸T | of less than 0.7 â 0.9 is usually applied. Any remaining event shape variables are combined into a multivariate discriminant that can either be used as selection criteria or as a p.d.f. observable in a ML fit. Fisher discriminants and neural networks are popular but Boosted Decision Trees (or Forests) have also been applied (see Chapter 4 for details). The number of variables is typically about six. Although discriminants with many more variables have been tried, they rarely bring any additional discrimination. The choice of variables depends on the mode under consideration, consistency with previously used discriminants, and ultimately on the prejudice of the analyst. It is important to check for correlations between the input variables and any other variables used in the ML fit. Variables that have been used over the years include (see Chapter 9 for many definitions): CLEO cones (momentum distribution in nine angular cones about the thrust vector); modified Fox-Wolfram moments (Abe, 2001c); the variable ST , the scalar sum of the transverse momenta, calculated with respect to the"
311,2524,0.99,The Physics of the B Factories,"at 90% C.L., where the range of the upper limit is due to the uncertainty of the parameterization used to describe the hadronic form factors and the unknown relative phase between the spin-one |F | and the spin-zero |FS | form factors. The results improve upon the previous limit from the CLEO experiment (Bonvicini et al., 2002) by one order of magnitude. Theoretical predictions for Im(Î·S ) are available in the context of MHDM with three or more Higgs doublets (Choi, Hagiwara, and Tanabashi, 1995; Grossman, 1994). In such models Î·S is related to the model parameters as (Choi, Hagiwara, and Tanabashi, 1995), Î·S â"
311,669,0.99,The Physics of the B Factories,"detector positions are intentionally modiï¬ed in a plausible range in both global displacement and rotation as well as random misalignment for each silicon sensor, and the change in ï¬tted values of the CP -violating parameters S and C (see Section 10.2) from the nominal value is assigned as an uncertainty from this source of systematic. The magnitude of this uncertainty on S and C is at most a few per mille. In extreme cases, for example modes such as B 0 â Ï+ Ïâ that suï¬er from a signiï¬cant contribution from mis-reconstructed signal in the ï¬nal state, the eï¬ect of the silicon detector alignment is somewhat larger: â¼ 0.01. The reason for this is that some of the mis-reconstructed signal in this ï¬nal state has a biased reconstructed vertex position, resulting from the inclusion of low-momentum tracks reconstructed at the extremities of the helicity angle distributions (see Chapter 12). Sometimes these low momentum tracks are incorrectly assigned from the rest of the event to a signal B candidate, rather than including the correct tracks from the signal side. Different alignment sets change the reconstruction rate of this component of mis-reconstructed signal, and thus induce a bias on the measured observables S and C. 15.3.2 Beamspot position, z scale and boost As discussed in Chapter 6, the beamspot location can be used to improve constraints on vertex reconstruction, and is used when reconstructing the tagging B meson vertex. The dominant contribution to the systematic uncertainty when adding this constraint comes from the limited knowledge of the vertical position of the beamspot. The knowledge of the beamspot is included in the vertex ï¬t via the addition of an extra term in the Ï2 of the track ï¬t. The limitation in the absolute knowledge of the beamspot location therefore translates into a systematic uncertainty on the reconstructed value of Ît, and hence propagates through onto the measured observables S and C in a timedependent CP asymmetry analysis. Detailed studies of the beam-spot position calibration were performed at the B Factories (see Section 6.4). Knowledge of the mean vertical position is the dominant systematic uncertainty from the use of the beamspot in BABAR, while its spread is found to give a much larger eï¬ect in Belle. The corresponding systematic uncertainties in the measured values of S and C are estimated by modifying the position and uncertainty on the vertical beamspot position according to the variations seen in data. For example, BABAR varies this position by Â±20Î¼m, as well as increase the uncertainty on this quantity by 20Î¼m to evaluate the systematic uncertainty arising from the use of the beamspot in vertex reconstruction. Belle changes the beamspot position uncertainty to a factor of 2 larger or smaller value than the nominal one, 21Î¼m. The relative change in S (C) from its nominal value (S â¼ sin 2Ï1 , C â¼ 0) is found to be 0.13% (0.06%) in BABAR and 0.3% (0.08%) in Belle for B decays to ccÌs ï¬nal states. Other important factors impacting the measurement of S and C are the z scale determined from the vertex detector, and the boost factor. Detailed studies of control"
311,2196,0.99,The Physics of the B Factories,"In this fit, the mixing parameters are defined in the form (xâ² /r0 and y â² /r0 ), where r0 is the ratio between the CF and DCS amplitudes defined above. These are allowed to vary in the fit. The time-dependent p.d.f. for this fit is convolved with a decay time resolution function derived from a fit to the RS events. The D0 lifetime is also determined from RS events, and is found to agree with the world average (Beringer et al., 2012). Signal samples consisting of 658, 986 RS (purity 99%) and 3, 009 WS (purity 50%) candidates are selected. Major sources of background come from a variety of wrongly reconstructed D0 decays, wrongly associated slow pions or from a combination of both. In the WS sample, a small background also comes from events in which both K + and Ï â are mis-identified in the PID detectors. Simulated samples of these categories are used to determine the contributions of each in the data. The shape of background events in the RS and WS Dalitz plots are determined from M and Îm sideband regions in the data. For both the RS and WS fits, eï¬ciency variations over the Dalitz plot are estimated from MC samples generated uniformly over the phase space. The Dalitz plots for the RS and WS samples, together with the distributions of M and Îm for the WS sample are shown in Fig. 19.2.19. In each of the Dalitz plots, bands due to charged and neutral states for K â (890) and for charged Ï are easily seen. CF modes preferentially decay via K â Ï while DCS modes preferentially decay via KÏ amplitudes. Values for xâ² and y â² are obtained for the combined D0 and D0 samples. Separate values are also obtained from fits to each of the two subsamples (for the latter see more details in Section 19.2.7). In all cases, a value for r0 is required. This is derived from the ratio NWS /NRS , where NWS (NRS ) is the number of wrong-sign (right-sign) signal events observed and their respective time-integrated p.d.f.s. This procedure introduces a correlation between the xâ² and y â² values obtained from the fit. Uncertainties in these mixing parameters are, therefore derived from the values obtained in a similar way for 106 pairs of values for (xâ² /r0 , y â² /r0 ) randomly generated in accordance with the fit covariance matrix (assuming Gaussian errors and including systematic uncertainties). The major systematic uncertainties arise from uncertainties in resonance masses and widths, and KÏ S-wave parameters in the decay amplitude models, variations in the estimates for the numbers of WS and RS signal events and in parameters describing the time resolution. The mixing parameter results obtained are +0.57 xâ² = (+2.61â0.68 Â± 0.39)%"
311,2182,0.99,The Physics of the B Factories,"parameters for each final state are determined by fits to the t distributions of events in M sidebands. The MC is used to select the sideband region that best reproduces the timing distribution of background events in the signal region. The results of a simultaneous fit are shown in Fig. 19.2.15. The fitted lifetime of D0 â K â Ï + , Ï = (408.7 Â± 0.6(stat)) fs, is consistent with the world average of (410.1 Â± 1.5) fs. The value of yCP is determined to be yCP = (1.31 Â± 0.32(stat) Â± 0.25(syst))%. This result and the BABAR result in the D0 â K + Ï â decays, described in Section 19.2.2 represent the first experimental evidence for D0 â D0 mixing. Belle performed an updated measurement of D0 â KK/ÏÏ decay modes using the full available data sample (Staric, 2012a). Using a larger data sample a small bias on the measured lifetime depending on the D meson polar angle in the CM system, Î¸â , is observed. It is a consequence of small residual misalignments between the Belle tracking detectors. To reduce the systematic uncertainty due to such eï¬ects the measurement is performed in bins of cos Î¸â and the final value of yCP is obtained as a weighted average of the values in individual bins. The final result is yCP = (1.11 Â± 0.22(stat) Â± 0.11(syst))%,"
311,1233,0.99,The Physics of the B Factories,"formed. The beam energy substituted mass mES and the energy diï¬erence ÎE are calculated for these candidates (see Chapter 7). For time-dependent CP analyses, the flavor of the B candidates is determined using a flavor-tagging algorithm (Chapter 8) and the proper time diï¬erence Ît between the signal B and the accompanying B meson (Btag ), is measured (Chapter 6). Finally, the signal yields and other decay properties (polarization, CP asymmetries) are determined in a multi-variate maximum likelihood fit. The continuum process e+ eâ â qq (q = u, d, s, c) is the main source of background for the charmless B decays. In order to suppress this background, charmless analyses employ multi-variate discriminants based on event topology, which tends to be isotropic for BB events and jet-like for qq events. A detailed description of these methods is given in Chapter 9. Additional discrimination against the continuum background is provided by the output of the B-flavor tagging algorithms (Chapter 8). In Belle, the tag parameter r ranges from 0 to 1 and is a measure of the likelihood that the b flavor of the accompanying B meson is correctly assigned by the Belle flavor-tagging algorithm. Events with high values of r are well-tagged and are less likely to originate from continuum production. It is found that there is no strong correlation between r and any of the topological variables used above to separate signal from continuum. In BABAR, the background discrimination power arises from the diï¬erence between the tag eï¬ciencies for signal and continuum background in seven tag categories (ctag = 1 . . . 7) which is manifest in terms of a diï¬erent signal purity in each tag category (Section 9.5.3). After the signal candidates are identified, the proper time diï¬erence Ît between the signal B and Btag can be determined from the spatial separation between their decay vertices. The Btag vertex is reconstructed from the remaining charged tracks in the event and its uncertainty dominates the Ît resolution Ïât . The typical proper time resolution is Ïât  â 0.7 ps. The distribution of the proper times provides further discrimination against the continuum backgrounds, which are characterized by smaller values of |Ît| (Section 9.5.3). The parameters of the propertime distributions for signal modes, as well as the tagging eï¬ciencies and mistag fractions, are obtained in dedicated fits to events with identified exclusive B decays as discussed in Section 10.6. 17.7.3 B â ÏÏ and B â ÏÏ An isospin analysis needs as ingredients the measurements of branching fractions, given in Section 17.4, and CP violation parameters, described in this section. 17.7.3.1 B 0 â Ï + Ï â The B Factories started performing time-dependent analyses of B 0 â Ï + Ï â early in their lifetime, and these results were updated on a number of occasions. The following describes only the most recent publications by BABAR"
311,1045,0.99,The Physics of the B Factories,"(+0.0132 â0.0038 ps), the z scale of the detector (0.0070 ps), and analysis bias (0.0070 ps). Belle measures Îmd with a sample of partially-reconstructed B 0 â Dâ+ Ï â events in 29.1 fbâ1 (Zheng, 2003). They select B 0 â Dâ+ Ïhâ events with partial reconstruction of the decay Dâ+ â D0 Ïs+ , using only the hard pion (Ïhâ ) from the B 0 decay and the soft pion (Ïs+ ) from the Dâ+ decay. Using this partial reconstruction method, Belle obtains an order of magnitude more events compared to the full reconstruction of the Dâ+ . The flavor of the other B in the event is identified through a highmomentum lepton ltag from semileptonic decay. Hadronic events are selected by applying requirements on track multiplicity and total energy variables. The hard pion from the B decay must have a momentum in the range 2.05â2.45 GeV/c and the soft pion momentum must be below 450 MeV/c. Belle applies impact parameter requirements on Ïhâ and Ïs+ to suppress backgrounds from interactions of beam particles with residual gas in the beam pipe or the beam pipe wall. They require both tracks to have SVD information and to not be identified as leptons. The event kinematics are fully constrained by fourmomentum conservation in the decays B 0 â Dâ+ Ïhâ and Dâ+ â D0 Ïs+ , the masses of all particles in these decays, the B 0 energy, and the Ïhâ and Ïs+ momenta. Belle uses two variables, the missing D0 mass, MDmiss , and the cosine of the angle between the soft pion in the Dâ+ rest frame and the momentum of the Dâ+ in the center-of-mass frame cos Î¸Ïâ s . The MDmiss distribution for signal events peaks sharply at the nominal D0 mass, while background events spread towards smaller values. Signal events are required to have MDmiss > 1.85 GeV/c and 0.3 < | cos Î¸Ïâ s | < 1.05. The flavor of Brec is determined from the Ïh charge. The flavor of the other B in the event is determined from the charge of ltag . The tag lepton is required to have momentum greater than 1.1 GeV/c and to pass similar requirements on SVD hits and impact parameter as the Brec pions. Tag leptons are rejected if when combined with any other lepton in the event the pair has an invariant mass consistent with a J/Ï . Belle determines the Brec (Btag ) decay vertex from the intersection of the Ïh (ltag ) track with the beam spot accounting for the B meson flight distance. After all selection criteria Belle finds 3433 signal events over a background of 1466 events which are used in the Îmd measurement. Studies of MC-simulated events show that a significant fraction of the selected events come from B 0 â Dâ+ Ïâ decays. Belle simultaneously fits the Ît distributions of the mixed and unmixed events with an unbinned maximumlikelihood method. The B 0B 0 mixing frequency Îmd is the only free parameter in the fit. The B 0 lifetime is fixed to the world average. The signal Ît resolution function uses a triple-Gaussian p.d.f. (see Eq. 10.4.2) in the Ît residuals. The resolution function parameters are determined from decays of J/Ï to e+ eâ and Î¼+ Î¼â . Backgrounds are divided into peaking and non-peaking categories. Non-peaking background is dominated by random combinations of Ïhâ and Ïs+ with primary leptons from"
241,668,0.99,Second Assessment of Climate Change for the Baltic Sea Basin,"One of the most important purposes of regional climate modelling is increasing knowledge of the real world (Laprise 2005). This additional knowledge is commonly termed âadded valueâ (Feser 2006). Identiï¬cation of AV is not an easy task. Small-scale atmospheric ï¬elds are usually less energetic than large-scale ï¬elds (Laprise 2005), so scale decomposition is sometimes necessary to separate the ï¬ner scales. Di Luca et al. (2012) used a diagram, adapted from Orlanski (1975) and von Storch (2005), to illustrate the concept of AV for the range of scales represented by global and regional models, relative to the characteristic temporal and spatial scales of atmospheric processes (Fig. 10.9). Regional climate modelling is mainly expected to add value at regional dimensions below 300 km and temporal scales less than 30 min, which are absent in GCMs. Evaluation of a hypothesis of AV implies a comparison of the performance of the RCM with that of the driving GCM (Feser et al. 2011). To date, the number of studies in which the AV of RCMs is directly analysed is limited and only a few concerns the Baltic Sea region. RCMs could provide AV by adding variability at scales not well resolved by GCMs (at Fig. 10.9 referred to as AV1). However, RCMs can also improve climate simulation at scales resolved by both RCMs and GCMs. This component of AV is referred as AV2 in Fig. 10.9. Because separation of scales is usually made while assessing AV, it is convenient to analyse both types of AV separately (Di Luca et al. 2012). RCMs operate in a limited domain, and two-way interactions between the regional domain and the rest of the globe do not usually occur. In many simulations, the spectral nudging technique ensures that the large scales are not altered too much by the regional model. For all these reasons, AV2 has not been clearly identiï¬ed and existing analyses are usually limited to AV1."
372,1282,0.99,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"corresponding to the beginning of the EoR or earlier and should be detectable in all directions over the sky. However, there is also the cosmic background and the foreground noise from our Galaxy, and the level of these exceeds the distant hydrogen line signal by an estimated factor of 104 . For detection of a broad faint background of radiation, in contrast with detection of discrete sources, sensitivity can be increased by using a large number of small antennas, to maximize sensitivity to broad structural features. In the image domain, .l; m/, the third variable added is the frequency, ,, and in the spatial frequency domain, .u; v/, the corresponding conjugate variable, represents time delay. A basic concern is how redundancy in the array configuration can be chosen to maximize the sensitivity to different angular scales in the search for the reionization signal. Further discussion of the challenges associated with EoR imaging can be found in Parsons et al. (2010, 2012, 2014); Zheng et al. (2013), and Dillon et al. (2015)."
311,2346,0.99,The Physics of the B Factories,"(Fig. 19.3.11, top), where the helicity angle (Î¸H ) is defined in the rest frame of the Dâ as the angle between the primary pion and the slow pion from the Dâ decay. In this fit, the parameters of the D2â0 and Dâ (2600)0 are fixed to those measured in the D+ Ï â . This fit also determined the parameters of the D10 . A complementary fit with | cos Î¸H | < 0.5, shown in Fig. 19.3.11 (middle), is performed to discriminate in favor of the Dâ (2600)0 . To determine the final parameters of the D(2750)0 signal the total Dâ+ Ï â sample is refitted (Fig. 19.3.11 (bottom)), while fixing the parameters of all other BW components to the values determined in the previous fits. The broad resonance D1â²0 is known to decay to this final state, however, these fits were insensitive to its contribution due to its large width and because the background parameters are free. The fit results are summarized in Table 19.3.6. The Dâ (2760)0 signal observed in its decay to D+ Ï â is very close in mass to the D(2750)0 signal observed in Dâ+ Ï â . To have information on the spin of the observed resonances, the data are divided into 10 sub-samples corresponding to cos Î¸H intervals of 0.2. Each sample is fitted with all shape parameters fixed to the values determined from the fits to the total samples. The yields extracted from these fits are plotted for each resonance in Fig. 19.3.12. The cos Î¸H distributions of the D2â and Dâ (2600) are consistent with the expectations for natural parity, defined by P = (â1)J , and leading to a sin2 Î¸H like distribution. This observation supports the assumption that the enhancement assigned to the Dâ (2600) observed in the D+ Ï â and Dâ+ Ï â mass spectra belong to the same state, as only states with natural parity can decay to both D+ Ï â and Dâ+ Ï â . The cos Î¸H distribution for the D(2550)0 is consistent with pure cos2 Î¸H as expected for a J P = 0â state."
311,1557,0.99,The Physics of the B Factories,"gies of remaining calorimeter clusters. However, diï¬erent cluster energy thresholds are applied in diï¬erent regions of the calorimeter: 50 MeV in the barrel region, 100 MeV in the forward endcap and 150MeV in the backward endcap. MC modeling of eï¬ciencies and kinematic distributions is verified by studying B 0 â D(â)â â+ Î½ decays in events with a hadronic Btag . The signal yield is extracted using a two-dimensional, unbinned ML fit to the EECL and cos Î¸B distributions, where the p.d.f.s for the two distributions are treated as uncorrelated. A slight excess of signal events is obtained in the fit, with a significance of approximately 1.5Ï. A branching fraction upper limit of B(B 0 â invisible) < 1.3 Ã 10â4 at 90% C.L. is obtained, which is slightly worse than the expected sensitivity of 1.1Ã10â4 . The diï¬erence in sensitivity between the BABAR and Belle analyses (2.4Ã10â5 and 13Ã10â5 , respectively), roughly a factor of five, is thought to be primarily due to the diï¬erence in tag eï¬ciencies between the hadronic and semileptonic methods, but also reflects diï¬erent optimizations of the level of background between the two experiments.95 The observed distributions of Eextra (EECL ) are shown in Fig. 17.11.4 for both measurements. It is not clear at this point how the sensitivities of the two tag methods compare for B 0 â invisible, since neither experiment has performed these searches using both methods. The details of the signal selection, in particular the detector acceptance and extra energy environment, diï¬er suï¬ciently between BABAR and Belle that it is diï¬cult to draw firm conclusions regarding future B Factory sensitivities. 17.11.3 B 0 â Î³Î³ and Bs0 â Î³Î³ The B 0 â Î³Î³ mode is related to the b â dÎ³ process, as the bÌ and d quarks in the initial state B 0 annihilate The BABAR measurement optimizes the selection in order to achieve the most stringent upper limit, assuming no signal events to be found."
241,1445,0.99,Second Assessment of Climate Change for the Baltic Sea Basin,"where xi is the ith model-simulated response pattern estimated from a ï¬nite ensemble and therefore contaminated with sampling noise ui and u0 is the noise in the observation. As described by Allen and Stott (2002), the presence of this internal variability (noise) in the signal patterns may bias OLS estimates of scaling factor (a) towards zero slope, particularly if only a small ensemble is available to estimate signals. Using a total least squares (TLS) algorithm solves this problem. TLS minimises the perpendicular distance between the scatter points and the best-ï¬t line, not the vertical distance minimised by OLS. Therefore, the bias of best"
151,46,0.99,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"in this compartment. As CPY partitions into the adsorbed compartment, less is available for degradation, and the rate of desorption, described by the rate constant kdes becomes the rate limiting step. This transition from km to kdes creates the biphasic behavior in the model. Further details on the equations used the model set-up and typical results are given in SI Appendix C. The model results fit the data well (SI Appendix C; SI Table C-2) (Bidlack 1979). The resulting rate constant represents the entire data set for each soil, optimized simultaneously and represents a consistent model across all the soils considered. This provides a better representation of the half-life than the values in the original reports. As noted above, it is expected that the rate constants might be correlated with the physical and chemical properties of the soils such as % organic matter, etc. No significant correlation could be found among rate constants or half-lives with the KOC, or water-holding capacity. It has been suggested that there might be a correlation between the rate constant km for degradation of CPY, and pH (Bidlack 1979). This is expected, given the dependence of the abiotic hydrolysis on pH, which contributes to this process, but the correlation is not simple. A graph of half-life vs. pH is shown (Fig. 3). It is possible to consider the data in two groups; one group of soils has half-lives >30 d, which were pH dependent; the other group had shorter half-lives with a much weaker correlation to pH. The correlations for the two groups in the range from pH 5 to 8 are given in (1) and (2)."
216,283,0.99,Advances in Production Technology,"13.5 Use of Biomechanical Stress Curves in Context of Adaptive Workplace Design For further analysis of the ï¬ndings of the study, and to derive new approaches for workplace design, the data were examined in a variance analysis (ANOVA). The small sample size (n = 16), the data in part not following a normal distribution (Shapiro-Wilk test: p < 0.05) and, in some datasets, dissimilarity of variances (Levene test: p < 0.05) do violate the preconditions of an ANOVA, but it was carried out nevertheless. To meet the data situation under these conditions, an additional non-parametric test, the Friedman test, was performed with the Wilcoxon-ranking sum test as a post hoc test. The signiï¬cance level was matched on the basis of the Bonferroni correction (Î± = 0.016). Although its preconditions were violated, the ï¬ndings of ANOVA were conï¬rmed by the non-parametric test. Table 13.1 lists the results of the analysis of variances. They indicate the partly signiï¬cantly different body part related muscle activations. The table shows a comparison of pairs of muscle activation relative to body parts between two directions of movement (DoM: xâto the right, yâto the front, zâto the top) as a function of the coordinates of movement (CoM), body parts, and weights (M1, M2). The direction of movement with the comparatively higher muscle activation is indicated in all cases. The ï¬elds marked in colors characterize signiï¬cant differences (power â¥ 0.7) in direction-dependent muscle activation per body part. It is evident that, even with the simple movements studied, signiï¬cant body part related differences in muscle activation can occur. The number clearly increases for weight No. 2. Among the movements studied, most of the signiï¬cantly higher muscle activations were found in the shoulder and the neck. Moreover, movements"
175,1042,0.99,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","hydrological attributes that can be managed. Shown in this ï¬gure is the impact of hydroperiod duration on the habitat of three different species of periphyton located in different parts of the Everglades, and the impact of the duration of the hydroperiod as well as the number of years since the last dry period on a species of ï¬sh. There are other functions that would influence the growth of periphyton and ï¬sh, such as the concentrations of phosphorus or other nutrients in the water. These are not shown. Figure 9.8 merely illustrates the construction of habitat suitability indices. There are situations where it may be much easier and more realistic to deï¬ne a range of some hydrological attribute values as being ideal. Consider ï¬sh living in streams or rivers for example. Fish desire a variety of depths, depending on their feeding and spawning activities. Ideal trout habitat, for example, requires both deeper pools and shallower riffles in streams. There is no one ideal depth or velocity or even a range of depths or velocities above or below some threshold value. In such cases it is possible to divide the hydrologic attribute scale into discrete ranges and identify the ideal fraction of the entire stream or river reach or wetland that ideally would be within each discrete range. This exercise will result in a function resembling a probability density function. The total area under the function is 1. (The ï¬rst and last segments of the function can represent anything less or greater than some discrete value, where appropriate, and if so the applicable segments are understood to cover those ranges.) Such a function is shown in Fig. 9.10. Figure 9.9 happens to be a discrete distribution, but it could have been a continuous one as well. Any predicted distribution of attribute values resulting from a simulation of a water management policy can be compared to this ideal distribution, as is shown in Fig. 9.10. The fraction of overlapping areas of both distributions is an indication of the suitability of that habitat for that indicator species."
231,267,0.99,North Sea Region Climate Change Assessment,"Care should be taken if the precipitation fraction exceeding the 95th percentile (R95pTOT) is determined over a climatological period of several decades, since extremes may have increased disproportionally and thus the shape of the distribution may have changed. For example, an index S95pTOT, using the Weibull shape parameter instead of an explicit estimate of the 95th percentile, can be used (Leander et al. 2014). Northern Europe shows a (signiï¬cant) increase in R95pTOT, but this is far less pronounced for S95pTOT. Since R95pTOT cannot distinguish between a shift in the median of the probability distribution for precipitation and a change in only the tail of the distribution, trends are generally âmore negativeâ for S95pTOT, especially over southern Scandinavia, the Netherlands, Germany and the UK. Zolina et al. (2009) introduced a new index for R95pTOT, making use of a gamma distribution for wet day precipitation amounts and the associated theoretical distribution of the fractional contribution of the wettest days to the seasonal or annual total. The trend results for their new index are similar to R95pTOT. Another way of analysing changes in precipitation is by counting the number of wet days. An example of this is the index CWD (maximum number of consecutive wet days, here deï¬ned as the number of days with precipitation â¥1 mm). Trends in the station records for the period 1951â 2012 are shown in Fig. 2.16, which indicates that most of the stations in the North Sea region show a slight increasing"
311,2208,0.99,The Physics of the B Factories,"where PCM denotes the CM four-momentum of the initial e+ eâ frame of reference and ROE stands for the ârest of the eventâ, i.e. the sum of the CM system four-momenta of all detected neutral and charged candidates in an event except for the signal kaon and lepton. Neutrals with energy less than 70 MeV/c2 and charged tracks with an impact parameter larger than 5 cm (2 cm) in z (xy) are not used in this context, although they are used in the computation of event-shape variables. Two kinematic constraints are"
311,1672,0.99,The Physics of the B Factories,"Figure 18.2.3. Fit to (a) the KS0 K Â± Ï â and (c) the K + K â Ï + Ï â Ï 0 mass spectra in two-photon fusion events (del Amo Sanchez, 2011h). The solid curves represent the total ï¬t functions and the dashed curves show the combinatorial background contributions. The background-subtracted distributions are shown in (b) and (d), where the solid curves indicate the signal components. The prominent peak is the Î·c , while the small peak above 3 GeV/c2 is due to residual ISR J/Ï production. The peaks in the insets are (left to right) Ïc0 , Ïc2 , and Î·c (2S). The Ïc0 peak is not present in (a) and (b), since the decay to KS0 K Â± Ï â is not allowed for this state."
175,645,0.99,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","when this assumption may be reasonable. Often a more sophisticated multi-scaling model is appropriate (Gupta and Dawdy 1995a; Robinson and Sivapalan 1997). Generally the mean is employed as the index flood. The problem of estimating the pth quantile xp is then reduced to estimating the mean for a site Âµx, and the ratio xp/Âµx of the pth quantile to the mean. The mean can often be estimated adequately with the record available at a site, even if that record is short. The indicated ratio is estimated using regional information. The British Flood Studies Report (NERC 1975) calls these normalized flood distributions growth curves. Key to the success of the index-flood approach is identiï¬cation of sets of basins that have similar coefï¬cients of variation and skew. Basins can be grouped geographically, as well as by physiographic characteristics including drainage area and elevation. Regions need not be geographically contiguous. Each site can potentially be assigned its own unique region consisting of sites with which it is particularly similar (Zrinji and Burn 1994), or regional regression equations can be derived to compute normalized regional quantiles as a function of a siteâs physiographic characteristics and other statistics (Fill and Stedinger 1998). Clearly the next step for regionalization procedures, such as the index-flood method, is to move away from estimates of regional parameters that do not depend upon basin size and other physiographic parameters. Gupta et al. (1994) argue that the basic premise of the index-flood method, that the coefï¬cient of variation of floods is relatively constant, is inconsistent with the known relationships between the coefï¬cient of variation CV and drainage area (see also Robinson and Sivapalan 1997). Recently, Fill and Stedinger (1998) built such a relationship into an index-flood procedure using a regression model to explain variations in the normalized quantiles. Tasker and Stedinger (1986) illustrated how one might relate log-space skew to physiographic basin characteristics (see also Gupta and Dawdy 1995b). Madsen and Rosbjerg (1997b) did the same for a regional model of Îº for the"
311,1609,0.99,The Physics of the B Factories,"17.12.3.2 Rare decays The first charmless baryonic B decay observed was B + â ppK + , in an analysis of a 29.4 fbâ1 data sample (Abe, 2002f). One unexpected feature of this rare decay process is that the observed mass distribution of the baryonantibaryon pair is peaked near threshold as shown in Fig. 17.12.9. To ensure that the measured events are genuine non-b â c signals, the regions 2.850 < M (pp) < 3.128 GeV/c2 and 3.315 < M (pp) < 3.735 GeV/c2 are excluded to remove background from modes with Î·c and J/Ï mesons, and Ï â² , Ïc0 , and Ïc1 mesons, respectively. The mass distribution of vetoed events can be found in the inset plot of Fig. 17.12.9, where a J/Ï peak can be clearly identified. Since the eï¬ciency of particle identification varies with respect to the particleâs momentum, the overall reconstruction eï¬ciency is dependent on the mass of the baryon-antibaryon system. The partial branching fractions in bins of baryon-antibaryon mass are then summed to obtain the total branching fraction."
372,893,0.99,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"8.8.4 FX Correlator The designation FX indicates a correlator in which Fourier transformation to the frequency domain is performed before cross multiplication of data from different antennas. In such a correlator, the input bit stream from each antenna is converted to a frequency spectrum by a real-time FFT, and then for each antenna pair, the complex amplitudes for each frequency are multiplied to produce the cross power spectrum. A major part of the computation occurs in the Fourier transformation, for which the total number of operations is proportional to the number of antennas. In comparison, in a lag correlator, the total computation is largely proportional to the number of antenna pairs. Thus, the FX scheme offers economy in hardware, especially if the number of antennas is large (see Sect. 8.8.5). The principle of the FX correlator, based on the use of the FFT algorithm, was discussed by Yen (1974) and first used in a large practical system by Chikada et al. (1984, 1987). Description of system designed for the VLBA are given by Benson (1995) and Romney (1999). Two slightly different implementations of the FX correlator have been used. In one, both in-phase and quadrature components of the signal are sampled to provide a sequence of N complex samples, which is then Fourier-transformed to provide N values of complex amplitude, distributed in frequency over positive and negative frequencies. In the other, N real samples are transformed to provide N values of complex amplitude. However, the negative frequencies are redundant, and only N=2 spectral points need be retained. We follow the second scheme in the discussion below. Figure 8.22 is a schematic diagram of the basic operations of an FX correlator. The input sample stream from an antenna is Fourier transformed in contiguous sequences of length-N samples, where N is usually a power-of-two integer for efficiency in the FFT algorithm. The output of each transformation is a series of N complex signal amplitudes as a function of frequency. The frequency spacing of the data after transformation is 1=.Nts /, where ts is the time interval between samples of the signals. In the cross-multiplication process that follows the FFT stage, the complex amplitude from one antenna of each pair is multiplied by the complex conjugate of the amplitude of the other. These multiplications occur in the correlator elements in Fig. 8.22. Note that the data in any one input sequence are combined only with data from other antennas for the same time sequence. This leads to some differences in the effective weighting of the data in the FX and XF designs."
311,599,0.99,The Physics of the B Factories,"In addition to the combinatoric backgrounds, there exist fully or partially reconstructed backgrounds that originate from decays of the same class of parent meson to a ï¬nal state similar to the one under consideration. For example, in an analysis of the decay B 0 â KS0 Ï + Ï â there are potentially large backgrounds from many other B decays including B 0 â Î· â² (â Ï0 Î³)KS0 , B 0 â Dâ (â KS0 Ï â )K + , and B + â KS0 Ï + . In the ï¬rst of these examples the decay has been partially reconstructed but the energy of the missing photon is suï¬ciently small that the reconstructed B 0 candidate passes selection criteria. In the second case the decay is fully reconstructed but a kaon/pion misidentiï¬cation occurs. In the third case the decay is again fully reconstructed and combined with an additional soft pion from the rest of the event to form a signal candidate. Each of these scenarios can lead to very diï¬erent distributions of events in the Dalitz plot. In general, the distributions of backgrounds across the Dalitz plot are rather diï¬cult to model with parametric functions. Additionally, the precise nature of the backgrounds can vary dramatically from one analysis to another. Thus, the most common approach for modeling the Dalitz-plot distributions of the backgrounds is to use histograms obtained from either Monte Carlo simulation or sidebands in data. Often some form of smoothing or interpolation is applied to the histograms in order to limit the eï¬ect of statistical ï¬uctuations in the input data sample. In B decay analyses, it is found that most backgrounds (particularly the dominant combinatoric backgrounds) preferentially populate the corners and edges of the Dalitz plot. In order to increase the resolution of the histograms in these regions, adaptive binning techniques can be used and/or the histograms can be formed in the so-called âsquare Dalitz plotâ, which is discussed in detail in Section 13.4.1. 13.3.2 Efficiency The most obvious eï¬ect of detector acceptance is a reduction in the number of events detected. In three-body decays this is complicated by the fact that the kinematic properties of the decay products diï¬er accross the Dalitz plot. Thus, the acceptance as a function of the Dalitz plot variables is, in general, nonuniform. The typical acceptance function drops at the corners of the phase space, which correspond to the kinematic conï¬guration where one of the ï¬nal state particles is produced at rest in the frame of the decaying particle. Reconstruction eï¬ciency is typically smaller for such particles, especially if the decaying particle has a small boost in the laboratory frame. At BABAR and Belle, the eï¬ciency proï¬le is usually well modeled by the full detector simulation. The proï¬le is then modeled either by a parameterized form, such as a two-dimensional polynomial, or by a histogram. Either way, this allows the eï¬ciency as a function of the position in phase space, Îµ (m), to be included in the signal Dalitzplot model, where it multiplies the squared absolute value of the amplitude. When histograms are used they often"
349,281,0.99,Methods for Measuring Greenhouse Gas Balances and Evaluating Mitigation Options in Smallholder Agriculture,"where: SE is the standard error for the entire population ah is the area of the stratum h Sh is the variance of stratum h A is the total area of the study Scaling SOC stocks from a few point source measurements (fields) to the whole farm necessarily requires a series of assumptions unless all fields within the farm are sampled (which may be highly unpractical). Here, it is assumed that the center and perimeter of each field are georeferenced so that the fieldâs surface area can be determined. In the proposed scheme, samples within a given farm should be taken along the previously described land use intensity gradient (i.e., home gardens, close-distance, mid-distance, and remote fields) at their most spatially representative fields. If for a given section (i.e., close-distance fields), there is an occurrence of individual fields with annual and perennial vegetation (crops or trees), and the area of the smaller field is at least half the size of bigger field, then sampling should be conducted at both fields. The average SOC stock for the selected farm is then calculated considering both the mean SOC stock obtained for each section and the area occupied by each section. The calculation procedure is similar to the one described for the landscape scale, and it simply replaces strata by sections. Uncertainties in SOC stock assessments vary according to the scale and the spatial landscape unit. Goidts et al. (2009) demonstrated that scaling up field scale measurements to the landscape level increases the coefficient of variation of SOC estimates. However, the same work showed that such uncertainty may be smaller than errors associated to local spatial heterogeneity and analytical procedures."
246,71,0.989,Rewilding European Landscapes,"grazing density to account for human transformations in semi-natural grasslands. We expressed the dPNV value by subtracting from 1 the score calculated according to the described methodology. (Fig. 2.1c). Through agriculture, hunting, fishing and forestry, humans are removing significant quantities of biomass from the ecosystems. Primary productivity (PP) is the foundation of trophic networks and it influences the structure and functions of ecosystems in a domino effect across trophic levels (Haberl et al. 2004). Humans have reduced drastically the PP available to other species and this has changed the composition of the ecological communities (Barnosky 2008; Pereira et al. 2012). We map the proportion of human harvested PP out of the total potential PP in Europe as another indicator of wilderness and using the data analysed in Haberl et al. (2007). We calculated the harvested PP by extracting net PP remaining in ecosystems after harvest from the net PP of the actual vegetation. We then calculated the proportion of harvested PP by dividing net harvested PP by net PP of the potential vegetation. The data are calculated based on country-level statistics of the Food and Agriculture Organization (Haberl et al. 2007) while potential PP is estimated using the Lund-Potsdam-Jena dynamic global vegetation model (Sitch et al. 2003). Some abnormalities can be noticed in the harvested PP map which are due to the assumptions of the model and the FAO national level data. The map has to be interpreted with this limitation in mind (Fig. 2.1d). The four resulting maps based on the selected metrics show a common pattern of high human footprint in the lowlands of central Europe (Fig. 2.1). The most unaltered values of all metrics occur in high mountainous areas and Scandinavia. But the differences at intermediate values of wilderness provide a key signal to what are the strongest determinants of human footprint at regional level in Europe. For example, although the dPNV is very low in almost all of Scandinavia (Fig. 2.1c), the proportion of harvested PP is comparatively higher, consistent with high forestry harvest in the Nordic countries (Fig. 2.1d). The reverse pattern is noticeable in the Iberian Peninsula where although the drier climate restricts high harvesting of PP, the current vegetation is quite far from PNV as measured in our map and consistent with the degradation of the Mediterranean habitats (Myers et al. 2000). In the same region, the significant differences between the inland and coastal values of the night light impact and human access (Fig. 2.1a and b) indicates the high difference between the human population densities inland compared with the coastal regions. These differences in the distribution of human populations are masked in the PNV score and harvested PP maps (Fig. 2.1c and d). The map of artificial light (Fig. 2.1b) also points out to a discrepancy in the relative wilderness values in East and SouthEast Europe compared with the dPNV score map for example (Fig. 2.1c). The lower economic activity in this area results in lower light impact although the level of vegetation change is very high (Doll et al. 2006). The lowest wilderness areas in Europe have usually low scores for all the wilderness dimensions considered, and they represent mainly areas of high human densities and intense economic activity. Conversely, high wilderness areas are the wildest from all the points of view taken here. But the areas of intermediate wilderness values are strongly impacted by only one or two metrics with very low wilderness"
175,943,0.989,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","Benjamin and Cornell (1970). Equation 8.6 for E[I] shows that in the presence of substantial uncertainty, the mean of the output from nonlinear systems is not simply the system output corresponding to the mean of the parameters (Gaven and Burges 1981, p. 1523). This is true for any nonlinear function. Of interest in the analysis of uncertainty is the approximation for the variance Var[I] of indicator I. In Eq. 8.10 the contribution of Pi to the variance of I equals Var[Pi] times [âF/âPi]2, which are the squares of the sensitivity coefï¬cients for indicator I with respect to each input parameter value Pi. An Example of First-Order Sensitivity Analysis It may appear that ï¬rst-order analysis is difï¬cult because the partial derivatives of the performance indicator I are needed with respect to the various parameters. However, reasonable approximations of these sensitivity coefï¬cients can be obtained from the simple sensitivity analysis described in Table 8.3. In that table, three different parameter sets, Pi, are deï¬ned in which one parameter of the set is at its high value, PiH, and one is at its low value, PiL, to produce corresponding values (called high, IiH, and low, IiL) of a system performance indicator I. It is then necessary to estimate some representation of the variances of the various parameters with some consistent procedure. For a normal distribution, the distance between the 5 and 95 percentiles is 1.645 standard deviations"
372,816,0.989,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"Fig. 8.10 Examples of quantization characteristics for (left diagram) an even number of levels (eight), and (right diagram) an odd number of levels (nine). Units on both axes are equal to *. The abscissa is the analog (unquantized) voltage, and the ordinate is the quantized output. The dotted curves show the analog level minus the quantized level. Note that for even numbers of levels, the thresholds occur at integral values on the abscissa, whereas for odd numbers of levels, they occur at values that are an integer plus one-half."
175,621,0.989,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","of variation of a log-Pearson type 3 variate X as a function of the standard deviation ÏY and coefï¬cient of skew Î³Y of the log-transformation Y = ln (X). Thus the standard deviation ÏY and skew Î³Y of Y are in log space. For Î³Y = 0, the log-Pearson type 3 distribution reduces to the two-parameter lognormal distribution discussed above, because in this case Y has a normal distribution. For the lognormal distribution, the standard deviation ÏY serves as the sole shape parameter, and the coefï¬cient of variation of X for small ÏY is just ÏY. Figure 6.7 shows that the situation is more complicated for the LP3 distribution. However, for small ÏY, the coefï¬cient of variation of X is approximately ÏY. Again, the flood flow data in Table 6.2 can be used to illustrate parameter estimation. Using natural logarithms, one can estimate the log-space moments with the standard estimators in Eqs. 6.39a that yield ^ Â¼ 7:202 ^ Â¼ 0:5625 ^c Â¼ 0:337 For the LP3 distribution, analysis generally focuses on the distribution of the logarithms Y = ln(X) of the flows, which would have a"
311,1852,0.989,The Physics of the B Factories,"Î·Î¥ (1S), and Î³Î³(Ï 0 Ï 0 )Î¥ (2S). The models for these and signal are obtained from MC simulation. A clear excess exists in the region where Î¥ (1 3DJ ), and is fitted with the signal model. Large data control samples of dipion transitions to Î¥ (1S) and Î¥ (2S) final states, directly from the parent Î¥ (3S), are used to validate the p.d.f.s used in the fit to the spectrum. Where shifts are present between the p.d.f. parameters determined from MC or data, the shifts are applied as corrections. Only a small shift in the reconstructed mass of the Î¥ (2S) is observed. The yield of D-wave triplet states is as follows (deter+5.7 mined from the fit to the data): 10.6â4.9 Î¥ (1 3D1 ), 33.9â7.5 Î¥ (1 3D2 ), and 9.4â5.2 Î¥ (1 3D1 ). Fit biases for the yield of signal events are determined by applying the data model to 2000 data-sized MC samples with events randomly drawn from the simulation subsamples. The biases are found to typically be at the level of 1-2 events in the signal region, and these biases are subtracted from the signal yields. Multiplicative systematic uncertainties arise from various sources, with the largest of them being the photon reconstruction eï¬ciency (3.0%) and particle identification (2.0%). Additive systematic uncertainties arise from the p.d.f. shapes, and total 1.5-2.0 events in the signal yields. The statistical significance of the signal yield for the J = 2 D-wave triplet state is 6.5Ï (5.8Ï) including statistical (statistical and systematic) uncertainties. The quantum numbers of the state are determined by studying the Ï + Ï â mass distribution after subtracting the backgrounds in the region 10.155 < mÏ+ Ïâ â+ ââ < 10.68 GeV/c2 . The dipion mass distribution is shown in Fig. 18.4.12, compared to the shapes expected from an S-"
372,378,0.989,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"are usually made for cases in which the source is small compared with the width of the main beam, and for such measurements, the beam should be centered on the source. Faraday rotation of the plane of polarization of incoming radiation occurs in the ionosphere and becomes important for frequencies below a few gigahertz; see Table 14.1. During polarization measurements, periodic observations of a strongly polarized source are useful for monitoring changes in the rotation, which varies with the total column density of electrons in the ionosphere. If not accounted for, Faraday rotation can cause errors in calibration; see, for example, Sakurai and Spangler (1994). In some antennas, the feed is displaced from the axis of the main reflector, for example, when the Cassegrain focus is used and the feeds for different bands are located in a circle around the vertex. For circularly polarized feeds, this departure from circular symmetry results in pointing offsets of the beams for the two opposite hands. The pointing directions of the two beams are typically separated by % 0:1 beamwidths, which makes measurements of circular polarization difficult because Vv is proportional to .Rrr ! R`` /. For linearly polarized feeds, the corresponding effect is an increase in the cross-polarized sidelobes near the beam edges. In VLBI, the large distances between antennas result in different parallactic angles at different sites, which must be taken into account. The quantities m` and mt , of Eqs. (4.20) and (4.22), have Rice distributions of the form of Eq. (6.63a), and the position angle has a distribution of the form of Eq. (6.63b). The percentage polarization can be overestimated, and a correction should be applied (Wardle and Kronberg 1974)."
175,819,0.989,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","From the distribution shown in Fig. 7.1 it is obvious that the likelihood of different values of the random variable varies; values in the vicinity of r* are much more likely to occur than are values at the tails of the distribution. A uniform distribution is one that looks like a rectangle; any value of the random variable between its lower and upper limits is equally likely. Using Eq. 7.2, together with a series of uniformly distributed (all equally likely) values of p* over the range from 0 to 1 (i.e., along the vertical axis of Fig. 7.2), a corresponding series of random variable values, r*, associated with any distribution can be generated. These random variable values will have a cumulative distribution as shown in Fig. 7.2, and hence a density distribution as shown in Fig. 7.1, regardless of the types or shapes of those distributions. The mean and variance of the distributions will be maintained. The mean and variance of continuous distributions are"
175,1045,0.989,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","Ã°9:7Ã In Fig. 9.10, this is the red shaded area under the blue curve. To identify a representative set of indicator species of any ecosystem, the hydrologic attributes or âstressorsâ that impact those indicator species, and ï¬nally the speciï¬c functional relationships between the hydrologic attributes and the habitat suitability performance indicators, is not a trivial task. The greater the number of experienced individuals involved in such an exercise, the more difï¬cult it may be to reach"
311,1575,0.989,The Physics of the B Factories,"more than 99% of the background while retaining 11â26% of the signal, depending on the mode. Signal yield is estimated in the âsignal regionâ, 5.27 < mES < 5.29 GeV/c2 and â0.055(â0.035) < ÎE < 0.035 GeV for the e+ e+ and e+ Î¼+ modes (Î¼+ Î¼+ mode). The background region is defined as the complement of the analysis region excluding the signal region. The amount of background is determined by fitting the 2-dimensional (ÎE, mES ) p.d.f. to the data in the background region and then integrating the fitted p.d.f. over the signal region. There was no event observed in the signal region of any mode. Figure 17.11.10 shows the (ÎE, mES ) distribution of selected B + â Dâ Î¼+ Î¼+ candidates. Upper limits (at the 90% C.L.) are calculated based on a frequentist approach (Feldman and Cousins, 1998) including systematic uncertainties using the POLE program, (Conrad, Botner, Hallgren, and Perez de los Heros, 2003). The results are summarized in Table 17.11.2, and are in the range [1.1, 2.6] Ã 10â6 ."
34,254,0.989,"Habitats and Biota of the Gulf of Mexico: Before the Deepwater Horizon Oil Spill: Volume 2: Fish Resources, Fisheries, Sea Turtles, Avian Resources, Marine Mammals, Diseases and Mortalities","with the distances between the estuaries (Gold et al. 1993, 1999). Tagging studies suggest that juvenile red drum have limited dispersal but that adults can travel considerable distances in the Gulf of Mexico (Osburn et al. 1982; Overstreet 1983). Metapopulation structure may exist for the red drum in the Gulf of Mexico, and despite the likely complex spatial structure of the stock, red drum in the Gulf of Mexico is considered as a single stock, which implicitly assumes no spatial heterogeneity in the Gulf of Mexico red drum population. The impacts of this unrealistic assumption on the stock assessment and management of red drum are unknown."
372,409,0.989,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"the parabola. The effects can be largely canceled by inserting a compensating offset in a secondary reflector. For further details, see Chu and Turrin (1973) and Rudge and Adatia (1978). A basic point concerns the accuracy of the reflector surface. Deviations of the surface from the ideal profile result in variations in the phase of the electromagnetic field as it reaches the focus. We can think of the reflector surface as consisting of many small sections that deviate from the ideal surface by !, a Gaussian random variable with probability distribution e!! =2# ; p.!/ D p 2"" #"
311,1213,0.989,The Physics of the B Factories,"and K â â KÏ phases for B 0 â J/ÏK â , and Dalitz plot phases for B 0 â D(â)0 [KS0 Ï + Ï â ]h0 and Dâ Dâ KS0 and other three-body decays). The result in terms of angle is Ï1 â¡ Î² = (21.30 Â± 0.78)â¦ . The direct CP asymmetry parameter C is found to be consistent with zero in these channels, as expected in the SM. The consistency between Ï1 and other CKM angles and sides of the Unitarity Triangle demonstrates that the KM mechanism is the dominant source of CP violation in the SM. Kobayashi and Maskawa shared the 2008 Nobel Prize in physics for their work on the KM mechanism presented in (Kobayashi and Maskawa, 1973). The test of the CKM matrix by examining the agreement between diï¬erent measurements is discussed in Section 25.1. A number of other channels have been studied by the experiments at the B Factories. These are suppressed to various degrees in the SM compared to b â ccÌs transitions. They are either tree dominated modes with a penguin (or another tree) contribution that has a diï¬erent weak phase (J/ÏÏ 0 , D(â) D(â) or D(â) h0 ), or charmless modes (b â sq qÌ). The penguin-dominated modes are particularly sensitive to the presence of any postulated new heavy particles that could contribute to such a loop transition. The most precisely determined time-dependent asymmetry parameters from a loop dominated b â sq qÌ channel come from B 0 â Î· â² K 0 and K + K â K 0 with a precision of 0.07 on sin 2Ï1 . The uncertainties of other modes range from around 0.2 to 0.7. The sin 2Ï1 results obtained from these measurements are consistent with the value measured in the b â ccÌs golden channels. The naÄ±Ìve average of charmless decays is within one sigma of b â ccÌs results. However, it should be noted that the naÄ±Ìve average is not a good observable to use when searching for NP, as the hadronic uncertainties vary from mode to mode. The most recent measurements of these decays are consistent with the SM. No significant direct CP asymmetry is found in the channels discussed in this section. However some of these channels exhibit central values that are more than 2Ï from C = 0 (e.g., D+ Dâ for Belle and ÏKS0 for BABAR). The global Ï2 among the diï¬erent channels studied is consistent with the interpretation that these measurements are the result of a statistical fluctuation."
241,539,0.989,Second Assessment of Climate Change for the Baltic Sea Basin,"Relative Sea Level In recent years, changes in Baltic Sea RSL, as the most important factor in impact studies, have been the subject of a wider range of publications, mainly on the regional (e.g. national) to local (e.g. only one tide gauge) level. However, a comparison of the different results is hampered by different observation periods (and the treatment of data gaps therein) and analysis techniques. Thus, the cited RSL trend values that follow must be treated with great care. Navrotskaya and Chubarenko (2012) studied annual (and extreme) sea levels in the Russian sector of the Vistula Lagoon situated in the south-eastern part of the Baltic Sea. They found annual mean linear RSL trends in the range 1.7â1.9 mm yearâ1 (Baltiisk, 1860â2006; Kaliningrad, 1901â2006) to 3.6â3.7 mm yearâ1 (Kaliningrad, Baltiisk, Krasnoflotskoe, 1959â2006). On the sea coast (Pionerskii,"
372,1360,0.989,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"because the spatial sensitivity function of the antenna falls to low levels as a result of the tapered illumination of the reflector. Missing data at low .u; v/ values result in broad negative sidelobes of the synthesized beam, such that the beam appears to be situated in a shallow bowl-shaped depression. This effect is most noticeable when the field to be imaged is wide enough that there are several empty .u; v/ cells within the central area. The transfer function AN .u/ is the autocorrelation function of the field distribution over the antenna aperture and depends on the particular design of the antenna, including the illumination pattern of the feed. The solid curve in Fig. 11.7 shows AN for a uniformly illuminated circular aperture, which can be regarded as an ideal case. Since there is usually some tapering in the illumination of a reflector antenna, in practice, AN will generally fall off somewhat more rapidly than the curves shown. The function AN in Fig. 11.7 is proportional to the common area of two overlapping circles of diameter d, and the abscissa is the distance between their centers. In three dimensions, this function is sometimes referred to as the Chinese hat function, and its properties are discussed by Bracewell (1995). The dashed curves in Fig. 11.7 show the relative spatial sensitivity for an interferometer using two uniformly illuminated circular apertures of diameter d. Curve 1 is for a spacing of 1:4d between the centers of the apertures; curve 2 is for a spacing of 1:25d. If both total-power and interferometer data are obtained, it can be seen that the minimum sensitivity occurs for spacings of approximately half of the antenna spacing. One solution to increasing the minimum sensitivity in the spatial frequency coverage is the addition of total-power measurements from a larger antenna [see, for example, Bajaja and van Albada (1979), Welch and Thornton (1985), StanimirovicÌ (2002)]. StanimirovicÌ considered the requirements for the use of single-antenna measurements of fringe visibility and concluded that the diameter of the antenna"
0,84,0.989,Library and Information Sciences : Trends and Research,"The proximity matrix P is a k Ã k symmetric matrix. It serves as input data for the MDS analysis. Here k is the number of the valid words in the two categories and n is the number of the valid records in the categories. In the process of mapping the relationships in high dimensional space onto a low dimensional space, information loss and disparity is inevitable and must be controlled in a tolerable range. Therefore, an evaluation criterion was used to assess the reliability and effectiveness of projecting by MDS. The quality of the projection can be measured by the stress value (S), which is defined in Eq. (3). The smaller a stress S is, the better the relationships among the objects in the low dimensional space reflect the relationships among the objects in the high dimensional space and vice versa (Zhang and Zhao 2013)."
311,813,0.989,The Physics of the B Factories,"predictions. An important consistency check for the overall shape of F (k) is to give a reasonable description of the measured shape of the photon-energy spectrum in inclusive B â Xs Î³ decays (see Section 17.9), which at leading order in 1/mb is given in terms of the same function F (k) via an expression analogous to Eq. (17.1.62). In addition to the leading shape function F (k), several additional shape functions appear at O(ÎQCD /mb ) (Bauer, Luke, and Mannel, 2003). Apart from their ï¬rst few moments, very little is known about the form of these subleading shape functions. They thus introduce an uncertainty in the theoretical predictions that is hard to quantify in a systematic fashion. An even larger number of unknown shape functions appears at O(Î±S ÎQCD /mb ) (Lee and Stewart, 2005). Weak annihilation contributions could have a large impact at large q 2 and might be another source of theoretical uncertainties. However, recent analyses (Bigi, Mannel, Turczyk, and Uraltsev, 2010; Gambino and Kamenik, 2010; Ligeti, Luke, and Manohar, 2010) have used CLEOc data to constrain contributions from weak annihilation, resulting in a rather small impact. The corresponding uncertainty is below 2% for the total rate, translating into an uncertainty of less than 1% on |Vub | for the most inclusive analyses. For the determination of |Vub | theoretical predictions by diï¬erent groups are in use. A more detailed summary and comparison can be found elsewhere (Antonelli et al., 2010a). At their core, the diï¬erent calculations are all based on Eq. (17.1.62), but they diï¬er in the treatment of the perturbative and non-perturbative contributions. The BLNP approach (Bosch, Lange, Neubert, and Paz, 2004; Lange, Neubert, and Paz, 2005) preferentially treats the kinematic region p+ X âª pX where the pX and pX dependences of C factorize. This allows for the resummation of Sudakov double logarithms of p+ X /pX and pX /mB to NNLL. They also include the full O(Î±S ) corrections, for which the perturbative expansions are performed using the so-called shape-function scheme for mb , and a subset of the perturbative corrections in C are absorbed into F (k). The subleading shape functions are separately modeled and included in the predictions. The GGOU approach (Gambino, Giordano, Ossola, and Uraltsev, 2007) treats the p+ X âª pX and pX â¼ pX regions on the same footing. The coeï¬cient C is computed at ï¬xed order to O(Î±S ) and O(Î±S2 Î²0 ) (Gambino, Gardi, and Ridolï¬, 2006), where the perturbative expansion is performed using the kinetic scheme for mb . In this case no resummation eï¬ects at small p+ X are included. The eï¬ect of resummation as well as all contributions from subleading shape functions are absorbed into F (k). This results in three non-universal distribution functions Fi (k, q 2 ), which have subleading dependence on q 2 . In the dressed-gluon exponentiation (DGE) approach (Andersen and Gardi, 2006; Gardi, 2008) the perturbative expansion includes the NNLL resummation in moment space as well as the full O(Î±S ) and O(Î±S2 Î²0 ) corrections. It also incorporates an internal resummation of running coupling corrections in the Sudakov exponent. This"
311,1475,0.989,The Physics of the B Factories,"high sensitivity to specific Wilson coeï¬cients (Feldmann and Matias, 2003). These authors predict only a rather small isospin asymmetry in the SM, with a positive sign at low q 2 . In (Aubert, 2009j) BABAR reported evidence for a large negative isospin asymmetry in the q 2 range below the J/Ï, with about 3Ï significance in both B â Kâ+ ââ and B â K â â+ ââ . However, the latest BABAR results (Lees, 2012i) are more consistent with null asymmetry, as listed in Table 17.9.11. Belleâs results (Wei, 2009) with higher statistics are compatible with BABAR and also with null asymmetry. Note that the average AI values are still negative, and about 2Ï from zero, so higher-precision measurements are desirable. Neither experiment sees a significant asymmetry in the high-q 2 region. See the cited papers for measured isospin asymmetries in all q 2 bins. These are illustrated for Belle in Fig. 17.9.14. 17.9.7.3 Angular distributions in B â K â â+ ââ : formalism and theory In the B â K â â+ ââ decay mode the angular distribution contains useful information about the diï¬erent amplitudes. This is in contrast to B â Kâ+ ââ or B â K â Î³, where the angular distributions are fully constrained by angular momentum conservation. The decay B â K â â+ ââ (K â â KÏ) is completely described by four independent kinematic variables:90 the lepton-pair invariant mass squared, q 2 , the angle Î¸K of the K + relative to the B in the K â rest frame, the angle Î¸â of the â+ relative to the B in the di-lepton rest frame, and the angle Ï between the K â decay plane and the di-lepton plane. Summing over the spins of the final particles, the diï¬erential decay distribution can be written as (Kruger and Matias, 2005; Kruger, Sehgal, Sinha, and Sinha, 2000) d4 Î I(q 2 , Î¸â , Î¸K , Ï) . â dÎ¸K dÏ"
157,109,0.989,"The Economy as a Complex Spatial System : Macro, Meso and Micro Perspectives","exist: the interior symmetric (IS) equilibrium and three interior asymmetric equilibria (IA1 , IA2 , IA3 ). In this interval, the size of immobile local demand is sufficiently large so that all regions can keep the industrial sector, the relative dimension of which depends on the initial condition: it could be large for one region (as in an asymmetric equilibrium) or of equal size for all three (as in the symmetric equilibrium). â¢ Crossing L = FIA (where a âfold bifurcationâ occurs and at which IAi = IAâ²i ) the fixed points IAi and IAâ²i disappear and the only attractor of map Z is IS (see Fig. 2d). Finally, at L = P FBS a pitchfork bifurcation of BSi occurs (at which BSi = BAi = BAâ²i ); after this bifurcation the fixed points BSi become saddles. Thus, for L > FIA the only stable equilbrium is the symmetric equilibrium. The size of the immobile demand is sufficiently large, compared to the potentially shifting demand, so that there is scope for firms to locate in each region. Any initial condition involving positive shares of entrepreneurs leads to the complete dispersion of the economic activity across the three regions as dispersion forces overcome agglomeration forces.11 Note that this model does not involve trade between the regions and for this reason it represents a useful reference point to be compared with the other two models in order to isolate the effects of trade on the long-run distribution of the economic activity."
311,1667,0.989,The Physics of the B Factories,"and the third is due to the uncertainty of the branching fractions used in the calculation. Recently, Belle has updated the Î·c (2S) measurement in the decays B + â (KS0 K Â± Ï â )K + by using a much larger data sample (Vinokurova, 2011). Besides improving the statistical accuracy, this analysis accounts for Î·c (2S) interference with the non-resonant continuum for the first time in a model-independent way, thus providing more reliable measurements of the Î·c (2S) mass and width, and the branching ratio for the B + â Î·c (2S)K + decay. Indeed the decays B + â KS0 K Â± Ï â K + can occur without proceeding via a charmonium state; the amplitude for such decays can interfere with the Î·c (2S) signal, which has a non-vanishing width. Diï¬erent values of the interference phase can result in diï¬erent Î·c (2S) resonance line shapes, and can lead to significant variations in the number of Î·c (2S) events while the total number of observed events in the Î·c (2S) peak remains the same. In this study Belle jointly analyzes the MKS0 K Â± Ïâ spectrum and the distribution of the angle Î¸ between the KS0 and K + in the rest frame of the KS0 K Â± Ï â system. The angular analysis pro-"
311,2704,0.989,The Physics of the B Factories,"Knowledge of exclusive charmed meson production in e+ eâ annihilation is rather poor. The only exception is the nearthreshold region, where the small phase space limits the number of particles in the final state. Heavy quark eï¬ec2 m (GeV/c ) tive theory (HQET), based on heavy-quark spin symmetry, provides a description of these processes in terms of a universal form factor called the Isgur-Wise function. For Figure 21.3.23. The baryon form factors measured by BABAR large q 2 , however, the leading-twist contribution, violating versus the dibaryon invariant mass. Data taken from Aubert this symmetry, becomes dominant. In the intermediate-q 2 (2005ah, 2007az). region the contribution of the symmetry-violating terms remains significant. A calculation that takes this eï¬ect into account (Grozin and Neubert, 1997) predicts cross sections of about 2.5 pbâ1 for the e+ eâ â DTâ DâL and 21.4 Open charm production e+ eâ â DDâ processes, where the subscripts T and L inDue to intriguing discoveries of exotic charmonium like dicate transverse and longitudinal polarization of the Dâ , states (see Secs. 21.5 and 18.3) with masses at which con- respectively. The cross section of the e+ eâ â DD process ventional charmonium states are expected to decay pre- is estimated to be at least 1000 times smaller. dominantly into pairs of open charm mesons it is of great Exclusive meson production in e+ eâ annihilation is interest to explore the ISR production of these final states. diï¬cult to study experimentally due to its extremely Exclusive production of open charm has been stud- low cross-section and reconstruction eï¬ciency. A partial ied in two diï¬erent regimes at the B Factories: far from reconstruction method is therefore used: one D(â) methreshold, in e+ eâ annihilation at the CM energy of the son is fully reconstructed while the other remains unrecollider (Section 21.4.1), and from threshold up to 5â6 GeV, constructed. For definiteness, suppose the D(â)+ is the depending on the final state, using the ISR technique (Sec- fully reconstructed meson. The distribution of recoil mass tions 21.4.2â21.4.6). There is no general approach to these Mrecoil , where measurements: the method that yields maximum signifi2 cance needs to be determined case-by-case. The measure(D(â)+ ) = (ECM â E(D(â)+ ))2 â p2 (D(â)+ ), Mrecoil ment of e+ eâ â D(â)+ D(â)â production at the CM en(21.4.1) ergy (Section 21.4.1) introduced a partial reconstruction can be used for identification of the process. Signal events method that exploits the special properties of the Dâ de- are expected to cluster around the mass of the unreconcay. Here D(â) denotes a D or Dâ meson. Measurement structed D(â)â meson. This method provides better reof DD production in ISR events, on the other hand, was construction eï¬ciency than the exclusive reconstruction performed by full reconstruction of both charmed mesons of the event, but the background level is also much higher. (Section 21.4.2). Analyses of D(â)+ Dââ (Section 21.4.3), If the unreconstructed meson is a Dâ , one can reconâ charmed-strange (Section 21.4.4), and three-body charmed struct one of its decay productsâusually the pion, Ïslow meson cross sections (Section 21.4.5), and a study of charmed in D â D Ï construct recoil diï¬erslow baryon production (Section 21.4.6), use variants of these ence techniques. In ISR events of this type, the continuous spectrum ÎMrecoil = Mrecoil (D(â)+ ) â Mrecoil (D(â)+ Ïslow of photons emitted from the initial state provides access (21.4.2) to a range of energies above open charm threshold. This Since most of the uncertainties cancel in the diï¬erence, allows the measurement of cross sections without the ad- the peak at the nominal mass diï¬erence mDâ â mD in ditional systematic uncertainties due to variation of the the ÎMrecoil distribution remains narrow (â¼ 1 MeV/c2 ). detector and machine conditions from one energy point to The width is determined mostly by the slow pion reconanother during the relatively long time of the data col- struction accuracy. The use of the recoil mass diï¬erence lection. However, the electromagnetic suppression of ISR as a discriminating or a signal variable is a powerful tool processes and the reduced reconstruction eï¬ciency due for background suppression. It was used by the Belle colto the event topology present considerable challenges. Re- laboration in ISR analyses (see Section 21.4.3) and in the"
311,944,0.989,The Physics of the B Factories,"variables derived from the observables in the p.d.f.s. Sometimes, a two-dimensional p.d.f. is used. A third option is to create a p.d.f. based on one of the observables, where the p.d.f. parameters (e.g. means and widths) are dependent on the other observable. A standard set of cross-checks on the fit is performed. The p.d.f.s are used to generate a series of simulated data samples that are then fitted with the ML method. This reveals any problems with minimization, pulls and biases. The tests are repeated with data samples generated from the full MC simulated data; this can reveal problems with correlations between observables. The ML fit is sometimes performed on a calibration channel taken from the data, such as a charm decay to the same or similar final state as the B meson decay under consideration. In this case, the ML model is simplified (e.g. no angular observables are used), any charm vetoes are removed, and all the model parameters are floated, if possible. This can reveal any differences between the MC simulation and data in the mES and ÎE signal distributions, which can then be corrected for in the final fit. The systematic uncertainties for the result are often separated into two categories and will depend on the measurement under consideration. Additive systematics aï¬ect the fit yield and hence the significance of a branching fraction measurement. Multiplicative systematics aï¬ect the central value of the result but not the significance. In the additive category, we place uncertainties on the accuracy of the fixed parameters in the p.d.f.s, any ML fit biases in extracting the yields, model-dependent parameters (such as the mean and width of poorly known resonances), the presence or absence of uncertain resonances (such as the Ï(600)), interference, BB background yields, and uncertainty on the longitudinal polarization fL . In a large number of modes, the uncertainty on the fixed parameters extracted from the MC simulation is the dominant systematic (see Chapter 15 for more details on systematic error estimation). In the multiplicative category falls the reconstruction eï¬ciency uncertainties arising from diï¬erences between data and MC simulation from tracking, uncertainties in the branching fractions of any intermediate decays, charged particle identification, neutral particle (Ï 0 ) identification, and long-lived particle (KS0 ) identification. Also, the accuracy of the known BB cross-section, luminosity and limited MC statistics can contribute. If various sub-decays are combined (e.g. K â+ â KS0 Ï + or K + Ï 0 ) to form an overall measurement, the multiplicative systematics are correlated and must be added linearly. Many of the systematic errors associated with fL and ACP cancel since these two measurements are based on ratios of signal yields. The systematic uncertainty on ACP caused by the detector responding diï¬erently to positive and negative tracks or the presence of s and s in K â and K + respectively is generally considered to be 0.5% at most. Once calculated, the systematic error is convolved with the likelihood function with a Gaussian distribution with a variance equal to the total systematic error (see â Chapter 15). The signal significance is then defined as 2Î ln L,"
362,142,0.989,Cloud-Based Benchmarking of Medical Image Analysis,"of the Gold Corpus and are obtained by comparing Gold Corpus segmentations from different annotators to additionally performed segmentations of the same structures in the same volumes (double annotations). Inter-annotator agreement is finally derived by averaging the Dice coefficients of all double annotations performed for a specific structure and modality and are shown in Table 4.2. Missing values are due to the structure being out of field such as the trachea in MRT1cefs-abdominal volumes (MRT T1 weighted contrast-enhanced sequence with fat saturation of the abdomen) or bad contrast of the addressed structure in a certain modality. Organs such as the adrenal glands show, depending on the annotated modality, a Dice coefficient smaller than 0.5 (see the bold values in Table 4.2). The reason for this is probably that the adrenal glands have the best contrast in contrast-enhanced CT compared to other sequences. The CT volumes have overall the best average Dice coefficients also in small anatomical structures such as the adrenal glands. The adrenal glands, the thyroid gland, the pancreas and the bodies of the rectus abdominis muscles have the smallest average Dice coefficients which are smaller and/or equal"
233,475,0.989,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"The mean equivalent number of Raoâs QE retained by each fraction of the landscape is higher when phylogenetic diversity is accounted for (black-filled symbols are higher than empty symbols). This means that including phylogenetic diversity as a prioritization criterion improves the outcome of the Zonation solution from the perspective of evolutionary history. Our results also highlight that the scale at which the prioritization is conducted (European vs. National) does not appear to have a consistent impact on the mean QE retained in each fraction of the landscape (same colored symbols are close to each other for a given fraction). In other words, the choice whether to conduct a prioritization at the country level or at the continent level does not influence how much phylogenetic diversity is retained. We overlaid the Zonation rankings with maps of existing protected areas to see how well the priorities and protected areas coincide. The protected areas we considered in our analyses (WDPA categories IâIV) cover a total of 7.8 % of the land area in the study region. We compared them to the same amount of land area prioritized by the Zonation variants (Fig. 6), i.e., 7.8 % top fraction of the Zonation solutions. A large majority of currently protected land is not considered of high priority by any of the Zonation variants (light blue areas), and conversely, much of the Zonation priorities are unprotected (yellow-orange tones). The best matching areas are shown in red, and are sparsely located across the study region without any clear spatial trends. We plotted the mean proportions of cell area protected among the cells in different top fractions of Zonation solutions for each of the four main solutions (Fig. 7). For both of the European scale analyses the proportion protected did not seem to depend at all whether the cells were considered of high or low priority. Actually their pattern of distribution appeared near random. Instead, for the national scale analyses there was a consistent pattern of increasing protection with increasing rank in Zonation, for both the basic and phylogenetic diversity variants of Zonation. Topmost 1 % fractions had almost twice as much area under protection as compared with the mean across the whole study region."
320,221,0.989,Managing Protected Areas in Central and Eastern Europe Under Climate Change,"5. Decrease of territorial coverage: remnants of habitats which are already endangered; 6. Influence of neophytes: potential danger of neophytes due to new invasive species or changing territorial coverage; 7. Dependency on groundwater and surface water in water balance: sensitivity of habitats which depend on water to changing temperature and precipitation patterns. For each habitat type each criteria was evaluated from the experts between the values âlowâ (1), âmediumâ (2), and âhighâ (3) sensitive. Afterwards, these values are summed and categorised to describe the overall sensitivity of a habitat type. Thereby, the categories were named similar to the evaluation values (Table 8.3). This evaluation was done from regional experts for the Alpine, Continental and Pannonian biogeographical region. To get an overall impression of the status per region, the sensitivity values of each habitat type from each investigation area within HABIT-CHANGE were grouped using the statistical median according to their biogeographical region. The variability of the ecological envelope of habitats was assessed by indicator values which were derived from the characteristic species composition of the habitats. As above, the biogeographical regions define the type of plant indicator scheme used for the assessment (Table 8.4). This differentiation was made because indicator schemes are based on the plant species response to climatic (e.g. temperature) and edaphic (e.g. moisture) habitat parameters, which are varying between the biogeographical regions (Englisch and Karrer 2001). Following Ellenberg (1992), different authors adapted the ecological preference of plants for their region. Each scheme categorises this ecological preference into ordinal scaled systems. Temperature values as climatic parameter and moisture values as edaphic parameter were selected in the framework. The temperature describes the plants response to air temperature gradients during the vegetation period. Moisture values indicate the degree of soil moisture needed by the plant during the vegetation period. Since the approach should be locally valid and transferrable to other biogeographical regions, the indicator schemes were re-categorised into three values each (Tables 8.5 and 8.6). Thereafter, the categorised indicator values were used to calculate an overall indicator value based on the statistical median for each habitat type listed by the investigation area. The frequency of the categorised indicator values per habitat, investigation area and biogeographical region was used in the sensitivity assessment. The proportion of the categories defined the main direction, therefore also the sensitivity of the habitat against changes in direction of the other category (Table 8.7). For instance, freshwater habitats are characterised in their moisture by moist to wet category and therefore are sensitive to drought periods."
311,1566,0.989,The Physics of the B Factories,"hadron and the track of opposite charge originate from the D(â)0 . The quantity m(KÏ), the invariant mass of the combination of these two tracks computed assuming appropriate kaon and pion mass hypotheses, is required to be greater than 1.95 GeV/c2 , i.e. to exceed the D0 mass, eï¬ectively suppressing these decays although with a significant loss of signal eï¬ciency (see Figure 17.11.6). The remaining background is mainly from continuum q qÌ production and is further suppressed by using a multivariate likelihood selector based on event shape and PID quality criteria, and the scalar sum of any remaining energy in the calorimeter. Signal branching fractions are determined relative to high-branching fraction decays with similar topologies, specifically B + â D(â)0 â+ Î½ with D0 â K + Ï â . Signal yields in each of eight signal modes are determined within a mass window of Â±60 MeV/c2 in mÏ , centered around the nominal tau mass. For each signal mode, a likelihood function is obtained from the products of the Poisson p.d.f.s representing the yields in the e, Î¼ and Ï decay channels. Since models of LFV can produce signatures in which the charge of the tau is either correlated or uncorrelated with that of the charged hadron, BABAR reports results on both the signed (e.g. B + â h+ Ï â Î¼+ ) and unsigned (e.g. B + â h+ Ï â Î¼Â± ) branching fractions. No significant signals were found in any modes and upper limits (see Table 17.11.2) were obtained at the level of a few times 10â5 ."
311,1464,0.989,The Physics of the B Factories,"isospin symmetry, the K + Ï + Ï â mass distribution of the K â Ï enriched sample of more abundant charged mode B + â K + Ï + Ï â Î³ is fitted with known states: those with 1+ spin-parity of which K1 (1270) is the dominant resonance with a small K1 (1400) contribution, those with 1â from K â (1680), and those with 2+ from K2â (1430) as shown in Figure 17.9.10. As a part of the systematic error study, contributions from other possible modes were tested, and it was found that the inclusion of B â KS0 ÏÎ³ causes the largest shift, where Ï is a controversial scalar state also listed as f0 (500) by the Particle Data Group (Beringer et al., 2012). The absolute values of the amplitudes to the K â Ï and KÏ0 modes are deduced from the results of the K â Ï enriched sample and known branching fractions. The relative phase between K â Ï and KÏ for K1 (1270) is determined from a fit to the KÏ and ÏÏ mass distributions after integrating out the KÏÏ mass, in which the corresponding phases for K â (1680) and K2â (1430) are fixed to known values and that for K1 (1400) is determined from a scan to give the smallest Ï2 . The dilution factor is calculated by integrating the sum of decay amplitudes for B 0 decays, using the parameters obtained for B + decays. It can be seen in Figure 17.9.11 that the KÏ0 Î³ is the dominant contribution in the Ï + Ï â mass range [0.6, 0.9] GeV/c2 . The dilution factor in this range is found to be 0.83 +0.19 â0.03 , which is used as a correction factor to the measured CP asymmetry in the same mass range. Despite this complication, the KS0 Ï0 Î³ mode is statistically competitive to the K â0 Î³ mode as listed in Table 17.9.9 because the vertex is determined from Ï0 â Ï + Ï â . The dilution eï¬ect is corrected for in the S measurement, but the C coeï¬cient is not corrected as direct CP asymmetry does not necessarily originate only from CP eigenstates. Time dependent CP asymmetries can also be measured in b â dÎ³ decay modes. The weak phases due to Vtd that appear in the B 0B 0 mixing and b â d penguin diagrams cancel each other, so a time dependent CP asymmetry requires a new contribution to the phase beyond the SM that enters diï¬erently in mixing and penguin diagrams. As with the b â sÎ³ decays, this new contribution also has to have a significant right-handed amplitude. Although the rate for b â dÎ³ is only 4% of b â sÎ³, in the case of B â Ï0 Î³ a large fraction of this suppression is compensated with respect to B â K â0 Î³, because of the 1/9 factor for the KS0 Ï 0 fraction and the KS0 vertex requirement in the vertex detector volume. Belle has measured time-dependent CP asymmetries in B â Ï0 Î³ with 657M BB (Ushiroda, 2008). The results are given in Table 17.9.9. At present all the measurements of time-dependent CP violation are consistent with zero, and dominated by statistical errors that are 0.16 or greater. This is an area where larger samples at a super flavor factory would make a significant improvement. 17.9.7 Electroweak penguin decays b â s(d)â+ ââ The b â sâ+ ââ transition, where â+ ââ is an electron or a muon pair, provides a number of additional probes of"
311,938,0.989,The Physics of the B Factories,"quite possible that power corrections are O(1) eï¬ects relative to the perturbative calculation, preventing a reliable quantitative estimate. However, the direct CP asymmetry calculations are still LO calculations, contrary to the branching fractions, so the final verdict + [0.049 + 0.051i ]NLOsp + [0.067]tw3 must await the completion of the NLO asymmetry cal+0.217 +0.115 culation. Contrary to direct CP asymmetries, the S pa+ (â0.077â0.078 )i . (17.4.9) = 0.240â0.125 rameter that appears in time-dependent CP asymmeHere 0.220 represents the naÄ±Ìve factorization value. tries is predicted more reliably, since it does not require Loop corrections to the form-factor-like term in the the computation of a strong phase. This is exploited first line of Eq. 17.4.6 and the first two lines of Eq. 17.4.9 in computations of the diï¬erence between sin 2Ï1 from almost cancel this number, but generate a sizable imagb â s penguin dominated and b â ccs tree decays (Beinary part, i.e. scattering phase. The real part of the neke, 2005; Cheng, Chua, and Soni, 2005b). amplitude is regenerated by spectator-scattering in the 5. Polarization in B â V V decays was expected to be second line of Eq. 17.4.6 and the third and fourth line predominantly longitudinal, since the transverse helicof Eq. 17.4.9. It is evident that the strong interaction ity amplitudes are Î/mb suppressed due to the V-A dynamics of the color-suppressed tree amplitude is far structure of the weak interaction and helicity conserfrom the naÄ±Ìve factorization picture, and is governed vation in short-distance QCD. While this is parametby quantum eï¬ects. The theoretical uncertainty is corrically true (with one exception (Beneke, Rohrer, and respondingly large. Yang, 2006)), a closer inspection shows that the para3. The QCD penguin amplitude P that governs branchmetric suppression is hardly realized in practice for the ing fractions of decays to final states such as ÏK and penguin amplitudes (Beneke, Rohrer, and Yang, 2007; its vector-meson relatives is certainly underestimated Kagan, 2004). This leads to the qualitative prediction in leading order in the heavy-quark expansion. The (or rather, in this case, postdiction) that the longipower-suppressed but chirally-enhanced scalar penguin tudinal polarization fraction should be close to 1 in amplitude, and perhaps a (diï¬cult to disentangle) weak tree-dominated decays, but can be much less, even less annihilation contribution, is required to explain the than 0.5, in penguin-dominated decays, as is indeed penguin-dominated PP final states. While the scalar observed. However, quantitative predictions of polarpenguin amplitude is calculable, some uncertainty reization fractions for penguin-dominated decays must mains. An important observation is the smaller size be taken with a grain of salt, since they rely on modelof the PV, VP and VV penguin amplitudes as comdependent or universality-inspired assumptions of the pared to PP final states, which can be inferred from non-factorizing transverse helicity amplitudes. the measured branching fractions of hadronic b â s The remainder of this chapter is devoted to an overview transitions. This is a clear indication of the relevance of factorization, which predicts this pattern as a conse- of experimental techniques of importance to charmless B quence of the quantum numbers of the operators Qi . If decay measurements and provides a summary of two-body the penguin amplitude were entirely non-perturbative, and three-body final state data collected by the BABAR no pattern of this form would be expected. A similar and Belle experiments. A detailed comparison and interstatement applies to the Î· (â²) K (â) final states, where pretation of the data in the light of theoretical approaches factorization explains naturally the strikingly large dif- as discussed above is beyond the scope of this review. For ferences in branching fractions, including the large Î· â² K this reason we will generally refrain from making reference branching fraction, through the interference of penguin to specific theoretical papers in the following. amplitudes, although sizeable theoretical uncertainties remain. A flavor-singlet penguin amplitude seems to 17.4.3 Experimental techniques play a sub-ordinate role in these decays. 4. The situation is much less clear for the strong phases and direct CP asymmetries. A generic qualitative pre- The decays of B mesons to final states with two or three diction is that the strong phases are small, since they hadrons without a charm quark are loosely broken down arise through either loop eï¬ects (Î±S (mb )) or power cor- into âtwo-bodyâ, âquasi-two-bodyâ and âthree-bodyâ derections (Î/mb ). Enhancements may arise, when the cays. The âtwo-bodyâ analyses concentrate on long-lived leading-order term is suppressed, for instance by small final states such as ÏÏ, KÏ, KK, etc. As these modes can Wilson coeï¬cients. This pattern is indeed observed. be used to access the CKM angle Ï2 , they are covered Quantitative predictions have met only partial suc- in Chapter 17.7; only the observation of direct CP violacess. The observed direct CP asymmetry in the de- tion is discussed here. The âquasi-two-bodyâ category incay to Ï + Ï â , and the asymmetry diï¬erence in the de- cludes decays where one or both of the decay products is a cays to Ï 0 K + and Ï â K + are prominently larger than resonance. Final state particles that have been measured predicted. A comparison of all CP asymmetry results include scalar (S) particles (a0 (980), f0 (980), f0 (1370), shows a pattern of quantitative agreements and dis- f0 (1500), K0â (1430)); pseudoscalar (P) particles (K Â± , K 0 , agreements that are not presently understood. Since Ï Â± ,Ï 0 , Î·, Î· â² ); vector (V) particles (Ï, Ï, Ï, K â ); tensor Î±S (mb ) and Î/mb are roughly of the same order, it is (T) particles (K2â (1430), f2 (1270)); and axial-vector (A)"
231,685,0.989,North Sea Region Climate Change Assessment,"also appears to be a robust result from the multi-model ensemble projections reviewed in this chapter. However, the range in projected future temperature change depends on the choice of GCM and as the range in projected changes is of the order of the amplitude of the projected change itself, the magnitude of the change cannot therefore be considered robust. On smaller spatial scales a lower signal to noise ratio is typically expected. Projected regional patterns and seasonal modulation of temperature increase are variable and their future development is uncertain. The spatial patterns of sea level rise are also more diverse among the different regional projections (Pardaens et al. 2011a) but in the latest IPCC assessment some of the spread across normalised modelled sea level change patterns appears to have been eliminated. A general decrease in ocean pH was a consistent signal from two regional climate change projections of OA. Offshore inter-model differences in projected future ocean pH appear to be small compared to the magnitude of projected changes, which could be attributed to the strong impact of changes in atmospheric CO2 levels on ocean pH in comparison to other internal physical and biogeochemical effects. The projected increase in regional OA for the North Sea can thus be considered robust for offshore waters, despite the small number of studies available. In contrast, the importance of terrestrial impacts near the coast is increasing and the projections are adversely affected by the lack of terrestrial coupling and lack of information on river loads and total alkalinity changes. Wind changes have a strong impact, inter alia, on local sea level, storm surges, surface waves, primary production, circulation, advection of salt- and nutrient-rich water from the North Atlantic, mixing, stratiï¬cation, and offshore transport of river plumes. The North Sea is located in the land-ocean transition zone of the Northwest European shelf, which is characterised by very high variability due to the alternating dominance of the maritime climate of the North Atlantic and the continental climate (e.g. Backhaus 1989; Hawkins and Sutton 2009). There are several modes of variability that are particularly important for the North Sea; the North Atlantic Oscillation (NAO), the Atlantic Multidecadal Oscillation (AMO) and the Atlantic Meridional Mode (AMM, e.g. Grossmann and Klotzbach 2009). The large natural variability has a greater impact on the local North Sea wind ï¬eld than potential anthropogenic-induced trends, and strong natural climate variability from annual to multi-decadal scales (e.g. Arguez et al. 2009) is a particular challenge when developing projections of climate change in the North Sea. Regional projections for changes in wind in existing scenario simulations are not robust for the North Sea (e.g. Lowe et al. 2009; see also Chap. 5), with many GCMs still unable to accurately capture features such as the placing and timing of atmospheric pressure systems in the UK"
372,1896,0.989,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"Note that the interference-to-noise ratio of 0.01 here refers to the levels at the correlator input. In the case of total-power systems (single antennas) and the arrays considered in Sect. 16.3, for which the errors are additive, the criterion of an interference-to-noise ratio of 0.1 applies to the time-averaged output of the correlator or detector. This therefore results in lower (i.e., more stringent) thresholds than those for VLBI in Eqs. (16.25) and (16.26). A curve for VLBI is shown in Fig. 16.2, using typical values for TS . The harmful thresholds are approximately 40â50 dB less stringent than those for total-power systems."
362,361,0.989,Cloud-Based Benchmarking of Medical Image Analysis,"where Ï a, b is the shortest-path weight from a to b in A . The adjacency graph of structures according to which we define the spatial prior in our experiments is given in Fig. 11.6b. In Eq. 11.4, the area of the common surface of adjacent clusters |âCi â© âC j | is introduced, so that âa, b â L the sum of pairwise energies in (11.3) is equal to the area of the common surface between the corresponding pair of structures multiplied by the shortest-path weight. This definition ensures that the segmentation energy is independent of the CVT clustering resolution [13]."
311,1267,0.989,The Physics of the B Factories,"17.7.6 SU (3) constraint using B 0 â Ï+ Ïâ , and B + â K â0 Ï+ Table 17.7.2 summarizes the measurements of the timedependent asymmetries with their correlation coeï¬cient ÏS,C , branching fractions, and fL for the B Factory results on B 0 â Ï+ Ïâ and B + â K â0 Ï+ . The eï¬ect of correlated systematic uncertainties on the ratio of branching fractions of these two decay channels is negligible. The constraint obtained from the B Factory data on Ï2 following the BGRS procedure outlined in Section 17.7.1.4 is shown in Figure 17.7.10. There are two overlapping solutions consistent with the SM. These two solutions diï¬er by the magnitude of Î´P T , where values of Î´P T â¼ 0 correspond to the left of the two central peaks in the figure. The determination of Ï2 using this approach only weakly constrains this phase diï¬erence given the data from the B Factories, however QCD factorization calculations favor a small phase diï¬erence and the solution with |Î´P T | â¼ 0 is preferred (Beneke, Buchalla, Neubert, and Sachrajda, 1999, 2000, 2001). This favored solution is summarized in Table 17.7.2 for the individual and combined B Factory results where the requirement that |Î´P T | < 90â¦ is imposed. While the BABAR data give a more precise value of"
311,2906,0.988,The Physics of the B Factories,"hadronic event per unit momentum in the laboratory frame, cent bins. Smearing was evaluated using MC and corrected (1/Nevt )dni /dp, i = Ï, K, p. Each corrected cross section for by inverting the smearing matrix (from generated z is then transformed into the e+ eâ CM frame. Results bin a to reconstructed z bin). The statistical uncertainties from the six regions are compared as a cross check, and on the matrix elements were converted into systematic then combined to give the final measured cross sections, uncertainties of the multiplicities. Further corrections in(1/Nevt ) dni /dpâ . There is a small correction for residual clude in-flight decays, detector interactions as well as relepton contamination, and results are given in two ways, construction eï¬ciencies. Furthermore backgrounds from including or excluding the contributions from decays of non-QCD processes were estimated using MC and subKS0 and weakly decaying strange baryons; here we con- tracted. The eï¬ects of ISR events were removed by evaluating the fraction of events with center-of-mass energies sider the latter. The total systematic uncertainty on the pion cross sec- less than 0.5% below the nominal energy in the MC and tion is at the level of a few percent in the full momentum removing that fraction from the data sample. As the fracrange. It is dominated at low momenta by tracking eï¬- tion might depend on how well the MC describes the data ciencies, by background contamination between 0.75 and various MC parameter settings were considered and the about 3 GeV/c, and by particle identification at the high- spread obtained was assigned as systematic uncertainty. est momenta. The uncertainties on the kaon and proton Acceptance eï¬ects were corrected by fitting both, data cross sections have similar patterns, but are significantly and MC cos Î¸ distributions within the measured range, to larger, in particular for momenta below 0.2 GeV/c and estimate the fraction of non-reconstructed tracks. The BABAR analysis of the momentum spectrum of the above 4 GeV/c. The uncertainties all have large point to point corre- Î· meson begins with a similar hadronic event selection. lations. There is an overall normalization uncertainty of The Î· mesons are reconstructed in the Î³Î³ decay mode: 0.9% that does not aï¬ect the shape of any cross section. high quality neutral clusters with energy above 0.15 GeV Several uncertainties are fully correlated over the entire are selected, and all pairs of such photon candidates are pâ range, but vary slowly with pâ and can have broad ef- considered. If any pair has an invariant mass in the range fects on the shape. The uncertainties from the calibration 0.11 < MÎ³Î³ < 0.155 GeV/c2 , consistent with a Ï 0 decay, of the particle ID are correlated over ranges of a few bins, then both photon candidates are rejected. A pair is also rejected if | cos Î¸Î³ | > 0.8, where Î¸Î³ is the angle between and can lead to apparent structures. The Belle analysis (Leitgab, 2012, 2013) similarly re- either photon momentum and the boost direction of the quires events with at least 3 charged tracks, a visible en- laboratory system in the Î· rest frame. Surviving pairs of photons are binned by the pair moergy above 7 GeV and either jet mass179 above 1.8 GeV or the jet mass normalized by the visible energy to be above mentum in the CM frame pâ , and the invariant mass 0.25. Tracks are selected within the central detector with distribution in each bin is fitted with a sum of signal â0.511 â¤ cos Î¸ < 0.842, where the polar angle Î¸ is calcu- and background functions over the range 0.35 < MÎ³Î³ < lated in the laboratory frame, with a minimum momen- 0.75 GeV/c2 . The signal function is the sum of a Gaussian tum of a track of 500 MeV and at least three hits in the distribution and Novosibirsk180 distribution whose paramvertex detector. Tracks are also required to originate from eters depend on pâ in such a way as to reproduce the within distances of 1.3 cm radially and 4 cm longitudi- line-shape induced by photon energy loss in front of the nally from the interaction point. The particles were iden- EMC. The eï¬ciency, defined as the ratio of the yield fitted tified as pions, kaons, protons, electrons or muons via like- for the MC sample to the true number of Î· â Î³Î³ decays lihood ratios obtained from the information of the CDC, produced, ranges between 24% and 34%. The relative sysACC, TOF, ECL, and KLM (see Chapter 2). The charge tematic uncertainty includes a component due to normalseparated particle identification eï¬ciencies and fake rates ization of 6.2%, arising from the single photon eï¬ciency, were evaluated using a data driven method by relying on the event selection and the Î· â Î³Î³ branching fraction. Adknown decays of Dâ , Î and J/Ï âs (see Chapter 5). These ditional point-to-point systematic uncertainties arise from matrices for Ï, K, p, Î¼ and e were obtained in a fine 17 the fitting procedure and signal and background shapes. Ã 9 (p, cos Î¸) binning. Where not completely defined by They are 27% at pâ = 0, where backgrounds are very high, data, an interpolation between adjacent bins or extrapo- but then drop rapidly to well below 6% at 1 GeV/c. Rellation based on MC simulation (Pythia 6.2 for u, d, s, c ative statistical uncertainty drops from 15% at low pâ to production, a dedicated Ï + Ï â and electro-magnetic pro- 2% above 1.5 GeV/c. cess generators) was used. The extracted matrices were inverted; the uncertainties arising from the limited size of the data control samples were assigned as systematic Results uncertainties. Another important uncertainty in the Belle analysis arises from momentum smearing which migrates Both BABAR (Lees, 2013f) and Belle (Leitgab, 2013) pubthe contents of a certain z-bin over several, mostly adja- lished results of the light hadron fragmentation recently,"
32,205,0.988,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","9.2.2 Transition in Growth Behavior In the present model, the essential features of the ecosystem-like systems, the introduction of a new species and the interaction-dependent survival condition for each species, are took into account. And because the both processes are introduced in neutral way, i.e. giving no apparent advantage to grow or collapse. Therefore whether the system can grow under such process will purely illuminate the relation between the systemâs complexity and robustness. Simulation results indeed give a fascinating answer: both of the growth and collapse can happen, depending on the only one model parameter m. The system can grow to infinitely large size if the number of interactions per species is in a moderate range (for the case of taking the standard distribution for link weights, the range is 5  m  18), and, if not, it stays in a finite size (Fig. 9.2)."
200,45,0.988,"Earthquakes, Tsunamis and Nuclear Risks","Figure 2.4 shows the first, second, third, and fourth principal component loadings for the four parameters of observed seismicity, distribution of active faults, lower limit of the seismogenic layer, and Bouguer gravity anomaly in (a) the northeastern Honshu, (b) southwestern Honshu, (c) Kyushu, (d) Hokkaido, and (e) Kanto districts resulting from the principal component analysis. The first principal component loadings (F1) in Fig. 2.4a of northeastern Honshu showing a 34 % proportion indicate that the observed seismicity, distribution of active faults, and Bouguer gravity anomaly parameters have positive values and that the lower limit of the seismogenic layer has a negative value. This result means that the short-term observed seismicity matches well the estimated long-term"
311,1561,0.988,The Physics of the B Factories,"In comparison to B 0 â Î³Î³, the Bs0 â Î³Î³ decay is favored by â¼ |Vts /Vtd |2 , with the SM prediction B(Bs0 â Î³Î³) â¼ (0.5â1.0)Ã10â6 . Production of Bs mesons does not occur at the Î¥ (4S), hence a large sample of Î¥ (5S) events is required. BABAR did not collect significant data at this energy. However, Belle obtained a substantial sample (see Table 3.2.1). Using 23.6 fbâ1 of such data, Belle (Wicht, 2008) performed a search for Bs0 â Î³Î³ decays. Details of the measurement can be found in Section 23.3.6, only the main results are presented at this place. The signal yield is determined by an unbinned extended ML fit to mES and ÎE. Figure 17.11.5 shows the projections to the fit variables, mES and ÎE. No signal is observed. Including the systematic error of +21 â19 % which is dominated by the uncertainties in the number of Bs0 events in the Î¥ (5S) â bbÌ process (+16 â13 %), the 90% C.L. branching fraction upper limit is determined to be B(Bs0 â Î³Î³) < 8.7 Ã 10â6 . This limit is about an order of magnitude larger than the SM prediction."
311,1357,0.988,The Physics of the B Factories,"could be diï¬erent owing to possible distinct values for r, where r is the ratio of decay constants and form factors involved with the two diagrams shown in Fig. 17.8.12. Assuming r â 1 in the above equation, we can estimate R purely in terms of the CKM matrix elements to be 2%. It follows from Eq. (17.8.27) that the value of RD(â) h dictates the sensitivity of CP violation measurement in B â D(â)â hÂ± , as the sine term containing weak phases is essentially weighted by the factor R. Now because R is predicted to be small, the experimental precision on 2Ï1 + Ï3 expected from these measurements is poor. Furthermore, these measurements are susceptible to potential model uncertainties caused by the assumptions used in the calculation of R. However, when the decay proceeds through several interfering amplitudes such as the"
311,943,0.988,The Physics of the B Factories,"such that only a few events can be expected to appear in the data. Higher mass resonances that peak outside the invariant mass selection region can still feed-down to the signal region because: they have a large width, such as f0 (1370); through reflection, where a daughter particle is mis-identified, such as in B â ÏÏ + ; or where a resonance has a long range component e.g. the S-wave component of the K0â (1430). These backgrounds are often treated in a separate analysis that looks in the mass region above the resonance under consideration (since this is still blinded) and performs a ML fit to the higher mass region using mES , ÎE, the multivariate discriminant, and the reconstructed mass. Once the yield is extracted, the number of events in the resonance signal region is estimated by extrapolating the fitted mass p.d.f. (or a fit to the extracted s W eights, see Chapter 4) down to the low mass region and integrating. A further category of background occurs when the B meson decays to the same final state without passing through a resonance, such as B + â Ï + Ï â Ï + when looking for B + â Ï0 Ï + or B 0 â Ï + Ï â Ï0 when looking for B 0 â Ï0 Ï0 . These backgrounds also become important when D mesons are used as calibration channels as these ânon-resonantâ decays can be responsible for a significant number of the events underneath the calibration channel of interest. Strictly speaking, ânon-resonantâ means a decay in a Dalitz Plot that is uniformly distributed in phase space (see later and Chapter 13). However, this distinction is generally ignored and any final state which cannot be represented by a peaking structure is usually categorized as non-resonant. This has practical benefits when performing a fit as it is often diï¬cult to identify the source of smoothly varying distributions. A fit which uses more than one such distribution is likely to find that the background events flow between the diï¬erent distributions without aï¬ecting the significance of the signal. As a result, some papers will report a non-resonant measurement while others will simply consider it as part of the background. The signal modes, mis-reconstructed signal modes (if used), continuum background and BB backgrounds distributions are used in a ML fit to extract the signal yield, branching fraction, ACP , and longitudinal polarization fL . The observables used are usually mES , ÎE, the multivariate discriminant and the intermediate resonance masses. If an angular analysis is required, the helicity cos Î¸H of the resonances is also used. In this later case, the reconstruction eï¬ciency as a function of cos Î¸H must be taken into account, often by multiplying the expected true distribution by a polynomial of a suitable order. The eï¬ciency for the other variables is usually treated as uniform. The observables used in the p.d.f.s are usually assumed to be uncorrelated and the total p.d.f. is taken to be the product of the separate individual p.d.f.s. However, in some cases this assumption is invalid and the correlations need to be taken into account explicitly. If the correlation only exists between two observables and is reasonably linear, then the correlation can be reduced by using rotated"
238,266,0.988,Nanoinformatics,"included in the group leading to an increase in Î²0. The value of Î²0 remains constant for a certain range indicating that these are real features. A plot of the voxels at Î´ = 0.3 shows that it indeed captures real clusters of Sc. With further decrease of concentration threshold, a decrease in Î²0 is observed. This is because every voxel outside the Sc clusters has some minimal content of Sc and the inclusion of all exterior voxels results in one single connected component. We also observe a peak in the value of Î²1 at a low concentration of Î´ = 0.03. When we plot the isoconcentration surface for those voxels we ï¬nd that these represent cavities. These voxels with very low Sc concentration sit on the edge of the Sc clusters, and thereby, enclose Sc clusters within themselves. A similar trend is observed with Mg where for low concentration we see Mg Isosurface containing cavities that enclose Sc clusters, whereas for high concentration there are few voxels."
165,185,0.988,New Methods for Measuring and Analyzing Segregation,"following terms. The lower-left cell (A) âLow Prototypical Segregationâ and the upper-right cell (C) âHigh Prototypical Segregationâ both involve situations where group distributions on area proportion White (p) produce similar high levels of inequality in rank-order position (D) and quantitative difference (S). The lowerright cell (B) âDisplacement without Separationâ involves a high level of group inequality on rank order position on area proportion White (p) but a low level of group inequality on quantitative differences on area proportion White (p). The combination indicates that Whites are consistently ranked above Blacks on area proportion White â as indicated by the high value of D, but the quantitative differences involved are small and thus result in the low value of S. Thus, the rank-order differences on area proportion White do not translate into group separation because the two groups have similar distributions on area racial composition (p) and thus the two populations are living together, not apart from each other."
311,1906,0.988,The Physics of the B Factories,"Figure 18.4.24. (a) Product branching fractions as a function of the Higgs mass (Aubert, 2009ai). For each point, both the statistical uncertainty (from the central value to the horizontal bar) and the total uncertainty (statistical and systematic added in quadrature) are shown (from the central value to the end of the error bar). In (b), the corresponding 90% C.L. upper limits on the product of the branching fractions versus the Higgs mass values are shown, with total uncertainty (solid line) and statistical uncertainty only (dashed line). The shaded vertical region represents the excluded mass range corresponding to the ÏbJ (2P ) â Î³Î¥ (1S) states."
71,231,0.988,Advancing Culture of Living with Landslides: Volume 1 ISDR-ICL Sendai Partnerships 2015-2025 (Volume 1.0),"of the fragments that can be ï¬tted to power laws (Fig. 11). The rock fragments were measured directly in the ï¬eld one by one with a tape. In case the deposit formed a continuous young debris cover with a high number of blocks to be measured, the methodology proposed by Ruiz-Carulla et al. (2015a) was followed. The rockfall volume involved in these events ranges from 2.6 m3 to 10,000 m3. Despite the apparent similarity of the distributions shown in Fig. 11, they contain signiï¬cant differences. The ï¬rst one is the scaling factors of the tails whose values range between 0.5 and 1.3 (Table 4). The scaling factors are an expression of the intensity of the fragmentation process. This can be observed in Fig. 12, where the number of fragments generated by the breakage of each individual block is plotted against the exponents of the ï¬tted power laws of their volume distributions. There exists a positive correlation between the number of blocks generated and the exponents of the power law. The meaning of the exponents in the case of rockfall deposits is however, less evident as it is generated from an in situ block size distribution (IBSD) of the detached mass. The fragmentation in rockfalls is a function of several variables (Dussauge et al. 2003; Wang and Tonon 2010; Hantz et al. 2014): the presence of discontinuities in the initial rock mass and their persistence, their orientation at the time of impact, the energy and angle of impact and the ground stiffness. The highest exponent of the rockfall inventoried corresponds to the case of Vilanova de Banat with a value of 1.27 and 60,000 blocks generated from a volume of 10,000 m3. It has both the largest volume and highest height of fall (Table 4). As it will be shown latter, the"
213,325,0.988,Collider Physics Within The Standard Model : a Primer,"3.12 Results of the SM Analysis of Precision Tests The electroweak Z pole measurements, combining the results of all the experiments, plus the W mass and width and the top mass mt , are summarised in Fig 3.15, as of March 2012 [350]. The primary rates are given by the pole cross-sections for the various final states  0 , and ratios thereof correspond to ratios of partial decay widths: h0 D"
175,549,0.988,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","Clearly, the average visitation rate, V D Â¼ 78:25, the visitation rate corresponding to the average pool level VD(L) = 100, and the worst-case assessment VD(Llow) = 25, are very different. The median and the most likely are other measures that characterize a data set. They have the advantage that they are less influenced by extreme outliers. For the symmetric data set shown in Table 6.1, the median, most likely, and the mean are the same, namely 30. But if instead the probabilities of the respective pool levels were 0.30, 0.25, 0.20, 0.15, and 0.10, (instead of 0.10, 0.25, 0.30, 0.25, 0.10) the expected value or mean is 25, the value having the highest probability of occurring (the most likely) is 10, and the median or value that is greater or equal to half of the other values and less than or equal to the other half of the values in the data set is 20."
238,173,0.988,Nanoinformatics,"In this study, three-dimensional images of granular packings with several packing ratio ð are obtained by using XCT, and these images provide precise positional coordinates of grains. Our interest is to characterize the skeleton deformation structures of grain conï¬gurations during the crystallization process. For experimental details, please see the original paper. Figure 5.14 shows the two-dimensional persistence diagrams computed on the grain conï¬gurations for four packing ratios ð = 0.6, 0.63, 0.69, and 0.73. Here, we note that the packing ratio ð = 0.64 is known as the Bernalâs density at which sharp structural transition to jamming is observed. As we observe from the ï¬gure, the persistence diagram (d) at the crystallized state consists of two strong peaks at (0.288, 0.353) and (0.288, 0.5), and they correspond to the regular tetrahedral and the regular octahedral conï¬gurations, respectively. We note that the persistence diagram (c) is similar to D2 (î­LJ amo ) in Fig. 5.13 (the Lennard-Jones system), since both are classiï¬ed as random packing systems. The tetrahedral peaks are well preserved for all packing ratios, while the octahedral peaks only exist at (c) and (d). Actually, further studies show that the octahedral peaks are only observable for packing ratios ð > 0.64. Next, let us study the persistence diagram (c) at ð = 0.69 in detail. Figure 5.15a is the same persistence diagram at ð = 0.69, in which four curves (D1, D2, D3, and D4) corresponding to the boundaries are drawn. In the paper, we found the analytical expressions of the actual deformations of grain conï¬gurations corresponding to these curves. Figure 5.15b and c show those deformations. It follows from a discussion similiar to the silica glass case that distorted tetrahedra and octahedra are conï¬ned in the region bounded by D1-D4 and those deformations give geometric constraints during the crystallization process."
238,262,0.988,Nanoinformatics,"In the case of 1 nm, the voxel size is too small to accurately estimate the density. Due to statistical fluctuation in the distribution of atoms, there are many pockets of 1 nm3 throughout the sample where the concentration of Al and Cr are almost equal. As the voxel size is increased, the increase in volume averages out the noise and a clear interface starts emerging. At 1.6 nm most of the statistical noise vanishes and a very sharp interface is obtained, with nanometer scale fluctuations visible on the isosurface representing the interface (Fig. 7.10). As the voxel size is increased beyond this value, over smoothing of data starts occurring. The interface starts becoming diffuse and the graininess in the image disappears. At this stage there is ideally no statistical noise and the residual clusters scattered throughout the volume could potentially be capturing the presence of nanoclusters."
372,899,0.988,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"A 50% overlap recovers most of the lost signal-to-noise ratio but at a cost of doubling the processing time in the F engine. This overlap feature was available in the original FX VLBA processors but rarely, if ever, used (Romney 1995). Another approach is to simply channel-average the spectrum, but this wastes the resolution capability of the F engine. Note that with polyphase filter banks, scalloping for narrow spectral lines and signal-to-noise ratio loss are very small. Number of Operations. We can make an approximate comparison of the workload requirements of XF and FX signal processors by comparing the number of multiplications needed in each system. For this rather simplistic analysis, we assume that the data are streams of real numbers at the Nyquist interval appropriate for bandwidth &"", i.e., ts D 1=.2&""/. To make this comparison, we further assume that the number of lags computed in the X engine (lag correlator), N, is equal to the data segment length into the F engine. This makes the spectral resolution of both systems approximately equal (see Fig. 8.23 for exact responses). Consider the analysis of one second of data, i.e., 2&"" samples. For the XF system, a lag correlator is required for each baseline. Thus, 2N&"" multiplications are required for each baseline. Since Nts # 2&"", the edge effects in calculating the correlation function are negligible (i.e., all lags have almost the same number of multiplications approaching N), and the workload of the single Fourier transform at the end of the integration period is negligible compared with the workload of calculating the correlation function. Thus, the rate of multiplications (multiplies per second), rXF , is rXF D 2&""Nnb ;"
311,2660,0.988,The Physics of the B Factories,"21.3.5 Light meson spectroscopy Most of the multi-hadron final states feature a variety of internal sub-processes, with formation of several intermediate states, whose properties can be measured thanks to the large available statistics at the B Factories. In some cases, however, these studies are made diï¬cult by the presence of broad interfering intermediate states, and have been performed only in a qualitative way. As an example, the study of the two- and three-pion invariant mass distributions of the process e+ eâ â Ï + Ï â Ï 0 Ï 0 shows important contributions from Ï(780)Ï, a1 (1260)Ï, and Ï+ Ïâ intermediate states, which strongly interfere."
372,1612,0.988,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"function of phase and indicate how it is related to other functions that are used to characterize atmospheric turbulence. When the Reynolds number (a dimensionless parameter that involves the viscosity, a characteristic scale size, and the velocity of a flow) exceeds a critical value, the flow becomes turbulent. In the atmosphere, the Reynolds number is nearly always high enough that turbulence is fully developed. In the Kolmogorov model for turbulence, the kinetic energy associated with large-scale turbulent motions is transferred to smaller and smaller scale sizes of turbulence until it is finally dissipated into heat by viscous friction. If the turbulence is fully developed and isotropic, then the two-dimensional power spectrum of the phase fluctuations (or the !11=3 refractive index) varies as qs , where qs (cycles per meter) is the spatial frequency (qs , the conjugate variable of d, is analogous to q, the conjugate variable of .). The structure function for the refractive index Dn .d/ is defined in a fashion similar to the structure function of phase in Eq. (13.76); that is, Dn .d/ is the mean-squared deviation of the difference in the refractive index at two points a distance d apart, or Dn .d/ D hÅn.x/ ! n.x ! d/,2 i. Note that only the scalar separation d is important for isotropic turbulence. For the conditions stated above, Dn can be shown to be given by the equation Dn .d/ D Cn2 d 2=3 ;"
241,556,0.988,Second Assessment of Climate Change for the Baltic Sea Basin,"It is clear therefore that, depending on which approach is used, the same long-term sea level record may appear to exhibit both the presence and absence of a signiï¬cant acceleration in sea level rise. In cases where acceleration in sea level rise has been identiï¬ed, this should ideally be accompanied by an estimate of its level of statistical signiï¬cance. As in any statistical test, a general algorithm to attach a certain level of statistical signiï¬cance of an estimated value cannot exist, since the signiï¬cance level depends on the formulation of the nullhypothesis, against which the statistical test is conducted. This formulation is subjective, and different authors may consider different null-hypotheses to describe the statistical properties of the natural fluctuations in the rate of sea level change. Donner et al. (2012) reported different levels of statistical signiï¬cance for the estimated acceleration depending on whether the data were assumed to be serially correlated or serially independent. The authors also attempted to estimate the acceleration in the rates of change of the quantiles of the distribution of monthly sea level for a set of Baltic Sea coastal stations. The record length across the dataset is not uniform and so the spatial patterns of the acceleration estimate and its level of statistical signiï¬cance may be strongly influenced by data availability. When the analysis is applied to the period covered by all sea level records (1951â2000), none of the quantiles of the probability distribution display statistically signiï¬cant accelerations if the de-seasonalised monthly sea level data are assumed to be serially correlated. However, they do underline that statistical detection of acceleration in only 50 years of data may not be possible due to the short record length."
200,114,0.988,"Earthquakes, Tsunamis and Nuclear Risks","The depth dependency of the equations of (6.3) and (6.4) are similar, and the absolute value of the stress drop on SMGAs is about 4 MPa larger than the stress drop on asperities. We also derive the relations between seismic moment Mo [Nm] and total area of SMGAs Sa [km2] as shown in Fig. 6.3b in which Sa is the average for each earthquake. The equations derived by constraining the slop to be 1/3 are written as"
311,1690,0.988,The Physics of the B Factories,"The spectra of M (D(â) D(â) ) â¡ Mrecoil (J/Ï ) are shown in Figs 18.2.9 (a), (b), (c), and (d) for the four selected cases in turn. Enhancements near threshold are evident in each distribution. A fit to the M (DD) distribution finds a broad resonance-like structure near the threshold, tentatively denoted X(3880). However the significance of the broad peak is low (3.8 Ï), and the fit is not stable under variation of the background parameterization. Therefore, with the existing sample the resonant structure in this process cannot be reliably determined. The significance of the X(3940) signal found by the fit to the M (DDâ ) spectrum is 5.7 Ï (including systematic uncertainties). The X(3940) mass and width are M = (3942 + â 6 Â± 6) MeV/c + 26 and Î = (37 â 15 Â± 8) MeV. The insets in Figs 18.2.9 (a) and (b) show the background subtracted spectra with the signal functions superimposed. The M (Dâ Dâ ) spectrum has a clear broad enhancement near threshold, which is seen above the small combinatorial background and the X(3940) reflection. The observed enhancement, which has a significance of 5.1 Ï (including systematics), was interpreted as a new resonance and denoted X(4160). The X(4160) parameters + 111 are M = (4156 + â 20 Â± 15) MeV/c and Î = (139 â 61 Â± 21) MeV. Although the masses and widths of the X(4160) and Ï(4160) are not inconsistent, the latter cannot be produced in e+ eâ annihilation via a single virtual photon due to C-parity conservation, as explained above; annihilation"
311,987,0.988,The Physics of the B Factories,"ÎE changes significantly, this can be compensated for by using a derived observable such as ÎE/Ï(ÎE). For similar reasons, multivariate discriminants need to be carefully constructed from variables that are as independent as possible from the position of the event in the Dalitz Plot. As in quasi-two-body analyses, care must be taken with charm mesons that either decay to the same final state or are mis-reconstructed e.g. where a lepton is mistaken for a pion or kaon. This is particularly important in searches for highly suppressed modes such as B â â K + Ï â Ï â Aubert (2008aw). The charm background can usually be much reduced by applying mass range criteria about known resonances such as D mesons, J/Ï and Ï(2S). This will result in empty bands in the Dalitz Plot that must be carefully considered when calculating eï¬ciencies and migrations. Alternatively, some charm decays are deliberately kept in the Dalitz Plot. A motivation for this comes from resonances such as the Ïc0 that have no weak phase and so can be used in an interference analysis to extract the weak phase from the Dalitz Plot. Unfortunately, the branching fraction for B â Ïc0 h is too small to be useful currently. When the Dalitz Plot is represented as a Cartesian coordinate system, with the square of the mass of pairs of final state particles as the x and y axes, the phase space is roughly triangular in shape. Figure 17.4.17 illustrates the distribution of events extracted from data in the decay of B 0 â KS0 Ï + Ï â . The distribution of events on the Dalitz Plot is plotted after applying a constraint on the B meson mass (mES = mB ). This improves the resolution and ensures that all events fall within the kinematic boundaries of the Dalitz Plot. An alternative often used is a âsquareâ Dalitz Plot where one of the axes is transformed into a âhelicity-likeâ variable e.g. (Aubert, 2007v) or see Chapter 13. Although this transforms the distribution of resonances from simple bands parallel to the axes to more complex hyperboloids, the âsquareâ Dalitz Plot has a number of benefits. It can expand the region near areas where large variations are occurring such as in narrow resonances like the Ï. Bands near the Dalitz Plot edges also"
280,154,0.988,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"pupae. Dll showed comparable expression domains in 5th instar larval wings but had a broader domain of expression in the eyespot centers of WS forms in the early pupal stage. In addition, this gene also had a second domain of expression that corresponded to the much broader black disc of scales in an eyespot, which became visible later, around 12 h after pupation (Brunetti et al. 2001a; Monteiro et al. 2006). The larger group of cells expressing Dll clustered in the eyespot center, however, suggested that some time in between the late larvae and early pupal stages, the eyespot centers were becoming larger in response to temperature. A subsequent study looked at two other markers for eyespot development and found that Notch and Engrailed genes were expressed earlier in the eyespot centers of DS forms relative to their later expression in WS forms, suggesting that these genes could be downregulating eyespot size in DS forms (Oliver et al. 2013). The onset of Dll expression in the eyespot centers of WS and DS forms, however, was approximately the same (Oliver et al. 2013). A more recent study (Bhardwaj et al. 2017) showed that a fourth gene expressed in eyespot centers, the ecdysone receptor (EcR), showed an enlargement in its domain of expression during the second half of the wandering stage in WS forms. Cells in the center of dorsal forewing eyespots underwent cell division concurrently with the rise of 20E titers taking place at that stage of development. Other marker genes, such as Spalt, also increased their domains of expression at the same time, concurrently with local cell divisions. Cells in the dorsal eyespot centers of DS males, however, experiencing the lowest levels of 20E hormone, did not undergo cell division and produced a small eyespot center as well as an associated small eyespot. To test whether levels of 20E were directly responsible for the regulation of dorsal eyespot center size via a localized process of cell division, injections of 20E (into DS males) and CurcB (into WS forms) at 60% of wandering stage development were performed and confirmed an effect of 20E levels on the regulation of eyespot center sizes in WS individuals as well as in DS females, the odd sex with the abnormally large eyespots (Bhardwaj et al. 2017). The experiments above pin the critical stage of regulation of eyespot center size, and eyespot size for both dorsal and ventral eyespots, to the second half of the wandering stage of development. At this stage, rearing temperature leads to variation in 20E titers, which in turn leads to localized patterns of cell divisions in cells that express the EcR receptor (Bhardwaj et al. 2017). These localized patterns of cell division determine the size of the eyespot centers, which are critical determinants of the size of the complete eyespot pattern (Monteiro and Brakefield 1994), and thus impact final eyespot size. For many years, however, research into the physiological and genetic basis of eyespot size plasticity focused exclusively on the period of development following pupation, which is not as sensitive to temperature as the previous larval wandering stage (Kooi and Brakefield 1999; Monteiro et al. 2015). This period shows variation in timing of 20E titers in the seasonal form âgenetic mimicsâ as well as in the actual seasonal forms (Mateus et al. 2014; Oostra et al. 2011). In particular, titers of 20E are low during the first 24 h (WS) (and 48 h in the DS) after pupation, which is the developmental window believed to be important for eyespot ring differentiation at"
238,268,0.988,Nanoinformatics,"APT data is a point cloud data and in order to study hidden features like precipitates or grain boundaries, isosurfaces are often used. These isosurfaces are drawn at a particular concentration threshold. We calculate the uncertainty in spatial location of isosurfaces here and use visualization techniques that lead to the incorporation of uncertainty information in the ï¬nal image (Fig. 7.13). Isosurfaces were drawn by joining voxels which have the same value of density or concentration, as deï¬ned in Sect. 7.4. For uncertainty calculations in the APT data, we followed the approach described in Sect. 7.3 for calculating the error and difference from ideal density. Consider xi as the atom in a voxel with coordinates (xxi , xyi , and xzi ), the mean Âµx and standard deviation Ïx along the x-axis will be given as follows:"
311,614,0.988,The Physics of the B Factories,"of the resonances are ï¬xed in the ï¬t. Under these circumstances, the integrals of each of the Ar Aârâ² terms can be calculated prior to the ï¬t. The p.d.f. normalization can then be calculated by combining these cached terms and the current values of the complex coeï¬cients at each iteration of the ï¬t. Consequently, it is a common procedure to ï¬x the resonance parameters in the ï¬t. Where necessary, likelihood scans are used to determine the values of any less well-known parameters. Due to the complexity of the likelihood function and the large numbers of parameters involved in Dalitz-plot ï¬ts it is quite common for several local minima to appear in the parameter space. This can cause problems for the minimization routine in ï¬nding the global minimum. In addition, these local minima can be almost degenerate with the global minimum, leading to the need to quote multiple solutions. This can occur, for example, when ambiguities arise from broad overlapping states. The data can often be well described by two or more conï¬gurations of the magnitudes and phases of these states. The problem of ï¬nding the global minimum is usually overcome by performing multiple ï¬ts to a given data sample, each with diï¬erent (often randomized) starting values for the various parameters. One can then choose the case where the best likelihood was obtained as the global solution. This method also permits the exploration of the other local minima, which allows the results from other solutions to be quoted if they are not signiï¬cantly separated in likelihood from the global minimum. 13.4.4 Fit fractions The choice of normalization, phase convention, and amplitude formalism may not always be the same for different experiments or indeed among the diï¬erent ï¬tting packages used within a single experiment. Consequently, it is extremely important to provide as much conventionindependent information as possible to allow a more meaningful comparison of results. Fit fractions are quite commonly used, both for this purpose and for providing a means to estimate the branching fractions of the various decay modes involved. The ï¬t fraction for a component j is deï¬ned as the integral of the square of the decay amplitude for that component divided by the integral of the square of the entire matrix element over the Dalitz plot: |cj Aj (m)| dm FF j = !! DP (13.4.12) | k ck Ak (m)| dm Similarly, the ï¬t fraction for the conjugate process is deï¬ned to be: !! & &c A (m)&2 dm DP j j FF j = !! & (13.4.13) ck Ak (m)& dm"
311,1845,0.988,The Physics of the B Factories,"components, a background shape due to the threshold for KS0 production, and a combinatorial background. The resulting masses and yields are listed in Table 18.4.5. Systematic uncertainties in the hb (nP ) parameters arise from the fitting procedure, including polynomial order, fit interval and signal shape. An additional Â±1 MeV/c2 uncertainty in the mass measurements is added, based on the observed deviations of the masses obtained for previously known vector bottomonium states, as in Adachi (2012a). These updated mass measurements correspond to hyperfine splittings of ÎMHF (1P ) = (+0.8 Â± 1.1) MeV/c2 and ÎMHF (2P ) = (+0.5 Â± 1.2) MeV/c2 , where statistical and systematic uncertainties in mass are added in quadrature."
311,1574,0.988,The Physics of the B Factories,"ports a measurement of the B + â Dâ â+ ââ²+ decays (Seon, 2011), where â, ââ² = e or Î¼ in any combination. Since we have no prior knowledge nor widely accepted model for these decays, a 3-body phase-space model is assumed for the signal simulation. To find signal candidates, first an energetic same-sign lepton pair is chosen. The lepton momentum in the lab frame is required to be greater than 0.5(0.8) GeV/c for electrons (muons). Particle identification requirements select electrons (muons) with an eï¬ciency of approximately 90% and a misidentification rate of 0.1% (1%) for pions in the kinematic region of interest. The energy sum of the dilepton system in the CM frame is required to exceed 1.3 GeV: this has minimal eï¬ect on the signal eï¬ciency in the phase-space model. The lepton pair is then combined with a Dâ â K + Ï â Ï â decay candidate. Kaons (pions) are discriminated from pions (kaons) with an eï¬ciency of approximately 91% (95%) and a misidentification rate below 4% (6%) in the kinematic region of interest. The K + Ï â Ï â invariant mass (MKÏÏ ) is required to be within Â±10 MeV/c2 from the nominal Dâ mass. The B candidates are further required to lie within mES > 5.2 GeV/c2 and |ÎE| < 0.3 GeV (the âanalysis regionâ). The major background sources are from continuum processes and to a lesser degree from semileptonic B decays such as B â Dâ â+ Î½â X, in which a same-sign lepton from the decay products of the other B is combined with the signal B. These backgrounds are suppressed by a single likelihood ratio R using the four variables: the Fisher discriminant F of the modified FoxWolfram moments (see Section 9.3), the cosine of the polar angle of the B candidate flight direction in the CM frame, cos Î¸B , the missing energy Emiss of the event, and the diï¬erence Î´z between the impact parameters of the two leptons in the beam direction. The requirement on R, determined mode-by-mode by a MC study, eliminates"
311,1679,0.988,The Physics of the B Factories,"based on analysis of a data sample of 395 fbâ1 (Uehara, 2006). The resonance is observed in two-photon production, a mechanism providing a clean environment for studying resonances in direct formation (see Chapter 22 for the details), both in Î³Î³ â D0D0 and Î³Î³ â D+Dâ (see Fig. 18.2.5 (a) and (b), respectively). The final state charmed mesons are fully reconstructed. Twophoton events are separated from e+ eâ annihilation and ISR events by requiring that the transverse momentum of the DD system be small, as expected for two-photon events in the no-tag mode (i.e., where neither the outgoing electron nor the positron are detected). The resulting combined invariant mass distribution is fitted with a relativistic Breit-Wigner signal function (taking the mass resolution and reconstruction eï¬ciency into account) and a background component (Fig. 18.2.5 (c)). The statistical significance of the Z(3930) peak is 5.3 Ï. The measured mass and total width of the resonance are listed in Table 18.2.3. The systematic uncertainties are dominated by uncertainties in the D mass and the choice of the signal function lineshape. Belle performs an angular analysis to identify the spin of the observed resonance. If one defines Î¸ as the angle of a D meson relative to the beam axis in the Î³Î³ frame (equivalent to the DD frame), the cos Î¸ distribution for a scalar particle will be flat, while for a spin-2 resonance produced with helicity 2 along the incident axis, a distribution proportional to sin4 Î¸ is expected. Spin-1 is largely suppressed in two-photon events with quasi-real photons (Yang, 1950), thus this assignment is not considered. The Belle data significantly favor spin-2 over spin-0 assignment, while the production and decay mechanisms require positive parity and C-parity. The resulting quantum numbers, J P C = 2++ , suggest identifying this particle with the previously unobserved Ïc2 (2P ) charmonium state. Assuming production of a spin-2 state, Belle calculated the product of its two-photon width and the branching fraction into DD (Table 18.2.3). The systematic errors are primarily due to uncertainties in tracking and particle identification eï¬ciencies, the choice of fit lineshapes and the errors of D branching fractions."
45,274,0.988,Measurement and Control of Charged Particle Beams,"Thus, Bmag is the ratio of the area of the decohered beam to the area of the injected beam. The factor of 2 results from the numerator representing an rms area. Examples of Emittance Dilution due to Mismatch. Emittance dilution results if Bmag = 1 due to the difference in the transverse phase advance of the particles within the bunch. There are multiple sources of such phase advance variations. The two most commonly considered sources depend on the chromaticity or on the amplitude of the betatron oscillations. The chromaticity Î¾ (â¡ Qâ² /Q) = (ÎÏ/Ï)/Î´ characterizes the energy dependence of the phase advance where ÎÏ is the difference in the phase advance of a particle from the mean phase advance of the bunch and Î´ is the relative energy deviation of that particle compared to the mean energy of the bunch. The amplitude dependence of the phase advance due to sextupole or octupolar magnetic fields is approximately described by 2ÏÏ = 2ÏÏ0 â Î¼a2 ,"
311,2622,0.988,The Physics of the B Factories,"As we have already observed, for tagged ISR analyses the hadrons, produced in a cone around the direction opposite to the tagged photon, generally fall in an instrumented region of the detector. The detection eï¬ciency is thus only weakly dependent on the final state hadronsâ angular distribution in the reference frame where the hadronic system is at rest, and the uncertainties related to the theoretical models used for simulation are significantly reduced. This is in contrast with both untagged ISR analyses and direct e+ eâ measurements, for which the region at small polar angles is largely inaccessible. There are also prices to pay for using the ISR technique. As discussed in Section 21.2.3, a good mass resolution and absolute mass scale are obtained in ISR analyses, but in direct e+ eâ measurements these quantities are given respectively by the beam energy spread and by the beam energy setting, which are determined far more precisely. The sources of background events for ISR measurements are significantly larger than those for direct e+ eâ measurements. In the latter case, the main backgrounds to a given final state come from other e+ eâ â hadrons reactions, due to undetected low momentum particles or to wrong particle identification, but such backgrounds are limited by the requirement of four-momentum conservation. For ISR events, a source of background of the same kind is due to mis-reconstructed events from other ISR processes where one or more particles escape detection. The constraint of four-momentum conservation is much less eï¬ective in this case because of the relatively poor resolution on the measurement of the energy of the ISR"
311,2128,0.988,The Physics of the B Factories,"However, SU (3)flavor symmetry breaking causes diï¬erences beyond those in the CKM factors contributing to the various states. These can be estimated from the measured branching fractions as given schematically in the fourth column of the table. If one denotes the expected B for a final state i by Î»n B0i , the measured B is ri Î»n B0i . The factors ri = 1 point to SU (3) symmetry breaking. The contributions to the mixing amplitude as evaluated using the measured branching fractions is given in the last column. The sum over all the states yields Î»2 (r2 + r3 â 2 r1 r4 ) which is in general diï¬erent from zero. In order to precisely calculate the mixing parameters, one would of course need to take into account other possible intermediate states for some of which the branching fractions are not well known. Nevertheless it is possible to estimate the magnitude of the < O(10â3 ) and |y| < O(10â2 ) mixing parameters to be |x| â¼ (Falk, Grossman, Ligeti, Nir, and Petrov, 2004). These are in rough agreement with the expectation of the OPE < O(10â3 ) (Bigi and Uraltsev, 2001b). method, |x|, |y| â¼ In summary, theoretical expectations based on the SM are that the mixing rate in the D0 system is small, arising mainly from long distance contributions that are difficult to estimate. Despite this diï¬culty, the measurement of mixing in this system over a wide range of decay modes and with suï¬cient precision can provide important constraints on possible NP parameters. Importantly and uniquely, such constraints will be complementary to those from down-type FCNC processes. On the experimental side, observations of D0 mixing come predominantly from measurements of the time evolution of neutral D meson decays to final states f that are accessible to both D0 and D0 . In such cases, as illustrated in Fig. 19.2.3, direct decay and decay preceded by mixing interfere. Mixing can, in principle, also be observed in semi-leptonic decays of D0 mesons where the leptons have the wrong sign. The only way, in the SM, for such decays to occur is through D0 -D0 mixing, with a tiny rate â (x2 + y 2 )/2 â¼ 5 Ã 10â5 . At the B Factories, cc pairs are produced in the electroweak annihilation of electrons and positrons. In the fragmentation of primary quarks various species of charmed hadrons together with lighter hadrons are produced. In general, pairs of D0 and D0 mesons are not"
291,90,0.988,Visualizing Mortality Dynamics in the Lexis Diagram (Volume 44.0),"The patterns observed in the three panels at the bottom of Fig. 9.1 deviate from the ones for circulatory and respiratory diseases above. Motor vehicle accidents do not peak in winter but around July and August. Many people assume that the reason for the peak in all-cause mortality is due to suicides in winter. The middle panel at the bottom of Fig. 9.1 illustrates why this assumption is wrong for three reasons: (1) The seasonal pattern is less pronounced for suicide than for other causes. (2) If one can speak of a seasonal pattern at all, the peak occurs definitely not during winter. (3) The 30,000 observed deaths are less than 1.5% of all deaths; not enough to shape the pattern for all causes. Lung cancer, whose impact on mortality in the United States was discussed in previous chapters, isâlike many malignant neoplasmsâan example of no or only negligible seasonality. Figure 9.1 displays an aggregated picture of monthly deaths. In our analysis we want to investigate, however, whether the seasonal pattern for selected causes of death differs by age as well as whether the seasonal pattern changed over calendar time. The multiplicative model2 suggested by Eilers et al. (2008) to decompose seasonal data allows such an analysis. The model is, at its core, another application of smoothing data via P-splines (Eilers and Marx 1996) as in Chap. 5. It is rather flexible since it allows the estimation not only of counts but also of rates. Exposures are then included as log offsets if the latter is desired, similar to Camardaâs approach (2012, 2015) employed in Chaps. 5, 6, and 7. We use the model in its most simple form: The model is estimating counts assuming an annual unimodal pattern in the data. Not allowing for bimodal patterns or even higher frequencies should not induce any problems in our analysis since the causes in which we are interested in feature clear patterns with one peak and one trough (see Fig. 9.1). We model the expected value of death counts y over age a and time t, ta D E.yta /, to be Poisson distributed using a log-link function log .ta / D vta C fta cos .!t/ C gta sin .!t/ with ! D 2=p, where p is the period. In our case of monthly values p D 12. Further technical details are given in Eilers et al. (2008). The estimation yields three smooth matrices/surfaces, vta for the trend as well as the smooth cosine and sine surfaces fta and gta . The trend surface captures any major changes in the overall pattern that could be caused by varying population sizes, survival improvements, competing risks . . . . We are mainly not interested in this trend surface nor in the the actual sine and cosine surfaces. The two latter surfaces allow us, however, to obtain an estimate for the amplitude and the phase over age and time via simple trigonometric functions. The latter denotes the location of the annual peak of the death counts and is expressed in the difference in days from the 1st of January; i.e., a value of 30 corresponds to late January whereas -30 indicates that mortality is highest in the beginning of December."
311,343,0.988,The Physics of the B Factories,"The correction is computed using only events in which the D0 meson in the ï¬rst Btag decays into the K â Ï + ï¬nal state. This is cross-checked using a sample in which the D0 meson from the ï¬rst tag decays into the K â Ï + Ï â Ï + ï¬nal state only, yielding complementary results. In both of the above methods, and across several iterations of semileptonic recoil-based analyses, BABAR has found the correction to be very close to 1.0. This suggests both that the assumptions in the above two methods are largely accurate, and also that existing simulations of these and the background decays are adequate for the purposes of modeling the decays. The correction has an associated systematic error, which is typically determined by propagating the statistical uncertainty due to the ï¬nite sample sizes of the double-tag and single-tag samples. The uncertainty of the correction is about 4%. Belle (Sibidanov, 2013) uses fully reconstructed events to calibrate the eï¬ciency of the NB-based Btag reconstruction. One of the produced B mesons is reconstructed as hadronic Btag while the other B meson is reconstructed in the semileptonic decay mode Bsl â D(â) âÎ½. The number of double tagged events is therefore given by:"
71,874,0.988,Advancing Culture of Living with Landslides: Volume 1 ISDR-ICL Sendai Partnerships 2015-2025 (Volume 1.0),"deposit area, width, and runout, vertical distance between headscarp crown and the tip of the deposits, etc. Besides the well-developed bedrock landslides that had already failed, evidence of deep-seated gravitational slope deformations (DSGSD) that can, under certain conditions, convert into catastrophic rock slope failure (Fig. 8) are identiï¬ed, digitized and included in the database. The volume of the deposits was calculated for each case individually. Considering the unknown morphology of both the source zone and depositional area prior to slope failure, the accuracy of the volume assessment is estimated as Â±30 â 50%. However, such accuracy seems to be sufï¬cient for the statistical analysis of basic relationships between the magnitude of the phenomena (characterized by its volume) and its main geometrical parameters (runout, area, H/L ratio) and such parameter as the DBIâthe Dimensionless Blockage Index (Ermini and Casagli 2003). It is higher than the scatter of the landslide volume versus area relationships, which usually are about 1 order larger (Honious et al. 1997; Guzetti et al. 2008). Considering the signiï¬cant scatter of volume estimates, some smaller features were included in the database even though, after being identiï¬ed and outlined on space images, they were found to be smaller. In addition, small rockslides that form clusters accompanying larger slope failures were"
372,921,0.988,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where the approximation holds for N ' 1. After the calculation in Eq. (8.132) is performed, the N-point window is moved by M steps, and the process is repeated. Each segment of M points is thus processed r times. Therefore, the input and output data rates are the same, except when spectral values at negative frequencies are discarded. The calculation in Eq. (8.132) is expressed diagramatically in Fig. 8.24. This process may seem counterintuitive in the following sense. The data stream is severely decimated by the action of the commutator, which distributes the time samples among the branches, or âpartitions,â with a cycling period M. That is, the data samples into each of the M partitions are x.0/; x.1/; x.2/; x.M ! 1/;"
372,602,0.988,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"the fringes are stopped, but for the other, they are lost in the averaging at the correlator output because the fringe frequencies are high. Thus, for one playback, we have the signal of an SSB system and the noise of a DSB p system in each of the real and imaginary outputs, that is, an SNR of V=.2 2Ë,/ and a relative sensitivity of 1=.2Ë/ for each individual sideband. 8. Measurement of cross-correlation as a function of time offset. Digital spectral correlators that measure cross-correlation as a function of time delay are described in Sect. 8.8. In a lag-type correlator, the cross-correlation is measured as a function of time offset, implemented by introducing instrumental delays. The Fourier transform of the cross-correlation as a function of relative time delay between the signals is the cross-correlation as a function of frequency, as required in spectral line measurements. As mentioned in Sect. 6.1.7, it is necessary to use only simple correlators for this measurement. The range of time offsets of the two signals covers both positive and negative values, and the resulting measurements of cross-correlation contain both even and odd components. Fourier transformation then provides both the real and imaginary components of the cross-correlation as a function of frequency. The full sensitivity is obtained so long as the range of time offsets is comparable to the reciprocal signal bandwidth or greater; see Table 9.7. Note that in Table 6.1, we have not included the quantization loss discussed in Sect. 8.3.3. A demonstration of the sensitivity using a simple correlator when the measurements are made as a function of time delay is given by Mickelson and Swenson (1991). Of the cases included in Table 6.1, the SSB with complex correlator is the one generally used where possible, because of the sensitivity and avoidance of the complications of DSB operation. Cases 2 and 3 in the table are included mainly for completeness of the discussion. As mentioned earlier, for frequencies of several hundred gigahertz, the most sensitive type of receiver input stage may be an SIS mixer. This has an inherently double-sided response, and if necessary, a sideband can be removed by filtering or using a sideband-separating arrangement (Appendix 7.1). For DSB operation, the most important cases in Table 6.1 are 6a and 6b. The case in which the unwanted sideband is only partially rejected is discussed in Appendix 6.1."
311,1495,0.988,The Physics of the B Factories,"17.10.1 Overview In this section, we review the measurements of purely leptonic decays, B + â â+ Î½ (â = e, Î¼, Ï ), and the semileptonic B decays B â D(â) Ï Î½. As b â u and b â c quark transitions, these processes depend on the magnitudes of the CKM matrix elements Vub and Vcb , respectively, however both have potential sensitivity to physics beyond the SM. In extensions of the SM which include an expanded Higgs sector, in particular the type-II two Higgs doublet model (2HDM) such as in the minimal supersymmetric extension of the Standard Model (MSSM), these processes are potentially sensitive to a charged Higgs boson (H Â± ). A number of benchmark new physics models, such as 2HDM and MSSM, are discussed in Section 25.2. The presence of the H Â± can impact the experimentally observed branching fractions for these decay modes and, in the case of B â D(â) Ï Î½, also the kinematic distributions of final state particles. Figure 17.10.1 shows Feynman diagrams for these tree level processes."
241,474,0.988,Second Assessment of Climate Change for the Baltic Sea Basin,"Owing to the transient nature of the atmospheric conditions over the Baltic Sea, the flow ï¬eld is highly variable, and thus changes in the resulting circulation and upwelling are difï¬cult to observe. However, three-dimensional models, forced by realistic atmospheric forcing conditions and river run-off, have reached a state of accuracy such that the highly fluctuating current ï¬eld and associated evolution of the temperature and salinity ï¬elds can be realistically simulated. Changes in the characteristics of the large-scale atmospheric wind ï¬eld over the central and eastern North Atlantic can be described by the North Atlantic Oscillation (NAO, see Chap. 4, Box 4.1). Weakened westerlies across northern Europe, which is characteristic of the negative phase of the NAO, are a precondition for outflow from the Baltic Sea. Thus, negative phases of the NAO have the potential to indirectly affect circulation in the Baltic Sea and water mass exchange with the North Sea (Lehmann et al. 2002). The linear correlation between the volume exchange of the Baltic Sea and the NAO index is only r = 0.28 (r = 0.49 for the NAO winter index DJFM, December through March). A better relation of the wind ï¬eld over the Baltic Sea to the large-scale atmospheric circulation is given by the (BSI), which is signiï¬cantly related to the NAO. Furthermore, the BSI is highly correlated with the variation in water storage in the Baltic Sea and volume exchange with the Danish Sounds. For northern Europe, the NAO accounts for about 50 % of the dominant climate winter regimes, the âblockingâ"
311,2569,0.988,The Physics of the B Factories,"(20.6.27) which is the most precise among all the published measurements and is somewhat lower than all of them although within errors consistent with the other results. An analysis of the KS Ï â invariant mass spectrum shown in Fig. 20.6.5 (top) reveals the dominant contribution from the K â (892)â with additional contributions of higher-mass states at 1400 MeV. A satisfactory fit is obtained only if the existence of a broad scalar state, K0â (800), is assumed. For the first time the K â (892)â mass and width have been measured in tau decay:"
297,1369,0.988,The R Book,"The ï¬tted values produced by the model are most unlikely to match the values of the data perfectly. The size of the discrepancy between the model and the data is a measure of the inadequacy of the model; a small discrepancy may be tolerable, but a large one will not be. The measure of discrepancy in a GLM to assess the goodness of ï¬t of the model to the data is called the deviance. Deviance is deï¬ned as â2 times the difference in log-likelihood between the current model and a saturated model (i.e. a model that ï¬ts the data perfectly). Because the latter does not depend on the parameters of the model, minimizing the deviance is the same as maximizing the likelihood. Deviance is estimated in different ways for different families within glm (Table 13.1). Numerical examples of the calculation of deviance for different glm families are given in Chapters 14 (Poisson errors), 16 (binomial errors), and 27 (gamma errors). Where there is grouping structure in the data, leading to spatial or temporal pseudoreplication, you will want to use generalized mixed models (lmer) with one of these error families (p. 710)."
8,614,0.988,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","The model of [4] is a statistical bootstrap model with baryon number conservation and proper volumes of the constituent hadrons and hadron clusters. These proper volumes grow in proportion to the cluster mass. As the strong interaction is in this model represented by all possible particle reactions (hadron chemistry), the number of particles is not conserved and, in calculating the partition function, summed from 0 to infinity. The mass spectrum (listing all possible hadrons and clusters) turns out to be exponentially increasing with the cluster mass. It is this exponential increase which generates the singularity via an integration over masses up to infinity. In a recent paper [7], the results of [4] were criticized on the basis of the following argument: if particle volumes grow in proportion to the mass, the mass integration is necessarily cut off when the sum of all particle volumes reaches the externally given volume V. Likewise the sum over particle numbers is cut off. Thus, trivially, no singularity can occur. What is not trivial is that, as the authors show, the thermodynamic limit ln Z.Ë; V/ V!1 V"
311,1874,0.988,The Physics of the B Factories,"distributions of the hadronic final states are shown in Fig. 18.4.19 for the five Î·c decay modes from Î¥ (1S) and Î¥ (2S) data, respectively. The large J/Ï signal is due to the ISR process e+ eâ â Î³ISR J/Ï , while the accumulation of events within the Î·c mass region is small."
372,1126,0.988,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where the first term on the right side represents the signal and the second term represents the noise. When the array is phased, the signal-to-noise (power) ratio is na TA =TS , and when it is unphased, it is TA =TS . Thus, the collecting area of the phased array is equal to the sum of the collecting areas of the individual antennas, but when it is unphased, it is, on average, equal to that of a single antenna. A question of interest concerns the case in which the antennas have different sensitivities resulting from different effective collecting areas and/or system temperatures. This is a matter of practical importance even for nominally uniform arrays, since maintenance or upgrading programs can result in differences in sensitivity. Consider a phased array in which the individual system temperatures and antenna temperatures are represented by TSi and TAi , respectively. Here, TAi is defined as the signal from a point source of unit flux density,2 so TAi is a characteristic of the antenna alone and is proportional to the collecting area. We consider only the weaksignal case for which TA "" TS . For antenna i, the output voltage from a source of flux density S is Vi D si C -i , and we can write hs2i i D STAi and h-i2 i D TSi . It is convenient to think of the output of each antenna as providing a measure of the flux density of the source, which is equal to Vi2 =TAi . The expectation of the measuredpvalue of p S should be the same for each p antenna. The corresponding voltages are S D Vi = TAi for the signal and -i = TAi for the noise. In the crosscorrelation of the array output with another VLBI antenna, the signal-to-noise ratio at the correlator output is proportional to the signal-to-noise voltage ratio of the signal from the array. Thus, in combining the signal voltages in the array, we p in effect, interested in maximizing the signal-to-noise ratio in an estimate of S. Because the array antennas are not identical, we should use weighting factors wi in combining their signals. The weights should be chosen to maximize the signal-tonoise ratio of the combined array signals which, in voltage, is X wi Vi Rsn D"
213,224,0.988,Collider Physics Within The Standard Model : a Primer,"2.10.4 Other Ës .mZ / Measurements as QCD Tests There are a number of other determinations of Ës that are important because they arise from qualitatively different observables and methods. Here I will give a few examples of the most interesting measurements. A classic set of measurements comes from a number of infrared-safe observables related to event rates and jet shapes in eC e annihilation. One important feature of these measurements is that they can be repeated at different energies in the same detector, like the JADE detector in the energy range of PETRA (most of the intermediate energy points in the right-hand panel of Fig. 2.32 are from this class of measurements) or the LEP detectors from LEP1 to LEP2 energies. As a result, one obtains a striking direct confirmation of the running of the coupling according to the renormalization group prediction. The perturbative part is known at NNLO [213], and resummations of leading logs arising from the vicinity of cuts and/or boundaries have been performed in many cases using effective field theory methods. The main problem with these measurements is the possibly large impact of non-perturbative hadronization effects on the result, and therefore on the theoretical error. According to [99], a summarizing result that takes into account the central values and the spread from the JADE measurements at PETRA, in the range 14â46 GeV, is Ës .mZ / D 0:1172 Ë 0:0051 ; while from the ALEPH data at LEP, in the range 90â206 GeV, the reported value [164] is Ës .mZ / D 0:1224 Ë 0:0039 :"
49,80,0.988,Artificial Intelligence and Cognitive Science IV,"It is difficult to evaluate the results of unsupervised learning. In this case the emphasis is on consistency of results on the training and on the testing set. This means that when operator assigns a name to certain outputs these outputs will become active for the same objects in the training and in the testing set. In the single camera experiment together 40 frequent episodes were found on layer 0 (11 of length 2 - not used, 7 of length 3, 12 of length 4 and 10 of length 5). The sequences may be visualized as e.g. the sequence of appropriately post-processed cluster centroids. In the ideal case the centroids would represent different elementary shapes. Without Gabor filtering the centroids in our case were relatively uniform patches of various luminance meaning the changes in luminance are more important rather than changes of texture. That also means that changes in exposure would influence the behavior of the system negatively. Appropriate data preprocessing is needed to achieve robustness. Together 24 frequent episodes were found on layer 1 (20 of length 1 (cluster index), 3 of length 2 and 1 of length 3). Based on this the output vector had 24 elements. Variation of information on the output of layer 1 was lower which confirmed the expectation based on the memory-prediction theory. The sequence written by layer 1 on which the temporal data mining was performed"
295,23,0.988,interface oral Health Science 2016 : innovative Research On BiosisâAbiosis intelligent interface,"Youngâs modulus. Therefore, the Youngâs modulus of the alloys first decreases, and Tiâ17MoâST exhibits the lowest Youngâs modulus among the designed alloys. With a further increase in the Mo content, the athermal Ï-phase in the alloy disappears. Thus, the solid solution strengthening with Mo becomes the primary cause of the changes in the Youngâs modulus. The Youngâs modulus increases as the bcc lattice contracts with increasing Mo content. Therefore, the Youngâs modulus of Tiâ18MoâST is slightly higher than that of Tiâ17MoâST. The microstructural analysis indicates that deformation-induced Ï-phase transformation occurs in all the alloys. Therefore, the increase in the Youngâs moduli of these four alloys following CR is attributed to the deformation-induced Ï-phase transformation that occurred during CR. The lack of change in the Youngâs modulus of Tiâ13Mo alloy is considered to result from the combined effect of deformation-induced Ï-phase transformation and deformation-induced Î±â³-phase transformation."
297,1160,0.988,The R Book,"attach(data) plot(time,amount,pch=21,col=""blue"",bg=""brown"") abline(lm(amount~time),col=""green"") The curvature in the relationship is clearly evident from the poor ï¬t of the straight-line (green) model through the scatterplot (there are groups of positive residuals for low and high values of time, and a large group of negative residuals at intermediate times). Now we ï¬t the linear model of log(amount) as a function of time: model <- lm(log(amount)~time) summary(model) Call: lm(formula = log(amount) ~ time) Residuals: 1Q Median -0.5935 -0.2043 0.0067"
238,286,0.988,Nanoinformatics,"There are two parameters in the SVM, the margin of tolerance and penalty factor. The best parameters were selected from combinations where the margin of tolerance was 0.001, 0.01, 0.05 or 0.1, the penalty factor was 10, 100, 1000 or 10000, and the variance was 10â2, 10â3, 10â4 or 10â5, for a total of 64 different patterns. As a result, a margin of tolerance of 0.01, a penalty factor of 1000 and a variance of 10â4 were used as SVR parameters. The results of the regression analysis for the training data are shown in Fig. 8.3a. Most data lie along the grey line, indicating that the predicted energies are equal to the accurate energies and that the regression analysis succeeded in correctly constructing the predictor. To evaluate the accuracy of the constructed predictor, the predictor was applied to Î£13[001]/(230) as a test situation. The results predicted by the predictor are shown in Fig. 8.3b. Most of the predicted grain boundary energies also lie on the grey line, indicating that the constructed predictor is also suitable for the test data. This result implies that the constructed predictor has the potential to predict the energy of the grain boundaries prior to the structure and energy calculations. Here, we focus on the blue data point marked by the blue arrow in Fig. 8.3b. Based on the constructed predictor, the blue data point was predicted to provide the minimum grain boundary energy. It should be mentioned that the virtual screening method and the calculations of all candidates give the minimum grain boundary energy at the same blue data point. The predicted grain boundary energy is 0.96 J/m2, which is only 10% larger than that the minimum grain boundary energy obtained from all-candidate calculations. It is also noteworthy that the predicted rigid body translation state (X = 5.0 Ã, Y = 1.0 Ã, and Z = 0.0 Ã) is identical to the most stable rigid body translation state determined by all-candidate calculations. We succeeded in screening all possible candidates and selecting the most promising candidate conï¬guration for accurately provide the most stable structure. By performing the structure and energy calculation once for this rigid body translation state, a grain boundary energy and structure identical to those obtained from all-candidate calculations can be obtained. Namely, the stable grain boundary structure and energy can be determined with only a one-time calculation using the present virtual screening method, which is signiï¬cantly more efï¬cient than previously reported methods. Since the constructed prediction model (the predictor shown in Fig. 8.2) was established, this predictor was also applied to other GBs. Here, based on the constructed predictor, the structures and energies of 12 other [001]-axis-symmetric tilt CSL grain boundaries, Î£25[001]/(430), Î£25[001]/(710), Î£29[001]/(520), Î£29 [001]/(730), Î£37[001]/(610), Î£37[001]/(750), Î£41[001]/(910), Î£41[001]/(540), Î£53[001]/(720), Î£53[001]/(950), Î£61[001]/(11 1 0), and Î£125[001]/(11 2 0), are predicted. Figure 8.4 shows the results of the predicted grain boundary energies and a comparison with previously reported grain boundary energies [19, 20]. Based on previous studies, the grain boundary energy exhibits a convex proï¬le in relation to the misorientation angle Î¸. Small cusps are also present, namely energy drops at 16.26Â°, 28.07Â°, 36.87Â°, 53.13Â°, and 67.38Â° corresponding to Î£25[001]/(710), Î£17"
311,1821,0.988,The Physics of the B Factories,"center-of-mass energy s and in the continuum sample. Hadronic events are selected using criteria that preferentially select events with open bottom (B) mesons. Muon pairs are cleanly and eï¬ciently selected using the tracking system and calorimeter only; the muon identification system is not required. The 10.54 GeV data includes all event types that satisfy the hadronic selection, other than open bottom: continuum e+ eâ â q qÌ (the dominant background), ISR production of Î¥ , and two photon events. The ISR contribution is calculated using simulated events. The two-photon component, 2% of the continuum sample, is estimated from the direction of the missing-momentum vector. The continuum component is obtained from the 10.54 GeV data by subtracting the other two components. The eï¬ciency for open bottom events to satisfy the criteria is obtained from simulation. It is taken to be the average of all possible two-body final states. Half the spread is taken as a systematic error. The resulting values of Rb (s) are shown in Fig. 18.4.2. Note that radiative corrections have not been applied. Not shown in the figure are correlated systematic errors totaling 2.6%, with equal contributions from hadronic event and muon pair eï¬ciencies and Î¼+ Î¼â radiative corrections. The region 10.80â11.20 GeV is fit with a model containing two interfering relativistic Breit Wigner resonances representing the Î¥ (10860) and the Î¥ (11020), a flat interfering component, plus an addition flat component representing bbÌ continuum not interfering with the resonances (Fig. 18.4.3). The resulting mass and widths are 10.876 Â± 0.002 GeV/c2 and 43Â±4 MeV for the Î¥ (10860) and 10.996Â± 0.002 GeV/c2 and 37 Â± 3 MeV for the Î¥ (11020) (Aubert, 2009x). These widths are considerably narrower than the previous PDG values. The results are sensitive to the details of the fit model. For example, using a threshold function instead of the flat non-resonant component gives a slightly diï¬erent mass and a significantly larger width (74 Â± 4 MeV) for the Î¥ (10860). A proper coupled channel approach including"
311,2141,0.988,The Physics of the B Factories,"Values of the mixing parameters for the D0 meson system can diï¬er significantly from SM estimates in several NP models. In Golowich, Hewett, Pakvasa, and Petrov (2007) the authors examined a large number of such models and calculated the contributions of new particles and processes to the mixing parameters x and y. Due to the large uncertainties in SM calculations, the values are obtained for specific NP contributions alone.122 In this approach the parameters of a large majority of the models considered are additionally constrained by the measured values of the D0 mixing parameters. An example of the sensitivity of the value of x to the mass and CKM elements of a possible fourth generation bâ² quark is shown in Fig. 19.2.6.123 Not only x but also y can be sensitive to some of the NP models considered. As pointed out in Eq. (19.2.5), the mixing parameters in the SM vanish in the exact SU (3)flavor limit. Moreover, the contribution to the mixing parameters enters only as a second order eï¬ect in the SU (3)flavor breaking. Hence the NP contributions to y could be significant for the models in which the contributions do not vanish in the SU (3)flavor symmetry limit (Golowich, Pakvasa, and Petrov, 2007). An example is the R-parity violating SUSY model, where the slepton mediated interaction is not suppressed in the SU (3)flavor symmetry limit and could lead to values as high as |y| â 3.7% for MâÌ = 100 GeV/c2 . Section 25.2 is a more general discussion on how one can constrain benchmark NP models using constraints from the B Factories."
233,470,0.988,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"Similarly, pair-wise Spearman rank correlations for Zonation solutions done with the different 100 phylogenetic trees were also very high. The mean pair-wise correlation was 0.99985 and even the lowest pair-wise correlation was 0.99934. There were only a few regions across the study area where the rankings were not consistent (Fig. 3). We also tested whether the uncertainty of tree structure was related to the position in Zonation rank, that is, whether there may have been more or less uncertainty associated with top ranking cells. Pair-wise Spearman rank correlations of uncertainty with each of the main Zonation variants gave weak, positive correlations of 0.10 for the basic solution, 0.12 for the phylogenetic diversity analysis, 0.08 for the basic national scale analysis, and 0.07 for the national scale phylogenetic diversity analysis. As the tree uncertainty seemed to play a very minor role in the prioritization outcome, in the following analyses we used one tree only (see Fig. 1). To assess how well the Zonation priorities covered the different speciesâ ranges, we plotted the proportions of species ranges retained in the landscape at different fractions of cell removal (Fig. 4). The median of representation is higher for the analysis with phylogenetic diversity than for the one without (Fig. 4, black squares). This may seem surprising, but is explained by the fact that Raoâs QE correlates with species richness. Looking into the corresponding values for individual species"
273,193,0.988,Report on Global Environmental Competitiveness (2013),"x is the average value of sample data and Ï is the standard deviation of sample data. Of course, the actual distribution of indicators would not be strictly in normal state, but according to the Law of Large Numbers, even the indicator data is other state of distribution, such feature also exists. So, if certain sample value of the indicators goes beyond the range of 3 standard deviation of the average value, the value can be judged as the extreme value of the indicator and needs regression to within the range after treatment of re-check and revision."
297,1361,0.988,The R Book,"where the xs are the values of the p different explanatory variables, and the Î²s are the (usually) unknown parameters to be estimated from the data. The right-hand side of the equation is called the linear structure. There are as many terms in the linear predictor as there are parameters, p, to be estimated from the data. Thus, with a simple regression, the linear predictor is the sum of two terms whose parameters are the intercept and the slope. With a one-way ANOVA with four treatments, the linear predictor is the sum of four terms leading to the estimation of the mean for each level of the factor. If there are covariates in the model, they add one term each to the linear predictor (the slope of each relationship). Interaction terms in a factorial ANOVA add one or more parameters to the linear predictor, depending upon the degrees of freedom of each factor (e.g. there would be three extra parameters for the interaction between a two-level factor and a four-level factor, because (2 â 1) Ã (4 â 1) = 3). To determine the ï¬t of a given model, a GLM evaluates the linear predictor for each value of the response variable, then compares the predicted value with a transformed value of y. The transformation to be employed is speciï¬ed in the link function, as explained below. The ï¬tted value is computed by applying the reciprocal of the link function, in order to get back to the original scale of measurement of the response variable."
285,720,0.988,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","Entracking is not a rare phenomenon. We and others have observed it in various cell types and nuclei, and in a number of species. In the ventral CN of the cat, most types of projection neuron display this behavior to some degree (spherical and globular bushy cells, octopus cells, commissural multipolar cells, stellate cells). We have observed it in the medial nucleus of the trapezoid body (Mc Laughlin et al. 2008) and in other neurons of the superior olivary complex (e.g. Joris and Smith 2008). We have observed entracking in different species (cat, chinchilla, gerbil; see also studies cited above), and have limited evidence in the CN of macaque monkey. An important qualifier is that the entracking observed is not always the extreme form (exactly 1 spike/cycle), particularly at frequencies above a few 100 Hz. The defining property is that there is an effect of absolute frequency on average firing rate so that rate increases monotonically with frequency up to a certain maximum. Thus, in the brainstem, firing rate does not only depend on SPL and stimulus frequency relative to CF, as it does in the AN, but also depends on absolute stimulus frequency. For the extreme cases of this behavior a stronger statement can be made: SPL and stimulus frequency relative to CF have remarkably little effect, and absolute frequency is the overriding stimulus parameter determining response rate. Besides being present in various cell types, nuclei, and species, entracking has inherent properties that make it an attractive coding mechanism. It is remarkably invariant with SPL, i.e. once perfect entracking is reached, further increases in SPL do not affect average response rate: the rate-level function shows a limited (20 dB) dynamic range, and at higher SPLs the firing rate remains clamped at the stimulus frequency. At the population level, increases in SPL cause an increase in the number of entracking neurons. A second striking property is the low variability in firing rate. In some cases there is no variability: exactly the same number of spikes is generated in response to the same stimulus presented at the same or other SPLs."
175,1284,0.988,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","The parameter Î³ is greater than 0 and no greater than 1. In practice Î³ is very close to 1. Again the values of these parameters, including the number of reaches n, can be found using nonlinear optimization methods, such as genetic algorithms, together with a time series of observed reach inflows and outflows."
297,1859,0.988,The R Book,"dynamics, but the lags associated with signiï¬cant negative and positive feedbacks are extremely interesting and highly suggestive. The main prey species of the lynx is the snowshoe hare and the negative feedback at lag 2 may reï¬ect the timescale of this predatorâprey interaction. The hares are known to cause medium-term induced reductions in the quality of their food plants as a result of heavy browsing pressure when the hares are at high density, and this could map through to lynx populations with lag 4. The order vector speciï¬es the non-seasonal part of the ARIMA model: the three components (p, d, q) are the AR order, the degree of differencing, and the MA order. We start by investigating the effects of AR order with no differencing and no moving average terms, comparing models on the basis of the AIC: model10 <- arima(Lynx,order=c(1,0,0)) model20 <- arima(Lynx,order=c(2,0,0)) model30 <- arima(Lynx,order=c(3,0,0)) model40 <- arima(Lynx,order=c(4,0,0)) model50 <- arima(Lynx,order=c(5,0,0)) model60 <- arima(Lynx,order=c(6,0,0)) AIC(model10,model20,model30,model40,model50,model60) model10 model20 model30 model40 model50 model60"
142,562,0.988,Subseafloor Biosphere Linked To Hydrothermal Systems : Taiga Concept,"of plagioclase and clinopyroxene (+olivine) (Fig. 14.6). Differences in the major-element compositions between the oceans can be seen in the diagram. Gabbros from the ultraslow to slow-spreading ocean ridges (MARK area of the Mid-Atlantic ridge and Atlantis Bank of the Southwest Indian Ridge) have lower plagioclase anorthite contents for a given clinopyroxene Mg# than those in gabbros from the fast-spreading ocean ridge (Hess Deep and/or Pito Deep of East Pacific) (Perk et al. 2007) (Fig. 14.6). Low-anorthite content of plagioclase at the same Mg# of clinopyroxene can be considered as analogues to the parameter Na8 defined by Klein and Langmuir (1987), which is interpreted to be a good indicator of the extent of partial melting. Since a general trend of an increasing extent of partial melting is moved toward higher Ca, a higher anorthite content of plagioclase is expected to be crystallized from hypothetical"
307,144,0.988,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"3.2.1 Numerical Simulations with the Theoretical Closed State Blocker We consider a mutation characterized by  D 3 and we apply closed state blockers (see reaction scheme (3.5)) with parameters satisfying the relation (3.10). In Fig. 3.1, we show the results of these simulations using the Monte Carlo approach: The lower panel of the figure is the same as the upper panel, except that we focus on concentrations ranging from 80 to 91 M . We observe significant differences between the wild type solution and the solution representing the mutation. Furthermore, we observe that the drug works quite well. Similar results are given in Fig. 3.2, where the computations are based on the probability density approach: Here the lower panel focuses on very high concentrations ranging from 89 to 91 M. We also see"
372,880,0.988,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"of the sampling interval !s . We consider the case of Nyquist sampling, for which !s D 1=.2&""/. The measured cross-correlation refers to the quantized waveforms, and the analysis in Sect. 8.4.1 shows how this is related to the cross-correlation of the unquantized waveforms. For correlation levels that are not too large, the two quantities are closely proportional, so for simplicity, we assume that Eq. (8.113) represents the behavior of the measured cross-correlation. The measurements are made with 2N time offsets from !N!s to .N ! 1/!s between the signals, and Fourier transformation of these discrete values yields the cross power spectrum at frequency intervals of .2N!s /!1 D &""=N for Nyquist sampling. The N complex values of the positive frequency spectrum are the data required. Of these, the imaginary part comes from the odd component of the correlator output r.!/. Thus, in the correlation measurement, it suffices to use single-multiplier correlators to measure 2N real values of r.!/ over both positive and negative values of ! for one antenna with respect to the other. As an alternative to measuring only the real part of the correlation, complex correlators could be used to measure both the real and imaginary parts for a range of time offsets from zero to .N !1/!s . However, complex correlators require broadband quadrature networks. Measurement of the cross-correlation over the limited time offset range is equivalent to measuring r.!/ multiplied by a rectangular function of width 2N!s . The cross power spectrum derived from the limited measurements is therefore equal to the true cross power spectrum convolved with the Fourier transform of the rectangular function, that is, with the sinc function sin.$""N=&""/"
280,370,0.988,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"The vertical axis in Figs. 12.5a and 12.5b is the relative abundance (RA), which denotes the population rate of the form f. polytes to all two forms (f. cyrus and f. polytes). Field data, which is shown by solid circles in Fig. 12.5b, clearly shows the positive correlation between the AI and the RA, which means that the higher the population ratio of the model butterfly in an island, the higher the ratio of the mimetic female to all females in the island."
372,241,0.988,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"Thus, the output of the correlator can be expressed in terms of a fringe pattern corresponding to that for a hypothetical point source in the direction s0 , which is the phase reference position. As noted earlier, this is usually the center or nominal position of the source to be imaged. The modulus and phase of V are equal to the amplitude and phase of the fringes; the phase is measured relative to the fringe phase for the hypothetical source. As defined above, V has the dimensions of flux density (W m!2 Hz!1 ), which is consistent with its Fourier transform relationship with I. Some authors have defined visibility as a normalized, dimensionless quantity, in which case it is necessary to reintroduce the intensity scale in the resulting image. Note that the bandwidth has been assumed to be small compared with the center frequency in deriving Eq. (3.5). In introducing a coordinate system, the geometry we now consider is illustrated in Fig. 3.2. The two antennas track the center of the field to be imaged. They are assumed to be identical, but if they differ, AN .! / is the geometric mean of the beam patterns of the two antennas. The magnitude of the baseline vector is measured in wavelengths at the center frequency of the observing band, and the baseline has components .u; v; w/ in a right-handed coordinate system, where u and v are measured in a plane normal to the direction of the phase reference position. The spacing component v is measured toward the north as defined by the plane through the origin, the source, and the pole, and u is measured toward the east."
311,637,0.988,The Physics of the B Factories,"Systematic error estimation Editors: Wolfgang Gradl (BABAR) Pao-Ti Chang (Belle) Additional section writers: Adrian Bevan, Chih-hsiang Cheng, Andreas Hafner, Kenkichi Miyabayashi For most measurements at the B Factories, the estimation of systematic uncertainties is a very important and challenging part of the analysis. There are a number of eï¬ects which can systematically inï¬uence the result. The ones which are frequently encountered in measurements performed by the B Factories are discussed in the present chapter. Sources of systematic eï¬ects include the diï¬erence between data and simulation, the uncertainty on external input needed to convert a directly measured value (e.g. the number of signal events) to the desired quantity (e.g. a branching fraction), and the analysis procedure chosen to extract the signal (e.g. background model, ï¬t bias). In addition, physics processes can introduce discrepancies between the measured value and the parameter of interest. This is often the case because the signal model used is only an approximation of the true, underlying process. An example of this type of systematic uncertainty is the eï¬ect of tag-side interference in measurements of time-dependent CP asymmetries. Where possible, measured values are corrected for such systematic shifts, and there is a systematic uncertainty associated with the correction. Some of the systematic corrections are derived from control sample studies; their associated uncertainty is essentially statistical in nature and scales with the size of the corresponding control sample and therefore with the data sample available for analysis. Careful design of the analysis strategy can help to minimize the eï¬ect of systematic errors on the ï¬nal result. A particular systematic eï¬ect might cancel in the ratio of two observable quantities, such as the total number of produced B mesons in the measurements of rate asymmetries. Similarly, if the branching fraction of a decay is measured relative to a well-known decay mode with similar ï¬nal state topology, systematic uncertainties due to reconstruction or PID eï¬ciency cancel to a certain extent."
297,1215,0.988,The R Book,"Instead of ï¬tting continuous, measured variables to data (as in regression), many experiments involve exposing experimental material to a range of discrete levels of one or more categorical variables known as factors. Thus, a factor might be drug treatment for a particular cancer, with ï¬ve levels corresponding to a placebo plus four new pharmaceuticals. Alternatively, a factor might be mineral fertilizer, where the four levels represent four different mixtures of nitrogen, phosphorus and potassium. Factors are often used in experimental designs to represent statistical blocks; these are internally homogeneous units in which each of the experimental treatments is repeated. Blocks may be different ï¬elds in an agricultural trial, different genotypes in a plant physiology experiment, or different growth chambers in a study of insect photoperiodism. It is important to understand that regression and analysis of variance (ANOVA) are identical approaches except for the nature of the explanatory variables. For example, it is a small step from having three levels of a shade factor (say light, medium and heavy shade cloths) then carrying out a one-way ANOVA, to measuring the light intensity in the three treatments and carrying out a regression with light intensity as the explanatory variable. As we shall see later on, some experiments combine regression and ANOVA by ï¬tting a series of regression lines, one in each of several levels of a given factor (this is called analysis of covariance; see Chapter 12). The emphasis in ANOVA was traditionally on hypothesis testing. Nowadays, the aim of an analysis of variance in R is to estimate means and standard errors of differences between means. Comparing two means by a t test involved calculating the difference between the two means, dividing by the standard error of the difference, and then comparing the resulting statistic with the value of Studentâs t from tables (or better still, using qt to calculate the critical value; see p. 287). The means are said to be signiï¬cantly different when the calculated value of t is larger than the critical value. For large samples (n > 30) a useful rule of thumb is that a t value greater than 2 is signiï¬cant. In ANOVA, we are concerned with cases where we want to compare three or more means. For the two-sample case, the t test and the ANOVA are identical, and the t test is to be preferred because it is simpler."
307,454,0.988,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"Fig. 14.7 The probability density functions of the open, closed, and inactivated states for the burst mode model. The mutation severity index is given by  D 10; 30; and 100 and the black line represents the wild type. Note that we only show solutions for the values of the transmembrane potential where the solutions differ as a result of the mutations Table 14.3 Values of the parameters used in the model in Fig. 14.6. The remaining rates are as in Table 12.2 on page 184"
165,578,0.988,New Methods for Measuring and Analyzing Segregation,"cpw i = Î£pw i = Î£w i W , cpb i = Î£pb i = Î£b i B, and cpt i = Î£pt i = Î£t i T . The graph that results from plotting these values as described is similar to the segregation curve in one key respect; under conditions of exact even distribution, the curves for the White and Black population will coincide with the diagonal line for the total population. So the diagonal is a reference point for even distribution. A key difference from the segregation curve is that under conditions of uneven distribution, the curve for the cumulating proportion of the Black population will rise above the diagonal and the curve for the cumulating proportion of the White population will fall below the diagonal. Like the segregation curve, the areas between the curves and the diagonal in this graph have relationships to the values of G and D. This should not be surprising since the information plotted is very similar to the information plotted in the segregation curve. However, the visual representation here is distinct. One feature of this graphical device is that the diagonal directly reflects relative rank position on area proportion White (p). Thus, the contrast between the diagonal and the curves for Whites and Blacks provides a basis for grasping their differences in relative rank position. A curve that rises above the diagonal is skewed toward below average rank positions. A curve that falls below the diagonal is skewed toward above average rank positions. The implications of the curves for group means on relative rank position are depicted graphically by plotting two vertical lines; one indicates the value of mean relative rank for Whites (YW) and the other indicates mean relative rank for Blacks (YB). Under conditions of exact even distribution, these will necessarily coincide at the value of 0.50, the overall mean on relative rank for area proportion White (p). Where these two values differ, the value for YW exceeds 0.50 and is necessarily higher than the value of YB which falls below 0.50. As noted earlier, the logical range for YW is from 0.5 to Q + ( P 2 ) and the logical range of YB is from Q/2 to 0.5, and the maximum value for (YW â YB) is 0.5 which occurs under complete segregation. The graphs in the figure are organized by two rows and three columns. The three columns are for three conditions for segregation. The graphs in the first (leftmost) column are for the extreme condition of exact even distribution where the value of G is 0. The graphs in the third (rightmost) column are for the opposite extreme condition of complete segregation where the value of G is 100. The graphs in the middle column are for substantial, but not complete, segregation where the value of G is 0.900.9 The two rows are for two conditions of city racial composition. The top row is for a city where P and Q are both 0.50. The bottom row is for a city where P is 0.80 and Q is 0.20."
311,1044,0.988,The Physics of the B Factories,"The lepton from the B decay must have a momentum in the range 1.3â2.4 GeV/c, and the soft pion momentum must be between 60 and 200 MeV/c. By approximating the Dâ+ momentum from the Ïs+ momentum, they calcu2 late the square of the missing neutrino mass (mÎ½miss ) . The (mmiss ) distribution peaks at zero for signal events, while it is spread over a wide range of mostly negative values for background events. BABAR determines the Brec decay vertex from a vertex fit of the lrec and Ïs tracks, constrained to the beam spot position in the transverse plane, but accounting for the average B 0 flight distance. The decay point of Btag is determined from ltag and the beam spot following a procedure similar to that of the Brec decay vertex. The flavor of Brec is determined from the lrec and soft pion charges. The flavor of the other B in the event is determined from the charge of ltag . After all selection criteria BABAR finds 49,000 signal events over a background of 28,000 events in the region (mÎ½miss ) > â2.5 GeV2/c4 . Background studies are done with events in the region (mÎ½miss ) < â2.5 GeV2/c4 if no signal candidate is found in the event. BABAR simultaneously fits the distributions of (mÎ½miss ) , Ît, and its uncertainty Ïât , for mixed and unmixed events, with a binned maximum-likelihood method. Probabilities for a given event to belong to any of the identified background sources (e+ eâ â qq continuum, BB combinatorial, and B + peaking background) are calculated based on the background (mÎ½miss ) distributions. Signal is considered to be any combination of a lepton and a charged Dâ+ produced in the decay of a single B 0 meson. They further divide their signal events according to the origin of the tag lepton into primary, cascade, and decay-side lepton tags. A primary lepton tag is produced in the direct decay B 0 â Xl+ Î½ l , a cascade lepton tag is produced in the process B 0 â DX, D â lâ Y , and a decay-side tag is produced by the semi-leptonic decay of the unreconstructed D0 . The relative normalization between mixed and unmixed signal events is constrained based on the time-integrated mixing rate Ïd . The Ît signal p.d.f. for both unmixed and mixed events consists of the sum of p.d.f.s for primary, cascade, and decay-side tags each convoluted with its own resolution function. They use the standard three Gaussian resolution function with eventby-event Ît uncertainties. From the fit BABAR obtains ÏB 0 = (1.504 Â± +0.018 +0.007 ) ps and Îmd = (0.511 Â± 0.007â0.006 ) psâ1 , 0.013â0.013 where the first errors are statistical and the second are systematic. The statistical correlation between ÏB 0 and Îmd is 0.7%. The results include corrections of â0.006 ps on ÏB 0 and +0.007 psâ1 on Îmd due to biases from event selection, boost approximation, B â peaking background, and combinatorial BB background based on MC studies. The systematic error in Îmd is dominated by unâ1 certainties in the SVT alignment (+0.0038 ), the seâ0.0033 ps lected range of Ît and Ïât (0.0033 ps ), and analysis bias (0.0035 psâ1 ), whereas the largest systematic error sources in the ÏB 0 measurement are the SVT alignment"
373,95,0.988,Introduction to Data Science,"where Î± is the level of each test and k is the number of interim looks. Larger samples allow to increase the level of significance of the finding, because with a larger sample, the probability of a result is likely to increase. This effect is expected because the larger the sample size, the more accurately it is expected to reflect the behavior of the entire population."
311,3016,0.988,The Physics of the B Factories,"Figure 25.1.1. Results of global ï¬ts in the (Ï, Î·) plane, from CKMï¬tter and UTï¬t, showing the consistency of b â d, b â s and s â d ï¬avor-changing transitions with the Kobayashi-Maskawa mechanism for the common origin of the observed CP violation. The inputs of Tables 25.1.1 through 25.1.3 are used to obtain these plots. The second solution for the value of Ï1 is suppressed using the measurements of ï¬nal states that have an asymmetry dependence on cos 2Ï1 . The corresponding numerical results from these ï¬ts can be found in Table 25.1.4. Table 25.1.5. Compatibility of the individual inputs with their prediction from the global ï¬t. Input"
311,2772,0.988,The Physics of the B Factories,"22.2.2 Comparison with QCD predictions at high where Aa0 (980) , Aa0 (Y ) and Aa2 (1320) are the amplitudes energy of the a0 (980), a0 (Y ) and a2 (1320), respectively; BS , BD0 and BD2 are non-resonant (hereafter called âbackgroundâ) amplitudes for S-, D0 and D2 waves; and Ïs0 , Ïs1 , and Ïd2 Two-photon production of exclusive hadronic final states are the phases of resonances relative to background am- provides useful information about resonances, perturbaplitudes. The goal of the analysis is to obtain parameters tive QCD, and non-perturbative QCD. From the theoof the a0 (980) and a0 (Y ) and to check the consistency of retical point of view, a two-photon process is attractive the a2 (1320) parameters that have been measured well in because of the absence of strong interactions in the initial state. the past. Brodsky and Lepage (1981) (BL) numerically calcuThe background amplitudes are parameterized as seclated the amplitude for the hard exclusive Î³Î³ â M1 M 2 ond order polynomials in W for both the real and imagprocesses within the context of the perturbative-QCD inary parts of all waves. The arbitrary phases are fixed by choosing Ïs0 = Ïd2 = 0. for S- and D- waves. We (pQCD) for the first time. A similar formula is also disconstrain all the background amplitudes to be zero at cussed by Chernyak and Zhitnitsky (1984). In this pQCD the threshold in accordance with the expectation that the framework, the amplitude for Î³Î³ â M1 M 2 can be decross section vanishes in the Thomson limit which was scribed in a factorized form: originally discussed in the context of low-energy Compton MÎ»1 Î»2 (s, Î¸â ) = (22.2.4) scattering. The relativistic Breit-Wigner resonance ampli tude (see Chapter 13) is used for a resonance. dxdyÏM (x, Qx )ÏM (y, Qy )TÎ»1 Î»2 (x, y, Î¸â ), The data are fit using the minimizer Minuit (James and Roos, 1975). Many fits are done using diï¬erent, randomly chosen starting parameters. In this way, Belle search where s is the squared invariant mass of the di-meson sysfor a global minimum and locate ambiguous solutions. tem, ÏM (x, Qx ) is a single-meson distribution amplitude The resulting best fit obtained is displayed in Fig. 22.2.2. for a meson M . The squared amplitude |ÏM (x, Qx )|2 is  2 and D  2 , are reproduced proportional to a probability for finding a valence quark The measured spectra, S2 , D fairly well by the fit. and antiquark in the meson, carrying a fraction x and Table 22.2.1 summarizes the fit results for Î³Î³ â Î·Ï 0 1 â x, respectively, of the mesonâs momentum. Qx is the as well as the other processes measured by Belle: Î³Î³ â typical momentum scale in the process, â¼ min(x, 1 â Ï + Ï â , Î³Î³ â K + K â , Î³Î³ â KS0 KS0 , Î³Î³ â Ï 0 Ï 0 and Î³Î³ â x) s sin Î¸â . The term TÎ»1 Î»2 is a hard scattering ampliÎ·Î·. tude for Î³Î»1 Î³Î»2 â qqqq with photon helicities Î»1 and Î»2 . The main results in this table are as follows. Belle From the sum rule, the overall normalization is fixed as measures the two-photon widths for the scalar resonances  1 f0 (980) and a0 (980) for the first time with significant (22.2.5) dxÏM (x, 0) = fM /2 3, statistics; the f0 (980) is observed as a clear peak both in the Ï + Ï â and Ï 0 Ï 0 modes; the a0 (980) is measured clearly in the Î·Ï 0 mode. The measured two-photon widths where fM is the decay constant for meson M . are small compared to those of the f2 (1270) and a2 (1320). For mesons with helicity zero the leading-term calcuThis supports the di-quark anti-di-quark hypothesis for lation gives the following dependence on s and scattering these mesons. In addition, Belle finds several resonance angle Î¸â : states in the range 1.3â2.4 GeV with substantial cou pling to two photons. Belle perform a generic partial wave [(e1 â e2 )2 ]2 2 |FM (s)| analysis including possible interferences with non-resonant d| cos Î¸â | (1 â cos2 Î¸â )2 terms. Systematic errors for some resonance parameters are large, resulting from a preference for destructive interference between the resonance and other components."
244,61,0.988,"Advancing Human Assessment : The Methodological, Psychological and Policy Contributions of Ets","where Î¦â1(p) represents the inverse of the standard normal cumulative distribution corresponding to the pth percentile. ETS scientists Gulliksen (1950, p. 368), Fan (1952, p. 1), Holland and Thayer (1985, p. 1), and Wainer (1989, p. 7) have described deltas as having features that differ from those of average item scores: â¢ The delta provides an increasing expression of an itemâs difficulty (i.e., is negatively associated with the average item score). â¢ The increments of the delta index are less compressed for very easy or very difficult items. â¢ The sets of deltas obtained for a testâs items from two different examinee groups are more likely to be linearly related than the corresponding sets of average item scores. Variations of the item difficulty indices in Eqs. 2.1 and 2.2 have been adapted and used in item analyses at ETS to address examinee group influences on item difficulty indices. These variations have been described both as actual item difficulty parameters (Gulliksen 1950, pp. 368â371) and as adjustments to existing item difficulty estimates (Tucker 1987, p. iii). One adjustment is the use of a linear function to transform the mean and standard deviation of a set of DÌi values from one examinee group to this setâs mean and standard deviation from the examinee group of interest (Gulliksen 1950; Thurstone 1925, 1947; Tucker 1987):"
12,136,0.988,Self-Assembled Molecules â New Kind of Protein Ligands : Supramolecular Ligands,"Table 5.2 presents a comparison of RD parameters describing full-chain dimers and V/C domains present in the complex. Analysis of results indicates that the full-chain dimer does not contain a shared hydrophobic core in the sense of the fuzzy oil drop model (with all corresponding RD values in excess of 0.5). When analyzed as components of the complex, V domains exhibit high RD values, which suggests that they lack prominent monocentrichydrophobic cores. Two exceptions to this rule are 1LIL and 2Q1E. In the latter case, the discrepancy is due to altered composition of the V domain tetramer crystals, where each unit cell comprises two dimers with two domains per dimer. For this reason, 2Q1E will be frequently seen as an outlier in further analysis. C domains analyzed in the context of the dimer exhibit good agreement with the theoretical hydrophobic core structure. Interesting properties are revealed for the light/heavy chain complex in the Fab IgG fragment, where each domain is discordant in the context of the complex-wide hydrophobic core."
142,1503,0.988,Subseafloor Biosphere Linked To Hydrothermal Systems : Taiga Concept,"In Fig. 41.7, major element versus MgO variation diagrams are shown for the drill core and surface samples, together with volcanic rocks sampled from the other regions of SMT (Masuda and Fryer, Chap. 21). Most of the elements show positive or negative correlation with MgO contents, which can be related to general fractionation trends of volcanic rocks in the SMT (Fig. 41.7). More precisely, the on-axis samples (Snail and Yamanaka) and the Archaean samples seem to lie on a straight line. On the other hand, the Pika samples clearly show departure from the line (Fig. 41.8). The trace element data also show essentially the same tendency (Fig. 41.9), although the number of samples plotted in the minor element versus MgO variation diagrams is significantly smaller than those in the major element diagrams (Figs. 41.7 and 41.8). This result suggests the difference in magmatism between the Pika site and the other three sites. This leads us to consider that the basement rocks of the Archaean hydrothermal site are a differentiated product of on-axis magma, whereas those of the Pika hydrothermal site are not directly related to the on-axis magma (might be related to off-axis magmatism). This interpretation of rock chemistry is generally consistent with geographical features of the two offaxis hydrothermal vent sites; the Archaean site hosted by small seamounts at the foot of the spreading axis whereas the Pika site situated on a relatively large off-axis seamount (Fig. 41.2)."
372,1182,0.988,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"The first and second lines of Eq. (10.11) represent the measured and unmeasured areas of the .u; v/ plane, respectively. In the measured area, W.u; v/wu .u; v/ D 1. For the case of uniform weighting, wt D 1, so the integral on the first line is zero. This condition minimizes the squared difference between the true and observed intensity distributions on the third line. If I.l; m/ is an unresolved point source, then I0 .l; m/ is equal to the synthesized beam. The uniform weighting minimizes the squared difference, over 4"" steradians, between the synthesized beam and the response to a point source as it would be observed with unlimited .u; v/ coverage. In this sense, it is sometimes said that uniform weighting minimizes the sidelobes of the synthesized beam. However, as shown in Fig. 10.2, a Gaussian taper reduces the sidelobes outside of the main beam at the expense of widening the beam. Images derived from visibility data that are uniformly weighted within the measured area of the .u; v/ plane have been referred to as the principal solution or principal response (Bracewell and Roberts 1954). The related process of reducing the sidelobe response in optical imaging is called apodization, for which there is an extensive literature; see, for example, Jacquinot and Roizen-Dossier (1964) and Slepian (1965)."
26,39,0.987,Cognitive Supervision for Robot-Assisted Minimally Invasive Laser Surgery,"where Ï = 2 Â· r 2 /w2 (z). The quantity w(z) is called the spot size of the Gaussian Beam, defined as the radius at which the intensity of the TEM00 mode is 1/e2 of its peak value I0 . This set of solutions is commonly referred to as TEMlp . The order and the index of the Laguerre polynomial determine the shape of the intensity profile, as shown in Fig. 2.4. TEM00 beams present several interesting properties. The intensity profile of these beams maintain a Gaussian shape regardless of the selection of the cross section along the propagation axis z. Furthermore, these beams present a relatively low divergence; this is a measure of how large the variation of the spot size w(z) is along z. This concept is illustrated in Fig. 2.5, where the w(z) is plotted. The spot size w(z) of the beam follows an hyperbolic law, and presents a global minimum w0 where"
295,415,0.987,interface oral Health Science 2016 : innovative Research On BiosisâAbiosis intelligent interface,"Figure 19.15 shows the tensile properties of Ti-(8â17)MnMIM along with those of Ti-64 ELI [5]. Both ÏB and Ï0.2 values for Ti-(8â12)MnMIM are higher than the corresponding values for Ti-64 ELI [5]. However, the elongation of the Ti-MnMIM alloys is lower than that of annealed Ti-64 ELI. It is noted that the elongation of Ti-MnMIM, which contains higher O content, pores, and carbides, is lower than that of Ti-MnLM, which do not contain pores and carbides and have lower O content. Thus, the lower elongation of Ti-MnMIM can be attributed to the combined effects of a higher O content primarily, along with the presence of pores and carbides, which are inherent to the MIM process [23, 28, 29]. Figure 19.16 shows the compressive properties of Ti-(8â17)MnMIM. The compressive 0.2 % proof stress (Ïc0.2) and compressive strain (Îµc) values for Ti-(8â17) MnMIM are higher than those for Ti-64 [4], also shown in Fig. 19.16. The two main parameters that affect the compressive properties of an alloy are the Mn content (which causes solid solution strengthening) and the amount of the Ï phase (which causes precipitation strengthening). The higher volume fraction of athermal Ï phase in Ti-8MnMIM and Ti-9MnMIM causes their Ïc0.2 values to be higher than those of Ti-(12â17)MnMIM. As the volume fraction of the athermal Ï phase decreases in Ti-(12â15)MnMIM, the solid solution strengthening effect of the Mn balances the effect of the decreasing Ï phase. The increase in compressive strain with increasing Mn content is also due to the decrease in the volume fraction of the athermal Ï phase with increasing Mn content."
372,545,0.987,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"the upper and lower sidebands. Here the variation of &g causes ru and r` to rotate in opposite directions. To verify this statement, note that the real parts of the correlator output are given in Eqs. (6.11) and (6.15), and the corresponding imaginary parts are obtained by replacing $G by $G $ ""=2. Then with .#m $ #n / D 0 (no fringe rotation), consider the effect of a small change in &g . The contra-rotating vectors representing the two sidebands at the correlator output coincide at an angle determined by instrumental phase, which we represent by the line AB in Fig. 6.5b. Thus, the vector sum oscillates along this line, and the fringe-frequency sinusoids at the real and imaginary outputs of the correlator are in phase. Now suppose we adjust the phase term .2""!0 (& C $G / in Eq. (6.18) to maximize the fringe amplitude at the real output. This action has the effect of rotating the line AB to coincide with the real axis. The imaginary output of the complex correlator then contains no signal, only noise. From Eq. (6.18), it can be seen that the visibility phase $v is represented by the phase of the vector that oscillates in amplitude along the real axis. The phase can be recovered by letting the fringes run and fitting a sinusoid to the waveform at the real output. If the fringes are stopped, it is possible to determine the amplitude and phase of the fringes by ""=2 switching of the LO phase at one antenna. In Eq. (6.18), this phase switch action can be represented by #m ! .#m $ ""=2/, which results in a change of the second cosine function to a sine, thus enabling the argument in square brackets to be determined. However, in such a case, the data representing the cosine and sine components of the output are not measured simultaneously, so the effective data-averaging time is half that for the SSB, complex-correlator case. In Fig. 6.5b, a ""=2 switch of the LO phase results in a rotation of ru and r` by ""=2 in opposite directions, so the vector sum of the two sideband outputs remains on the line AB. Relative sensitivities of different systems are discussed in Sect. 6.2.5."
372,861,0.987,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"To illustrate the effect of oversampling, examples of the quantization efficiency (Q , derived using Eqs. (8.95), (8.100), (8.101), and (8.107), are shown in Table 8.3. These are for 2-, 3-, 4-, 8-, and 16-level sampling and values of Ë equal to 1, 2, 4, 8, 16, and 32. In each case, the value of * used is the one that maximizes (Q for Nyquist sampling, as given in Thompson et al. (2007).6 Note that as Ë is increased, the improvement gained by each further increase declines, because the correlation between adjacent samples increases, and thus, the new information provided by finer sampling becomes progressively smaller."
311,515,0.987,The Physics of the B Factories,"r(Ît; Î¼, Ï) =fC Â· Gauss(Ît; Î¼C , ÏC )+ (1 â fC â fO ) Â· Gauss(Ît; Î¼T , ÏT )+ fO Â· Gauss(Ît; 0, ÏO ), (11.3.2) where Î¼C,T and ÏC,T,O represent the means and widths of the corresponding Gaussian distributions, respectively, and fC and fO represent the fraction of events in the core and outlier component, respectively. While very few events are expected that are not described by the convolution of the physics model with a core and tail Gaussian resolution term, it is important to include a wide outlier term in the resolution model, as otherwise a single event that is âfarâ from both core and tail models has the potential to contribute disproportionally to the likelihood and can strongly and unduly inï¬uence the ï¬t result, even when outliers only contribute at the permille level to the event sample. A common pragmatic choice for the outlier term is a very broad Gaussian distribution, as shown in the example of Eq. (11.3.2), but other shapes have also been used. The resolution model of the previous example describes the average performance of the decay-time reconstruction. Since the decay-time diï¬erence is calculated from the distance between two decay vertices, the resolution in the time diï¬erence will depend on the number of tracks used in the vertex ï¬ts as well as their conï¬guration, and the vertex ï¬t procedure returns an estimate of the uncertainty on the decay-time diï¬erence for each event. A more precise inference on the physics parameter Ï of the model f can be made by taking into account this perevent uncertainty on the decay time diï¬erence â weighting events with a precise measurement of ÏÎt more strongly than those with a poorer measurement by modifying the resolution model as follows"
311,2811,0.987,The Physics of the B Factories,"The meson-photon transition form factors for Î· and Î· â² have been measured by BABAR in the e+ eâ â e+ eâ Î· (â²) (del Amo Sanchez, 2011f) and e+ eâ â Î· (â²) Î³ (Aubert, 2006w) reactions. The decay modes Î· â² â Ï + Ï â Î·, Î· â Î³Î³ and Î· â Ï + Ï + Ï 0 are used to reconstruct Î· â² and Î· mesons, respectively. For single-tag + â + â + â 0 Among systematic uncertainties from diï¬erent sources e e â e e Î· events, Î· â Ï Ï Ï is the only decay available analysis. events with neutral Î· dethat are assigned to the cross section, the biggest con0 cays, the BABAR trigger and tributions come from the extraction of the Ï yield with the fit and the uncertainty of the trigger eï¬ciency; they background filters. In contrast to the e+ eâ â e+ eâ Ï 0 process, the QED depend largely on the Q2 regions. The total systematic background for the processes e+ eâ â e+ eâ Î· (â²) is almost uncertainty for the combined cross section is between 8% and 14% (and between 4% and 7% for the form factor), fully rejected by the requirement that charged-pion candidates be identified as pions. The hadron background depending on the Q2 region. from e+ eâ annihilation is suppressed by the requirement The Belle result for the transition form factor is shown of electron identification. As a result, after applying the in Fig. 22.7.3 together the results of the previous measuretransverse-momentum and recoil-mass conditions e+ eâ â ments. It is compared with the asymptotic QCD predice+ eâ Î· (â²) events are selected with low non-peaking backtion shown by the dashed line. Belle has applied a fit to a ground. The numbers of events containing true Î· and Î· â² parameterization with an asymptotic limit, Q2 |F (Q2 )| = are determined from the fit to the Ï + Ï + Ï 0 and Ï + Ï â Î· BQ2 /(Q2 + C). The obtained result for the asymptotic mass distributions with a sum of an Î· (â²) resolution funcvalue, B = 0.209 Â± 0.016 GeV, is slightly larger than the tion and a linear non-peaking background distribution. QCD prediction but still consistent with it. The fit curve The fit is performed in 11 Q2 intervals from the Q2 range is shown in the figure. 4â40 GeV2 . Above 40 GeV2 all observed Î· (â²) candidates The values of Q |F (Q )| measured by Belle agree with are expected to originate from background. The fitted the previous measurements (Aubert (2009y), Behrend et al. number of Î· and Î· â² events in the Q2 range 4â40 GeV2 < 9 GeV2 . The is about 3000 and 5000, respectively. (1991); Gronberg et al. (1998)), for Q2 â¼ For Î· â² events the only observed source of peaking backapparent systematic shift between Belle and BABAR cor2 responds to a 2.3Ï diï¬erence in the Q region between ground is e+ eâ annihilation. The contribution of e+ eâ an9 GeV2 and 20 GeV2 , taking into account both statisti- nihilation is estimated using events with the wrong sign of cal and systematic uncertainties in the two measurements. the eÂ± Î· â² momentum z-component, and subtracted. This The Belle result does not show a rapid growth of the form background is important only in the two highest-Q2 interfactor beyond the asymptotic prediction of QCD, in con- vals (Q2 > 25 GeV2 ), where it reaches about 10%. For Î· events, three sources of peaking background are trast to the BABAR result, and is closer to the theoretical predictions. studied and subtracted. These are the e+ eâ annihilation"
311,2424,0.987,The Physics of the B Factories,"to each. Because the Îc (2980) is close to threshold on the scale of its natural width, especially with an intermediate Î£c , the available phase space changes significantly across the resonance. Diï¬erent handling of this threshold behavior is the reason for the mild tension in the fitted Îc (2980) masses between (Chistov, 2006b) and (Aubert, 2008f). The masses measured in the Îc (2645)Ï + final state (Lesiak, 2008), which is far from threshold, are consistent with the BABAR treatment, although the widths are smaller than either experiment saw in Î+ c KÏ. Requiring an intermediate Î£c reduces the background levels, and by doing this BABAR was able to identify two further candidate states, the Îc (3055) and Îc (3123). The latter had a limited statistical significance (3Ï), and needs confirmation."
175,1238,0.987,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","positions of all reach intervals that will be at the corresponding boundaries of segment i at the end of time period t. This requires computing the velocities through each of the intermediate segments or volume elements. Second, the changes in the amounts of the modeled quality constituents, e.g., temperature, organics, nutrients and toxics, are calculated assuming plug flow during the time interval, Ît, using the appropriate differential equations and numerical methods for solving them. Finally, all the multiple incoming blocks of water with their end-of-period constituent concentrations are completely mixed in the segment i to obtain initial concentrations in that segment for the next time step, t + 1. This is"
311,1734,0.987,The Physics of the B Factories,"section, one needs to know how often the second cc-pair fragments into Dâ+ or D0 -mesons. Using the Lund fragmentation model (SjoÌstrand, 1994) Belle calculated the ratio of the J/Ï cc and inclusive J/Ï X production cross +0.15 Â±0.12. This result clearly sections to be equal to 0.59 â0.13 demonstrates that, contrary to NRQCD predictions, the dominant diagram for J/Ï production is e+ eâ â J/Ï cc. In 2009, using an order of magnitude larger data sample (673 fbâ1 ) Belle measured the cross sections for the processes e+ eâ â J/Ï cc in a model-independent way (Pakhlov, 2009). In the study of associated production of a J/Ï with charmed hadrons, all the ground state charmed mesons (D0 , D+ , Ds+ ) and the Îc -baryon were used. As two charmed hadrons are produced in cc fragmentation, the e+ eâ â J/Ï cc cross section is given by the sum of double-charmonium e+ eâ â J/Ï (cc)res cross"
285,325,0.987,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","In most cases a2/a1 is significant, in both guinea pigs and humans. This suggests either two subpopulations of fibres and/or neural elements (peripheral vs. central site of excitation) with different latencies and proportions, or repeated firings of the same neurons. The latter option is supported by the following arguments (see also in Ramekers et al. 2015). First, the interval between the two components, Âµ2â Âµ1, is about 0.5 ms, which is around the absolute refractory period estimated from masker-probe responses. Second, the N2âN1 interval of the eCAP waveform has been found to correlate with recovery measures (Ramekers et al. 2015). Alternatively, a reduced relative contribution of the second component with increasing stimulation intensity and more basal position within the cochlea as observed in humans could be explained by the former option. The late component can be ascribed to excitation peripheral to the cell body (Stypulkowski and van den Honert 1984) which is thought to occur to a larger extent near threshold (Briaire and Frijns 2005)."
311,814,0.987,The Physics of the B Factories,"approach eï¬ectively corresponds to a perturbative model for the leading shape function, with non-perturbative corrections only included via its moments. Therefore, it tends to be more predictive than the other approaches, resulting in smaller theoretical uncertainties within the framework. However, the intrinsic uncertainties due the assumptions inherent in the framework are not estimated. Another approach based on Sudakov resummation has been proposed in (Aglietti, Di Lodovico, Ferrera, and Ricciardi, 2009). It employs the so-called analytic coupling in the infrared. The full O(Î±S2 ) corrections to the b â uâÎ½ spectrum are only known in the limit p+ X âª pX (Greub, Neubert, and Pecjak, 2010), and are currently not included in the determination of |Vub |. In case of BLNP, their eï¬ect turns out to be much larger than expected from the perturbative uncertainties at O(Î±S ), resulting in an increase of |Vub | by 8%. On the other hand, the O(Î±S2 Î²0 ) terms often dominate the O(Î±S2 ) corrections, and their inclusion in the GGOU and DGE approaches does not lead to similarly large corrections. A resolution of this apparent discrepancy will probably have to await a calculation of the complete O(Î±S2 ) corrections. All the above approaches choose speciï¬c model parameterizations of the shape function(s), and it is unclear to what extent the model variations used to estimate the shape function uncertainties reï¬ect the actual limited knowledge of their form, particularly at subleading order in 1/mb . Also, the theoretical uncertainties do not include explicit estimates of the possible size of O(Î±S ÎQCD /mb ) corrections. Given all the above, it is possible that the theoretical uncertainties currently quoted for |Vub | might be underestimated. On the other hand, the diï¬erent theoretical frameworks yield values of |Vub | that are compatible within uncertainties with each other and across a variety of diï¬erent experimental cuts. Imposing an additional lower cut on q 2 restricts the decay kinematics to the part of the small mX region where X â¼ pX . Formally, this allows the application of the OPE in terms of local operators (Bauer, Ligeti, and Luke, 2001). In practice, the resulting OPE still has rather large 1/m2b and higher order corrections, and some residual shapefunction eï¬ects must be included. Nevertheless, this approach provides an important cross check on the extracted value of |Vub |."
311,2114,0.987,The Physics of the B Factories,"obtained from the unbinned likelihood fits are listed in Table 19.1.27 with statistical and systematic uncertainties. Only systematic uncertainties associated with the signal and background p.d.f.s are included in the systematic uncertainty for the yields. The curves representing the fits are overlaid in the figures. The most significant signal is + â seen in the distribution for Î+ c â pÎ¼ Î¼ ; the signal yield has a statistical-only significance of 2.6Ï as determined from the change in log-likelihood with respect to zero assumed signal events. With 35 diï¬erent measurements, a 2.6Ï deviation is expected with about 25% probability. 19.1.10 Summary of charmed meson decays Charm decays open the road to investigate the flavor physics of up-type quarks, which is complementary to the weak interactions of the (bottom and strange) down type quarks. Since the B decays are dominated by the b â c transitions, and the e+ eâ â cc cross section is comparatively high, the B factories also generated plenty of charm which allowed detailed measurements with highly competitive precision. FCNC processes of up-type quarks are predicted to be heavily GIM suppressed, which motivated measurements and searches of rare and even forbidden decays of charm at the B factories. Although for many FCNC processes it is diï¬cult to make a precise theoretical prediction, the B factories added a lot of new information on these decays, constraining significantly the limits on physics beyond the Aside from the weak interactions also many studies of QCD related issues have been performed. The charm quark is neither heavy enough to be cleanly treated within"
311,1711,0.987,The Physics of the B Factories,"and total widths (Aubert, 2004d). In this analysis the J/Ï is reconstructed in the dimuon channel only, because the e+ eâ final state has much larger backgrounds from radiative Bhabha events. The directly measured quantity is Îee Ã B(J/Ï â Î¼+ Î¼â ),103 which is found to be equal to (0.3301 Â± 0.0077 Â± 0.0073) keV. Then using the known leptonic branching fractions it is possible to derive both the electronic width Îee = (5.61 Â± 0.20) keV, and the total width Îtot = Îee /B(J/Ï â e+ eâ ) = (94.7 Â± 4.4) keV. The statistical and systematic uncertainties are combined in quadrature. The BABAR Îee Ã B(J/Ï â Î¼+ Î¼â ) result is one of three measurements contributing to the current world average, which is (0.334Â±0.005) keV (Beringer et al., 2012)."
359,202,0.987,"Micro-, Meso- and Macro-Dynamics of the Brain","where r is the radial distance from the line, q the longitudinal distance from the end of the line, and l Â¼ Îs + q is the distance from the origin of the line. Holt and Koch (1999) analyzed the accuracy of the LSA and found it to be highly accurate except at very close distances (i.e., about 1 Î¼m) to the cable (see also Rosenfalck 1969; Trayanova and Henriquez 1991; Fig. 3b). The LSA has been the primary method of calculating extracellular voltages arising from transmembrane currents (Gold et al. 2006, 2009; Holt 1998; Holt and Koch 1999; Pettersen and Einevoll 2008; Fig. 3c). Notably, the aforementioned relationships assume that the extracellular medium in the brain is described via electrostatics and not by much more elaborate elements of electrodynamics. Furthermore, a widespread assumption is that the extracellular medium is isotropic and homogeneous. What evidence exists for such claims to be made? It turns out that this question has remained unresolved, with a number of studies reporting an anisotropic and homogeneous Ï (Nicholson and Freeman 1975; Logothetis et al. 2007) (Fig. 3d, e) to strongly anisotropic and inhomogeneous (Goto et al. 2010; Hoeltzell and Dykes 1979; Ranck 1973) and, finally, even of capacitive nature (BeÌdard and Destexhe 2009; BeÌdard et al. 2004; Gabriel et al. 1996). Part of the difficulty in determining the properties of Ï, especially at the local, microscopic scale, has to do with the inhomogeneity of the brain as a structure. In that sense, the questions to be answered are where, in what species, in what frequency band and at what spatial scale should Ï be measured. The danger is that measuring Ï over larger volumes leads to possibly quite different results (attributed to averaging) than recording Ï over tens of Î¼m. Moreover, measuring Ï within distances of tens micrometers, i.e., the relevant spatial scale for signals related to spiking, poses significant technical challenges given the large number of sites (both for current injection and voltage recording) that need to be positioned within Î¼m-distances and the resulting tissue deformation/damage. Recently, detailed whole-cell patch recordings of excitatory and inhibitory neurons in rat somatosensory cortex slices were performed in parallel to positioning"
372,1590,0.987,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"has an absorption coefficient of 29,000 dB km!1 , or 0.069 cm!1 . The values of $n and Ä±n are 1:44 "" 10!6 and 0:7 "" 10!6 , respectively. In the atmospheric windows above 400 GHz, where radio astronomical observations are possible only from very dry sites, the refractive index can be noticeably different from the value at lower frequencies. The normalized refractivity is shown in Fig. 13.9. Equation (13.68) is an important result of very general validity. We derived it from a specific model [Eq. (13.58)] that led to an approximately Lorentzian profile for the absorption spectrum. In practice, line profiles are found to differ slightly from the Lorentzian form, and more sophisticated models are needed to fit them exactly. However, Eqs. (13.68) and (13.69) could be derived from the Kramersâ Kronig relation. The low-frequency value of the index of refraction, as given by Eq. (13.9), results from the contributions of all transitions at higher frequencies. Summing the contributions [see Eq. (13.69)] of many lines, each characterized by parameters $n1 , 'i , Ëmi ; and #0i ; we obtain the low-frequency value of the index of refraction: nS D 1 C"
32,671,0.987,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","The values of hki and hCi in my network are independently tunable with the reduction process, which has other side effects. The originally connected networks fall into pieces. Separate clusters appear, which are smaller networks without connections to other parts of the system. Increasing the reduction strength  the number of clusters Nc is increasing according to power law, where the exponent depends on the number of links only, independently from their role in the growing process (Fig. 29.5a). Large number of clusters can occur depending on  and the system size N. Based on the simulation results the value of Nc can be characterized by the following form Nc /"
169,145,0.987,Riverine Ecosystem Management : Science For Governing Towards a Sustainable Future (Volume 8.0),"Qts1 < Qts2) and decreasing ï¬ow (DC: Qts1 > Qts2). Continuous time steps with equal trends are deï¬ned as a single ï¬uctuation event (Fig. 5.3). For each event a set of parameters related to ï¬uctuation intensity (Table 5.1) is calculated by the algorithm: the highest ï¬ow change within a time step represents parameter (1)âmaximum ï¬ow ï¬uctuation rate (MAFR). Parameter (2)âmean ï¬ow ï¬uctuation rate (MEFR) is calculated by the event amplitude divided by the number of time steps. Parameter (3)âthe amplitude (AMP) of an event is deï¬ned as the difference between the ï¬ow maximum (Qmax) and the ï¬ow minimum (Qmin). Parameter (4)âï¬ow ratio (FR) is deï¬ned as (Qmax)/(Qmin). The duration (DUR) of an event (5) is simply the number of continuous time steps with equal ï¬ow trend. In addition, timing and daylight condition are determined for every single event. This method to detect and characterize ï¬ow ï¬uctuations using hydrograph curves offers a wide range of applications: intensity, timing, and frequency of ï¬ow ï¬uctuations can be detected automatically and in a standardized way. As a consequence the hydrological situation at speciï¬c river sections can be compared to each other,"
372,1794,0.987,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"Refractive scattering is thought to be responsible for the slow amplitude variations observed in some pulsars and quasars at meter and decimeter wavelengths. This realization solved the long-standing problem of understanding the behavior of âlong-wavelength variables,â which could not be explained by intrinsic variability models based on synchrotron emission. The identification of two scales in the interstellar medium provides strong support for the power-law model. The two scales provide a way of estimating the power-law index, because the relative importance of refractive scattering increases as the power spectrum steepens. It is interesting to note that these two scales arise from a power-law phenomenon, which has no intrinsic scale. The scales are related to the propagation and depend on the wavelength and distance of the screen. In addition to amplitude scintillation, refractive scattering causes the apparent position of the source to wander with time. The amplitude and timescale are about equal to &s and tref ; respectively. The character of this wander depends on the powerlaw index of the fluctuations. Limits on the power-law index have been established from the limits on the amplitude of image wander in the relative positions among clusters of masers (Gwinn et al. 1988). Rare sudden changes in the intensity of several extragalactic sources, called Fiedler events, or extreme scattering events (Fiedler et al. 1987), are probably"
255,354,0.987,Railway Ecology,"Spatial Distribution and Activity Within Saltpans The use of a speciï¬c type of pond (feeders, evaporators and crystallisers) in winter did not show signiï¬cant differences with type of saltpan (impacted or control). Although the full GLMM was signiï¬cantly different from a null model (Table 12.1), none of the variables presented signiï¬cant interactions between the type of pond and the combined interaction between time of year and the type of saltpan. Likewise, although the full GLMM for activity differed signiï¬cantly from the corresponding null model without the interaction term, none of the interaction terms was statistically signiï¬cant (Table 12.1). This suggests that shorebirds did not change signiï¬cantly their spatial distribution and activity within impacted saltpans during the construction and operation phases."
297,1868,0.987,The R Book,"We need to extract the 54 variables that refer to the speciesâ abundances and leave behind the variables containing the experimental treatments (plot and lime) and the covariates (species richness, hay biomass and soil pH). This creates a smaller dataframe containing all 89 plots (i.e. all the rows) but only columns 1 to 54: pgd <- pgdata[,1:54] There are two functions for carrying out PCA in R. The generally preferred method for numerical accuracy is prcomp (where the calculation is done by a singular value decomposition of the centred and scaled data matrix, not by using eigen on the covariance matrix, as in the alternative function princomp). The aim is to ï¬nd linear combinations of a set of variables that maximize the variation contained within them, thereby displaying most of the original variation in fewer dimensions. These principal components have a value for every one of the 89 rows of the dataframe. By contrast, in factor analysis (see p. 813), each factor contains a contribution (which may in some cases be zero) from each variable, so the length of each factor is the total number of variables (54 in the current example). This has practical implications, because you can plot the principal components against other explanatory variables from the dataframe, but you cannot do this for factors because the factors are of length 54 while the covariates are of length 89. You need to think about this until the penny drops."
391,966,0.987,Ocean-Atmosphere Interactions of Gases and Particles,"The global distribution of sea salt aerosols (SSA) is generally estimated using chemical transport models (CTMs) or General Circulation Models (GCMs). Emissions are calculated by integrating a size-dependent source function of SSA at each time step over several size bins. Simultaneously, transport and depositional loss of SSA are also calculated. Models then solve the continuity equation for mass conservation in each size bin. The resulting global distribution of SSA is highly dependent not only on the assumed sea-salt source function (see reviews by Lewis and Schwartz 2004; OâDowd and de Leeuw 2007; de Leeuw et al. 2011b) but also on the upper size range of particles, and meteorological fields, as demonstrated in the model intercomparison study of Textor et al. (2006). These authors found a large inter-model range of 3â18 Tg for the global SSA burdens calculated in 16 different CTMs. Calculated SSA concentrations can differ by factors of 2â3 when different source functions are used in the"
311,1730,0.987,The Physics of the B Factories,"by charmed hadrons (Abe, 2002j). The J/Ï is reconstructed in its â+ ââ (â = e, Î¼) decays, with its mass constrained to the nominal value to improve the momentum resolution. BB background is suppressed by requiring pâJ/Ï > 2 GeV/c, where pâJ/Ï is the J/Ï momentum in the CM system. Backgrounds from QED e+ eâ â â+ ââ (Î³) processes are suppressed by requiring the number of tracks in each event to be larger than 4. Unexpectedly, Belle found that a significant fraction of J/Ï âs are produced together with another charmonium state in two-body reactions of the type e+ eâ â J/Ï Î·c (Fig. 18.2.17 (d)). More often, charmed mesons are found in the events with reconstructed J/Ï (Fig. 18.2.17 (c)). To identify the first type of process (with the double charmonium final state) it is not necessary to reconstruct both mesons, which inevitably leads to substantial eï¬ciency loss. Relying on four-momentum conservation, the mass of the system produced together with J/Ï can be calculated using only the measured J/Ï momentum. The mass of the system recoiling against the J/Ï candidateâthe ârecoil massââ is defined as Mrecoil (J/Ï) = ( s â EJ/Ï )2 â pâJ/Ï (18.2.3)"
311,2827,0.987,The Physics of the B Factories,"Figure 23.2.2. The ratio of the number of hadronic events (R2 < 0.2) to the number of Bhabha events as a function of the e+ eâ CM energy, from Drutskoy (2007a). Only statistical errors are shown. The curve is the result of the ï¬t to a sum of a Breit-Wigner function and a constant."
275,391,0.987,Foundations of Trusted Autonomy,"effect of reducing the entire column to zero, as per the example in Table 13.13, with the resulting distribution of the classification estimates redistributed without user control, into the next most similar categories. Table 13.14 illustrates a best effort test (error average 0.806) achieved with the aim of âcreating errors in cell (2,5) and minimising correct classifications in cell (5,5)â. As can be seen in Table 13.14, the desired effect is achievable, however, the consequence of the desired weakness in correctly classifying (5,5) produces more significant errors at both (5,3) and (5,8), which may be the ânext weakestâ points."
311,335,0.987,The Physics of the B Factories,"reconstructed D mesons are required to be within Â±3Ï (Â±2.5Ï) at BABAR (Belle) of their nominal mass value. As explained in Section 7.2 the cosine of the angle between the B meson and the D(â) â candidate momenta, cos Î¸B,Dâ deï¬ned in Eq. (7.2.2), is a powerful discriminant. In case the Dâ and the neutrino are the only decay products of the B then cos Î¸B,Dâ must lie in the physical region between Â±1. If additional decay products from the cascade of a higher mass charm state down to the D0 go unreconstructed then this will force the value of cos Î¸B,Dâ to be smaller. In order to keep such candidates events with cos Î¸B,Dâ between â2.5 and +1.1 are usually accepted. The positive limit is allowed to be slightly outside of the physical region to account for detector and reconstruction eï¬ects. Of course, for the reconstruction of exclusive channels (B â â D0 ââ Î½ â , B â â Dâ0 ââ Î½ â , B 0 â D+ ââ Î½ â and B 0 â Dâ+ ââ Î½ â ), the selection is tightened to only consider the physical region. A typical B â â D0 ââ Î½ â X selection at BABAR yields an eï¬ciency of approximately 6 Ã 10â3 with a mode dependent purity which averages to â¼ 60%. For the neutral B reconstruction the eï¬ciency is typical half that of a similar charged B selection. The loss of a neutrino in the semileptonic tagging mode limits the constraints that can be imposed compared to the case when all of the B meson decay products are reconstructed. For example the signal B direction cannot be found as is possible for hadronic B reconstruction. However, this constraint is not of paramount importance in the analysis of signal decay modes to ï¬nal state with more than one neutrino like for example B + â Ï + Î½Ï or B 0 â Î½Î½). The knowledge of signal B momentum enables calculation of missing mass which is a very powerful variable to separate signal B decays with a single neutrino in the ï¬nal state from background decays, but becomes weak when multiple neutrinos are present in the signal B decay. 7.4.3 Inclusive Btag reconstruction As discussed in the previous two sections the reconstruction of the recoil B meson using the hadronic and semi-"
32,327,0.987,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","Where Ë is a shape parameter and  is a scale parameter. If Ë D 1, Eq. (14.6) corresponds to Eq. (14.4). If Ë D 2, Eq. (14.6) corresponds to Eq. (14.5). The green curves in Fig. 14.5 show distributions of Eq. (14.6). We selected the parameters Ë D 1:6;  D 0:9 for BS D 0:5 and Ë D 1:2;  D 0:9 for BS D 10. The shape of the distributions of the logarithmic differences of two values is concerned with the correlation between those two values. The left side of Fig. 14.6 shows a scatter plot of ln S.x; y/ versus ln S.x C BS; y/ or ln S.x; y C BS/. From this figure, we observe agglomeration effect that many people live near places where many other people also live. The correlation coefficient is able to interpret as an index of agglomeration effect. The right side figureâsp data are transformed from the left side figureâs data by dilating both axis data 2 and rotating clockwise 45Ä± . The horizontal axis of the right side figure is the logarithmic summation between the nearest neighbor populations. The vertical axis of the right side figure is the logarithmic difference between the nearest neighbor populations. The red bars are the standard deviation inside each segment, which is equally divided by the horizontal axis. The correlation of the left side figure represents the correlation between the population and the nearest neighbor population. If this correlation is strong, the population near the large population is large. It is considered that the strengthen of this correlation is one of the indices which represents degree of the agglomeration of population. The deviation of the distribution of the vertical axis of"
311,1155,0.987,The Physics of the B Factories,"17.6.6 Ï1 from charmless quasi-two-body B decays The time-dependent CP asymmetry parameter S measured in charmless decays to CP eigenstates via b â sqÌq penguin transitions is also equal to S = âÎ·f sin 2Ï1 in the SM. These decays are particularly sensitive to new physics because any unobserved heavy particle could contribute an additional penguin loop and alter the value of the measured weak phase. If the measured S in one or a group of charmless decays deviates significantly from that in tree-dominated processes, it could be a signature of new physics eï¬ects. The comparison between loop and tree-dominated decays, however, must to be made with careful estimates of the SM corrections from higher order topologies. The key issue in the theoretical understanding of these CP asymmetries is the tree-to-penguin ratio, both in short- and long-distance interactions. The typical deviations in theoretical calculations are below a few percent, and the corresponding uncertainty can be as small as one or two percent. The modes that benefit from the least theoretical uncertainties are Î· â² KS0 , ÏKS0 , and KS0 KS0 KS0 (Beneke, 2005; Cheng, Chua, and Soni, 2005a,b). Of the charmless decays of interest, two-body and quasitwo-body final states are the simplest states to study experimentally. The term âquasi-two-bodyâ refers to a final state that includes a resonance whose interference with any other amplitude is ignored (details in Section 17.4.5). The experiments at the B Factories have studied the CP odd states B 0 â Î· â² KS0 , ÏKS0 , Ï 0 KS0 and the CP -even states B 0 â Î· â² KL0 and Ï 0 KL0 . Measurements of timedependent asymmetries in three-body decays are discussed in Section 17.6.7. Due to the similarity between the experimental techniques used to reconstruct the B 0 â Ï 0 KS0 and B 0 â KS0 KS0 decays, the latter measurement is included in this section. The 2KS0 mode is dominated by a b â dsÌs penguin transition. Assuming top-quark dominance in the virtual loop, the time-dependent CP asymmetry parameters in this decay are expected to vanish, i.e. SKS0 KS0 = CKS0 KS0 = 0 (Fleischer, 1994). If a significant discrepancy is observed, this would be a clear signature of new physics (Giri and Mohanta, 2004). Measurements of time-dependent asymmetry parameters of B mesons decaying into Î· â² K 0 , ÏKS0 , Ï 0 K 0 , and KS0 KS0 are described in the following. 17.6.6.1 B 0 â Î· â² K 0 The branching fraction of the B 0 â Î· â² K 0 decay was first measured by CLEO (Behrens et al., 1998) and was surprisingly large compared to naÄ±Ìve expectations. This result is confirmed by both Belle (Abe, 2001d) and BABAR (Aubert, 2001d). Because of the large branching fraction, this mode provides the most precise time-dependent CP asymmetry parameter measurement of any b â sqÌq decay mode. The first measurements were made in 2002 by Belle (Chen, 2002) and in 2003 by BABAR (Aubert, 2003i). For these measurements, the Î· â² candidates were reconstructed via Î· â² â Î·Ï + Ï â and Î· â² â Ï0 Î³ decays, with Î· â Î³Î³ and Ï0 â"
311,2453,0.987,The Physics of the B Factories,"represents the coupling of R to the final state with net helicity Î»f , and Ïi,i are the diagonal density matrix elements inherited from the charmed baryon. If R â HP is a strong decay then |AJ1 | = |AJâ 1 | and so Î² vanishes. In addition to the technique outlined above for measuring the spin of a resonance, charmed baryon decays can be used more generally to measure properties such as the mass and width of intermediate resonances in multi-body decays. 19.4.3.2 Spin of the Î© â The method introduced in Section 19.4.3.1 was used by BABAR in an elegant way to measure the spin of the Î© â"
311,2829,0.987,The Physics of the B Factories,"23.2.3 Calculation of the number of Bs0 mesons in a data sample It is important to determine the number of Bs0 mesons in a data sample with high precision because the corresponding uncertainty is usually the dominant systematic uncertainty in Bs0 branching fraction measurements. Taking into account the hadronic event classification discussed above, the following parameters are required in order to obtain the number of Bs0 mesons in a data sample taken at a given CM energy: 1. The integrated luminosity of the data sample Lint . 2. The bb production cross section Ï (e+ eâ â bb), also denoted as Ïbb . (â)0 (â)0 3. The fraction of bb events containing a Bs B s pair, usually referred to as fs . 4. The fractions of events produced through specific pro(â)0 (â)0 duction channels over all Bs B s events: fB 0B 0 , s s fB â0B 0 , and fB â0B â0 ."
179,1252,0.987,"Habitats and Biota of the Gulf of Mexico: Before the Deepwater Horizon Oil Spill: Volume 1: Water Quality, Sediments, Sediment Contaminants, Oil and Gas Seeps, Coastal Habitats, Offshore Plankton and Benthos, and Shellfish","7.6.4.3 Macrofauna Quantitative investigations of the macrofauna were initiated in the mid-1960s (Rowe and Menzel 1971; Rowe 1971; Rowe et al. 1974; Pequegnat 1983). The published surveys used an anchor dredge or a van Veen grab to sample specific areas of the seafloor, followed by sediment sieving with a 0.42 mm mesh sieve. Since those early publications, the sieve size generally prescribed in studies supported by MMS in deep water has been reduced to 0.3 mm, meaning that total abundances of smaller organisms would have increased in the later studies (Recall that all the continental shelf studies used 0.5 mm sieves). These small changes, while affecting densities, probably have not affected biomass estimates (Rowe 1983). The most recent studies have used a GOMEX corer (Boland and Rowe 1991) or a spade corer (Escobar-Briones et al. 2008b, c), whereas some of the present ongoing sampling has gone to a multicorer (Barnett et al. 1984). The Gulf of Mexico macrofauna biomass follows a log-normal relationship with depth, whether measured as wet weight, dry weight, or organic carbon (Rowe and Menzel 1971). The slope of the log-normal line appears to be the same regardless of which measure is used, but the slope of the densities can be less than that of the weight measures, indicating that abundances do not decline as fast as biomass; that is, animals in some ocean basins are getting smaller with depth. Recall that this was true of the microbiota and the meiofauna as well. It appears that the rate of decline of biomass with depth is a general feature on most continental margins, but the height of the line (the origin at shallow intercept on the shelf) above the x-axis is a function of the rate of PP in the surface water (Rowe 1971; Wei et al. 2010a). Thus, the biomass regression in the Gulf is steep but somewhat below most other ocean basins, a clear indication that the Gulf of Mexico is an oligotrophic ecosystem, with several exceptional habitats. Most of the historical biomass measurements in the Gulf of Mexico (Figure 7.57) have been incorporated into a single database for the purpose of predicting macrofaunal biomass across large scales of depth and region (Wei et al. 2010b, 2012a). The densities and biomass are dominated by worms (Figure 7.58), either polychaetes or nematodes (Figure 7.59). It is presumed that animal densities decline with depth because food becomes limiting (Rowe 1971, 1983). A log-normal relationship has been described for most of the worldâs oceans, including the Gulf of Mexico. The height of the line is related to the levels of PP in the surface water (Rowe 1971), as well as input from the margins (Walsh et al. 1981; Deming and Carpenter 2008; Santschi and Rowe 2008). Submarine canyons appear to concentrate organic matter, thus enhancing their biomass and animal abundances, especially in the Gulf of Mexico (Roberts 1977; Soliman and Rowe 2008; Escobar-Briones et al. 2008a; Rowe and Kennicutt 2008). The biomass in the southern Gulf of Mexico is decidedly lower than that in the northern Gulf of Mexico, as illustrated in Figure 7.60 from Wei et al. (2012a), using data from EscobarBriones et al. (2008a). This reflects the source of the water (the Caribbean via the YucataÌn Strait) and the resulting low PP due to nitrate limitation. The high variance among the southern"
307,296,0.987,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"Fig. 8.11 This figure shows how the expected concentrations of the dyad (given by x) and the JSR (given by y) change as functions of the mutation severity indices. The curve denoted by Ecc starts at the circle that represents the expected values of x and y in the case of both the LCC and RyR being closed. The starting point represents the wild type and the curves represent the two mutations (or combinations of them) and similarly for the curves starting at the circles next to Eoc , Eco , and Eoo . All curves are computed using V D 0 mV"
391,764,0.987,Ocean-Atmosphere Interactions of Gases and Particles,"Fig. 4.22 Relative change (%) of different variables in response to dust addition during eight bioassay experiments in the tropical Atlatic Ocean. Relative change was calculated as 100  (DâC)/C, where D and C are the mean value of the variable in the dust and the control treatments, respectively. Boxes and bars enclose the 25thâ75th and 5thâ95th percentiles, respectively, the vertical dotted line is the mean, and the vertical continuous line is the mode (n Â¼ 8). Variables shown are total"
297,1507,0.987,The R Book,"The trend of increasing parasitism with density is very clear. In these plots, the width of the sector indicates how many of the data fell in this range of population densities; there were equal numbers of hosts in the ï¬rst two bins, but twice as many in the highest density category than in the category below, with a peak of just over 80% parasitized. Alternatively, if you want a smooth curve you can use the conditional density plot cdplot like this:"
200,115,0.987,"Earthquakes, Tsunamis and Nuclear Risks","Sa for strike-slip, reverse, and all types of faults are about 0.91, 0.73, and 0.80 times of Sasp by Somerville et al. [4]. Although the standard error is large, Sa for each reverse fault is smaller than Sasp by Somerville et al. [4]. SMGAs are source models for strong motions in the period range from 0.1 to 5 s, while the asperities are source models for strong motions in the period longer than about 2 s. Therefore, the result that total area of SMGAs is smaller than total area of asperities is interpreted by frequency-dependent source radiations [35]. Short-period spectral level A which means the flat level of acceleration source spectrum [36] is proportional to stress drop and square root of total area of SMGAs (or asperities). Considering the equations of (6.1), (6.2), (6.5), and (6.6), A for reverse faults is larger than A for strike-slip faults. Satoh [35] showed the same results from strong motion records for big crustal earthquakes in Japan using the spectral inversion method. McGarr [37] showed that peak ground velocities PGVs normalized by Mo1/3 and hypocentral distances depend on focal depths and are larger for reverse faults than normal faults. He pointed out that these results are expected from frictional laws. In addition he pointed out that data of strike-slip faults were insufficient in his analysis, but the normalized PGVs for strike-slip faults would lie between those for reverse and normal faults. Our results are qualitatively consistent with McGarrâs results, although site effects were not considered in McGarrâs results."
311,2380,0.987,The Physics of the B Factories,"are summarized in Table 19.3.10. BABAR further combined these results with their measurements of the product B of the B â D(â) Ds1 (2460)+ decays (Aubert, 2004ad), measured similarly to the described Belle analysis. This allowed a first measurement of the Ds1 (2460)+ decay rates: B(Ds1 (2460)+ â Dsâ+ Ï 0 ) = (56 Â± 13 Â± 9)%, B(Ds1 (2460)+ â Ds+ Î³) = (16 Â± 4 Â± 3)%. (19.3.19) Belle studied the production of the Ds1 (2536)+ meson in B â D(â) Ds1 (2536)+ decays, with D(â) being either D0 or D(â)â , using a data sample of 657 Ã 106 BB pairs (Aushev, 2011). The Ds1 (2536)+ is reconstructed in its dominant decay modes, Dâ+ KS0 and Dâ0 K + . Figure 19.3.23 shows the M (Ds1 (2536)) spectra for the B candidates satisfying the ÎE-mES selection, for each D(â) flavor and the Ds1 (2536)+ decay mode separately. All these distributions are fitted simultaneously, with the signal Ds1 (2536)+ described as a BW function convolved with a double Gaussian function describing the mass resolution. The measured Ds1 (2536)+ mass and width were respectively 2534.1 Â± 0.6 MeV/c2 and 0.75 Â± 0.23 MeV/c2 , consistent with their PDG values (Beringer et al., 2012). A fit with the Ds1 (2536)+ partial width ratio kept as a free parameter, yields: B(Ds1 (2536)+ â Dâ0 K + ) = 0.88 Â± 0.24 Â± 0.08, B(Ds1 (2536)+ â Dâ+ K 0 ) (19.3.20) in agreement with the BABAR study (Aubert, 2008bd). Table 19.3.11 summarizes ratios of branching fractions, calculated using the latest measurements of the B â D(â) Ds(J) branching fractions, as well as the Ds1 (2536)+ measurements by Belle (Aushev, 2011) and BABAR (Aubert, 2008bd). In these calculations, 100% branching fracâ tions are assumed for the Ds0 (2317)+ â Ds+ Ï 0 and Ds1 (2536)+ â Dâ K decay modes. Within the factorization model and in the heavy quark limit, these raâ tios should be of order unity for the Ds0 (2317)+ and Ds1 (2460)+ , whereas for the Ds1 (2536)+ they are predicted to be very small (Datta and OâDonnell, 2003b; Le Yaouanc, Oliver, PeÌne, and Raynal, 1996). The decay pattern for the Ds1 (2536)+ follows these expectations, whereas for the Ds0 (2317)+ and Ds1 (2460)+ the ratios are rather diï¬erent from unity and therefore such an approach"
372,1409,0.987,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"This chapter is concerned with the techniques by which angular positions of radio sources can be measured with the greatest possible accuracy, and with the design of interferometers for optimum determination of source-position, baseline, and geodetic1 parameters. The total fringe phase of an interferometer, where the effect of delay tracking is removed, can be expressed in terms of the scalar product of the baseline and sourceposition vectors D and s, respectively, as"
219,749,0.987,Economics of Land Degradation and Improvement â A Global Assessment for Sustainable Development,"where s denotes the observed state and t is the year of observation. Our dependent variable Y is âwaste landâ which is the area affected by soil erosion. We regressed this with the host of influencing agricultural variables captured by the vector x, such as number of cultivators per unit of area, cropping intensity, fertiliser consumption or fertiliser subsidy, percentage of irrigated area, and yield. We control for a state-dependent characteristics, GDP, population density, poverty ratio and literacy rate. All the variables except the dummies have been used in logarithmic form in the estimation. Additionally to the state-level analysis, we also perform an analysis of drivers of land degradation at household level. More speciï¬cally, the unit of observation is a The selected states are: Andhra Pradesh, Bihar, Gujarat, Haryana, Karnataka, Madhya Pradesh (including Chattisgarh), Maharashtra, Orissa, Punjab, Rajasthan, Tamilnadu, Uttar Pradesh and West Bengal."
372,611,0.987,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"Note that the integrals must be taken over the whole radio-frequency passband, denoted by the subscript RF, which includes both sidebands in the case of a DSB system. We assume that the passband function HRF .!/ is identical for all antennas. The values of l and m in the intensity function in Eq. (6.74) are multiplied by the factor !=!0 , which varies as we integrate over the passband, being equal to unity at the band center. Thus, one can envisage the integrals in the square brackets in Eq. (6.74) as resulting in a process of averaging a large number of images, each with a different scale factor. The scale factors are equal to !=!0 , and the range of values of ! is determined by the observing passband. The images are aligned at the origin, and thus the effect of the integration over frequency is to produce a radial smearing of the intensity distribution before it is convolved with the beam. The response to a point source at position .l; m/ is radially elongated by a factor equal to l2 C m2 (!=!0 . For distances from the origin at which the elongation is large compared with the synthesized beamwidth, features on the sky become attenuated by the smearing, so there is an effective limitation of the useful field of view. The measured intensity is the smeared distribution convolved with the synthesized beam. Details of the behavior of the derived intensity distribution can be deduced from Eq. (6.74). For example, suppose that the beam contains a circularly symmetrical sidelobe at a large angular distance from the beam axis and that in an image, the response to a distant source causes the sidelobe to fall near the origin. Is the sidelobe broadened near the origin? Since the distant source is elongated, the sidelobe will be smeared in a direction parallel to that of a line joining the source and the origin, as"
175,772,0.987,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","information and in the transformation of that information into decisions, recommendations, and conclusions. (b) In systems studies, sources of error and uncertainty are sometimes grouped into three categories 1. Uncertainty due to the natural variability of rainfall, temperature, and stream flows which affect a systemâs operation. 2. Uncertainty due to errors made in the estimation of the modelsâ parameters with a limited amount of data. 3. Uncertainty or errors introduced into the analysis because conceptual and/or mathematical models do not reflect the true nature of the relationships being described. Indicate, if applicable, into which category each of the sources of error or uncertainty you have identiï¬ed falls. 6:2 The following matrix displays the joint probabilities of different weather conditions and of different recreation beneï¬t levels obtained from use of a reservoir in a state park: Weather"
8,481,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","19.5 Summary and Conclusions Our model uses only three basic experimental facts: 1. The strong interactions are strong enough to produce many resonances and even fireballs. We assume that the latter are only an âextrapolationâ of the resonances to very high energies. 2. The strong interactions have a range of the order of the Compton wavelength of the pion. 3. In high-energy collisions, the duration of contact is in general so short that a thermodynamical equilibrium (in the sense of Fermiâs statistical theory) cannot be reached. From (1) it follows that particles are to be considered as (quasi) distinguishable, while (2) determines the volume in which the system is enclosed, and (3) allows one to treat the longitudinal and transverse motion as (nearly) independent. All the rest is straightforward and simple statistical thermodynamics with the following results: â¢ A universal highest temperature T0  m (corresponding to 1012 K) governs all high-energy processes involving strong interactions (and only these; no highest temperature exists for gravitational, weak, and electromagnetic interactions since they do not produce the many resonances which make the particles distinguishable). â¢ The transverse momentum distribution in high-energy collisions of hadrons is a Boltzmann distribution with constant temperature T0  m independent â of the primary energy (1  Elab  106 GeV), â of the number of particles involvedâfor two particles it gives elastic scattering, for many particles the jets, â of the centrality (= degree of thermal equilibrium) of the collision."
311,857,0.987,The Physics of the B Factories,"and the third uncertainty is purely from theory considerations. There is good agreement among the values of |Vtd /Vts | obtained from exclusive and inclusive analyses of radiative penguin processes. The farthest outlier from the central value of |Vtd /Vts | is obtained from the average of the Ï0 mode. However, all results are in reasonable agreement with each other. While the total uncertainty in the current results for the exclusive and inclusive approaches is comparable, the relatively very small inclusive theory uncertainty will make it a more sensitive observable at future ï¬avor facilities that plan to integrate much larger datasets than available at Belle or BABAR. Comparing these results with the |Vtd /Vts | value from mixing, there is also good agreement, albeit with substantially larger uncertainties for the radiative decays results. For any future Belle inclusive analysis, it seems reasonable to assume that the uncertainty will be similar to that for their exclusive analysis, just as at BABAR. This would allow for more precise comparisons between |Vtd /Vts | from rare radiative decays and from mixing. 17.2.3 Summary A direct determination of |Vts | and |Vtd | from a measurement of the decays t â s and t â d at LHC is diï¬cult, and will likely remain so at least in the near future. Indirect methods involving virtual top quarks are therefore required to measure these CKM matrix elements. At the B Factories, the FCNC transitions b â s and b â d in radiative penguin processes have been used to obtain measurements of the ratio |Vtd /Vts |, while the value of |Vtd | has been obtained from measurements of Bd mixing. Extracting the values of the CKM elements from these processes necessarily assumes there are no contributions from physics beyond the SM and it is diï¬cult to distinguish possible NP contributions, which may enter at the same order as the lowest order SM processes. The major uncertainties in the existing measurements originate from ignorance of the hadronic matrix elements. The current method for extracting |Vtd | and |Vts | from ÎB = Â±2 processes relies heavily on lattice calculations, and any further experimental improvements in Îms/d measurements will need to be matched by corresponding improvements in the lattice calculations. Likewise, for improvement in the precision of |Vtd | and |Vts | extracted from radiative penguin processes, signiï¬cant advances in the theoretical methods will be necessary. Experimentally, it may be possible at future super ï¬avor factories to make a fully inclusive branching fraction measurement of b â dÎ³, as well as b â sâ+ ââ and b â dâ+ ââ , which will help to reduce theory and model dependences. In b â sâ+ ââ and b â dâ+ ââ decays, additional amplitudes arise from diagrams similar to Fig. 17.2.1 but with a Z boson replacing the photon (see Section 17.9 for a discussion of these modes). Because the contribution of these additional electroweak amplitudes becomes greater, and the contribution from the photon pole decreases, with increasing invariant mass of the di-lepton"
8,117,0.987,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Laboratory, and at CERNâs Large Hadron Collider (LHC) are testing these new concepts. Nuclei, rather than protons, are used in these experiments in order to maximize the volume of quark deconfinement. This allows a clearer study of the signature of the formation of a new phase of matter, the quark-gluon plasma (QGP). In past years both CERN and RHIC communities have presented clear evidence for the formation of the deconfined QGP state in which the hadron constituents are dissolved. The current experimental objective is the understanding of the physical properties of this new phase of matter. This requires the use of novel probes, which respond to a change in the nature of the state within the short time available. More precisely, the heating of hadronic matter beyond the Hagedorn temperature is accompanied by a large collision compression pressure, which is the same in magnitude as the pressure in the very early Universe. In the subsequent expansion, a collective flow velocity as large as 60 % of the velocity of light is exceeded. The expansion of dense QGP phase occurs on a timescale similar to that needed for light to traverse the interacting nuclei."
311,1126,0.987,The Physics of the B Factories,"analysis. Figure 17.6.6 shows the mES and pâB distributions for the Belle analysis. Vertex reconstruction and B meson flavor tagging algorithms (described in Chapters 6 and 8) are applied to the selected signal candidates. Time-dependent CP asymmetry parameters are extracted from fits to the distributions of proper decay time diï¬erence between signal and tagged B mesons as described in Chapter 10. BABAR extracts the time-dependent asymmetry parameters (S and C) from a simultaneous fit to both the BCP and Bflav (see Section 10.2) samples with 69 additional free parameters, where tagging and resolution parameters are transparently propagated into the CP analysis as part of the final statistical error. Belle takes a multi-step approach: the final fit includes only S and C as free parameters, and all the fit model parameters, which include signal fractions, flavor tagging performance parameters, and proper time diï¬erence resolution function parameters are fixed to the values determined from separate fits to the Bflav and BCP samples. Eï¬ects arising from the uncertainties of these parameters are included in the final result as systematic errors. The results of the time-dependent CP asymmetry measurements are summarized in Table 17.6.1 for each decay mode, and for the combined set of modes. As described in Section 17.6.8, the time-dependent full angular analysis of the B 0 â J/Ï K â0 decay can provide a value for cos 2Ï1 in addition to sin 2Ï1 . The angular information presented in the table has been averaged over, resulting in a dilution of the measured CP asymmetry by a fac-"
165,123,0.987,New Methods for Measuring and Analyzing Segregation,"of the three and leads to milder nonlinearity in the y-p relationship and larger changes in y as p moves from 20 to 70. In each group comparison the changes in y as p moves from 20 to 70 are smaller than the 50 point increase in y observed for S for the same group comparisons. This is because the y-p relationship is linear for S and nonlinear for H. The nonlinearity in the y-p relationship for H creates a large region in the middle portion of the range of p where the slope of the curve is less than 1.0 and thus changes in y are smaller than changes in p.2 In addition, the degree to which changes in y are smaller than changes in p varies across the three segregation comparisons because the nonlinearity in the y-p relationship varies; specifically, the departure from linearity is more pronounced when the two groups in the comparison are more unequal in size and thus changes in y over the middle range of p are smaller in these group comparisons. The function y = f ( p ) for the Hutchens index (R) also generates values of y that fall on a smooth, ever-rising, backwards âS-curveâ. The curve is similar in form to the curve seen for the Theil index (H). But the nonlinearity in the curve for R is noticeably more pronounced. Accordingly, the patterns for the scoring of y for R are similar to those just noted for H, but âamplifiedâ. For example, as with H, changes of a fixed amount in contact with Whites (p) translate into different impacts on y depending on the initial starting value of p and relative size of the two groups. Thus, the graphs in Fig. 5.1 indicate that a family that moves from an area that is 20 % White area to an area that is 70 % White area would experience an increase of 24.2 points on scaled contact with Whites (y) in the White-Black comparison, 25.7 points in the White-Latino comparison and 18.3 points in the White-Asian comparison. The changes in y are even smaller than the changes in y noted for H because the departure from linearity in the y-p relationship for R is greater. This âflattensâ the y-p curve over the middle range of p even more and causes changes in y to be smaller than changes in p. As seen with H, the changes in y vary across the different segregation comparisons; they are larger when groups are more equal in size and smaller when groups are more unequal in size. The function y = f ( p ) for the gini index (G/2) also produces an ever-rising, backwards âS-curveâ. However, in contrast to the functions for H and R, this curve is irregular rather than smooth. This is because G tracks percentile scores for p and these depend not on the specific value of contact with Whites (p) itself, but instead on how values of p translate into rank position on contact with Whites. In the case of White-Black segregation, for example, this is determined by the number of Whites and Blacks living in areas where p higher and the number of Whites and Blacks living in areas where p is lower. The nonlinearity of the function for G/2 is more pronounced than that seen for the functions for H and R and this produces larger departures from the diagonal line for S. As a result, it is reasonable to say that scoring y as the percentile transformation of p is as the most âdramaticâ rescaling of contact of those considered here. Thus, the graphs in Fig. 5.1 indicate that a family"
372,1629,0.987,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"For example, the range of seasonal median values for -0 for the ALMA site is 0.045â0.17 mm. Since the diameter of the ALMA antennas is 12 m, the range of ( is 0.2â0.600 from Eq. (13.111), which is independent of wavelength. The timescale of this effect is d=vs , where vs is the wind speed. At a wavelength of 1 mm, the beamwidth is about 2000 , so the ratio (=.b has the range of 1% to 3%. There is no effect on the amplitude of the incident electric field because the phase fluctuations arise in a layer close to ground. However, the fractional changes in antenna gain at the half-beamwidth point would range from 1.5% to 5%, which could have an effect on the quality of mosaic images derived from array observations under some conditions. For further details, see Holdaway and Woody (1998). Methods of realtime correction of anomalous refraction have been proposed by Lamb and Woody (1998)."
372,1911,0.987,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where we assume that the antenna response A1 .!/ has a constant value A0 over the source, and the subscripts R and I denote the real and imaginary parts of the visibility. This result follows from the definition of visibility that is given for a twodimensional source in Sect. 3.1.1. Thus, the correlator output is proportional to the square of the modulus of the complex visibility. For a more detailed discussion following the same approach, see Hanbury Brown and Twiss (1954). An analysis based on the mutual coherence of the radiation field is given by Bracewell (1958). Some characteristics of the intensity interferometer offer advantages over the conventional interferometer. The intensity interferometer is much less sensitive to atmospheric phase fluctuations, because each signal component at the correlator input is generated as the difference between two radio frequency components that have followed almost the same path through the atmosphere. The phase fluctuations in the difference-frequency components at the detectors are less than those in the radio frequency signals by the ratio of the difference frequency to the radio frequency, which may be of order 10!5 . In the conventional interferometer, such phase fluctuations can make the amplitude, as well as the phase, of the visibility difficult to measure. Similarly, fluctuations in the phases of the local oscillators in the two receivers do not contribute to the phases of the difference-frequency components. Thus, it is not necessary to synchronize the local oscillators or even to use high-stability frequency standards, as in VLBI. These advantages were helpful, although by no means essential, in the early radio implementation of the intensity interferometer. Had the diameters of the sources under investigation then been of order of arcseconds, rather than arcminutes, the characteristics of the intensity interferometer would have played a more essential role. The serious disadvantage of the intensity interferometer is its relative lack of sensitivity. Because of the action of the detectors in the receivers, the ratio of the signal power to the noise power at the correlator inputs is proportional to the square of the corresponding ratio in the RF (predetector) stages, the exact value being dependent on the bandwidths of these and the post-detector stages (Hanbury Brown and Twiss 1954). In a conventional interferometer, it is possible to detect signals that"
311,2865,0.987,The Physics of the B Factories,"Figure 23.3.5. Distributions for the Bs0 â Dsââ Ï+ candidates. Top: Mbc = mES and ÎE distributions, as in the previous ï¬gure. Bottom: distributions of the cosine of the helicity angles of the Dsââ (left) and Ï+ (right) with mES and ÎE restricted to the Bsâ0B â0 s kinematic region. The components of the total p.d.f. (blue solid line) are shown separately: the black-dotted curve is the background and the two red-dashed curves are the signal. The large (small) signal component corresponds to the longitudinal (transverse) signal. Plots are from Louvot (2010)."
311,652,0.987,The Physics of the B Factories,"15.1.4 Ï 0 reconstruction The reconstruction eï¬ciency of Ï 0 âs in the decay channel Ï 0 â Î³Î³ can diï¬er between data and simulation mainly for the following reasons (see Section 2.2.4 for the description of the electromagnetic calorimeters): â Imperfect modeling of the material distribution in the detector. A photon can undergo pair production in the material of the detector before reaching the calorimeter. If the produced tracks are reconstructed in the tracking detectors, the corresponding clusters in the calorimeter, if any, are tagged as being produced by a charged track and the photon candidate is lost. Even if the reconstruction algorithms still ï¬nd a photon candidate, the energy resolution might be degraded, leading to a Ï 0 candidate with an incorrectly reconstructed energy or mass. â Imperfect modeling of photon shower shape. In order to discriminate electromagnetic from hadronic showers, shower shape variables such as the lateral moment,47 the number of crystals in a shower etc. are used. Showers tend to be somewhat narrower in simulation than in data, creating a small eï¬ciency diï¬erence between data and MC. â Split-oï¬s. The particle showers created by hadrons interacting with the material in the calorimeter contain a fraction of neutral hadrons. Such secondary hadrons can travel a sizable distance in the calorimeter before interacting with the material and depositing (a part of) their energy. These so-called split-oï¬s leave the signature of a calorimeter cluster without an associated track pointing to it, which is hard to distinguish from a real photon. Cluster split-oï¬s occur close to tracks, and the secondary showers usually have low energies. Detailed modeling of hadronic showers is diï¬cult, thus split-oï¬s present a further potential source of systematic diï¬erence between data and simulation. â Additional background in data. Real data events typically contain more (soft) photon candidates, most of which originate from beam-related background. This background consists primarily of electrons and positrons from radiative Bhabha scattering which hit elements of the detector or the beam line, producing neutrons with energies in the MeV range, which then can produce low energy showers in the calorimeter. These additional photon candidates increase the number of Î³Î³ combinations in data, giving rise to more Ï 0 candidates, especially at low Ï 0 momentum. The data-MC eï¬ciency ratio is ï¬rst measured in very clean events in which the presence of a Ï 0 can be predicted with little background. Possible diï¬erences between the Ï 0 reconstruction eï¬ciency in such events and highmultiplicity events with higher background must then be"
71,1393,0.987,Advancing Culture of Living with Landslides: Volume 1 ISDR-ICL Sendai Partnerships 2015-2025 (Volume 1.0),"Results and Discussion Landslide Susceptibility Assessment, Validation and Classification The classiï¬ed landslide type-based susceptibility maps resulting from the application of the Information Value method to the set of seven independent predisposing factors and to each landslide modelling group (RS, STS and D-STS) are presented in Fig. 3a, b, c, respectively. The ï¬nal landslide susceptibility map for the Loures municipality resulting from the integration of the above-mentioned landslide susceptibility maps is shown in Fig. 3d. Table 2 summarizes the area corresponding to each landslide susceptibility class of the three landslide type-based susceptibility maps and of the ï¬nal landslide susceptibility map. The areas classiï¬ed as Very high and High susceptible to slide occurrence validate the 70% of landslide occurrences within the municipality and are the ones that should integrate the NER. Considering only landslide type-based susceptibility maps, these classes correspond to 11.2, 10.3, and 16.5% of the Loures territory, respectively. In all models the Low and Very low susceptibility classes are dominant ranging from 66 to 78.6% of the total area. The ï¬nal municipal landslide susceptibility map is similar to the landslide type-based susceptibility maps. In fact more than half of the municipality (51.8%) is associated to Low or Very low landslide susceptibility (Table 2), not justifying special concerns, on the assumption that future interventions in the territory will not cause drastic changes in the current topography. The Moderate susceptibility class is larger in area (26% of total area) when compared with the landslide type-based susceptibility maps. Interventions in these areas are possible, but should be avoided both, slope cuts and embankments not supported by an engineering geology project. The High and Very high susceptibility is observed in 22.1% of the territory. Due to its geomorphological and geotechnical characteristics, these areas are not suitable for building structures or infrastructures implementation. The overall landslide susceptibility models ï¬t, expressed by the corresponding success-rate curves obtained in the validation process are shown in Fig. 4. The level of adjustment of each landslide type-based susceptibility model to the respective landslide inventory is also observed by the AUC values calculated for each success-rate curve. The landslide susceptibility model that presents the highest adjustment is the one constructed for the"
311,740,0.987,The Physics of the B Factories,"upper limit of integration may be considered large for the moment. The contributions Î1/mn describe corrections to the axial vector current for ï¬nite-mass quarks. The Î1/m2Q contributions can be conveniently written as Î¼2Ï (Î¼) â Î¼2G + 2 , Î1/m2Q = mc mb (17.1.18) where Î¼2G â 3(m2B â â m2B )/4 and Î¼2Ï (Î¼) are matrix elements of the chromomagnetic energy and kinetic energy of the b quark in the B meson. The meaning of the scale Î¼ in Î¼2Ï (Î¼) is explained below. The 1/m3Q contributions have a similar expression (see, e.g., Gambino, Mannel, and Uraltsev (2010)) with analogs of Î¼2G and Î¼2Ï that are related to moments of the inclusive semileptonic distribution. For Ç« â« ÎQCD , the hadronic states in the excitation integral are dual to quark-gluon states. Introducing a scale Î¼ to separate this short-distance part from the longdistance part (which must be treated non-perturbatively), one writes"
285,217,0.987,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","The MPI model has been used to model the performance of the SCE strategy for five virtual CI users. All virtual CI user differs from each other in their most comfortable and threshold levels used in the map. That means that the speech processor of each virtual CI user will generate different electrodograms for the same vowels. Next, formant features were extracted from the electrodograms based on the spectral contrast between formants 1 and 2. Noise was added to the spectral contrast features (jnd noise). Three amounts of noise were added 0.01, 10 and 50 % of the magnitude of the formants extracted. Figure 2 presents the results predicted by the model in percentage of correct vowels identified for five different SCE factors (0, 0.5, 1, 2 and 4). From the modelling results it can be observed that maximum performance is achieved for SCE factors 2 and 4. The other interesting aspect is that there is no difference in performance across the five different virtual CI users for âNoise Factorâ 0.001 (Fig. 2a). That means that the feature extraction (i.e. the extraction of the formants) is robust to the different electrodograms of each virtual CI user. Differences in performance between the five virtual CI users can only be observed for âNoise Factorâ 0.1 and 0.5, meaning that the âjnd noiseâ is the only parameter explaining the differences. From the MPI modelling results we decided to use SCE factors 2 and 4 (equivalent to increasing the original spectral contrast of the spectrum by 3 and by 5 in a dB scale) to be investigated in CI users."
157,187,0.987,"The Economy as a Complex Spatial System : Macro, Meso and Micro Perspectives","Moreover, Debarsy et al. (2012) derive the algorithms to calculate partial derivatives that can quantify the magnitude and timing of dependent variable responses in each region at various time horizons t + T to changes in the explanatory variables at time t. They also distinguish between two different interpretative scenarios, one where the change in explanatory variables represents a permanent or sustained change in the level and the other where we have a transitory (or one-period) change. In particular, the T -period-ahead (cumulative) impact arising from a permanent change at time t in the k-th variable is1 :"
165,154,0.987,New Methods for Measuring and Analyzing Segregation,"These results support the general conclusion that correlations between indices are consistently weaker when using broader, more heterogeneous samples of cities and when using index scores computed for blocks instead of tracts. As a final check, I replicated these results using alternative versions of index scores that corrected for index bias (discussed in Chaps. 14 and 15), a potential concern when using index scores computed from block-level data. The relevant results were fundamentally similar and strengthen the conclusion I offer here. I now answer the questions posed in the heading for this section of the chapter, âWhen do different indices agree?â and âWhen can they disagree?â The previous discussion provides a preliminary answer. Indices are more likely to agree in studies that focus on large metropolitan areas and compute index scores using tract-level data. Conversely, indices are more likely to disagree in studies that use broader samples and/or compute index scores with block-level data. But why is this so? Two findings provide clues. One is that cities in the Massey and Denton sample have higher levels of relative minority presence and the other is that correlations among indices are consistently higher when the relative size of the minority population is larger. Among the CBSAs segregation comparisons that meet the criterion of having at least 1,500 in population for the minority group, relative minority presence is consistently higher in the subset of CBSAs in the Massey and Denton subsample and this is true for all three White-Minority comparisons considered. This is consequential because correlations among indices are higher when pairwise minority group proportions are moderate-to-high.5 Evidence for this is presented in Fig. 6.4 and in Table 6.4. Table 6.4 is organized in three panels. The top panel gives correlations among index scores computed from block-level data for the subset of White-Minority segregation comparisons where the two groups in the comparison are similar in relative size; specifically, these are the subset of 510 segregation comparisons where the pairwise proportion for the smaller group in the comparison is in the range of 0.30â0.50. The key finding documented here is simple and compelling; the correlations among all of the indices are extremely high. The weakest relationship observed is between G and R with a simple linear correlation of 0.9697 and a squared correlation of 0.9403. Figure 6.4 presents the scatterplots for these same relationships. It documents that the relationships are even stronger than the simple linear correlations suggest as the lower correlations involve relationships that are very close but mildly nonlinear. When the nonlinearities are taken into account, all relationships are near exact. For example, the G-R combination has the lowest squared linear correlation (0.9403) but regressing G on R and the square root of R yields a multiple R-square statistic of 0.9859. The middle panel of Table 6.4 presents results for White-Minority segregation comparisons where the pairwise proportion for the smaller group in the comparison is the range of 0.10â0.30. The key finding documented here is that, while the correlations are generally lower, they all remain very high. Thus, the lowest squared"
297,1293,0.987,The R Book,"Note that the two signiï¬cant interactions from the aov table do not show up in the summary.lm table (Waterâ Daphnia and DetergentâDaphnia). This is because summary.lm shows treatment contrasts, comparing everything to the Intercept, rather than orthogonal contrasts (see p. 430). This draws attention to the importance of model simpliï¬cation rather than per-row t tests in assessing statistical signiï¬cance (i.e. removing the non-signiï¬cant three-way interaction term in this case). In the aov table, the p values are âon deletionâ p values, which is a big advantage. The main difference is that there are eight rows in the summary.aov table (three main effects, three two-way interactions, one three-way interaction and an error term) but there are 24 rows in the summary.lm table (four levels of detergent by three levels of daphnia clone by two levels of water). You can easily view the output of model1 in linear model layout, or model2 as an ANOVA table using the opposite summary options: summary.lm(model1) summary.aov(model2) In complicated designed experiments, it is easiest to summarize the effect sizes with plot.design and model.tables functions. For main effects, use plot.design(Growth.rate~Water*Detergent*Daphnia)"
71,214,0.987,Advancing Culture of Living with Landslides: Volume 1 ISDR-ICL Sendai Partnerships 2015-2025 (Volume 1.0),"Assessment of the Largest Credible Volume The assessment of the largest credible volume is crucial for the management of rockfall risk. Residential areas located below rockfall susceptible slopes have often developed strategies of risk mitigation using a combination of land use planning, stabilization and protective works (protection fences, embankments). The design of these measures and the delimitation of the hazardous areas are based on analyses for a range of expected potential rockfall volumes (Corominas et al. 2005; Abbruzzese et al. 2009; Agliardi et al. 2009; Li et al. 2009). Although some risks analyses (i.e. Hungr et al. 1999) have shown that the highest risk is associated to mid-size rockfall events (1â103 m3), the occurrence of rockfall events larger than the used for the design of the protective works would not be manageable and the population might be exposed at an unacceptable risk level. The question posed is what the largest credible rockfall volume can be. It is usually characterized by volumes of rock masses of several orders of magnitude greater than the events commonly observed in the study area. It must be kept in mind that the largest credible rockfall event is a reasonable largest event, not the largest conceivable event. The analysis of the failure of a rocky slope is intrinsically linked to the knowledge of the fracture pattern of the massif which, on one side determines the volume of kinematically unstable rock mass and on the other side determines the mechanism of rupture. The instability mechanism may involve displacement along existing discontinuities either fully persistent or not and brittle failure of intact rock. It may involve single large blocks bounded by discontinuities or rock masses composed of smaller blocks. Figure 5 shows some conceptual schemes of the fracture patterns associated to the failure mechanisms. Figure 5a: simple planar failure is a rock mass affected by a fully persistent joint set. The volume mobilized is directly determined by the orientation and dip of the discontinuity and the surface topography. This"
285,82,0.987,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","Elderly listeners (ENH and EHI) showed higher thresholds than young listeners (YNH) in the TIME task (with both low- and high-frequency stimuli) and the HUGGINS task. This confirms earlier findings that the sensitivity to temporal structure declines without accompanying elevated audiometric thresholds (e.g., Hopkins and Moore 2011). Thresholds for the LEVEL task also tended to be higher in the elderly than the young, consistently with previous finding (e.g., He et al. 1998). Despite these consistent declines in the performance of the monaural tasks with age, the present study failed to find significant age effect in the lateralization tasks ( ITD and ILD). The thresholds for the ILD and LEVEL tasks correlated for both the low- and high-frequency stimuli. This confirms the results of our earlier study (Ochi et al. 2014), suggesting that the efficiency of level coding in the auditory periphery is a major factor accounting for inter-individual variation of ILD sensitivity. The ITD task showed a correlation with the TIME task for the high-frequency stimulus; when the factors of age and hearing-impairments were controlled. This again is consistent with the finding of our earlier study (Ochi et al. 2014), suggesting that a listenerâs ITD sensitivity is well accounted for by the listenerâs sensitivity to temporal (envelope) structure. Despite a relatively large number of participants and greater range of threshold values in the TIME task, however, we failed to find a correlation between the ITD and TIME tasks for the low-frequency stimulus. The thresholds for the HUGGINS task showed a significant positive correlation with those for the TIME task ( r = 0.50, p = 0.004), but not with those for the ITD task ( r = 0.11, p = 0.553). This suggests that the performances of the TIME and HUGGINS tasks capture inter-individual variation of the efficiency of temporal-structure processing, but that of the ITD task is determined primarily by other factors. The HUGGINS and ITD tasks are similar"
18,190,0.987,Radiation Monitoring and Dose Estimation of the Fukushima Nuclear Accident,"k describes a ratio of the fixed radioactive materials and r describes a gradient of the decrease. Fitted lines are shown as solid lines in Fig. 10.8, which show different gradients and saturation values. k and r for each point are shown in Table 10.1. The fixation ratio of the cushion area dose rate is highest and that of the normal asphalt area is lowest in the three points. From these analyses, it became clear that the dose rates on paved areas decrease faster than the physical decay of radioactive nuclides that were speculated to come"
311,414,0.987,The Physics of the B Factories,"nonlinear correlations, neural networks (NN, see Chapter 4 for a description of multivariate methods) and other nonlinear discriminant algorithms are used. As an illustration, typical charmless 3-body analyses use, in addition to the L0 and L2 monomials, variables such as |cos Î¸B | and |cos Î¸T | in their ï¬nal MVA. Figure 9.4.5 illustrates the discriminating power achieved with a NN based on these four variables, used in several Dalitz-plot analyses of charmless 3-body B decays in BABAR (see Chapter 13 for a description of Dalitz-plot analyses). In these analyses, the NN output is used both for selection and in the maximum-likelihood ï¬t. At the ï¬rst stage, this NN provides a â¼ 1.9Ï separation between signal and background. A cut at NN > â0.4 is then applied to remove roughly 75% of continuum background while retaining a 90% signal eï¬ciency; on top of enhancing its signal-to-background content, this cut also reduces the sample size to a value that is suitable for the CPU constraints aï¬ecting multidi-"
107,95,0.987,"Symbiotic interaction : 5Th international Workshop, Symbiotic 2016, Padua, Italy, September 29â30, 2016, Revised Selected Papers","100 instances of 7 features each. The feature corresponding to the age of the Internet users is linearly divided into two groups, which are below 35 years and above 35 years. The education level feature can assume also two values, which are the higher education and the secondary education levels. About the number of years of Internet usage, this feature has integer values from 0 to +â. In particular, in the dataset it varies from 1 to 9. Consequently, it is discretized by employing an Equal-Width Discretization [8], obtaining 3 different ranges, named as high Internet usage (> 6 years), middle Internet usage (from 4 to 6 years), and low Internet usage (< 4 years). The features corresponding to the response time in solving the 4 different types of image-based CAPTCHA have decimal values varying from 0.0 to +â. A test has been conducted for discretization of these features with multiple methods, including Equal-Width Discretization. It demonstrated that most of the methods fail in this task because they obtain a poor discretization. On the contrary, discretization by adopting the K-Medians clustering algorithm [4], whose advantage is finding the natural groups of data based on their characteristics, performed successfully, identifying 3 different ranges. The aim of the K-Medians is to determine K ranges from the values of the features, by minimizing a function J. It measures the total sum of L1 norm between each value in a certain range and its centroid, which is its median value. We run the K-Medians algorithm 10 times with K = 3 and selected the set of ranges with the lowest value of J as the optimal solution. Hence, K-Medians found 3 ranges corresponding to low response time (< 12.34 s), middle response time (from 12.34 s to 27.30 s), and high response time (> 27.30 s). Table 1 reports the feature values, eventually discretized. Table 1. Possible values and corresponding ranges for each feature of the dataset Features"
311,2233,0.987,The Physics of the B Factories,"As a result of D0 â D0 mixing, CP asymmetries for D mesons depend upon the interval of decay time, t, over which the asymmetry is integrated. At the B Factories, time resolution is comparable with the D0 lifetime, ÏD0 . Therefore, no D0 decay vertex separation requirement is imposed on the samples used in any of the analyses and integration times include t = 0. In contrast, hadron collider experiments (CDF, LHCb, etc.) impose decay length based selections to reduce large combinatorial backgrounds. There, the integration times begin at t = tmin > 0. In all cases, the upper end of the range, tmax is large and can be taken as infinite. To first order in the small parameter y, the values for AfCP measured can be approximated (see, for example Gersabeck, Alexander, Borghi, Gligorov, and Parkes, 2012) as combinations of afdir and aind that are linear in tmin , thereby allowing separate values for afdir and aind to be estimated.134 The feasibility of using measurements of CP asymmetries as a function of time (as distinct from values ACP integrated over finite time periods) has also been studied (Bevan, Inguglia, and Meadows, 2011). Many decay modes can be used to study weak phases in D0 meson decays in much the same way that such measurements were used in the B Factory measurements of the CKM phases Ï1â3 . Such measurements will be feasible using samples about 100 times larger than those of the B Factoryâs. 19.2.6.1 Using data to measure detector induced asymmetries In the experimental determination of the physics parameter AfCP other asymmetries not originating from the CP violation may enter and have to be corrected for. These include detector induced asymmetries, for example an asymmetry in the reconstruction eï¬ciencies of positively and negatively charged tracks, as well as the forward-backward (FB) asymmetry due to the Î³ â âZ 0 interference in e+ eâ â cc. The former may be induced by diï¬erent cross-sections for the interaction of particles and anti-particles in the"
311,718,0.987,The Physics of the B Factories,"B physics The main objective of the B Factories was to perform measurements of the decays and CP asymmetries of B mesons. While the asymmetric set-up and high luminosities of the B Factories allowed us for the ï¬rst time to perform statistically signiï¬cant measurements of time-dependent CP asymmetries, the symmetric predecessors of the B Factories, DORIS and CESR, had already produced some data on B decays. Experiments at LEP and the Tevatron had provided a proof of principle of the time-dependent CP asymmetry measurement in the golden mode B 0 â J/ÏKS0 , and improved our knowledge of Bd0 mixing. Most of the time, the B Factories took data near the Î¥ (4S) resonance, which decays almost exclusively into B 0B 0 and B +B â pairs. As a consequence, the overwhelming majority of B Factory measurements relate to these states: these measurements are described in the following sections. However, some data has been taken at the (â)0 (â)0 Î¥ (5S) resonance, which also decays into Bs B s pairs. Measurements of Bs decays performed with this data are discussed in Chapter 23. There are many ways to arrange this vast amount of material. The scheme adopted for this book uses the Unitarity Triangle as an organizing principle. We start from a discussion of the ways the sides of the triangle are constrained, including theoretical methods as well as experimental results in the corresponding sections. Hence we start with the measurements determining the magnitude of the CKM matrix elements Vcb , Vub , Vts , and Vtd . This is followed by a discussion of the decay rates of charmed and charmless non-leptonic processes, including a comparison with theoretical expectations. The reason for this is that many charmed and charmless non-leptonic decay modes are used in the measurement of CP asymmetries, and therefore should be discussed before moving on to review work related to the angles of the Unitarity Triangle. Before treating the CP asymmetries related to the angles of the Unitarity Triangle, we discuss measurements of B lifetimes and B 0 â B 0 mixing, which are needed to understand the time-dependent analyses performed for the extraction of the angles. Searches for CP T and other symmetry violations which are based on the lifetime- and mixing-measurement techniques are then presented. Following on from this one will ï¬nd the description of measurements of CP violation, i.e. the extraction of the angles Ï1 , Ï2 , and Ï3 . The end of this chapter is devoted to special processes. These are either rare decays related to ï¬avor changing neutral current transitions of the b quark, processes involving Ï leptons or baryons in the ï¬nal state, or decays which are very rare or forbidden in the Standard Model."
362,359,0.987,Cloud-Based Benchmarking of Medical Image Analysis,"In Eq. 11.3, t1 and t2 are temperature hyperparameters, and Ni is the neighbourhood of the variable i â S . The first and second sums in Eq. 11.3 correspond, respectively, to organ intensity and location (atlas) likelihood energies, and the third is the energy of a prior distribution of label configurations expressed as a Markov random field [18] with respect to the graph G . We shall define these terms in detail."
273,199,0.987,Report on Global Environmental Competitiveness (2013),"that the correlation coefficient between the four pillar and the subordinate individual indicators is relatively larger. More number of correlation coefficient that passes the significance test indicates that many original indicators show higher correlation. But, except that the individual indicators show certain correlation, the correlation between sub-index and between pillars are not high, which means little influence on the calculation of comprehensive evaluation score and the reliability of both scores and rankings of GEC."
30,120,0.987,Determinants of Financial Development,"more general than an additive effects model, the traditional one-way or two-way fixed effects model.44 Since the common factors are unobservable, standard regression methods are not applicable for an equation like (3.11). Estimation of models with a common factor structure is still at its early stage of development. Pesaran (2006) estimates this type of model directly by proxying the common factors with weighted cross section averages (Subsection 3.4.4 discusses this in detail). In spite of its convenience in not involving estimation of common factors, the Pesaran (2006) approach is confined to the single factor case. Among others, Bai and Ng (2004) and Moon and Perron (2004) seek to estimate the common factors. Their approaches have advantages in accommodating multiple common factors that may coexist in the economy, effectively contributing to panel unit root testing, panel cointegration testing and estimation of models in a more general setting. Below is a brief description of common factor analysis resulting from Bai and Ng (2004). To overcome possible cross section dependence in panel unit root testing, Bai and Ng (2004) propose a PANIC approach â Panel Analysis of Non-stationarity in Idiosyncratic and Common Components. Essentially they assume the DGP of a series zit (which could be yit or xit for this case) has a common factor structure in the sense that the series is the sum of an unobserved deterministic component (dit ), an unobserved common component (Î»i ft ) and an idiosyncratic component (eit ) as follows:"
311,1416,0.987,The Physics of the B Factories,"17.9.2.7 Constraints on new physics from B(B â Xs Î³) The SM prediction for the extrapolated branching fraction (Eq. 17.9.20) and the latest experimental average now have similar levels of uncertainty, and are consistent at the 1Ï level. This finding implies very stringent constraints on NP models (Section 25.2). As examples we quote â In the type-II two-Higgs doublet model (THDM) the bound on the charged Higgs mass is MH + > 380 GeV/c2 at 95% C.L. See (Misiak et al., 2007) and the improved computation in (Hermann, Misiak, and Steinhauser, 2012). [Note a slightly older worldaverage branching fraction was used in the latter work.] â In minimal universal extra-dimension model (Haisch and Weiler, 2007), the bound on the inverse compactification radius is 1/R > 600 GeV at 95% C.L. [This limit should be recomputed to reflect improvements in the world-average branching fraction since 2007.] In both cases, the bounds are much stronger than those previously derived from other measurements â but see Section 17.10.2.2 regarding an even stronger constraint on the THDM. Constraints on various supersymmetric models have been reviewed in (Altmannshofer, Buras, Gori, Paradisi, and Straub, 2010; Hurth, 2003). Bounds on the little Higgs model with T -parity have also been presented (Blanke, Buras, Duling, Recksiegel, and Tarantino, 2010). Finally, model-independent analyses in the eï¬ective field theory approach with the assumption of minimal flavor violation (DâAmbrosio, Giudice, Isidori, and Strumia, 2002; Hurth, Isidori, Kamenik, and Mescia, 2009) also show the strong constraining power of the B â Xs Î³ branching fraction. 17.9.3 Exclusive b â sÎ³ As already discussed, B â K â Î³ was the first b â sÎ³ decay to be observed, and the K â (892) resonance is the only one clearly visible in the Xs mass spectrum of B â Xs Î³ (Fig. 17.9.6). Contributions to B â Xs Î³ with Xs heavier than K â have also been studied in detail. Some of the heavier Xs final states are from resonances, but these are harder to disentangle, and non-resonant contributions also seem to be large, as the sum of the measured resonant contributions is far from saturating the total B â Xs Î³ decay width. 17.9.3.1 B â K â Î³ The B â K â Î³ signal is reconstructed in the four K â final states, K + Ï â , KS0 Ï 0 , K + Ï 0 , KS0 Ï + , and their charge conjugate modes; KS0 Ï 0 is a CP eigenstate that can be used to study time-dependent CP violation. The signal is identified by the kinematic variables mES and ÎE. There is a combinatorial background that is suppressed by event shape variables, and then has to be subtracted. There are also small âpeakingâ backgrounds from"
179,1231,0.987,"Habitats and Biota of the Gulf of Mexico: Before the Deepwater Horizon Oil Spill: Volume 1: Water Quality, Sediments, Sediment Contaminants, Oil and Gas Seeps, Coastal Habitats, Offshore Plankton and Benthos, and Shellfish","demersal fishes. This information is embodied in numerous reports, government documents, and peer-refereed papers, as summarized in Table 7.4 and in the review by Rabalais et al. (1999b). Sampling locations were organized along the coast in transects that bisected the shelf, from depths as shallow as 6 m (19.7 ft) out to the edge of the shelf at depths approaching 200 m (656 ft). Recurrent groups or assemblages were determined among these sites, and maps were then used to illustrate the groupings. The entire northern Gulf of Mexico coastline exhibited some common features: (1) highest densities of macrofauna were encountered at the inshore locations, (2) lowest densities were at the outer-shelf margin, (3) macrofaunas were dominated by diverse assemblages of polychaete annelid worms followed by amphipod crustaceans and bivalve molluscs in lesser numbers, and (4) principal faunal groups were aligned parallel to the coastline within depth intervals in a predictable fashion. About 20 % of the dominant macrobenthos are shared between the three northern Gulf study areasâSouth Texas Outer Continental Shelf (STOCS), Mississippi Alabama Marine Ecosystem Study (MAMES), and the Mississippi, Alabama, Florida (MAFLA) ecosystem studiesâand Rabalais et al. (1999b) suggest that there is regional endemism within the macrofaunal component of the benthic communities. However, that degree of overlap in similar species is substantially higher than might be expected, given the differences in the habitats (Figure 7.1). The STOCS investigation on the south Texas shelf, summarized in Flint and Rabalais (1981), was designed to gain a quantitative understanding of how the shelf ecosystem food web functions relative to supplies of inorganic plant nutrients, phytoplankton productivity, stocks of zooplankton, and fate on the sea floor. The data clearly demonstrate that meager nutrient supply (nitrate) supports relatively low PP because chlorophyll a concentrations were consistently below 1 mg C/m3 all year, with the exception of single modest spikes during brief spring and fall blooms. A carbon budget was created to illustrate how an estimated 103 g C/m2/year of new production (a high value given the low chlorophyll a values) is cycled through the food web to the economically important brown shrimp (Farfantepenaeus aztecus) population. Modest gradients of ammonium (NH4) at the seafloor suggested that benthic-pelagic coupling"
311,2120,0.987,The Physics of the B Factories,"The reason for the small rate of mixing of D0 mesons lies in the fact that they are the only flavored neutral mesons composed of up-type quarks. The GIM mechanism, as explained below, is even more eï¬cient for the case of up-type quark FCNCâs. For the same reason measurements of mixing in the D0 system yield complementary constraints on possible contributions from new physics (NP) processes beyond the SM to those arising from the measurements of FCNCâs of down-type quarks (B or K mesons). In 2007 the B Factories established evidence for mixing in the neutral charm mesons system, and those results were published back-to-back in Phys. Rev. Lett. as (Aubert, 2007j) and (Staric, 2007). These results are discussed in Sections 19.2.2 and 19.2.3, respectively. 19.2.1.2 Mixing A general description of oscillations of pseudoscalar neutral mesons is given in Section 10.1. In the following we emphasize some of the specifics of the D0 system. The mixing parameters are defined in Eq. (19.2.1). â In the absence of CP violation (q = p = 1/ 2 in Eq. 10.1.2), D1(2) is the CP -even (odd) state if one adopts the phase convention CP |D0  = |D0  and CP |D0  = |D0 .119 The amplitude for the process of Fig. 19.2.1, D0 |H âC=2 |D0 , can be schematically written as Vci Vcj Vuj F(m2W , m2i , m2j ), (19.2.2) i,j=d,s,b"
372,722,0.987,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"7.4 Polarization Mismatch Errors The response of two antennas to an unpolarized source is greatest when the antennas are identically polarized. Small variations in the polarization characteristics of one antenna relative to another occur as a result of mechanical tolerances. These variations lead to errors in the assignment of antenna gains in a manner similar to the variations in frequency responses. To examine this effect, we calculate the response of two arbitrarily polarized antennas to a randomly polarized source, which is given by the term for the Stokes parameter Iv in Eq. (4.29). Definitions of symbols are in terms of the polarization ellipse (see Fig. 4.8 and related text). The position angle of the major axis is , the axial ratio is tan ., and subscripts m and n indicate two antennas of an array. As an example, we consider antennas with nominally identical circular polarization for which we can write .m D &=4C"".m and .n D &=4C"".n , where the "" terms represent the deviations of the corresponding parameter from the ideal value. The required response is Gmn D G0 Åcos. m """
12,125,0.987,Self-Assembled Molecules â New Kind of Protein Ligands : Supramolecular Ligands,"The fuzzy oil drop model has been extensively described in numerous publications [3, 12, 13]. The description presented below should therefore be regarded only as a brief introduction. The traditional notion of a âhydrophobic coreâ refers to a concentration of hydrophobic residues at the geometric center of the protein, along with exposure of hydrophilic residues on its surface. When dealing with globular proteins, this configuration can be described with a 3D Gaussian, where the origin of the coordinate system coincides with the geometric center of the molecule and a separate Ï coefficient is defined for each principal axis, delineating an ellipsoid capsule. The molecule itself should be oriented in such a way as to align its orthogonal dimensions (longest diagonals) with axes of the coordinate system. Values of Ïx, Ïy and Ïz are calculated as 1/3 of the separation between the origin of the system and the position of the most distant atom along each axis. This is schematically depicted in Fig. 5.1. In accordance with the three-sigma rule, over 99% of the total volume of the Gaussian is captured by applying a cutoff distance of 3Ï in each principal direction. The value of the Gaussian at any point within this ellipsoid capsule is interpreted as local theoretical hydrophobicity (also referred to as the âidealizedâ distribution). The theoretical hydrophobicity distribution should be confronted with the observed distribution, which depends on local interactions between each residue and its neighbors. These calculations are based on the positions of the so-called effective atoms (averaged-out positions of all atoms comprising a given residue) and the intrinsic hydrophobicity of each amino acid. Each effective atom collects interactions with its neighbors, with a cutoff distance of 9 Ã. Theoretical (T) hydrophobicity is expressed by the following formulae: HË t j ="
214,283,0.987,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"GCMs and regional climate models are dynamical models. They use the equations of motion and thermodynamics to determine the rates of change of physical quantities (e.g., water vapor, temperature or heat, cloud water, carbon). There are also statistical models of climate. Statistical models of climate take observations from the past and try to predict the future with various forms of regression or correlation analysis: ï¬tting past data on temperature and precipitation for example, to a function that is used as a predictor of the future. Usually this is done for downscaling, ï¬tting temperature at one point to a larger scale temperature or flow pattern that can be predicted by a dynamical model. Regional Climate Modeling: Downscaling Use of a regional model at high resolution is an example of at type of analysis called downscaling.3 Downscaling uses ï¬ner-resolution information to improve the results of a coarse-scale model. It is effective especially when the improvement in resolution affects the simulation because of small-scale features at the surface: as is the case in regions of varied topography (mountains or coastlines). Use of a regional or local area model is known as dynamical downscaling. Downscaling can also be statistical downscaling. For example, if you want to know the temperature high on a mountain, you could develop a statistical relationship between the temperature for the whole region and this particular point based on observations over the past 50 years, and then adjust climate model output for the region in the same way to get a simulated record at that station that takes into account the unique local features (high altitude). Both methods of downscaling would be particularly useful for representing climate in regions of variable topography: either at high altitude, Wilby, R. L., Wigley, T. M. L., Conway, D., Jones, P. D., Hewitson, B. C., Main, J., & Wilks, D. S. (1998). âStatistical Downscaling of General Circulation Model Output: A Comparison of Methods.â Water Resources Research, 34(11): 2995â3008."
297,1227,0.987,The R Book,"The function sapply is used to calculate the mean yields for the three soils (contrast this with tapply, below, where the response and explanatory variables are in adjacent columns in a dataframe): sapply(list(sand,clay,loam),mean)"
365,146,0.987,Climate Smart Agriculture : Building Resilience To Climate Change,"continues to be updated on a monthly basis through the maturation stage of the crop. The most important month of the growing season is usually reproduction, and therefore the influence of this period has a strong relationship to yield. The benefit of the interactive term is multifold. Specifically, linear statistical models tend to be meancentric, which means they are challenged to capture extreme events. The quadratic component of their interaction generally captures these extreme events in the model. The models are generally run at the district level. Moreover, each country is unique in the way that it reports yield data. The spatial resolution of the yield data provided by a country serves as the basis of calibration in the model. Both deviation from expected yield and actual yield prediction are presented in the findings of the report. The expected yield has been trended to account for linear improvement of seed stock and improved agricultural practices. These trends are removed, since they are independent of the weather. An example report or the corn belt of the USA during the 2015 growing season is presented below. Figure 3a shows the predicted deviation from trended (expected) corn yields for the center of the corn-belt in the United States at the end of August 2015. The reasons this region is chosen are twofold; it produces one of the highest yields and is one of the most important growing areas for corn in the world and the sophisticated procedure for calculating yield by the United States Department of Agriculture (USDA) provides one of the best data sets for calibrating the yield prediction"
80,199,0.987,Innovations in Quantitative Risk Management (Volume 99.0),"with Îº := cmax /c0 â (0, 1). We call Îº the time-change correlation parameter. In particular, correlation coefficients ranging from zero to c j ck /cmax are possible, and the correlation does not depend on the point in time t. (iii) Due to the common time change, the compound Poisson processes Z (1) , . . . , Z (d) are stochastically dependent. Moreover, it can be shown that the dependence structure of the d-dimensional process (Z (1) , . . . , Z (d) ) is driven solely by the time-change correlation parameter Îº. A striking advantage of introducing dependence among the jumps in this manner is that the time-changed processes Z (1) , . . . , Z (d) remain in the class of compound Poisson processes with exponential jump heights, which ensures that the marginal processes maintain a tractable structure. In particular, the characteristic functions of the univariate log-price processes in a two-sided Î -OU-BNS model are still at"
372,553,0.987,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"which is equal to (6.26) when second-order terms in d&g =dt are neglected. (Recall that for, e.g., a 1-km baseline, the highest possible value of d&g =dt is 2:42 # 10""10 .) For the lower sideband, (6.26) and (6.27) apply if the signs of both !RF and !1 are reversed, and again the frequencies at the correlator input are equal. Thus, the overall effect is that the fringes are stopped for both sidebands."
360,329,0.987,Compositionality and Concepts in Linguistics and Psychology,"Fig. 3 Determining the CT point for RED HAIR and RED CAR: Typicality values of different hues for RED (see on top) are used to describe typicality for HAIR and CAR as a function of typicality for RED (a and b). The reddest local maximum point(s) of these functions is the critical typicality (CT) point (def. (7), example (6)). The CT point affects the determination of acceptability for the complex concept (Sect. 7)"
233,467,0.987,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"Fig. 2 Zonation priority maps for mammals in Europe. The red tones represent the best 10 % of the solution and blue tones indicate the lowest 50 % of cells. (a) Is the basic, core-area Zonation solution for our data where conservation value in Zonation optimization is only based on species richness normalised by range size. (b) Is the Zonation solution where the conservation value of a cell is weighted with the medium variant of the phylogenetic diversity, i.e. the inverse of the equivalent number of Raoâs quadratic entropy for the local community in each cell is used as cell costs. (c) Shows the national level basic Zonation priorities and (d) is the national analysis with phylogenetic diversity included"
311,476,0.987,The Physics of the B Factories,"The likelihood is then treated as a function of the parameters p and q. For measurements consisting of an ensemble of data points the likelihood of the ensemble is simply the product of the likelihood of each observation: L(p, q) ="
311,1040,0.987,The Physics of the B Factories,"positively charged lepton is taken as the first lepton (z1 ). For SS events Belle uses the absolute value of Îz. BABAR applies a beam spot constraint to the two lepton tracks to find the primary vertex of the event in the transverse plane. The positions of closest approach of the two tracks to this vertex in the transverse plane are computed and their z coordinates are denoted z1 and z2 , where the subscripts refer to the highest and second highest momentum leptons in the Î¥ (4S) rest frame. The vertex fit constrains the lepton tracks to originate from the same point in the transverse plane, thereby neglecting the nonzero transverse flight length for B 0 mesons. As a consequence, the Ît resolution function is Îz dependent, becoming worse at higher |Îz|. Neglecting this dependence introduces a small bias that BABAR accounts for in the systematic uncertainty. BABAR and Belle use binned maximum likelihood fits to the Ît and Îz distributions, respectively, of the selected dilepton candidates to extract Îmd . BABAR fits the shapes of the Ît distributions with the p.d.f.s for OS and SS dilepton events as given in Eq. (17.5.22). Belle fits the Îz distributions and constrains the integrated mixing probability to Ïd . Their Îz distributions are described by converting the constrained signal Ît distributions PÂ± (Ît) to Îz distributions using Eq. (17.5.12) and convolving them with the Îz resolution function. The constrained signal Ît distributions are given by eâ|ât|/ÏB0 [1 Â± cos(Îmd Ît)] , 4ÏB 0 eâ|ât|/ÏB+ Pch (Ît) = NÎ¥ (4S) fch b2ch Ç«ch (17.5.23) 2ÏB + PÂ± (Ît) = NÎ¥ (4S) f0 b20 Ç«Â±"
372,976,0.987,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"A similar analysis is given by Rogers (1976). The variation of %g with time results in fringe oscillations at the correlator output. The fringe frequency, .1=2&/d*12 =dt, is constant across the receiver bandwidth because the (instrumental) delay tracking removes the (geometric) delay-induced phase variation across the band. For the upper and lower sidebands, the rate of change of phase has opposite signs; note the term 2&$LO %g in Eqs. (9.16) and (9.17). See also Fig. 6.5 and the related discussion. In VLBI, the natural fringe frequency is fast enough that the fringes would be lost in the final averaging of the correlated data, so rotation of the phase to stop the fringes is applied before the correlator in Fig. 9.2. In a double-sideband system, if the fringes are stopped for one sideband, the fringe frequency is doubled for the other sideband. However, it is possible to obtain the data from each sideband by processing the data twice with appropriate fringe offsets each time. In VLBI, the source position and other parameters are not always known with sufficient accuracy when the observation is made, so in Fig. 9.2, the fringes are stopped after recovery of the data streams to permit trial of different fringe rotation rates. This involves applying a phase shift to the quantized signals at the correlator input or output (see Sect. 9.7.1). The effect on the cross-correlation function or the cross power spectrum can be described as multiplication by e""j2&$LO %g for the upper sideband and filtering to select the low-frequency term. This process results in a complex correlation function: .%/ D #$ exp"
311,760,0.987,The Physics of the B Factories,"dm2X m2n .(17.1.41) X = ÎE>Ecut E>Ecut Here, dÎ/dm2X is the diï¬erential width as a function of the mass squared of the hadronic system X. For both types, n is the order of the moment. For n > 1, the moments can also be deï¬ned relative to Eâ  and m2X , respectively, in which case they are called central moments. The OPE cannot be expected to converge in regions of phase space where the momentum of the ï¬nal hadronic state is O(ÎQCD ) and where perturbation theory has singularities. This is because what actually controls the expansion is not mb but the energy release, which is O(ÎQCD ) in those cases. The OPE is therefore valid only for sufï¬ciently inclusive measurements and in general cannot describe diï¬erential distributions. The lepton energy moments can be measured very precisely, while the hadronic mass moments are directly sensitive to higher dimensional matrix elements such as Î¼2Ï and Ï3D . In most cases, one has to take into account an experimental lower threshold on the lepton momentum. The leptonic and hadronic moments give information on the quark masses and on the non-perturbative OPE matrix elements, while the total rate allows for the extraction of |Vcb |. The reliability of the inclusive method rests on our ability to control the higher order contributions in the double series and to constrain quark-hadron duality violation, i.e. eï¬ects beyond the OPE, which exist but are expected"
217,924,0.987,Finite Difference Computing With Pdes : a Modern Software Approach,"Let jj  jj be the standard Euclidean vector norm. Four termination criteria are much in use:  Absolute change in solution: jju  u jj  u  Relative change in solution: jju  u jj  u jju0 jj, where u0 denotes the start value of u in the iteration  Absolute residual: jjF .u/jj  r  Relative residual: jjF .u/jj  r jjF .u0 /jj To prevent divergent iterations to run forever, one terminates the iterations when the current number of iterations k exceeds a maximum value kmax . The relative criteria are most used since they are not sensitive to the characteristic size of u. Nevertheless, the relative criteria can be misleading when the initial start value for the iteration is very close to the solution, since an unnecessary reduction in the error measure is enforced. In such cases the absolute criteria work better. It is common to combine the absolute and relative measures of the size of the residual, as in jjF .u/jj  rr jjF .u0 /jj C ra ; (5.21)"
151,315,0.987,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"The Granular Pesticide Avian Risk Assessment Model (GranPARAM) was used to estimate exposure and fate of birds as a result of consuming pesticide granules in CPY-treated agricultural fields. The model as originally described (Moore et al. 2010b, c) has been updated for this assessment. GranPARAM simulates the grit ingestion behavior of individual birds and determines how many pesticide granules and the associated dose each bird ingests during the 24-h period immediately following CPY application. Each bird in a GranPARAM simulation is assumed to be actively foraging for grit in and around the agricultural field to which CPY has been applied. The scheme that GranPARAM follows to model granule ingestion behavior is depicted in Fig. 5. In GranPARAM, each bird is randomly assigned a daily grit intake rate from a large database of grit counts for the species being considered and estimated grit retention rate. This step defines the number of medium- and coarse-sized particles (i.e., particles in the same size range as Lorsban 15G granules) that the individual ingests during the peak day of the simulation. For CPY, the peak day was assumed to be the 24 h immediately following application. The work of Stafford et al. (1996) and Stafford and Best (1997) showed that most pesticide granules are incorporated into soil, and thus unavailable to birds, within 1-d of application. Rainfall accelerates this process (Stafford and Best 1997). GranPARAM relies on estimates of granule counts on the soil surface immediately after application, which clearly represents the maximum possible exposure for birds (Solomon et al. 2001). Each site of application of the granular formulation is randomly assigned a soil texture (e.g., Silt-Loam) with a probability equal to the occurrence of that texture fraction in the crop-capable acreage in the region of interest. The database in the model was originally for corn, but has been expanded to include other crops and areas to which CPY is applied. Once the soil texture category is assigned, the application site is then randomly assigned a specific soil particle size profile (% of soil mass represented by various particle size categories) from a large soils database of measurements. This step defines the levels of medium- and coarse-sized sand particles available as natural grit. For each exposure scenario (Table 3), the method of application, rate of application, incorporation efficiency, bird species, region of interest, and other aspects of the analysis included in the simulation were defined (Fig. 5). The rate of application of CPY defines the relative numbers of medium- and coarse-sized granules applied. The method of application (e.g., in-furrow, band, broadcast) determines the spatial placement of these granules and the number available as a source of particles to birds. The choice of bird species determines the number of particles ingested."
79,40,0.987,Cosmic Ray Neutron Sensing : Estimation of Agricultural Crop Biomass Water Equivalent,"Locate the areas of interest in which the aforementioned polygons overlap. This is easiest to do by overlaying a basic satellite image of any particular study area. Once the appropriate areas have been located, use the âIdentifyâ button (ArcGIS) or similar function if using any other image processing software, to identify the GrWDRVI value of each polygon that overlaps the area of interest. These numbers now can be averaged to determine the mean index value for each study site. The reflective behavior of plant material changes over the course of a growing season (see Sect. 2.1). This means that the linear relationship between biomass and the GrWDRVI changes from the beginning and peak of the growing season (denoted as âGreen-Upâ in this publication), to the end of the growing season (denoted as âSenescenceâ in this publication) [2â4]. Calculation of biomass must be done with separate equations to reflect the differences in each relationship. Additionally, GrWDRVI values below 0.25 are not to be used due to the fact that the biomass is too small during these growth stages and the satellite data cannot accurately predict plant biomass. Note: the equations are given here as derived from 11 years of observation from Nebraska, USA [2]: Maize Biomass Green - Up"
311,3058,0.987,The Physics of the B Factories,"Figure 25.2.3. Constraints on the charged Higgs mass in the Type II 2HDM from the B Factory measurements of B(B â Ï Î½) and B(B â Xs Î³). The x-axis represents the present experimental value of B(B â Ï Î½) normalized to the SM prediction (see text for details). The vertical yellow regions are excluded by the current world average experimental value, B(B â Ï Î½)exp = (1.15 Â± 0.23) Ã 10â6 , at 95% C.L., while the 1Ï, 2Ï, 3Ï errors on the same experimental value are denoted by the dotted, dashed, and solid lines, respectively. The horizontal yellow region is excluded by the B(B â Xs Î³) measurement (see Eq. 25.2.9). The grey and the black lines correspond to the predictions of the Type II 2HDM given in Eq. (25.2.12), respectively, with labels denoting the diï¬erent values of tan Î²."
372,1185,0.987,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"The nominal range of R is â2 to 2. R D 2 makes S very small with respect to w so that the weighting approaches natural weighting, whereas R D !2 makes S large with respect to w so that the weighting approaches the uniform weighting. R D 0 produces an rms that is midway between the values for R D !2 and 2. R is called the robustness factor because as it increases, the image is more immune to errors in calibration or errors due to radio frequency interference, because the effect of a bad point in a cell with few data points is deemphasized as R increases. An example of how the synthesized beamwidth and rms noise vary with R is shown in Fig. 10.3. In the vicinity of R D 0, which is the normal default value, the beamwidth and rms noise are most sensitive to changes in R. For the example shown in Fig. 10.3, the beamwidth increases by 5%, and the rms noise decreases by 45% as R increases from â0.5 to 0.5. For inhomogeneous arrays such as those used in VLBI, the gain in sensitivity can increase markedly for little increase in beamwidth."
311,1266,0.987,The Physics of the B Factories,"lutions appear in the fits for reasons analogous to the ÏÏ and ÏÏ constraints on Ï2 . The first of the two solutions quoted for BABAR is compatible with the results of Standard Model based fits to the Unitarity Triangle to be presented in Section 25.1. In Belle the solution most compatible with the results of Standard Model based fits is the last quoted one. In BABAR a MC technique is used to estimate a probability region for the bound on |ÎÏ2 |. The CP -averaged rates and CP asymmetry parameters used in estimating the bounds are generated according to the experimental distributions. The input values of branching fractions are those presented in Section 17.4.5.5 while CP asymmetry parameters are from this section (Eq. 17.7.26). For the decay constants the following values are used: fÏ = (130.4 Â± 0.2) MeV (Amsler et al., 2008), fK = (155.5 Â± 0.9) MeV (Amsler et al., 2008), fa1 = (203 Â± 18) MeV (Cheng and Yang, 2007), and fK1 = 207 MeV (Bloch, Kalinovsky, Roberts, and Schmidt, 1999). For fK1 an uncertainty of 20 MeV is assumed. For the constant Î» the value 0.23 (Amsler et al., 2008) is used. For each set of generated values, the bound on |ÎÏ2 | is evaluated. The limits on |ÎÏ2 | are obtained by counting the fraction of bounds within a given value and the results are |ÎÏ2 | < 11â¦ (13â¦ ) at 68% (90%) probability (Aubert, 2010d). Combining the solution near 90â¦ , consistent with the results of global CKM fits, with the bound on |Ï2 âÏeff 2 | we measure the weak phase Ï2 = (79 Â± 7 Â± 11)â¦ . This solution is in agreement with the value of Ï2 found in the analyses of B â ÏÏ, B â ÏÏ, and B â ÏÏ decays. This measurement is currently limited by statistics and a substantial improvement of its precision may come from a future super flavor factory."
311,1760,0.987,The Physics of the B Factories,"example, Swanson (2006) found 0.06â0.29. Braaten and Lu (2008) subsequently described the R < 0.1 prediction as the result of a âconceptual errorâ and found that in the molecular model R could be studied only together with the lineshapes of X(3872) decays. As no significant separation is seen between the masses of the peaks in the K + Ï + Ï â J/Ï and KS0 Ï + Ï â J/Ï final states, we assume in what follows that both are due to the decay of a single X(3872) state. In the mass and width measurements presented below, the results from K + Ï + Ï â J/Ï dominate. Belle observed the decay X(3872) â Dâ0 D0 in the 0 0 0 Ï D D final state at the higher mass M = (3875.2 Â± Â± 0.8) MeV/c2 (Gokhroo, 2006; the final error re0.7â1.6 flects the then-current uncertainty in the D0 mass). Subsequent analyses by both BABAR (Aubert, 2008bd) and Belle (Aushev, 2010) confirmed the observation, also adding the Î³D0 D0 final state, and finding a mass of M = (3873.8 Â± 0.5) MeV/c2 if the two results are averaged. As this is significantly larger than the value observed in the discovery mode Ï + Ï â J/Ï (see below) there has been some speculation that Dâ0 D0 and Ï + Ï â J/Ï are produced by the decay of two distinct parent particles (see for example the discussion in Aubert, 2008bd). While this is possible a priori, there are two related problems with using this model to interpret the data: â Expected lineshape: In a decay X(3872) â Dâ0 D0 the Dâ0 will in general be oï¬-shell, because of the proximity of the Dâ0 D0 threshold. The eï¬ect on the decays is pronounced if the X is below threshold, and study of the Ï 0 D0 D0 and Î³D0 D0 lineshapes (which can have a complicated structure in general) is required to distinguish between an X state which is below threshold and an above-threshold âvirtual stateâ (see for exam-"
365,157,0.987,Climate Smart Agriculture : Building Resilience To Climate Change,"flow). In order to keep the equation as simple as possible, yet robust, the regression is based on one variable and tested in two basins of very different climatologyâs, topographyâs, land use patterns and annual water supply cycles. An important consideration between the gauge and BWI values is a lagged relationship between water accumulating near the surface and detected downstream at the gauge. The lag between the water input upstream and the detection of changes in flow downtstream is based on numerous empirical observations and theory that flow models are more accurate when they include the prior month(s) due to the time lapse for the water accumulate into the major stem of the river (Demirel et al. 2013). The number of prior months used in the predictions of flows is directly related to the size of the basin, the influence of snow melt and its topography. Therefore, a lagged term is included in Equation 2, where Qm(BWI) is the discharge at a station for month m While n is the number of previous month(s) averaged together with the concurrent month BWI value. Qm ( BWI ) = g ( d )"
314,234,0.987,Building a Resilient and Sustainable Agriculture in Sub-Saharan Africa,"where Xi is the set of variables belonging to the three dimensions of vulnerability apart from land tenure variables, Yi is a variable reflecting land tenure security (the percentage of crop land which is owned by the farmers themselves), Î²0, Î²1, and Î²2 are the vectors of the coefficients to be estimated, and Ïi + Î³it is the error term. All the variables used cannot be included in the regression for the sake of degree of freedom. Therefore, relevant regressors are chosen among the variables used to build vulnerability index through stepwise analyses. Panel specification tests are run to select the appropriate model (Baltagi 2008). Land tenure security is expected to negatively and significantly influence vulnerability to climate shocks. It should be noted that the variable capturing land tenure may be endogenous. Therefore, this chapter accounts for this likely endogeneity and use as instruments the departments in which the communities belong. Indeed, land tenure may vary with respect to the geographic settings. Moreover, every model has to be tested for sensitivity and uncertainty. A Monte Carlo analysis (Metropolis and Ulam 1949) is performed to assess the uncertainty within the vulnerability index calculation model. Monte Carlo method calculates new results by relying on repetitive random sampling (Metropolis and Ulam 1949). The sensitivity of the vulnerability indicator to any variability in the input dataset is investigated through the change and omission of certain indicators."
311,1244,0.987,The Physics of the B Factories,"Measurements of the polarization fraction and the fraction of ÏÏÏ non-resonant events were performed by Belle +0.034 in (Somov, 2006) and found to be 0.941â0.040 Â± 0.030 and (6.3 Â± 6.7)%, respectively. The latest Belle measurements of the CP asymmetry parameters are based on a data sample of 535 million BB pairs (Somov, 2007). The analysis is organized into two steps. During the first step the yields of signal and background components are obtained using an unbinned extended ML fit to the three-dimensional (mES , ÎE, R) distribution. A total of 176843 events are selected for the analysis. The fit yields NÏÏ+ÏÏÏ = 576 Â± 53 events. During the second step the CP asymmetry"
12,157,0.987,Self-Assembled Molecules â New Kind of Protein Ligands : Supramolecular Ligands,"Figure 5.12 shows a hypothetical mechanism of multimolecular fibril generation. The status of Î²-structural fragments (as shown in Fig. 5.9) suggests the possible propagation of local maxima and local minima of hydrophobicity in contrast to expected distribution for these fragments. As it is shown in Fig. 5.12. the approach of two units representing similar characteristics is able to make possible propagation of linear hydrophobicity/hydrophilicity propagation. The red fragments on Fig. 5.12 are those shown in Fig. 5.9. According to 3D presentation the fragment 53â60 (distinguished as pink) is expected to fit its structure to the partner from the next unit (domain). Loose N-terminal fragments, devoid of hydrophobic stability (as confirmed by molecular dynamics simulations and experimental studies [21, 26]), may align with one another, creating a new Î²-interface especially due to its localization on the edge of the domain. The crystal structure of the V-V dimer does not correspond to the actual conformation of these domains in an amyloidfibril. Certain structural changes are expected in the N-terminal fragment (Fig. 5.6), but also in the domain as a whole (see [13] for a discussion of potential changes expected for fibril formation). In light of this fact, it is difficult to speculate about the final structure of the V-V amyloid â although conformational rearrangements proposed for transthyretin [13], converting its crystal structure into an amyloid, appear equally possible in the IgG V domain (as seen in 3BJL). The proposed supramolecular CR binding mechanism â one of many possible â is superficial in nature and does not require the dye to penetrate the amyloid. This explains why CR is able to adhere to amyloids formed by separate domains, as well as by identical Î²-structural fragments comprising a single domain. Due to the specific type of inter-chain interactions occurring in amyloids, intercalation of the dye is unlikely, and while CR may potentially dock in a suitable cavity (as proposed in"
84,614,0.987,Eye Tracking Methodology,"display (this may take another 33 ms for a system with update rate of 30 frames per second). Loschky and McConkie (2000) conducted an experiment on a gaze-contingent display investigating spatial, resolutional, and temporal parameters affecting perception and performance. Two key issues addressed by the authors are the timing of GCDs and the detectability of the peripherally degraded component of the GCD. That is, how soon after the end of an eye movement does the window need to be updated in order to avoid disrupting processing, and is there a difference between the window sizes and peripheral degradation levels that are visually detectable and those that produce behavioral effects? In all experiments, monochromatic photographic scenes were used as stimuli with a circular, high-resolution window surrounded by a degraded peripheral region. An example of Loschky and McConkieâs GCD is shown in Fig. 24.7a. In one facet of the experiment, it was found that for an image change to go undetected, it must be started within 5 ms after the end of the eye movement. Detection likelihood rose quickly beyond that point. In another facet of the study concerning detection of peripheral degradation, results showed that the least peripheral degradation (inclusion of four of four possible levels) went undetected even at the smallest window size (2â¦ ), where the opposite was true with the highest level of degradation: it was quite detectable at even the largest window size (5â¦ ). The GCD was also evaluated in terms of performance effects, in the context of visual search and scene recall tasks. In the end it was found that the generation of an imperceptible GCD was quite difficult in comparison to the generation of a GCD that does not deteriorate performance. Although greater delays (e.g., 15 ms) and greater degradation (inclusion of only three of four possible levels) produce detectable visual artifacts, they appear to have minimal impact on performance of visual tasks when there is a 4.1â¦ high-resolution area centered at the point of gaze. Parkhurst et al. (2000) investigated behavioral effects of a two-region gazecontingent display. A central high-resolution region, varying from 1 to 15 degrees, was presented at the instantaneous center of gaze during a visual search task. An example of Parkhurst et al. display is shown in Fig. 24.7b. Measures of reaction time, accuracy, and fixation durations were obtained during a visual search task. The authorsâ primary finding is that reaction time and accuracy co-vary as a function of the central region size. The authors note this as a clear indicator of a strategic speed/accuracy tradeoff where participants favor speed in some conditions and accuracy in others. For small central region sizes, slow reaction times are accompanied by high accuracy. Conversely, for large central region sizes, fast reaction times are accompanied by low accuracy. A secondary finding indicated that fixation duration varies as a function of central region size. For small central region sizes, participants tend to spend more time examining each fixation than under normal viewing conditions. For large central regions, fixation durations tend to be closer to normal. In agreement with reaction time and accuracy, fixation duration is approximately normal (comparable to that seen for uniform resolution displays) with a central region size of 5â¦ . For screen-based VR rendering the work of Watson et al. (1997) is particularly relevant. The authors studied the effects of Level Of Detail (LOD) peripheral"
285,726,0.987,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","The main goal of this chapter is to introduce a new way of thinking about pitch coding, grounded in CNS physiology. If there is a robust representation of pitch in the dominant ISI distribution in the AN (Cariani and Delgutte 1996a, 1996b), and if some neurons convert ISI directly into a corresponding firing rate, then it seems possible that the dominant ISI interval is coded as predominant firing rate. Evidently, the scheme proposed here is incomplete. Questions arise how and where a butte profile would be read out; how such a representation would mesh with the spectral representation needed for F0 above ~ 500 Hz; how phase-invariant this representation would be; etc. We conclude with some interrelated issues. An important issue is the effective bandwidth of central neurons. Several CN neuron types integrate over wide frequency regions (Godfrey et al. 1975; Winter and Palmer 1995): partials that are resolved at the level of the AN may be unresolved in these CN populations. The autocorrelogram-like display in the dominant ISI hypothesis sums across frequency channels (Cariani and Delgutte 1996a, 1996b): for such an operation a wide bandwidth would be beneficial. Another issue is whether there is a specific physiological subset of neurons or even a separate brainstem nucleus which codes periodicity via entracking. Entracking is observed in a diversity of structures and neuron types, suggesting a distributed mechanism, but this does not exclude the existence of a brainstem âpitch centerâ specialized in this form of encoding. For CN neurons showing entracking, convergence of multiple inputs from the AN is obviously required; the degree of entracking in responses beyond the CN suggests that there are multiple stages of such convergence. A strong form of the butte hypothesis is based on perfect entracking; a weaker form only requires a monotonic relationship between firing rate and pitch-related period without attaining equality. One of the most critical issues is phase invariance, which we see"
311,995,0.987,The Physics of the B Factories,"does not wholly eliminate the correlations. Systematic uncertainties that are unique to the Dalitz Plot are: the asymmetries in the background; limited statistics from the sidebands used to form the continuum histograms (if histograms are used); the mass rejection regions; diï¬erences in the continuum shape between the sideband and the signal region; and charge bias introduced either by the detector response or the selection criteria. A model dependent error derived from performing fits with an alternative set of resonances is sometimes quoted either in quadrature with the systematic error or its own. As with quasi-twobody modes, an important systematic is associated with uncertainty on the parameters that are fixed in the fit. If a resonance is deemed to be significant, the mass and width may still not be well known. Rather than float the mass and width, a series of fits can be performed with the mass and width fixed at diï¬erent values and the change in the likelihood used as a guide to the best values. Even so, it may be necessary to modify a model after unblinding, particularly to remove resonances that are not significant. 17.4.7 Three-body and Dalitz decays Approximately seven B 0 and eleven B Â± Dalitz Plots have been investigated by BABAR and Belle. It is impossible to do justice to the wealth of information available. Decays involving three pions, particularly B â ÏÏ, are important for the measurement of Ï2 and are considered in Chapter 17.6. Decays with an Î·, Î· â² , Ï, f0 (980), or K â in the final three-body state are itemized in the tables and figures of this section but are not described in detail. Instead, this section concentrates on modes with one or more kaons in the final state. B meson decays to three-body final states B â Khh proceed predominantly via b â u tree-level diagrams (T and C diagrams in Fig. 17.4.1) and b â s(d) penguin diagrams (P in Fig. 17.4.1). The other diagrams can contribute but are expected to be much smaller. Final states with an odd number of kaons (s-quarks) are expected to proceed dominantly via b â s penguin transitions as the b â u transition is color-suppressed. If there are two kaons, the decay proceeds through the color-allowed b â u tree diagram and the b â d penguin decay with no b â s penguin contribution. As a result, these Dalitz decays provide an excellent opportunity to understand the relative contribution of tree and penguin amplitudes in charmless decays. This is shown in Fig. 17.4.18 where the extracted values of sin 2Ï1 in b â s penguin transitions are compared to b â ccs decays. Table 17.4.10 summarizes the reported branching fractions and asymmetries. In many cases, no resonances have been found in a Dalitz Plot and so consequently it has only been possible to give a branching fraction (or upper limit) and a CP asymmetry for the whole Dalitz Plot. Figure 17.4.19 shows the relative values of the reported branching fractions so far measured."
86,305,0.987,Nuclear Back-End and Transmutation Technology For Waste Disposal : Beyond The Fukushima Accident,"In Eq. (17.18), the term containing âR is called the direct term; the second term is the number density term, which represents the effect of the change of nuclide number densities caused by cross-section changes; the third term shows the effect of the change of flux from to cross-section changes; the fourth term shows the effect of the change of adjoint flux caused by cross-section changes, and the last term shows the effect of constant power production even when there are cross-section changes. The adjoint number density N* is calculated from the end of a burn-up period to the beginning of the period, and the generalized flux and generalized adjoint flux are calculated at each burn-up step. The adjoint number density N* is not continuous but has a discontinuity at each burn-up step. To calculate true burn-up sensitivities S relative to infinite-dilution cross sections, we introduce e S to Eq. (17.14) to obtain S."
110,489,0.987,Finance for Food : Towards New Agricultural and Rural Finance,"A value chain perspective is also recommended when assessing a projectâs poverty impacts since there are possible spillover effects across the supply chain (for example, direct and indirect labor effects). The impact on vulnerable populations could also be considered (such as impacts on womenâs employment and childrenâs education). A list of potential variables that could be included to develop a poverty scorecard is presented in Table 1. Methodology When assessing a projectâs potential impact on the poor, complexity can arise in terms of weighting the outcomes. It is possible that some enterprises/projects perform better in terms of geographical targeting, but do not do well in terms of their potential impact on gender. Consequently, we propose the use of the statistical method of Principal Components to determine the weights for the different outcomes (variables) considered. Principal Component analysis is a statistical technique that creates new variables that are linear combinations of the original variables. The new variables are referred to as the âprincipal componentsâ and are uncorrelated (orthogonal) to each other. The number of principal components generated is equal to the number of original variables. The first principal component accounts for most of the variation in the data, the second principal component accounts for most of the variance that has not been accounted for by the first principal component, and so forth. Generally, one or two principal components are needed to account for more than half of the variation in the data. As a rule of thumb for project poverty scoring, we suggest using all first principal components necessary to account for at least half of the data variation. Recall that each component is a weighted sum of the variables considered to measure poverty reduction. Thus, higher values for a component denote a higher poverty impact, whereas lower values denote a lower poverty impact. Steps to Construct a Poverty Scorecard Given a set of projects that meet a sustainability threshold, the steps to construct a poverty score can be summarized as follows: 1. Collect all the necessary information from the projects to construct the geographic, employment, and spillover indicators (variables) over which the projects will be evaluated. 2. Normalize the indicators by subtracting the mean and dividing by the standard deviation. 3. Obtain the covariance matrix of the normalized indicators. ( X i  X )(Yi  Y ) (n  1) i 1"
32,317,0.987,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","14.3 Population Distribution in Japan How to divide space is critical when examining populationâs size distribution. Dividing space by municipal level is standard for investigating the size distributions of cities. In this study we do not use such spatial division method. We adopted square blocks of the same size as a spatial division method and divided a particular region into identical sized square lattices. Then we aggregated the population inside the square blocks and observed its population distribution. We can control the spatial divisionâs scale using this method. We use parameter BS [km], which denotes the size of one side of the square blocks. Figure 14.2 shows a complementary cumulative distribution function (CCDF) PrfX  xg"
372,832,0.987,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"of levels increases. However, this is for the case of an ideal rectangular passband, which in a practical receiving system may be closely approximated. Figure 8.11 shows the quantization efficiency (Q as a function of the threshold spacing *. If the constant voltage spacing between adjacent thresholds for both input and output values is not maintained, the individual levels can sometimes be adjusted to obtain an improvement in (Q of a few tenths of a percent, decreasing with increasing number of levels. The values of (Q in Table 8.2 are in agreement with results by Jenet and Anderson (1998), who give detailed calculations of performance for twoto eight-bit quantization, for both uniform and nonuniform threshold spacing. See also Appendix 8.3 for optimization in the case of four-level quantization. In recent designs of radio telescopes, the level increment * is frequently chosen so that signals at levels much higher than the rms system noise can be accommodated within the range of levels of the quantizer. This preserves an essentially linear response to interfering signals so that they can be eliminated or mitigated by further processing. For example, with 256 levels (8-bit representation) and * D 0:5, we find that (Q D 0:9796. The range of Ë128 levels then corresponds to Ë64#, i.e., Ë36 dB above the system noise, for a % 2% sacrifice in signal-to-noise ratio."
232,476,0.987,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"heavy-metal volume fraction (HMVF). For the heterogeneous systems, the VVF is given by b/d, representing the averaged fracture volume fraction, or the fracture porosity in rock. For the homogeneous system, VVF represents the void space fraction that is ï¬lled with water and heavy metal precipitations, equivalent to the porosity of a porous rock. The HMVF is deï¬ned in a similar way, representing the volume fraction of heavy metal precipitations in the entire core. The volume fraction of the solid-phase of the rock then equals to (1-VVF), and the water volume fraction is given by (VVF-HMVF). By deï¬nition, the HMVF must be smaller than VVF, because the volume of precipitation cannot exceed the available void space in the rock. Two types of host rocks are considered in the present study: average sandstone and magnetite-hematite-bearing pelitic gneiss containing 15% iron. For the heterogeneous systems, the fracture aperture takes values of 0.1, 0.2, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, and 10.0 cm. For given compositions and geometry for rock and heavy metal, calculations have been ï¬rst performed for various VVF and HMVF parameters, assuming that the mass of heavy metal in the core is 250 MT, which is the total mass of the damaged fuels form three reactor cores. The discrete keff results have been used to generate a keff contour plot by interpolation. By deï¬ning a nominal sub-criticality criterion keff < 0.98, the super-critical region can be determined in the parametric space. Within the super-critical parameter range, MCNP calculations have been conducted to obtain the critical mass of heavy metal deposition. More detailed descriptions about the model parameters can be found in [2]."
311,1886,0.987,The Physics of the B Factories,"18.4.6.7 Exclusive Î¥ (1S) and Î¥ (2S) decays into light hadrons A number of channels in Ï decays have been studied, most of which satisfy predictions about their properties to within experimental errors. One example of a property which does not conform to expectation arises from the comparison of Ï decays into vector-pseudoscalar (VP) and vector-tensor (VT) final states: ÏÏ, K â KÌ, Ïa2 (1320) and Ïf2 (1270). The rates of decay to these final states deviate from expectations, such as those implies by the â12% ruleâ (Section 18.4.6.1). It is interesting, therefore, to see if similar patterns of deviation occur in the bottomonium system by studying similar final states of Î¥ decay. Although 82% of the Î¥ (1S) and 59% of the Î¥ (2S) decays are expected to be light-hadron final states, little experimental information exists on exclusive decays of the Î¥ resonances below the BB threshold. This situation is very diï¬erent in charmonium sector, where numerous channels have been measured and used to perform model tests. The Belle Collaboration published first observations of exclusive, light-hadron final states of the Î¥ (1S) and Î¥ (2S) (Shen, 2012). A large number of final states were studied and the key results are summarized in Table 18.4.10. The measurements are mostly consistent with the prediction from pQCD (Section 18.4.6.1), Q21 = 0.77 Â± 0.07. The one measured mode that demonstrates a deviation from the prediction is ÏÏ + Ï â , which is consistent with the prediction at the level of 2.6Ï. For the final states measured so far, the predictions from pQCD appear to be reliable within the experimental uncertainties."
175,644,0.987,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","Research has demonstrated the potential advantages of âindex floodâ procedures (Lettenmaier et al. 1987; Stedinger and Lu 1995; Hosking and Wallis 1997; Madsen and Rosbjerg 1997a). The idea behind the index-flood approach is to use the data from many hydrologically âsimilarâ basins to estimate a dimensionless flood distribution (Wallis 1980). Thus this method âsubstitutes space for timeâ using regional information to compensate for having relatively short records at each site. The concept underlying the index-flood method is that the distributions of floods at different sites in a âregionâ are the same except for a scale or index-flood parameter that Ã°nrÃ LÃ°hjr; n; x1 ; . . .; xr Ã Â¼ FÃ°TjhÃ f Ã°x1 jhÃf Ã°x2 jhÃ . . . f Ã°xr jhÃ reflects the size, rainfall, and runoff characterisÃ°6:104Ã tics of each watershed. Research is revealing"
241,541,0.986,Second Assessment of Climate Change for the Baltic Sea Basin,"southern region is not uniform and displays a clear gradient in the north-easterly direction (Fig. 9.7, lower right panel). The spatial pattern in long-term trends in sea level should, in general, also reflect the spatially varying ï¬ngerprints of sea level change due to recent mass changes in the polar ice sheets and, accordingly, the change in the gravity ï¬eld of the Earthâs surface (Tamisiea et al. 2003; Milne et al. 2009). As demonstrated by Mitrovica et al. (2001), the pattern caused by the melting of the Greenland Ice Sheet is negligible for the Baltic Sea region, as its zero-line intersects this region, and the remaining variations are below the accuracy of the derived RSL rates, ranging between 0.1 and 0.3 mm yearâ1. On the other hand, the pattern caused by the melting of the Antarctic Ice Sheet does affect the Baltic Sea region, but can be expected to be nearly constant over the entire region (see also Chap. 14). There exists, however, considerable uncertainty about the sign of the mass balance of the Antarctic Ice Sheet in the twentieth century, due to the competing effects of melting, calving and precipitation. Richter et al. (2011) also analysed the variation in the annual mean RSL at long tide gauge records, such as the Polish tide gauges Swinoujscie and Kolobrzeg (Fig. 9.7, upper right panel). Both time series show consistent behaviour with a slight negative trend throughout the ï¬rst decades of the time series up until 1860, followed by an increasing trend of around 1 mm yearâ1. The authors suggested, as a possible explanation for this trend, climatic effects related to the Little Ice Age. This is consistent with Ekman (2009 and references therein) for the GIA-corrected ASL trend of 1.01 mm yearâ1 for the Stockholm time series. (Figure 9.7 includes, for illustration, the RSL Stockholm series that displays a strong negative trend due to the GIA). However, it should be remembered that due to decadal"
360,360,0.986,Compositionality and Concepts in Linguistics and Psychology,"Here the partial-split situation and âsmallerâ situations are of low acceptability as with walk and write. However, the CT point is the joint situation, where plausibly, both distributivity and the WALK& SING concept attain maximal acceptability. In between these two points, the function increases monotonically. This function for the complex concept DIST WALK& SING is described in Fig. 9b."
151,91,0.986,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"frequency of detection. The key issue in this context is not one of presence/absence, because CPY and CPYO can be monitored in air at concentrations as little as 0.001 ng mâ3, which are much less than thresholds for adverse effects. Risk depends on the magnitude of concentrations, especially in media where organisms might be exposed and thus are potentially at risk. It can be difficult to assimilate ranges in concentrations in the atmosphere and the variety of concentration units of differing magnitudes in sampled media. Accordingly, here, the feasibility was assessed of compiling a more readily comprehendible depiction of multi-media environmental concentrations by expressing the concentrations as ranges and converting concentrations in various media to fugacities. Fugacity is essentially partial pressure and can be deduced for all media and compared directly, without difficulties introduced by the use of different concentration units for individual compartments of the environment. Using fugacity as a synoptic descriptor of concentrations in the ecosystem has been applied previously to multi-media concentrations of organochlorines in the Great Lakes (Clark et al. 1988). It is, of course, possible to calculate multimedia equilibrium concentrations using partition coefficients directly, rather than using fugacity as an intermediate, but the equilibrium status of two phases with units such as ng mâ3 in air and mg kgâ1 in vegetation may not be obvious. Ideally, to demonstrate directly the trend of decreasing concentrations, the data should be plotted as a function of distance from source, but because sources are often uncertain and concentrations vary with time as a function of transformation of the material at the location of release, this is rarely possible. The approach adopted here was to compile a distribution of reported concentrations to gain perspective on the range in magnitude of concentrations at various distances from points of release, at least for ecosystems for which sufficient monitoring data have been compiled. Accordingly, Table 4 depicts the distribution of reported concentrations for air, rain, snow, water bodies, soils, sediments, and biota on a decade scale. In some cases, products of transformation are included and in others they were specifically excluded. Some of the data were reported graphically or as ranges, so numerical values were sometimes difficult to establish. Locations for which information was available varied geographically and often lacked information on current and recent meteorology such as wind speed, temperature, and precipitation. Some values reported for each concentration range are approximate because reports gave only"
393,270,0.986,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 2,"Linear model Linear models derived from regression model use weighted observable and nonrandom quantities to infer a value for an unknown quantity. Sometimes is the sum of these values extended with the random variable that express errors. Expected value for that variable is 0. TFIDF-based model TFIDF expresses the importance of the words to the document. Weighted terms from a document are represented as a vector of weights. Typically the similarity between compared documents is calculated as an angle between vectors. The cosine function is used to evaluate it. Markov model Markov model is based on probability distribution. Any state is represented by its probability. Probability of the state is propagated from one state to the other one, i.e. next event depends only on the probability of previously observed events. Special kind of Markov model is Hidden Markov model where states are hidden. Neural networks The structure of the networks, non-linear thresholds and the weights of the edges between the nodes make them capable in combination with content-based filtering to represent user's characteristics. Then, the neural network can be learned where the nodes represent the source in that the user is interesting and edges represent strength of the association among sources. Classification Classification divides objects described with attributes into the classes. Objects that are in the same class have some common traits or their attributes are close. On the other hand objects with different attributes are located in variant classes. Rule induction The method for the rule induction consists of learning sets of the rules. The rules are inducted by using variable techniques upon learning set. Several representations of rules are possible, e.g. direct representation, decision trees, representation in terms of conditional probabilities."
244,565,0.986,"Advancing Human Assessment : The Methodological, Psychological and Policy Contributions of Ets","seven in mathematics (one base-year form; three alternative forms in each followup), five in reading (two alternative forms per follow-up), and three each in science and history/citizenship/geography (one form per round). The version of the multiplegroup PARSCALE used at that time only saved the subpopulation means and standard deviations and not the individual expected a posteriori (EAP) scores. The individual EAP scores, which are the means of their posterior distributions of the latent variable, were obtained from the NAEP B-group conditioning program, which uses the Gaussian quadrature procedure. This variation is virtually equivalent to conditioning (e.g., see Mislevy et al. 1992, as well as Barone and Beaton, Chap. 8, and Kirsch et al., Chap. 9, in this volume) on a set of dummy variables defining from which ability subpopulation an individual comes. In summary, this procedure finds the item parameters that maximize the likelihood function across all groups (forms and grades) simultaneously. The items can be put on the same vertical scale because of the linking items that are common to different forms across years, or adjacent forms within year. Using the performance on the common items, the subgroup means can be located along the vertical scale. Individual ability scores are not estimated in the item parameter estimation step; only the subgroup means and variances are estimated. Next, NAEPâs B-group program was used to estimate the individual ability scores as the mean of an individualâs posterior distribution. (A detailed technical description of this procedure may be found in Rock et al. 1995). Checks on the goodness of fit of the IRT model to the observed data were then carried out. Item traces were inspected to ensure a good fit throughout the ability range. More importantly, estimated proportions correct by item by grade were also estimated in order to ensure that the IRT model was both reproducing the item P-plus values and that there was no particular bias in favor of any particular grade. Since the item parameters were estimated using a model that maximizes the goodness-of-fit across the subpopulations, including grades, one would not expect much difference here. When the differences were summed across all items for each test, the maximum discrepancy between observed and estimated proportion correct for the whole test was .7 of a scale score point for Grade 12 mathematics, whose score scale had a range of 0 to 81. The IRT estimates tended to slightly underestimate the observed proportions. However, no systematic bias was found for any particular grade."
372,844,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"To determine %.!/ as a function of the correlation coefficient %, we need to consider the probabilities of occurrence of the unquantized variables x and y within each quantization interval. First, consider the case in which the number of quantization intervals is even and equal to 2N. Thus, there are N positive intervals plus N negative ones. The mean value of the products of pairs of the quantized values, hOxyO i, is obtained by considering each of the 2N & 2N D 4N 2 possible pairings of the levels of xO and yO . Only half of these need be calculated, since if the x and y values are interchanged, the probability remains the same. The probability of the unquantized variables x and y falling within any pair of intervals is given by integration of the Gaussian bivariate probability distribution, Eq. (8.1), over the corresponding range of x and y. In Eq. (8.1), x and y have variance # and cross-correlation coefficient %. Here, we are concerned with samples of x and y taken at the Nyquist interval !s , and n is the number of Nyquist intervals between the pairs of samples considered. For a rectangular passband of width &"", the correlation coefficient is given by %.n!s / D"
228,333,0.986,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"10.2 Ordered Fuzzy Candlesticks Generally, in our approach, a fixed time interval of financial high frequency data is identified with Ordered Fuzzy Numbers and it is called ordered Fuzzy Candlestick (OFC). The general idea is presented in Fig. 10.1. Note that the orientation of the OFN shows whether the ordered Fuzzy Candlestick is long or short. Information about movements in the price are contained in the shape of the f and g functions. In this sense, functions f and g do not depend directly on the variable tick but depend on the relationship between the parameters A and B. In the following sections the details of constructing the ordered Fuzzy Candlestick are presented. Previous works listed two cases of construction of ordered Fuzzy Candlesticks. The first assumes that the functions f and g are functions of predetermined type; moreover, the shapes of these functions should depend on two parameters (e.g.,"
311,2913,0.986,The Physics of the B Factories,"in the measured cross sections is about 15% less than predicted, but this could be due to issues with the flavor dependence. Similar results are obtained for the Î· meson from ALEPH and L3 data at 91 GeV (Adriani et al., 1992; Barate et al., 2000), and compared with the Jetset predictions. Again, other data and models give the same conclusions. The flavor dependence is smaller, and the discrepancy at the Z 0 is larger than for K Â± , perhaps indicating a failure of the models. For protons, also shown in Fig. 24.1.4, the Jetset model is tested with one parameter value changed, the probability for a given string break to produce a diquarkantidiquark, rather than quark-antiquark, pair, from 0.1 to 0.085, which provides a good description of the higherenergy data. Here, the simulated high-xp scaling violation between 10.54 and 34 GeV is about the same as for the pions, but that between 34 and 91 GeV is slightly larger since fast protons are expected to be produced predominantly in uu and dd events. The prediction for 10.54 GeV rises well above the BABAR data, exceeding it by as much as a factor of 4.5 at z =0.9. Similar behavior is seen for Jetset with default parameters, HERWIG, and UCLA at high xp . This indicates that we do not understand the scaling properties of protons, or perhaps of baryons or heavier hadrons in general. These data can be used to test the predictions of MLLA QCD combined with the ansatz of LPHD (Azimov, Dokshitzer, Khoze, and â Troyan, 1985), by transforming to the variable Î¾ = ln( s/2pâ ). This representation emphasizes the low momentum region (large Î¾). It is predicted that: the Î¾ distribution would be approximately Gaussian over a range of â¼ 1 unit around itâs peak position Î¾ â ; a distorted Gaussian should describe the distribution over a wider range; Î¾ â should decrease â exponentially with hadron mass at a given CMâenergy s and Î¾ â should increase logarithmically with s for a given hadron. Conventionally, Î¾ â is found by fitting a Gaussian distribution to the data over sets of points within 0.5â1 units of the approximate peak position. Next, the widest roughly symmetric range about this position is found in which a Gaussian fit gives a good Ï2 , and this range is then extended as far as possible in one direction. Results of such fits and the ranges are listed in Table 24.1.1. Acceptable fits were found over ranges at least 1 unit wide, consistent with the prediction. Table 24.1.1. Results of the Gaussian and distorted Gaussian (where a skewness term and a kurtosis term are added) ï¬ts to the Î¾ distributions. The ï¬t ranges and the peak positions Î¾ â are reported (Muller, 2004). Particle"
311,511,0.986,The Physics of the B Factories,"where n is the selected component of a model consisting of nc components (e.g. signal and one or more backgrounds). In this expression the indices j, k run over the nc model components, Fj is the p.d.f. for component j in the observables y, Nk is the expected number of events for the k th component, and Vnj is the inverse of the covariance matrix Vnj in these yield parameters. The matrix Vnj is obtained from the data, either through a numeric summation over the per-event contributions using Eq. (11.1.10), or from HESSE following a maximum likelihood ï¬t to the data. Note that s W eights can be negative, as Vnj is not positive deï¬nite. The predicted distribution for any component j in observable x is given by the histogram of events in x where each event contributes with a weight s Pn (y). An example is shown in Figure 11.2.4, where for a 3dimensional model in observables mES , ÎE, F, the p.d.f. in mES for signal and background are compared with the s Plots in this observable, calculated using s W eights that use exclusively the data and the model prediction in observables ÎE, F. In this example the data was simulated and has been sampled from the model itself and perfect agreement is observed between the p.d.f. and the s Plot prediction. When applied on samples of observed data, discrepancies between the s Plot and the direct model prediction may occur, which may be indicative of disagreements between data and model."
307,342,0.986,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"In Table 10.2, we compare the statistical properties of the solutions based on different theoretical blockers. We see that the mutation significantly increases the open probability but leaves the expected value of the transmembrane potential more or less unchanged. The standard deviation, however, is significantly reduced by the mutation. Both the open and closed state blockers are able to significantly reduce the effect of the mutations, as illustrated in Fig. 10.3. However, the closed state blocker is slightly better at this than the optimal open state blocker."
372,1615,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"are uncorrelated, as described in Sect. 13.1.6. dout is nominally the scale size of clouds, a few kilometers. However, some correlation remains out to the scale size of weather systems and beyond. The structure function is formally an ensemble average. For practical purposes, the turbulent eddies are assumed to remain fixed as the atmospheric layer moves across an array. This is the frozen-screen hypothesis, sometimes attributed to Taylor (1938). Practically, the rms fluctuations in phase increase with time up to the cross time tc D d=vs , where vs is the wind speed parallel to the baseline direction corresponding to d. tc is called the corner time, beyond which the rms fluctuations flatten out and D+ .d/ can be estimated. Atmospheric fluctuations on scales larger than d cover both receiving elements and do not contribute to the structure function. An example of the structure function as a function of time measured at the ALMA site by the 300-m satellite site-testing interferometer is shown in Fig. 13.12. tc & 20 s, implying a wind speed of about 15 m s!1 ."
311,941,0.986,The Physics of the B Factories,"thrust axis, of particles outside a 45 cone around the B thrust axis, divided by the scalar sum of their momenta (Jen, 2006); the polar angles of the B meson momentum vector and the B meson thrust axis with respect to the beam axis; the angle between the B meson thrust axis and the thrust axis of the rest of the event; and the ratio of the second- and zeroth-order momentum-weighted polynomial moments of the energy flow around the B meson thrust axis (Aubert, 2004a). Although not strictly event shape variables, some success has been achieved by using two additional inputs to the neural network: the flavor of the other B meson as reported by a multivariate tagging algorithm (Aubert, 2005i); and the boostcorrected proper-time diï¬erence between the decay vertices of the two B mesons divided by its error. The multivariate discriminant can be trained with Monte Carlo (MC) simulation for the signal, and qq continuum MC, oï¬-resonance data or sideband data for the background. The discriminant can sometimes be used as a selection criterion as well as a p.d.f. as a simple cut on the output can eliminate a substantial part of the background (of the order of 20%-40%) with little signal loss. Instead of using the tagging information in the event-shape, Belle have sometimes used the B meson flavor tagging output (Kakuno, 2004) to calculate a figure of merit; signal retention of greater than 60% with background rejection greater than 90% has been achieved (Jen, 2006). B meson decays to charm have large branching fractions and final states that are either the same as the mode under consideration or easily mis-reconstructed. These charm backgrounds can be suppressed by reconstructing the charm candidate from combinations of tracks and applying a veto around the nominal mass (typically â¼ 40 MeV/c2 for the D meson). The helicity distribution is an important variable that can be used to identify particles of a particular spin, extract the longitudinal polarization fL , or simply as a selection criterion. The helicity angle Î¸H of the resonance is defined as the angle between the momentum vector of one of the resonanceâs daughter particles and the direction opposite to the B meson momentum in the resonance rest frame (Kramer and Palmer (1992)). The choice of daughter must be consistent from event to event (either based on charge or flavor) and care must be taken to avoid any unexpected ordering in momentum or azimuthal angle introduced by the track finding algorithms. It is often necessary to limit the range of the helicity angle. At values of | cos Î¸H | > 0.9 the signal reconstruction eï¬ciency starts to fall oï¬, as one of the daughter tracks of the resonance has a low momentum. At the same time, backgrounds created from combinations of tracks start to increase. If the resonance decays to particles of diï¬ering mass, then the momentum selection criteria on the daughter particles will cause the cos Î¸H distribution to be skewed, requiring careful compensation for the change in eï¬ciency. The allowed range of cos Î¸H is mode dependent but typically events are rejected if cos Î¸H is greater than 0.7 â 0.9, with diï¬erent ranges for negative and positive cos Î¸H . In Vector-Vector (VV) decays, the longitudi-"
295,411,0.986,interface oral Health Science 2016 : innovative Research On BiosisâAbiosis intelligent interface,"from the Ti and Mn powders, which contain 0.16 and 0.77 mass% O, respectively. The C content of the alloys is of approximately 0.06 mass%. This is likely due to C pickup during the debinding process [23]. Furthermore, Mn evaporation is also observed to occur during fabrication by MIM, as the Mn content of the alloys is lower than the nominal content [16]. Figures 19.9 and 19.10 show the optical micrographs and XRD profiles of the Ti-(8â17)MnMIM, respectively. Small closed pores, large interconnected pores, and elongated precipitates can be observed in each alloy. Both the pores (ellipses) and precipitates (arrows) are mostly located at the grain boundaries. Only the diffraction peaks of the attributed to Î² planes are observable in the XRD profiles of Ti-(8â17) MnMIM. Again, the presence of the athermal Ï phase has been confirmed by the TEM observations. Figure 19.11 shows the SAED patterns and DF images of the diffraction spots or streaks of the Ï phase of Ti-(8â17)MnMIM. Clear diffraction spots produced by the athermal Ï phase can be observed in the SAED patterns of"
231,209,0.986,North Sea Region Climate Change Assessment,"Situated in northern central Europe, the North Sea exhibits large climate variability with inflow of a wide range of air masses from arctic to subtropical. For this reason, it is difï¬cult to differentiate between natural and externally forced variability, despite large amounts of historical data. This chapter examines past and present studies of variability and changes in atmospheric variables over the instrumental period; roughly the last 200 years. Research areas lacking consensus in the scientiï¬c community are highlighted to stimulate further research. The main driver of atmospheric variability in the North Sea region is the North Atlantic Oscillation (NAO). Despite its apparent long-term irregularity, the NAO exhibits extended periods of positive or negative index values. No consensus exists with respect to the size of the fraction of interannual NAO variance that cannot be explained by random forcing and is therefore probably influenced by external forcing. Slowly varying natural factors with an effect on European climate, such as the Atlantic Multidecadal Oscillation (AMO), may superimpose long-term trends on atmospheric variability and so be difï¬cult to distinguish from the anthropogenic climate change signal. The source of atmospheric and surface data influences the results obtained, even in the comparatively data-rich North Sea region. Based on reanalysis data, several studies ï¬nd positive trends in storm activity over the North Sea region and a northeast shift in storm tracks over the past few decades. However, studies based on direct or indirect historical records of long-term variations in pressure, wind or wind-related proxies, mostly do not identify robust long-term trends. This counter-intuitive result is explained by uncertainties in the long-term historical wind and atmospheric pressure observations, and additional uncertainties arising from the lack of quality control when digitising old data as well as potential biases in the reanalyses due to the fact that the underlying amount of available data is not constant in time. Nevertheless, the northeast shift in storm tracks appears to be a new phenomenon. In contrast, the increase in wind speed and storminess in the latter half of the 20th century does not seem to be unprecedented within the context of historical observations. There are indications of an increase in the number of deep cyclones (but not in the total number of cyclones). There are also indications that the persistence of circulation types has increased over the past century. Temperatures have increased both over land and over the North Sea. There is a distinct signal in the number of frost days and the number of summer days. While there is a clear winter and spring warming signal over the Baltic Sea region, this is not as clear for the North Sea region. As expected, the"
175,895,0.986,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","particular set of goals or objectives will be considered in the future and the relative importance of each of them. Rather than contrasting âknowledgeâ uncertainty versus natural variability versus decision uncertainty, one can classify uncertainty in another way based on speciï¬c sources of uncertainty, such as those listed below, and address ways of identifying and dealing with each source of uncertainty. Informational Uncertainties: â¢ imprecision in specifying the boundary and initial conditions that impact the output variable values â¢ imprecision in measuring observed output variable values Model Uncertainties: â¢ uncertain model structure and parameter values â¢ variability of observed input and output values over a region smaller than the spatial scale of the model"
238,82,0.986,Nanoinformatics,"In this chapter, a machine learning-based selective sampling procedure for PES evaluation is introduced and applied to proton conduction in BaZrO3 to demonstrate its efï¬cacy. The region of interest governing the ionic conduction is deï¬ned in the two ways: (1) a low-PE region and (2) a low-FN region. For the low-PE region, the performance of the selective sampling based on the GP model greatly depends on the descriptors. Employing the preliminary PES (prePES) is signiï¬cantly effective, which is evaluated by single-point DFT computations in a smaller supercell. The GP3(xyz + prePES) sampling requires 394 DFT computations to sample all the low-PE grid points (353 points) in a grid with 1768 points for the asymmetric unit of BaZrO3 crystal. This is a 78% reduction in the computational costs. However, the deï¬ned region of interest, i.e., the low-PE region, comprises 20% of the conï¬guration space. Consequently, the reducible computational cost is limited to 80%. The region of interest should, therefore, be redeï¬ned as it becomes smaller in the conï¬guration space. For the low-FN region, the region of interest contains only 15"
311,2732,0.986,The Physics of the B Factories,"In order to avoid any possible bias and to firmly establish this observation, the BABAR analysis blinds the enhancement region [4.2, 4.4] GeV/c2 , and optimizes the âseAn important exotic charmonium candidate, the Y (4260), lection criteria by maximizing the quantity N/(3/2 + B) has been observed in the Ï + Ï â J/Ï final state. The discov- (Punzi, 2003b), where 3/2 corresponds to the search for a ery and subsequent studies are reported in Sections 21.5.1.1 3Ï signal, N is the total number of Î³ISR Ï(2S), Ï(2S) â and 21.5.1.2 respectively. The claimed broad structure Ï + Ï â J/Ï candidates in the 20 MeV/c2 Ï + Ï â J/Ï mass Y (4008) is discussed in the latter section. range that brackets the Ï(2S) nominal mass, and B is the number of (background) events in the Ï + Ï â J/Ï mass regions [3.8, 4.2] GeV/c2 and [4.4, 4.8] GeV/c2 , scaled to the width of the excluded region. The selection criteria are op21.5.1.1 The Y (4260) discovery timized taking advantage of the features of ISR emission, The discovery by Belle of the surprisingly narrow X(3872) that is, a recoil mass close to zero, and a small transverse + â resonance from the study of B â J/Ï Ï + Ï â K decays component of the visible momentum in the e e CM, including photon reconstructed. Exactly (Choi, 2003), discussed in Section 18.3.2, renewed experi+ â mental interest in charmonium spectroscopy. In order to four tracks consistent with production at the e e interunderstand the X(3872) when its quantum numbers were action point are allowed: two oppositely charged tracks hardly known in 2004, BABAR searched for X(3872) â identified as pions, and a pair of identified leptons (ei+ â + â Ï + Ï â J/Ï in the ISR process e+ eâ â Î³ISR Ï + Ï â J/Ï , where ther e e or Î¼ Î¼ ) whose reconstructed invariant mass + â J/Ï decays to â â , using a data sample corresponding to is within an optimized interval around the J/Ï peak. Ad232 fbâ1 (Aubert, 2005y). The analysis was performed re- ditional cuts on kinematic variables of the hadronic system quiring exclusive reconstruction of the hadronic final state, are applied to further reject background sources. but not explicit detection of the ISR photon. The large and In order to improve the mass resolution, the four tracks clean ISR Ï(2S) â Ï + Ï â J/Ï sample provides a good are refitted with a constraint to a common vertex, and the control sample for validation and selection criteria opti- lepton pair kinematically constrained to the J/Ï mass. mization. In a subsample of 123 fbâ1 of data, as shown in The resulting Ï + Ï â J/Ï mass-resolution function is wellFig. 21.5.1, no evidence for the X(3872) was found, but described by a Breit-Wigner distribution with a full width an enhancement was seen around 4.3 GeV/c2 . at half maximum increasing from 4.2 MeV/c2 at the Ï(2S) 21.5.1 Y family states in ISR Ï + Ï â J/Ï"
297,1511,0.986,The R Book,"r The response is bounded (by 1 above and by 0 below). r By calculating the percentage, we lose information on the size of the sample, n, from which the proportion was estimated. R carries out weighted regression, using the individual sample sizes as weights, and the logit link function to ensure linearity. There are some kinds of proportion data, such as percentage cover, which are best analysed using conventional linear models (assuming normal errors and constant variance) following arcsine transâ formation. The response variable, y, measured in radians, is sinâ1 0.01 Ã p, where p is percentage cover. If, however, the response variable takes the form of a percentage change in some continuous measurement (such as the percentage change in weight on receiving a particular diet), then rather than arcsine-transforming the data, it is usually better treated by either"
103,34,0.986,Solar Particle Radiation Storms Forecasting and Analysis : The Hesperia Horizon 2020 Project and Beyond (Volume 444.0),"of the (Tylka and Lee 2006) model for the case in which the injection of coronal seed ions at Q-Perp shocks is suppressed. The energy dependence of the normalized Fe/O ratio i.e. (Fe/O)n is shown for different values of the impulsive suprathermal fraction R in the seed population. In the Q-Par shock event (Fe/O)n 1 at lower energies (E < 2 MeV/nucleon), while (Fe/O)n monotonically decreases with increasing E. In contrast, in the Q-Perp shock (Fe/O)n is between 1 and 8 at lower energies, depending on the impulsive suprathermal fractions. With increasing energy the normalized ratio exhibits a complex variation e.g. approaching a plateau or reaching a minimum and further increasing afterwards. Tylka et al. (2005) hence assumed that the high-energy Fe/O ratio could be used as a crude proxy for shock geometry, with Fe-poor and Fe-rich events corresponding to Q-Par and Q-Perp shock geometries, respectively. It should be noted that these explanations have not taken into account the IP transport effect, which could further distort the Fe/O ratio that emerged from the CME-shock acceleration process (e.g. Tylka et al. 2013). Recently, (Tan et al. 2017) examined 29 large SEP events with peak proton intensity Jpp (>60 MeV) > 1 pfu during solar cycle 23. The emphasis of their examination was put on a joint analysis of the Ne/O and Fe/O data in the 3â40 MeV/nucleon energy range as covered by the Wind/LEMT and ACE/SIS sensors in order to differentiate between the Fe-poor and Fe-rich events at higher energies that emerged from the CME-driven shock acceleration process, after correcting the IP transport effect. One of the main findings of this work is presented in Fig. 1.10 in which the plot of the source plasma temperature T as very recently reported by Reames (2016) versus the normalized Ne/O ratio i.e. (Ne/O)n at E D 30 MeV/nucleon is shown. T is well correlated with (Ne/O)n with the linear correlation coefficient (CC) D 0.82. Therefore, the (Ne/O)n value at high energies should be a proxy of the injection energy in the shock acceleration process, and hence the shock â¢Bn according to the models of Tylka and Lee (2006) and Schwadron et al. (2015)."
365,534,0.986,Climate Smart Agriculture : Building Resilience To Climate Change,"The AgMIP RIA methods are designed to assess vulnerability of farm households to climate change. We define a climate as a probability distribution of weather events that occur at a specific place and during a defined period of time. A change in climate is a change in the probability distribution of weather events. These changes are often described in terms of the mean temperature over a period of time such as a day, month or year, but can also be changes in temperature extremes, the variability of weather events, and other aspects such as rainfall amount and intensity and wind velocity. Impacts of climate change are quantified as gains and losses in economic wellbeing (e.g., farm income or per capita income) or other metrics of well-being (e.g., changes in health or environmental quality). In this framework, some or all individuals may gain or lose from a change, and we say the losers are vulnerable to loss from climate change. The AgMIP RIA methodology is designed to quantify the proportion of the population that are losers, as well as the magnitude of loss. It is important to note, however, that in a heterogeneous population there are typically some gainers and some losers, and thus the net impact may be positive or negative. The AgMIP RIA method is designed to quantify climate vulnerability by modeling a heterogeneous population of farm households rather than modeling a ârepresentativeâ or average or typical farm. This approach begins with the representation of impacts on the farm household using the concept of economic gains and losses (other metrics of impact can be also be used depending on available data, e.g., the impact on health of household members). As Fig. 3 shows, the AgMIP RIA approach uses a statistical representation of the farming system in a heterogeneous region or population to quantify the distribution of gains and losses, e.g., due to climate change. Figure 4 illustrates this idea with two loss distributions. The area under the distribution on the positive side of zero is the proportion of losers and is the measure"
372,739,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"canceled. For example, if the second phase switching is done at the central location, the timing of the second switching should be delayed relative to the first one by %tr . 3. Both the first and second switchings in any signal path should be delayed by the geometric delay of the corresponding antenna %g so that, at the correlator input, the switching transitions in the unwanted components are aligned in time from one antenna to another. The delay %g varies with time as the antennas track a source. Requirement 2 above is concerned only with the relative accuracy of switchings within the same signal path from one antenna to the correlator. This is the simplest case because it is concerned only with offsets in two switchings of the same Walsh function. Consider the effect of a small time offset Ä± in the relative timing of the first and second switchings. For each transition, the timing difference causes the correlator output voltage to be reversed for a period Ä± and thereby cancels an equivalent interval of the unreversed output. Hence, for each transition, there is an effective loss of signal for a period 2Ä±. The average fractional loss of sensitivity is 2nt Ä±=%tb , where nt is the number of transitions within the time base % tb (i.e., twice the Walsh sequency). Thus, for a tolerable limit of, e.g., 1% correlation loss, the tolerable value of Ä± can be determined for any given time base and maximum sequency used. Since the correlation loss is proportional to nt , use of the lowest sequencies within the Walsh set helps to minimize loss in sensitivity. For arrays in which the numbers of antennas and the baseline lengths are not too long, the delaying of the switchings by %g (as noted in the third requirement above) can often be neglected. This introduces a timing error %g that is greatest for the longest baselines. The effect of this error can be minimized by using the lowest values of nt for the antennas for which the geometric delay is greatest. Requirements 1 and 3 are concerned with the relative timing of transitions at different antennas, i.e., between different Walsh functions. The effect of a timing offset on the rejection of the unwanted components depends on the loss in orthogonality of the Walsh functions used for different antennas. This is more complicated than the effect of an offset on two identical Walsh functions discussed above. The loss in orthogonality depends upon the sequencies of the two functions involved and is greatest for sequencies in the middle range of the Walsh set, as shown by Emerson (2005). Pairs consisting of a function with an even sequency and one with an odd sequency remain orthogonal in the presence of time shifts, but such combinations are possible for no more than half of the pairs in a complete Walsh set. Of the other pairs, some remain orthogonal with time offsets, as can be shown by numerical trials, and some do not [as shown in Fig. 3 of Emerson (2005)]. It is clearly beneficial to use equal numbers of odd and even sequencies in an array so that for approximately half of the antenna pairs, the orthogonality is independent of time offsets."
165,384,0.986,New Methods for Measuring and Analyzing Segregation,"S. Recall that D can take high values when S is low. Based on this, I classified outcomes on D and S into four categories. The first is a baseline category of âconcordantâ as occurs in prototypical segregation where displacement from even distribution is substantially polarized. The other three categories capture D exceeding S by increasingly large amounts. Holding D constant, distribution across the three categories of D-S discrepancy indicates variation in the extent to which displacement from uneven distribution is dispersed and produces lower levels of group separation and neighborhood polarization. I then estimated the regression of the White-Minority difference on exposure to neighborhood poverty on D and the three categories of D-S discrepancy. The multiple R-square for the regression was 0.502 compared to 0.417 when using D alone. This indicates that knowing that D is discordant from S added to the ability to predict the White-Minority difference in exposure to neighborhood poverty over what could be predicted from knowledge of D alone. As expected, the pattern of the effects indicated that when D was high in relation to S, the White-Minority difference in exposure to neighborhood poverty was lower (all effects were statistically significant at p < 0.001 ). The impact of the largest D-S discrepancy category was â4.3 which is clearly large in relation to the value of 6.9 for interquartile range of 6.9 for the dependent variable. I obtained similar results for the regression predicting the White-Minority difference on neighborhood income rank. The multiple R-square for the regression using D and the three categories of D-S discrepancy as predictors was 0.483 compared to 0.383 when using D alone. The results indicated that knowing that D was high in relation to S added to the ability to predict White-Minority difference in exposure to neighborhood income rank over what could be predicted from knowledge of D alone (all effects statistically significant at p < 0.001 ). As expected, discrepant categories had lower levels of White-Black inequality on income rank and the impact of the largest D-S discrepancy category was â4.0 which is clearly large when compared to the value of the interquartile range of 5.8 for the dependent variable. I next estimated parallel regressions where S and categories of D-S discrepancy were used to predict White-Minority disadvantage in exposure to poverty and neighborhood income rank. The results were different and quite revealing. For the regression of the White-Minority difference on exposure to neighborhood poverty the multiple R-square for the regression was 0.529 compared to 0.512 when using S alone. This signals that knowing D was high relative to S increased the ability to predict the White-Minority difference in exposure to neighborhood poverty by only a small amount over what could be predicted from knowledge of S alone. The coefficients for the three categories of discrepancy were all statistically significant (all at p < 0.001 ) but impacts were more modest than in the parallel analysis focusing on D as the largest effect here was 1.9 which was less than half the size of the largest effect of â4.3 seen in the parallel analysis focusing on D. I found similar results for the regression predicting the White-Minority difference on neighborhood income rank. The multiple R-square for the regression using S and the three categories of D-S discrepancy as predictors was 0.508 compared to 0.494"
311,986,0.986,The Physics of the B Factories,"ing of the amplitude properties is required, for instance the spin, the scattering amplitude can be expressed in terms of partial-wave amplitudes. The drawback of the S-matrix formalism is that it is not unitary and as a result the sum of the amplitudes of the resonances in the Dalitz Plot can be greater or less than the inclusive Dalitz Plot amplitude depending on whether the overall interference is constructive or destructive. The individual branching fractions are therefore often reported as fit fractions (FF), defined as the integral of a single amplitude squared divided by the coherent matrix element squared for the whole Dalitz Plot (Section 13.4.1). An alternative parameterization uses the K-matrix formalism which is unitary by construction but has a drawback that the masses and widths can be different to the S-matrix results. The K-matrix formalism is more commonly used in Dalitz Plot analyses of D meson decays (section 13.2.2). This is because many of the resonances in the D meson Dalitz Plot contain a large number of events and the S-matrix approximation of a BreitWigner or similar shape for the decay of the resonance is no longer adequate, especially when the resonances overlap in the Dalitz Plot. The selection criteria for three-body decays are very similar to that employed for quasi-two-body analyses. An obvious exception is that the B meson decay is treated as a decay to the three final state particles and no intermediate resonance vertex is formed when reconstructing the B meson. As the number of neutral final state particles increases the importance of any constraint from the beam spot on the B meson vertex position also increases. In quasi-two-body analyses, event shape variables and multivariate discriminants can be used to extract the signal yield because the reconstruction eï¬ciency is flat in the small volume of phase space under consideration. In Dalitz Plot analyses, this is no longer true and variables that depend on momentum vectors are correlated with position in phase space. Even variables like mES and ÎE need to be treated carefully. Some analyses deal with the problem using an elliptical selection region in (mES , ÎE). Others rotate (mES , ÎE) about a point to eliminate the linear correlation component. If the event-by-event resolution on"
311,1317,0.986,The Physics of the B Factories,"for 1065 dof. The values are large, but both experiments find that the main features of the Dalitz plot are well reproduced, with some significant but numerically small discrepancies at the peaks and dips of the distribution, which are used later to assign systematic uncertainties. BABAR has estimated that most of their excess in Ï2 /ndof , ÎÏ2 /ndof â 0.16, arises from imperfections in modeling experimental eï¬ects â mostly eï¬ciency variations at the boundaries of the Dalitz plot, and invariant mass resolution â rather than the amplitude model (Aubert, 2008l)."
285,36,0.986,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","FÃ¼llgrabe et al. (2015) also assessed sensitivity to ENV cues by measuring thresholds for detecting sinusoidal amplitude modulation imposed on a 4-kHz sinusoidal carrier. Modulation rates of 5, 30, 90, and 180 Hz were used to characterize the temporal-modulation-transfer function (TMTF). On average, thresholds (expressed as 20log10m, where m is the modulation index) were 2â2.5 dB higher (worse) for the older than for the younger subjects. However, the shapes of the TMTFs were similar for the two groups. This suggests that increasing age is associated with reduced efficiency in processing ENV information, but not with reduced temporal resolution for ENV cues. Schoof and Rosen (2014) found no significant difference in either processing efficiency or the shape of TMTFs measured with noise-band carriers between young (19â29 years) and older (60â72 years) subjects with near-normal audiograms. It is possible that, with noiseband carriers, amplitude-modulation detection is limited by the inherent fluctuations in the carrier (Dau et al. 1997), making it hard to measure the effects of age."
372,360,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"4.7.4 Matrix Formulation The description of polarimetry given above, using the ellipticity and orientation of the antenna response, is based on a physical model of the antenna and the electromagnetic wave, as in Eq. (4.29). Historically, studies of optical polarization have developed over a much longer period. A description of radio polarimetry following an approach originally developed in optics is given in Hamaker et al. (1996) and in more detail in four papers: Hamaker et al. (1996), Sault et al. (1996), Hamaker (2000), and Hamaker (2006). The mathematical analysis is largely in terms of matrix algebra, and in particular, it allows the responses of different elements of the signal path such as the atmosphere, the antennas, and the electronic system to be represented independently and then combined in the final solution. This approach is convenient for detailed analysis including effects of the atmosphere, ionosphere, In the matrix formulation, the electric fields of the polarized wave are represented by a two-component column vector. The effect of any linear system on the wave, or on the voltage waveforms of the signal after reception, can be represented by a 2 "" 2 matrix of the form shown below: "" # % &% & a 1 a 2 Ep (4.44) a 3 a 4 Eq where Ep and Eq represent the input polarization state (orthogonal linear or opposite circular) and Ep0 and Eq0 represent the outputs. The 2 "" 2 matrix in Eq. (4.44) is referred to as a Jones matrix (Jones 1941), and any simple linear operation on the wave can be represented by such a matrix. Jones matrices can represent a rotation of the wave relative to the antenna; the response of the antenna, including polarization leakage effects; or the amplification of the signals in the receiving system up to the correlator input. The combined effect of these operations is represented by the product of the corresponding Jones matrices, just as the effect on a scalar voltage can be represented by the product of gains and response factors for different stages of the receiving system. For a wave specified in terms of opposite circularly polarized components, Jones matrices for these operations can take the following forms: exp. j(/ Jrotation D exp.!j(/ 1 Dr Jleakage D D` 1 Gr 0 Jgain D 0 G`"
311,2803,0.986,The Physics of the B Factories,"e+ eâ â e+ eâ Î³ process in the specific kinematical configuration in which one of the final electrons moves along the collision axis while the other electron and photon are emitted at large angles. The VCS trigger selects events in which the detected electron plus photon system has a small transverse momentum and the recoil mass close to zero. For most of e+ eâ â e+ eâ Ï 0 events, the close photons from Ï 0 decay cannot be separated by the trigger cluster algorithm and are identified as a single photon. Therefore, the VCS trigger has relatively large eï¬ciency for events of the process under study (50â80%, depending on the Ï 0 energy). The second feature is the large QED background. The main background source is VCS. There is also a sizable background from the e+ eâ â e+ eâ Î³Î³ process in which one of the final electrons is soft and one of the photons is emitted along the beam axis. The photon from the QED process, together with a soft photon (from beam background, for example) may give the invariant mass close to the Ï 0 mass. Special selection criteria described in detail in Aubert (2009y) are applied to suppress QED background. After QED background suppression, the signal events are selected by the requirements that there are electron and Ï 0 candidates in an event with energies above 2.0 and 1.5 GeV, respectively, and that the electron plus Ï 0 system has a small transverse momentum and a recoil mass close to zero. To avoid systematic uncertainty due to possible data-simulation diï¬erences in detector response near the detector edges, the e+ eâ â e+ eâ Ï 0 cross section is measured in the region Q2 > 4 GeV2 , where the detection eï¬ciency for signal events is greater than 5%. The Q2 region from 4 to 40 GeV2 is divided into 17 intervals. For each interval, the number of signal events is determined from the fit to the two-photon invariant mass spectrum with a sum of a Ï 0 resolution function and a polynomial background distribution. For Q2 > 40 GeV2 , no evidence of a signal over background is found in the two-photon invariant mass distribution. The total number of events with a Ï 0 in the Q2 range 4â40 GeV2 is about 14000. Some events containing a Ï 0 may arise from background processes such as e+ eâ annihilation, vector-meson bremsstrahlung e+ eâ â e+ eâ V , and two-photon processes with higher multiplicity final states such as e+ eâ â e+ eâ Ï 0 Ï 0 . The e+ eâ annihilation background is estimated using the diï¬erence in the distributions of the eÂ± Ï 0 momentum z-component for signal and background. In twophoton events with a tagged positron (electron), the momentum z-component is negative (positive), while annihilation events are produced symmetrically. The annihilation background is assumed to be equal to the number events with the wrong sign of the eÂ± Ï 0 momentum zcomponent and is found to be negligible. The largest bremsstrahlung background is expected to arise from the process e+ eâ â e+ eâ Ï with Ï decaying to Ï 0 Î³. The background is estimated from the number of data events with an extra photon, in which the invariant mass of the Ï 0 Î³ system is close to the Ï mass. This background is also found to be negligible."
213,141,0.986,Collider Physics Within The Standard Model : a Primer,"2.7.2 The Final State in eC e Annihilation Experiments on eC e annihilation at high energy provide a remarkable opportunity for systematically testing the distinct signatures predicted by QCD for the structure of the final state averaged over a large number of events. Typical of asymptotic freedom is the hierarchy of configurations emerging as a consequence of the smallness of Ës .Q2 /. When all corrections of order Ës .Q2 / are neglected, one recovers the naive parton model prediction for the final state: almost collinear events with two back-to-back jets with limited transverse momentum and an angular distribution 1 C cos2  with respect to the beam axis (typical of spin 1/2 parton quarks, while scalar quarks would lead to a sin2  distribution). To order Ës .Q2 /, a tail of events is predicted to appear with large transverse momentum pT  Q=2 with respect to a suitably defined jet axis (for example, the thrust axis, see below). This small fraction of events with large pT consists mainly of three-jet events with almost planar topology. The skeleton of a three-jet event, to leading order in Ës .Q2 /, is formed by three hard partons qNqg, the third being a gluon emitted by a quark or antiquark line. To order Ës2 .Q2 /, a hard perturbative non-planar component starts to build up, and a small fraction of four-jet events qNqgg or qNqqNq appear, and so on. Event shape variables defined from the set of 4-momenta of final state particles are introduced to describe the topological structure of the final state energy flow in a quantitative manner [154]. The best known event shape variable is thrust (T) [192], defined as i jpi  nT j (2.78) T D max P i jpi j where the maximization is in terms of the axis defined by the unit vector nT : the thrust axis is the axis that maximizes the sum of the absolute values of the longitudinal momenta of the final state particles. The thrust T varies between 1/2, for a spherical event, to 1 for a collinear (2-jet) event. Event shape variables are important for QCD tests and measurements of Ës , and also for more practical purposes, like a laboratory for assessing the reliability of event simulation programmes and a tool for the separation of signals and background."
151,345,0.986,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"There are a number of bird species that frequent agroecosystems besides those included as focal species in this assessment (see Best and Murray 2003). The USEPA (USEPA 2005) used estimates of mortality for the combination of focal species and bird sensitivities in each modeled exposure scenario to approximate the cumulative distribution of outcomes for the complex of species using treated fields. The approach assumes that the focal species included in the modeling exercise are representative of the birds and their exposures occurring in the fields (USEPA 2005). This assumption is supported by the selection of focal species known to occur on the treated crops by actual survey (e.g., Best and Murray 2003). According to the USEPA (2005), the outcomes of the three modeled sensitivity assumptions (low, median and high sensitivity) âcan be viewed as a stratified sample from the population which estimates the limits and mid points of the cumulative risk distribution and therefore provides a reasonable approximation of the distribution.â The resulting cumulative distribution of acute risk for banded application on corn at the maximum application rate of 1.12 kg haâ1 (1 lb ai Aâ1) is shown in Fig. 8. Results of simulations using LiquidPARAM indicate that several species of birds, if highly sensitive, would experience up to approximately 30% mortality. Similar results were predicted for alfalfa, almond, apple/cherry, grape, grapefruit and soybean (Table 5). For orange, somewhat greater risk is expected in the bird community because this crop has the greatest application rate allowed on the Lorsban Advanced label (i.e., 6.27 kg haâ1 (5.6 lb ai Aâ1)) (Fig. 9). Although the results of the LiquidPARAM modeling indicated some acute risk to the most sensitive species for several crops listed on the Lorsban Advanced label (Table 5), the evidence from field studies that used the corresponding application rates (i.e., corn, grapefruit, and orange) indicate that flowable CPY poses little risk to birds (Dittrich and Staedtler 2010; Frey et al. 1994; Gallagher et al. 1994; Selbach and Wilkens 2008b; Wolf et al. 2010). Thus, it would appear that LiquidPARAM"
372,1642,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"For longer baselines, other power laws will be more appropriate. With the frozen-screen approximation, the power-law exponent can be determined from the power spectrum of the fluctuations. An example is shown in Fig. 13.17 (see also Bolton et al. 2011). Thus, in extrapolating D+ .d/ from a singlespacing measurement, one does not have to depend on the theoretical values of the exponent of d but can use the measurements of D+ .&/ to determine the range and variation [see Eq. (13.108) and Table 13.2]. For the example shown in Fig. 13.17, the power-law slope for frequencies above 0.01 Hz is 2.5, slightly below the value of 8/3 or 2.67 predicted for Kolmogorov turbulence. The spectrum flattens at frequencies below 0.01 Hz because of the filtering effect of the interferometer. Fluctuations larger than the baseline, 100 m in this case, cause little phase effect. For the corner frequency fc D vs =d, the wind speed along the baseline direction can be inferred to be about 1 m s!1 . Table 13.4 shows a compilation of the measurements of the structure function referred to a baseline of 100 m. The range of values reported for a fiducial baseline"
372,862,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"8.4.3 Quantization Levels and Data Processing At this point, it is useful to put into perspective the characteristics of quantization schemes, which are summarized in Tables 8.2 and 8.3. It should be remembered that the assumption % # 1 was used in determining these values. In considering the relative advantages of different quantization schemes, we note first that both the quantization efficiency (Q and the receiving bandwidth &"" may be limited by size and speed of the correlator system. The overall sensitivity is proportional to (Q &"". Consider two conditions. In the first, the observing bandwidth is limited by factors other than the capacity of the digital system. This can occur in spectral line observing or when the interference-free band is of limited width. The sensitivity limitation imposed by the correlator system then involves only the quantization efficiency (Q in Table 8.2, and the choice of quantization scheme is one between"
187,183,0.986,Managing The Complexity of Critical infrastructures : a Modelling and Simulation Approach (Volume 90.0),"The degree distribution P(k) is introduced deï¬ned as the (relative or absolute) frequency of nodes of degree k. According to this property a graph can be classiï¬ed as regular, random or scale free. Figure 13 shows the difference between the node degree distribution of random and scale-free graphs. In Fig. 14 two examples of graphs are depicted (Fig. 15). The functional form of P(k) contains relevant information on the nature of the network under study. It has widely shown that ârealâ spontaneously-grown networks (i.e. grown with no external design or supervision) tend to show a power-law decaying P(k). In this type of networks (named âscale-freeâ networks), loosely connected nodes (leaves) and highly connected ones (hubs) co-exist. Scale-free networks are known to exhibit a high level of robustness against random faults of their elements, while showing a large vulnerability related to the removal of speciï¬c components: hub removals induce dramatic impacts on the graph connectivity. âRandomâ graphs, in turn, are those whose P(k) has a poissonian proï¬le. The ârandom graphâ approximation, although being used to map most of ârealâ networks, has been discovered to represent very few real systems [15]. Different statistical indices may be introduced to describe the degree distribution. For instance it is possible to compute the range of the node degrees using the"
8,785,0.986,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","particles (quark bags) only. Thus one may view [7â9] the hadronic gas phase as being an assembly of many different hadronic resonances, their number in the interval .m2 ; m2 C dm2 / being given by the mass spectrum .m2 ; b/dm2 . Here the baryon number b is the only discrete quantum number to be considered at present. All bagâbag interaction is contained in the mutual transmutations from one state to another. Thus the gas phase has the characteristic of an infinite component ideal gas phase of extended objects. The quark bags having a finite size force us to formulate the theory of an extended, though otherwise ideal multicomponent gas. It is a straightforward exercise, carried through in the beginning of the next section, to reduce the grand partition function Z to an expression in terms of the mass spectrum .m2 ; b/. In principle, an experimental form of .m2 ; b/ could then be used as an input. However, the more natural way is to introduce the statistical bootstrap model [7], which will provide us with a theoretical that is consistent with assumptions and approximations made in determining Z. In the statistical bootstrap, the essential step consists in the realization that a composite state of many quark bags is in itself an âelementaryâ bag [1, 10]. This leads directly to a nonlinear integral equation for . The ideas of the statistical bootstrap have found a very successful application in the description of hadronic reactions [11] over the past decade. The present work is an extension [1, 9, 12] and application [1, 13] of this method to the case of a system containing any number of finite size hadronic clusters with their baryon numbers adding up to some fixed number. Among the most successful predictions of the statistical bootstrap, we record here the derivation of the limiting hadronic temperature and the exponential growth of the mass spectrum. We see that the theoretical description of the two hadronic phasesâthe individual hadron gas and the quark-gluon plasmaâis consistent with observations and with the present knowledge of elementary particles. What remains is the study of the possible phase transition between those phases as well as its observation. Unfortunately, we can argue that in the study of temperatures and mean transverse momenta of pions and nucleons produced in nuclear collisions, practically all information about the hot and dense phase of the collision is lost, as most of the emitted particles originate in the cooler and more dilute hadronic gas phase of matter. In order to obtain reliable information on quark matter, we must presumably perform more specific experiments. We will briefly point out that the presence of numerous s quarks in the quark plasma suggest, as a characteristic experiment, the observation Î hyperons. We close this report by showing that, in nuclear collisions, unlike pp reactions, we can use equilibrium thermodynamics in a large volume to compute the yield of strange and anti-strange particles. The latter, e.g., Î, might be significantly different from what one expects in pp collisions and give a hint about the properties of the quark-gluon phase."
311,2134,0.986,The Physics of the B Factories,"for diï¬erences between D0 and D0 , and CP asymmetries are measured. The decay rates in Eqs (19.2.11) and (19.2.12) depend also on the CP violating parameter q/p (cf. Chapter 10). In general, CP violation in the SM for processes involving charmed hadrons is expected to be tiny. This is conveniently seen in the parameterization of the CKM matrix given in Eq. (16.4.4). CP violation arises from the phase in the CKM matrix, and the elements of the matrix related to the first two generations of quarks, which appear in the charmed hadron processes, are almost real. Hence the magnitude of the CP violating eï¬ect is expected to be small. As an example, consider the Cabibbo suppressed decay D0 â Ï + Ï â , shown in Fig. 19.2.4. The relevant CKM phase entering the ratio of amplitudes for this decay and its charge conjugate is"
12,131,0.986,Self-Assembled Molecules â New Kind of Protein Ligands : Supramolecular Ligands,"Here, RD < 0.5 indicates the presence of a hydrophobic core. Figure 5.2 presents a graphical representation of RD values, restricted (for simplicity) to a one dimensional form. DKL (as well as O|T, O|R and RD) may be calculated for specific structural units (complex, single molecule, single chain, selected domain). In such cases the bounding ellipsoid is restricted to the selected fragment of the protein. It is also possible to determine the status of polypeptide chain fragments within the context of a given ellipsoid. This procedure requires prior normalization of Oi, Ti and Ri values belonging to the analyzed fragment. The procedure described above will be consistently applied in the analysis presented in this chapter. The status of selected polypeptide chain fragment will be studied to evaluate their participation in forming a hydrophobic core. In particular, secondary folds which satisfy RD < 0.5 are thought to contribute to the moleculewide hydrophobic core. When the opposite is true (i.e. RD > 0.5), the given fragment can be considered unstable. It appears that fragments which exhibit higher-than-expected hydrophobicity may, when exposed on the surface, be engaged in protein complexation (forming parts of the interface). Calculations concerning fragments of the polypeptide chain requires prior normalization of Ti, Oi and Ri values belonging to the selected fragment. The results tell us whether the given fragment contributes to the molecule-wide hydrophobic core."
241,581,0.986,Second Assessment of Climate Change for the Baltic Sea Basin,"9.5.3.2 Spatio-Temporal Variations Several observation sites reveal short-term (weekly) features in the wave activity that reappear regularly, for example, a relatively calm period at the end of December and beginning of January in the northern Baltic Sea (Soomere et al. 2011). These features are probably site-speciï¬c and only persist for a few decades (Soomere et al. 2012). The extensive seasonal variation in wind speed (Mietus 1998) causes substantial variability (by a factor of two in coastal areas and up to three in offshore regions) in wave height at monthly scales (Schmager et al. 2008; Soomere and RÃ¤Ã¤met 2011) (Fig. 9.17). The calmest months are April to July and the windiest October to January (see also Chap. 4). The most extensive interannual and decadal variations in wave properties exist in the visually observed wave data (Fig. 9.18). The appearance and spatial coherence of such variations has undergone major change. Short-term (interannual and up to 3 years) variability in annual mean wave height displays a consistent pattern (with a typical spatial scale of more than 500 km) from the southern Baltic Proper to the eastern Gulf of Finland from the mid-1950s to the mid-1980s. The coherence is lost in the mid-1980s (Soomere"
302,159,0.986,Freshwater Microplastics : Emerging Environmental Contaminants?,"Size is another parameter usually measured for microplastics, but no unified criteria are currently available. Different size classes were reported by different authors, which make it difficult to compare the data from different works [50]. Due to the restriction of the sampling methods used, usually only microplastics >0.333 mm (mesh size of the manta trawl net) are assessed in neustonic samples collected by trawling. Smaller microplastics can be examined for sediment and biota samples as density separation combined with filtration is used. Whereas the examination of microplastics <0.05 mm will get increasingly difficult, advanced instruments such as Raman microscopy, micro-Fourier transform infrared spectroscopy (Î¼-FTIR), or scanning electron microscope (SEM) with energy dispersive spectroscopy (EDS) should be used [36]. Generally, microplastic abundance increases with decreasing size [51â53]. In lake Hovsgol, 0.355â0.999, 1.00â4.749, and >4.75 mm size classes accounted for 41, 40, and 19% of the total plastics, respectively [30]. In the freshwaters of Wuhan, 0.05â0.5, 0.5â1, and 1â2 mm size classes together accounted for over 80% of the total microplastics, and 0.05â0.5 mm microplastics were the most abundant in most of the studied waters [32]. In Yangtze Estuary, 0.5â1, 1â2.5, 2.5â5, and >5 mm size classes made up 67, 28.4, 4.4, and 0.2% of the total plastics, respectively [33]. In the estuary of Minjiang, Oujiang, and Jiaojiang, the smallest size class (0.5â1.0 mm) was also found the most abundant followed by the 1.0â2.0 mm size class, and these two size classes together accounted for over 70% of the total plastics [34]. However, among the four size classes (0.112â0.3, 0.3â0.5, 0.5â1.6, 1.6â5 mm), 0.5â1.6 mm microplastics were the most abundant from the majority of site in the Three Gorges Reservoir, which made up 30â57% of the total microplastics [22]. While for microplastic samples from the lakeshore sediment of the Siling Co basin, different size distribution patterns were observed from different sampling sites [23]. The patterns of microplastic size distribution can be related to the sources of microplastics and might also reflect the degree of weathering. A higher degree of weathering might result in a higher abundance of smaller particles. Biofouling and hydrodynamic conditions were also believed to affect the size distribution of microplastics [54â56]."
297,1775,0.986,The R Book,"As you can see, the parameter estimates are very close to those obtained by the linear model (intercept = 11.799 Â± 1.268 rather than 11.7556 Â± 1.0408; slope = â1.230 Â± 0.263 rather than â1.2167 Â± 0.2186). The unreliability estimates are slightly greater than in the linear model, and the deviance is substantially greater (36.51 rather than 20.07), but the interpretation is unaffected. A plot of the jags model object produces strip diagrams with credible interval bars for the parameters, the deviance and tau, with the results from the different chains in different colours: plot(model)"
145,173,0.986,"Evolution, Monitoring and Predicting Models of Rockburst : Precursor Information for Rock Failure","When stress was at a low level (below 0.2rc), the spatial fractal dimension, close to 3, was relatively large indicated that AE events in this stage were more evenly distributed in the interior space of rock samples. With the increase of stress, rock sample entered the cracks initiation stage and cracks stabilization expansion stage (0.2â0.7rc). Because of less AE events, D value of this stage decreased slowly. With the increase of stress, rock specimen entered the unsteady expansion stage (0.7â0.9rc), the internal damage increased and the fractal dimension decreased continuously. When stress value reached about 80% of the peak stress, D value decreased rapidly, indicated that the internal events began to accumulate near the macroscopic main fracture surface. When stress reached the peak stress, the macro-fracture surface was formed gradually, and D value dropped to the minimum value, which was close to 2.2. By analyzing the relationship curve between the spatial fractal dimension and the stress level of AE event, the spatial fractal dimension decreased linearly with the increase of stress level. Therefore, the least squares method was used to ï¬t the curve, and the ï¬tted curve was shown in Fig. 6.18. The ï¬tted equation was as follow:"
391,116,0.986,Ocean-Atmosphere Interactions of Gases and Particles,Fig. 1.1 Schematic representation of the major pathways within the marine sulphur cycle and the impact of four different regimes on the relative contribution of each pathway and ultimately on the fraction of DMSP that is emitted to the atmosphere as DMS. âLow DMSPâ and âHigh DMSPâ refer to
311,1905,0.986,The Physics of the B Factories,"The low-mass Higgs boson models discussed earlier in this section predicted that the branching fraction for Î¥ (1S) â Î³A0 could range as high as â¼ 10â3 (the range of possible branching fractions is dependent on the specific NMSSM model used). The measurements from the BABAR collaboration put strong constraints on the upper range of this kind of decay down to the level of 10â6 , removing a few orders of magnitude of possible range from the top level of predicted branching fractions."
80,347,0.986,Innovations in Quantitative Risk Management (Volume 99.0),"where Î»ÌU is a random variable since it is based on parameters of the mixture copula model which are themselves functions of the data and therefore random variables. Such a simple linear projection will then allow one to interpret directly the marginal linear contributions to the upside or downside risk exposure of the basket obtained from the model, according to particular pairs of currencies in the basket by considering the coefficients Î±i j , i.e. the projection weights. To perform this analysis, we need estimates of the pairwise tail dependence in the upside and i| j i| j downside risk exposures Î»ÌU and Î»ÌL for each pair of currencies i, j â {1, 2, . . . , d}. We obtain this through non-parametric (model-free) estimators, see [8]. Definition 5 Non-Parametric Pairwise Estimator of Upper Tail Dependence (Extreme Exposure)"
233,92,0.986,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"with a concentration of threatened phylogenetic distinctive and rare species. Here, the PE measure was combined with probabilities of extinction. Their âimperilled phylogenetic endemismâ (IPE) index is the sum over all branches of branch length times its probability of extinction (product of extinction probabilities of all descendents) times the inverse of its range-extent. Gudde et al. (2013) claimed to âquantify where on the landscape at-risk evolutionary history is concentrated.â However, their âimperilled phylogenetic endemismâ (IPE) index appears to have the weakness that it could highlight places that have no threatened branches at all. As a revealing example, suppose that area A has 20 species, all of IUCN âleast concernâ (see IUCN 2006, 2012). Suppose that this corresponds to a low probability of extinction of 0.025 (for methods and discussion, see Mooers et al. 2008; Faith and Richards 2012). Each species is found in only ten areas. Suppose that area B has ï¬ve species, all IUCN âcritically endangeredâ (probability of extinction assumed to be a higher 0.4). Each species is found in 50 areas, but all are found together in this one area. Suppose also that each species is at the end of a branch of some unit length. Also, for simplicity, I will ignore deeper branches (assuming that all species have numerous secure sisters). IPE in this simple case is equal to the product of the number of branches, the probability of extinction and the inverse of the number of cells containing a given branch. Application of IPE gives area A the higher priority; the IPE score equals 20 times 0.025 times 1/10 or 0.05. IPE gives area B the lower priority; the IPE score equals 5 times 0.4 times 1/50 or 0.04. Application of IPE therefore would ignore the opportunity to save, with a reserve based around area B, ï¬ve critically endangered species. Instead, IPE would give preference to an area with 20 non-threatened species! This reveals the key limitation of the approach. IPE is supposed to reï¬ect a concentration of range restricted, threatened species. Gudde et al. (2013) argued that âour mapping does indeed quantify where at risk PD is concentratedâ. However, IPE, in the example above, actually quantiï¬ed where not-at-risk PD was concentrated! This weakness of IPE is similar to that of EDGE (see above and Faith 2008). Both methods suffer the weakness that phylogenetic overlap of species is not effectively taken into account. For EDGE type assessments, an existing probabilistic PD approach (Witting and Loeschcke 1995) performs better (Faith 2008; see also May-Collado and Agnarsson 2011; Kuntner et al. 2011). In the ï¬nal section, I examine the prospects for using this âexpected PDâ approach to address some conservation assessment problems that have been unsuccessfully treated by the ED type methods. The PE measure is relevant to another study that attempts to integrate range extent and threat information into PD assessments. In their global study on conservation of phylogenetic diversity of birds, Jetz et al. (2014) devised a measure related to ED to provide scores for regions or areas. Their âEDRâ score for a species is simply the ED value divided by the range (number of occupied cells) of the species. Total EDR for a given region then is the summed EDR of all species occurring in the region. Jetz et al. ask, âUnder an objective of minimizing global PD loss, how do ED and EDR perform as metrics for a rule-based approach to taxon- and"
65,106,0.986,Handbook of Ocean Wave Energy,"To understand how the ocean waves may influence the performance of a wave energy converter, it is useful to consider the temporal, directional and spectral characteristics of the ocean waves and how these may influence the relationship between the average omni-directional wave power and the average power generation. Firstly, the temporal characteristic of a wave climate is how the sea-states that make up a wave climate vary in time as illustrated in Fig. 3.7 for the signiï¬cant wave height. In general, the more consistent the wave climate is, the more attractive it becomes (for a particular average wave power) because the WEC and power generating plant can remain closest to its optimal operating conditions and thus maximise the system efï¬ciency. However, the sea-states will vary due to changes in the metrological conditions that generate the winds and associated waves. Not surprisingly, the stability of metrological conditions varies across the world so that the wave climate is more consistent in some locations than others. This variability may be primarily associated with daily, seasonal and/or annual variations in the sea-states, each of which will have a slightly different impact on the power generation and its utility. Thus, it is clear that for all locations the temporal characteristics are an important element of the wave climate, which can result in different power generations for the same average incident wave power."
233,473,0.986,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"Fig. 5 Mean phylogenetic diversity measured as the equivalent number of Raoâs QE across cells at different top fractions of the landscape according to Zonation. The values have been standardized from an original mean QE of 5.06 across all cells. The different prioritizations converge at top fraction equal to 1, as that represents the mean value across all cells in the landscape. If cells were removed in random order, the points would form a flat line at this level"
120,26,0.986,Genome Editing in Neurosciences,"Fig. 1 Proposed model describing the effect of multiple Parkinsonâs disease (PD)-associated risk variants on SNCA expression (modified from Soldner et al. 2016). The schematic illustrates the genomic organization of the SNCA locus, including the PD-associated risk variants SNCA-Rep1 and the risk SNPs rs356168 and rs3756045, both located in a distal enhancer element in the fourth intron of SNCA. The analysis described in Soldner et al. (2016) suggests that the brain-specific transcription factors (TF) EMX2 and NKX6-1 show sequence-dependent binding at rs356168 with preference for the A-allele. The efficient TF binding in carriers of the protective A-allele results in a suppressed distal enhancer element and, consequently, in reduced expression of SNCA associated with reduced risk to develop PD. In contrast, the reduced TF binding in carriers of the PD riskassociated G-allele at this variant leads to a more active distal enhancer, resulting in increased expression of SNCA associated with an increased risk to develop PD. Notably, neither the repeat length of SNCA-Rep1 nor the PD-risk variant at rs3756054 significantly affects SNCA expression, suggesting that these elements are in linkage disequilibrium (LD) with other functional riskmodifying variants"
307,97,0.986,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),2.2.4 Changing States: The Effect of the Markov Model We have now handled the advection of the probability listed as c) above and how changes due to the opening or closing of the channel affect the probability density function remains to be seen. Recall that the reaction scheme of the Markov model is given by
29,112,0.986,"Micro-, Meso- and Macro-Connectomics of the Brain","maps of the visual field, where the primary visual area V1 and the higher order area MT act as anchors, a concept that has been generalized recently to a tethering hypothesis where conserved, regionally localized patterning centers ensure the observed stereotypic localization of primary areas during the massive cortical expansion that accompanies phylogenesis (Buckner and Krienen 2013). The tethering hypothesis speculates that the primary cortical areas would be integrated into the cortical network in a very different fashion from the association cortex, the latter being characterized by a greater abundance of long-distance connections. Our results do not support this speculation, but they do suggest a major difference. Whereas the primary cortical areas are located in the fans of the bowtie, the association cortex is part of the high-density cortical core and is part of the knot of the bowtie (Ercsey-Ravasz et al. 2013). The above considerations go some way in explaining the developmental and phylogenetic basis of the high functional clustering of areas, thereby forming distinct constellations of areas centered on visual, auditory, somatosensory, motor and cognitive functions. The recent tract tracing data in both macaque and mouse and the network analysis of inter-areal connectivity begin to provide a coherent picture of the high-density cortical network. The anatomy tells us that there are many more connections than previously suspected, including numerous low-weight long-distance connections that can only be detected by connectomic approaches (Markov et al. 2014b; Oh et al. 2014; Zingg et al. 2014). It would be wise to resist the temptation to ignore such connections. The variables of functional and structural parameters, including synaptic weights and transmission probability, EPSPs, spine sizes, firing rates, correlations of population synchrony and axon diameters, show skewed log normal distributions (Buzsaki and Mizuseki 2014). Hence, at multiple levels, assemblies of many weak and few strong elements seem to be a characteristic feature of what makes brains work. With regards to the weak interareal connections, while their band-width will exclude dense information transfer, there is ample possibility for them to play a role in contraction dynamics of oscillatory coherence (Wang and Slotine 2005) and hence in shaping communication across the cortex (Fries 2005). The potential importance of the long-distance weak connection in the cortex, at least superficially, echoes that of the strength of weak ties in social networks, reputed to be important in integrating the individual into the social fabric (Granovetter 1973)."
241,471,0.986,Second Assessment of Climate Change for the Baltic Sea Basin,"There is a long-term approximate advectiveâdiffusive balance in the deep water of the Baltic Sea (Stigebrandt 2001). Advective supplies of new deep water tend to increase the salinity while diffusive fluxes tend to decrease the salinity. However, this is not in balance over short timescales due to the discontinuous character of the advective supply of deep water. Since tides are usually weak in the Baltic Sea, most of the energy sustaining turbulence in the deep-water pools must be provided by the wind. Based on long-term modelling of the large-scale vertical circulation in the Baltic Proper, Stigebrandt (1987, 2001) concluded that under contemporary conditions the basinwide vertical diapycnal diffusivity (or diapycnal mixing coefï¬cient) in the deep-water pools can be reasonably well described by j Â¼ minÃ°a=N; jmax Ã where Î± and jmax are constants and N is the Brunt-VÃ¤isÃ¤lÃ¤ frequency. In the horizontally integrated model for the Baltic Proper, Stigebrandt (1987) tuned Î± to equal 2 Ã 10â7 m2 sâ2."
32,313,0.986,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","14.1 Introduction Population distribution has been studied for many decades. Zipfâs law [1], which argues that the size distribution of a cityâs population is a power-law, is known well [2â6]. However, a problem exists: how to define the area of cities when we observe population distributions. The tail of a power-law distribution is composed of megacities. By dividing megacities into several smaller cities, the distributionâs tail becomes thin. Because of the different definitions of a city, population distribution is not a power-law distribution but a log-normal one [7â9]. City areas have been decided by geographical, historical, and administrative factors. Rozenfeld et al. proposed a method that decided a cityâs area by a city clustering algorithm [10]. In this research, we divide spatial regions by a method that ignores the shape of cities to find the properties of population distribution that do not depend on countries or local regions. We investigated population distribution using a spatial division method by identically sized squares. This approach resembles a previous method [9]. In our case, we control the scale of the spatial division by changing the size of the squares and clarify the universal properties concerned with population agglomeration. Populationâs universal properties can be observed by changing the scale of the spatial division. We introduce logarithmic differences between the nearest neighbor two square blocks in terms of population. The regional dependence of these values in terms of the shape of the distributions vanishes for small size scales. The property of the distribution of logarithmic differences is concerned with the correlation coefficient of the population in two squares. This correlation is one index to measure population agglomeration. In this research, we investigate Japanese population data. In Sect. 14.2, we introduce eight regions to investigate local properties inside Japan. In Sect. 14.3, we compare several distributions concerned with population among these eight regions. Next we compare Japan and Europe in Sect. 14.5 and show the universal properties concerned with population in both cases."
311,1049,0.986,The Physics of the B Factories,"the second is systematic. The central value has been corrected by (â0.002Â±0.002) psâ1 to account for a small variation of the background composition as a function of mES . An additional correction of (â0.007Â±0.003) psâ1 has been applied to account for a bias observed in fully-simulated MC events due to correlations between the mistag rate and the Ît resolution that are not explicitly included in the likelihood function. Belle measures Îmd = (0.528 Â± 0.017 Â± 0.011) psâ1 in a sample of 29.1 fbâ1 . The largest contributions to the systematic uncertainty in the Belle measurement come from the uncertainties in the signal Ît resolution function parameters (0.008 psâ1 ) and limited MC statistics (0.005 psâ1 ). In the BABAR fit the parameters of the signal and background Ît resolutions functions are allowed to vary, and their contribution to the uncertainty on Îmd is included as part of the statistical error. The largest remaining systematic uncertainties come from uncertainties in the B 0 lifetime (0.006 psâ1 ) and in the alignment of the SVT (0.005 psâ1 )."
232,529,0.986,Resilience: A New Paradigm of Nuclear Safety: From Accident Mitigation to Resilient Society Facing Extreme Situations,"3 Veriï¬cation of the ERP Model To verify the accuracy of the ERP model applied to the E-MPS method quantitatively, we analyzed a hydrostatic pressure problem in a rectangular vessel. The numerical results were compared with the theoretical solution p Â¼ qjgjh where q is the fluid density, g is the gravitational acceleration vector, and h is the depth of the static water surface. The initial conï¬guration, in which the depth of the rectangular vessel is 0.1 m and the width is 0.04 m, is illustrated in Fig. 1. In the E-MPS computation, weak compressibility causes vertical vibrations of the fluid surface. To reach the static state as quickly as possible, we chose a relatively high value for the kinematic viscosity. The conditions used in the analysis are listed in Table 1, in which l0 is the initial particle spacing. Figure 3 shows the pressures of fluid particles computed by (a) the ERP model, (b) the ERP model using only the repulsive force, (c) Haradaâs model, and (d) Yamadaâs model. These are the results at the 200,000th step, at which the pressure ï¬eld can be regarded to be in a steady state. In Fig. 2, snapshots obtained"
393,444,0.986,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 2,"As it was shown in overview of several clustering techniques in the previous sections we might often face scalability problems during clustering. We recall that the clustered data-set scale on three axes: 1) large number of instances, 2) many or too complex instance attributes, 3) lot of clusters. Local clustering aims to tackle scalability problems related to high number of instances. The second mentioned scalability problem occurs if estimation of the proper similarity among the instances is a too expensive operation (string comparison). Reliable identification of similarity between scientific citations is a very expensive operation, although the dissimilarity of two citations can be computed very cheaply. A two-pass algorithm was developed for application when dissimilarity can be estimated much more effectively than similarity between a vertex pair (McCallum et al., 2000). The method is generic enough to be well combinable by other approaches. The idea is to create so called canopies in the first pass of the algorithm. Canopies are a kind of rough overlapping clusters which can be computed very effectively even if we compare all n 2 pairs. In the second pass, a rigorous, but more expensive, similarity function is executed on every instance pair, which occurs in the same canopy (we remind, that an instance can be included in more than one canopy). Virtually any instance-based clustering algorithm can be used in the second pass of the algorithm. From a theoretical standpoint, by setting certain properties of the inexpensive distance metric, accurate clustering solution can still be recovered with the canopies approach (McCallum et al., 2000). The necessary guarantees needed for avoiding the decay of the clustering quality depend from the clustering algorithm chosen in the second pass. If c is the number of canopies, f is the average number of canopies to which the instances are included, k is the number of clusters and n the number of instances, the"
311,2941,0.986,The Physics of the B Factories,"where a shorthand notation was used for the fragmentaâ¥h tion functions of hemispheres 1 and 2, i.e. H2â¥ = H1,q (z2 ) and similarly for the unpolarized fragmentation functions D2 = D1,q (z2 ); also the favored and disfavored FFâs are denoted by superscripts fav and dis, respectively. A simiL lar notation holds for the AU 12 amplitude. Anselmino et al. (2007) extracted the corresponding favored and disfavored Collins fragmentation functions from the Belle data and found that they are both sizeable and of opposite sign. Using this they were able to extract the quark transversity distribution from the HERMES (Airapetian et al., 2005) and COMPASS (Alexakhin et al., 2005) data for the first time. In this extraction some improvements in the knowledge of the Collins function are still needed. For example, the intrinsic transverse momentum dependence is not known and was only estimated. Recent results have been shown by BABAR (Garzia, 2013), with an analysis similar to that performed by Belle and based on a data sample corresponding to an integrated luminosity of about 468 fbâ1 collected at the Î¥ (4S) and 40 MeV below. A general consistency between the BABAR and Belle asymmetries measured as a function of the fractional energies is observed. The z-range explored by BABAR extends from 0.15 to 0.9. In addition, BABAR performed a study of the azimuthal asymmetries as a function of the transverse momentum of the pions with respect to the thrust axis. As an example,"
30,295,0.986,Determinants of Financial Development,"186 Notes 36. The summary below is heavily drawn from DemirgÃ¼Ã§-Kunt and Levine (1996, 1999). 37. The precision of the principal component analysis used to derive this new index depends on having a relatively large number of variables. Given that there are only three indices on which the principal component analysis is based, the new index of financial development is almost the mean of the three individual indices. 38. Two measures for the efficiency of financial intermediation widely used are Overhead Costs, the ratio of overhead costs to total bank assets, and Net Interest Margin, the difference between bank interest income and interest expenses, divided by total assets. Due to the incompleteness of the available data, they are not included in this analysis. 39. In the growth and convergence context, both the panel data analysis of Caselli et al. (1996) and the cross section analysis of Mankiw et al. (1992) find a negative effect of initial income on growth, but the former identifies a much larger effect than the latter, implying a 10% convergence rate relative to 2â3% suggested by Mankiw et al. (1992). 40. Starting from a general model with three lags of the dependent and independent variables and testing the null hypothesis of the coefficients being zero for the longest lag, we end up with one lagged independent variable and one lagged dependent variable appearing in the model for this context, given that the relevant specification tests are satisfied. 41. Caselli et al. (1996) treat some variables like the investment rate and population growth rate as predetermined and argue that these variables are potentially both causes and effects of economic growth. 42. Alonso-Borrego and Arellano (1999) propose the symmetrically normalized GMM estimator and the Limited Information Maximum Likelihood estimator. Recently Kruiniger (2008) has developed the Maximum Likelihood estimator and Newey and Windmeijer (2009) have proposed the new variance estimator for the generalized empirical likelihood estimator. 43. Bond et al. (2001) and Bond (2002) illustrate that in principle the firstdifferenced GMM estimates for the AR(1) coefficient should lie between the within group estimates (being downwards biased) and the OLS estimates (being upwards biased) from a straightforward pooled regression. 44. For the case of r=2, when ft = (1 Î·t ) and Î»i = (Î±i 1), we have Î»i ft = Î±i + Î·t , where Î±i and Î·t are the individual effect and time effect, respectively. 45. Bai (2004) suggests that differenced data can also be used to calculate the number of factors."
70,813,0.986,Optics in Our Time,"Achieving the maximum space-time correlation in photon-number ï¬uctuations, we place D1 and D2 at equal longitudinal and transverse coordinates, Ï1 Â¼ ~ Ï2 . z1 Â¼ z2 and ~ . Figure 16.8 reports a typical measurement of the polarization correlation in photon-number ï¬uctuation correlation. In this measurement, we ï¬xed Î¸1 Â¼ 45â and rotated Î¸2 to a set of different values. The black dots are experimental data, the red sinusoidal curve is the theoretical ï¬tting of cos2 Ã°Î¸1  Î¸2 Ã based on Eq. (16.44) with a  92:5 % contrast. For other values of Î¸1 6Â¼ 45â we have observed the same sinusoidal correlation function. . Figure 16.9 reports a measurement of hÎn1 Ã°Î¸1 Ã În2 Ã°Î¸2 Ãi by scanning the values of Î¸1 and Î¸2 (2-D scanning). Based on these measurements, we conclude that our observed polarization correlation is the same as that of the Bell state jÎ¦Ã°Ã¾Ã i. Apparently, the post-selection measurements of the reported experiment has âentangledâ a product state of polarization into the Bell state jÎ¦Ã°Ã¾Ã i. To explain the experimental observation, we start from the analysis of chaoticthermal light. Chaotic-thermal light may come from a natural thermal light source, such as the sun, or from a pseudo-thermal light source, usually consisting of a laser beam, either CW or pulsed, and a fast rotating ground glass containing a large number of tiny scattering diffusers (usually on the order of a few micrometers). For a natural thermal light source, each radiating atom among a large number of"
241,607,0.986,Second Assessment of Climate Change for the Baltic Sea Basin,"to the extent of change in these environmental variables, predicting the future climate requires reliable estimates of the future composition of the atmosphere and land use. As the concentration of GHGs and aerosols in the future atmosphere is so difï¬cult to predict because of the many influencing factors, scenarios are developed based on projections of the future evolution of the world population and economy (see Chap. 11, Sect. 11.2) and it is these scenarios that are used as the basis for projections of future climate. Beside the uncertainty related to the limited information on land use, and the atmospheric concentrations of GHGs and aerosols, there are also other sources of uncertainty in models. These include limited amounts of input data and their limited accuracy. Due to the chaotic nature of the climate system, a very small difference in initial conditions can generate different climate features, as each simulation generates a different set of realisations. If this were the only source of uncertainty, the differences between simulations should remain within the range of typical climate variability. However, this is not the case. Many sub-grid-scale processes must be simulated in models in a more or less complex form and are not well described by the models. For example, simulations of cloud formation, their optical and radiative features, and the creation of precipitation still carry considerable model error. For climate models to be useful, they need to be evaluated. As future climate predictions cannot be evaluated by direct comparison with observations, models are evaluated by comparing simulations with observations of the past climate. In theory, this should make it possible to select the best model, but this is not the case in practice. One model can usually describe a particular parameter better than another model, while the second model better describes a different variable or even the same variable, but in another part of the world. There are no objective ways to choose the best model, because none are able to exactly reproduce the observed mean climate and its variability. Differences between simulations and real climate data can be estimated on the basis of a so-called reference period (in the past) for which observational data are available. The differences, usually referred to as âbiasesâ, vary in space and typically also in daily and annual cycles. The models describe climate at a set of grid points. Because of numerical constraints in GCMs and RCMs, model results at neighbouring grid points are more correlated than actual measurements from two observation points at the same distance (DÃ©quÃ© 2007). This is one reason why the distributions of simulated variables are usually smoothed in comparison with measured station data. Simulations tend to underestimate the highest values and overestimate the lowest (DÃ©quÃ© 2007). This means that the bias is different in different parts of the distribution."
238,442,0.986,Nanoinformatics,"applicable for alumina scale. The activation energies for oxygen in the scale are also assumed to be the same as those obtained from the oxygen permeation experiments, as listed in Table 11.1, regardless of whether the alumina scale was doped with Y or not. Consequently, PO2 and AO*/Sgb in Eqs. (11.10) and (11.17) for scale can be determined by solving the simultaneous equations using the proï¬ling position (x/L) and the corresponding oxygen GB diffusion coefï¬cients. The oxygen GB diffusion data for Y-doped scale formed on ODS-MA956 alloy [6] was determined at an x/L of approximately 0.88â0.96; however, there was no description of the measurement ranges for other types of scales [7]. Thus, these ranges for all the scales in this study are assumed to be equal to that for Y-doped scale [6], which adopts the middle value (x/L = 0.92) of the measurement range because such a depth proï¬ling is generally performed in a zone just near the scale surface. As a result, PO2 and AO*/Sgb for scale formed on the RE-free alloy at 1373 K, i.e., non-doped scale, are 1.6 Ã 10â17 Pa and 13.94 Ã 10â4 mol sâ1 Paâ1/6, respectively. The calculated AO*/Sgb value is almost equal to that determined from the oxygen permeation experiments, as given in Table 11.1. Figure 11.11 shows Arrhenius plots of the GB diffusion coefï¬cients for oxygen, together with data from the literature [6, 7]. Table 11.2 summarizes the measurement conditions and activation energies for the GB diffusion data in Fig. 11.11. The dashed line a, which is determined by substitution of PO2 = 1.6 Ã 10â17 Pa in Temperature (ÂºC) 1700 1600 1500"
311,992,0.986,The Physics of the B Factories,"butions have turned out not to be very satisfactory and other more complex functions are called upon. This can partly be explained as the influence of poorly understood resonances (such as the Ï/Îº or the higher mass resonances mentioned above). As a result ânon-resonantâ has come to mean anything that is not modeled by a resonance. In practical terms, this means the distributions often have to come from MC simulations, oï¬-resonance data or sideband data, or a combination of all three. In the case of sideband data, MC samples must be used to remove events from B meson decays that are also present and to determine possible diï¬erences in the background shape between the sideband and signal regions. Linear interpolation between bins can be used where needed. The backgrounds are constructed separately for both the B 0 and B 0 events and a p.d.f. or histogram is formed taking into account any asymmetry that might be present in the background distributions (see, for example Eq. 20 in Aubert (2009h)). The observables that are used in the ML depend on the analysis under consideration. Typically, a combination of ÎE, mES , multivariate discriminant, position in the Dalitz Plot and charge (flavor) of the B meson candidate is used. Sometimes a cut is applied to the observable first (e.g. on the multivariate discriminant) and then this observable is excluded from the fit. This usually happens for observables that are correlated with position in the Dalitz Plot. As in two-body and quasi-two-body decays, certain D meson decays to the same or similar final state can be used as a calibration channel and allow for correction to fitted parameters derived just from MC simulation. Although many of the resonances in the Dalitz Plot can be predicted from previous quasi-two-body measurements, there is still a large uncertainty in the number and type of resonances that should be included in any particular model. Examples include the exact parameterization of the non-resonant three-body decay component, the Ï/Îº with masses in the region 400 â 600 MeV/c2 and widths that are large and uncertain, the Ï(782), the Ïc0 and Ïc2 , and the higher mass partners of the Ï, f0 (980), and K â . The addition of a resonance to the model that is not present in the data can be just as problematic as any exclusion of a resonance that is present. The problem is exacerbated if a blind fit is being performed. One technique is to use the log-likelihood reported by a particular model fitted to the data or to calculate a Ï2 statistic based on the number of events predicted from a fit and the number of real events in a bin in the Dalitz Plot. The statistical significance of the presence of a component can be estimated by evaluating the diï¬erence Î ln L between the negative log-likelihood of the nominal fit and that of a fit where the amplitude and ACP is set to zero. This is then used to evaluate a p value which is the integral from 2Î ln L to infinity of the p.d.f. of the Ï2 distribution. An important goal of the Dalitz Plot analysis is the extraction of CP asymmetries either from a time-integrated or time-dependent analysis. Consequently, the resonances are parameterized not just in terms of their widths and masses but as functions of the decay dynamics, angular"
179,302,0.986,"Habitats and Biota of the Gulf of Mexico: Before the Deepwater Horizon Oil Spill: Volume 1: Water Quality, Sediments, Sediment Contaminants, Oil and Gas Seeps, Coastal Habitats, Offshore Plankton and Benthos, and Shellfish","A numerical scoring system was developed to integrate information on (1) primary symptoms: decreased light availability (chlorophyll a concentrations and problematic epiphytic and macroalgal growth), algal dominance (diatom/dinoflagellate ratios and benthic to pelagic dominance ratios), and increased organic matter decomposition (chlorophyll a concentrations and problematic macroalgal growth) and (2) secondary symptoms: loss of submerged aquatic vegetation (spatial coverage and trends), harmful algae (nuisance and toxic blooms), and low dissolved oxygen (anoxia, hypoxia, and stress) to determine the overall status of eutrophic symptoms in each estuary. This scoring system was implemented in three phases according to the methods described in detail the report First, a single index value was computed from all primary symptoms. The scoring system gave equal weight to all three symptoms and considered the spatial and temporal characteristics of each. The scores for the three symptoms were then averaged, resulting in the highest values being assigned to estuaries having multiple primary symptoms that occur with great frequency, over large spatial areas of the estuary, and for extended periods of time. Likewise, the lowest scores indicate estuaries that exhibit few, if any, characteristics of the primary symptoms Next, a single index value was computed from all secondary symptoms. The scoring system again gave equal weight to all symptoms and their spatial and temporal characteristics. The highest score of any of the three symptoms was then chosen as the overall secondary value for the estuary. This weights the secondary symptoms higher than the primary symptoms, because the secondary symptoms take longer to develop, thereby indicating a more chronic problem, and being more indicative of actual impacts to the estuary Finally, the range of numeric scores assigned to primary and secondary symptoms was divided into categories of high, moderate, and low. Primary and secondary scores were then compared in a matrix so that overall categories could be assigned to the estuaries Estuaries having high scores for both primary and secondary conditions were considered to have an overall âhighâ level of eutrophication. Likewise, estuaries with low primary and secondary values were assigned an overall âlowâ level of eutrophication. Scores were then assigned to the remaining estuaries based on interpretations of each estuaryâs combined values"
311,792,0.986,The Physics of the B Factories,"Their choice avoids unphysical singularities which are generated at q 2 = t+ by the outer function in a truncated BGL parameterization. Further, Bourrely, Caprini, and Lellouch (2009) optimize the parameter t0 such that the semileptonic domain is mapped onto a symmetric interval in z. With the choice t0 = (mB + mÏ )( mB â mÏ )2 , the value of |z| < 0.279. Although the BCL parameterization has a simpler functional form, the constraint on the series is more complicated than Eq. (17.1.50) in that it is no longer diagonal in the series index k. We use the BCL parameterization to obtain |Vub | in Section 17.1.4.3. A diï¬erent approach suggested by Flynn and Nieves (2007a,b) uses the OmneÌs parameterization, allowing one to express the form-factor shape in terms of the elastic BÏ scattering phase shift and the value of f+ (q 2 ) at a few subtraction points below the BÏ production threshold. Lattice QCD form-factor calculations State-of-the-art LQCD computations now regularly include the eï¬ects of three light dynamical quarks. Often calculations are done in the isospin limit with two lighter degenerate quarks and one heavier quark with a mass close to the physical strange quark; these are referred to as â2+1â ï¬avor simulations. In practice, limited computational resources prohibit calculations with simulated values of the u- and d-quark as light as those in the real world. LQCD calculations must also be done at ï¬xed, nonzero values of the lattice spacing. Hence one generates data with a sequence of light-quark masses (down to â¼ mstrange /10 for current B â Ï calculations) and a sequence of lattice spacings (down to a â¼ 0.09 fm for current B â Ï calculations) and extrapolates the remainder of the way to the physical masses and zero lattice spacing. Because these limits are interrelated, it is now standard to use model-independent functional forms derived in Chiral Perturbation Theory (ÏPT) for the speciï¬c lattice quark formulation being used (i.e. including discretization corrections) to guide the extrapolation (see, e.g., Aubin and Bernard, 2007, for the case of B â Ï). This procedure leaves a remaining systematic uncertainty in the physical matrix element due"
151,146,0.986,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"Detectable concentrations of the organophosphorus insecticide CPY in air, rain, snow and other environmental media have been measured in North America and other locations at considerable distances from likely agricultural sources, which indicates the potential for long range transport (LRT) in the atmosphere. This issue was addressed by first compiling monitoring results for CPY in all relevant environmental media. As a contribution to the risk assessment of CPY in remote regions, a simple mass balance model was developed to quantify likely concentrations at locations ranging from local sites of application to more remote locations up to hundreds of km distant. Physical-chemical properties of CPY were reviewed and a set of consistent values for those properties that determine partitioning and reactivity were compiled and evaluated for use in the model. The model quantifies transformation and deposition processes and includes a tentative treatment of dispersion to lesser atmospheric concentrations. The model also addressed formation and fate of CPYO, which is the major transformation product of CPY. The Characteristic Travel Distance (CTD) at which 63% of the original mass of volatilized CPY is degraded or deposited-based on a conservative concentration of â¢OH radicals of 0.7 Ã 106 molecules cmâ3 and a half-life of 3 h, was estimated to be 62 km. At lesser concentrations of â¢OH radical, such as occurs at night and at lesser temperatures, the CTD is proportionally greater. By including monitoring data from a variety of media, including air, rain, snow and biota, all monitored concentrations can be converted to the equilibrium criterion of fugacity, thus providing a synoptic assessment of concentrations of CPY and CPYO in multiple media. The calculated fugacities of CPY in air and other media decrease proportionally with increasing distance from sources, which can provide an approximate prediction of downwind concentrations and fugacities in media and can contribute to improved risk assessments for CPY and especially CPYO at locations remote from points of application, but still subject to LRT. The model yielded estimated concentrations that are generally consistent with concentrations measured, which suggests that the canonical fate and transport processes were included in the simulation model. The equations included in the model enable both masses and concentrations of CPY and CPYO to be estimated as a function of distance downwind following application. While the analysis provided here is useful and an improvement over previous estimates of LRT of CPY and CPYO, there is still need for improved estimates of the chemical-physical properties of CPYO. Based on the persistence in water, soils, and sediments, its bioconcentration and biomagnification in organisms, and its potential for long-range transport, CPY and CPYO do not trigger the criteria for classification as a POP under the Stockholm convention or a PB chemical under EC 1107/2009. Nonetheless, CPY is toxic at concentrations less than the trigger for classification as T under EC1107/2009; however, this simple trigger needs to be placed in the context of low risks to non-target organisms close to the areas of use. Overall, CPY and CPYO are judged to not trigger the PBT criteria of EC 1107/2009. Acknowledgements The authors wish to thank the anonymous reviewers of this paper for their suggestions and constructive criticism. Prof. Giesy was supported by the Canada Research Chair program, a Visiting Distinguished Professorship in the Department of Biology and Chemistry and"
365,845,0.986,Climate Smart Agriculture : Building Resilience To Climate Change,"than five percent were water harvesting, irrigation, non-yield related strategies such as migration, and shift in farming practice from crop production to livestock herding or other sectors. We use this information from the survey to create the variable adaptation. This is equal to 1 if a farm household adopted any of the above strategies, and to 0 otherwise. As mentioned, detailed production data were collected at different production stages (i.e., land preparation, planting, weeding, harvesting, and post-harvest processing). Most of the sample population is composed of rainfed farms (less than 9 per cent of them have access to irrigation). Ethiopian rural households face high weather and climatic variability. Significant spatial variations exist in agroecological conditions, including topography, soil type, temperature, and soil fertility (Hagos et al. 1999).6 The farming system in the survey sites is very traditional with plough and animalsâ draught power. Labor is the major input in the production process during land preparation, planting, and post-harvest processing. Labor inputs were disaggregated as adult male labor, adult female labor, and children labor. The three forms of labor were aggregated as one labor input using adult equivalents.7 Monthly rainfall and temperature data were collected from all the meteorological stations in the country for the period 1970â2000. Then, the Thin Plate Spline method of spatial interpolation was used to impute the household specific rainfall and temperature values using latitude, longitude, and elevation information of each household. The Thin Plate Spline is a physically based two-dimensional interpolation scheme for arbitrarily spaced tabulated data. The Spline surface represents a thin metal sheet that is constrained not to move at the grid points, which ensures that the generated rainfall and temperature data at the weather stations are exactly the same as data at the weather station sites that were used for the interpolation. In our case, the rainfall and temperature data at the weather stations are reproduced by the interpolation for those stations, which ensures the credibility of the method (see Wahba 1990). This method is one of the most commonly used to create spatial climate data sets (e.g., Di Falco et al. 2011; Deressa and Hassan 2009). Its strengths are that it is readily available, relatively easy to apply, and accounts for spatially varying elevation relationships. However, it only simulates elevation relationships and has difficulty handling very sharp spatial gradients, which can be typical of coastal areas. Given that our area of the study is characterized by significant terrain features, and no climatically important coastlines, the choice of the Thin Spline method is reasonable (for more details on the properties of this method in comparison to the other methods see Daly 2006)."
233,275,0.986,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"in the range [1, N] for any measures of species importance and all orders q â¥ 0. Since the range depends on N, the phylogenetic beta diversity cannot be used to compare phylogenetic differentiation among assemblages across multiple regions with different numbers of assemblages. To remove the dependence on N, several transformations can be used to transform the phylogenetic beta component onto [0, 1] to measure local overlap, regional overlap, homogeneity and turnover. We give a summary of these four transformations below and tabulate formulas and the relationship with previous measures in Table 1 for the two most important classes. The formulas for the special cases for q = 0, 1 and 2 are also displayed there. 1. A class of branch overlap measures from a local perspective: 1- q"
372,1258,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"10.5.2 Variation of Spatial Frequency Over the Bandwidth The effect of using the center frequency of the receiver passband in calculating the values of u and v for all frequencies within the passband is discussed in Sect. 6.3.1. Consider, for example, a single discrete source for which the visibility function has a maximum centered on the .u; v/ origin and decreases monotonically for a range of increasing u and v. If we use the frequency at the band center ,0 to calculate u and v for a frequency at the high end of the band, that is, , > ,0 , then the values of u and v will be underestimated. The measured visibility will fall off too quickly with u and v, and the central peak of the visibility function will be too narrow. Hence, the width of the image in l and m will be too wide. Thus, if the source radiates a spectral line at the blueshifted side of the bandwidth, the angular dimensions may be overestimated and similarly underestimated at the redshifted side. This effect can be described as chromatic aberration. As discussed in Sect. 6.3, for observations with a spectral line (multichannel) correlator, the visibility measured for each channel can be expressed as a function of the .u; v/ values appropriate for the frequency of the channel. This corrects the chromatic aberration but causes the .u; v/ range over which the visibility is measured to increase over the bandwidth in proportion to the frequency. Thus, the width of the synthesized beam (i.e., the angular resolution) and the angular scale of the sidelobes vary over the bandwidth. The variation of the resolution can, if necessary, be corrected by truncation or tapering of the visibility data to reduce the resolution to that of the lowest frequency within the passband."
372,1601,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where a is a constant, and -m D 2""adm =). The form of Eqs. (13.78) is shown in Fig. 13.11a. This form can be derived by assuming a multiple-scale power-law model for the spectrum of the phase fluctuations. There is a limiting distance dm beyond which fluctuations do not increase noticeably, a few kilometers, roughly the size of clouds. This limit is called the outer scale length of the fluctuations. Beyond this dimension, the fluctuations in the path lengths become uncorrelated. First, consider an interferometer that operates in the domain of baselines shorter than dm . The measured visibilities Vm are related to the true visibilities V by the"
311,224,0.986,The Physics of the B Factories,"used in classifying an event. The technology of digital error correction may be used for this, in a method referred to as âerror-correcting output codesâ (ECOC) (Dietterich and Bakiri, 1995). An event is classiï¬ed by evaluating each of the classiï¬ers to give a vector consisting of the numbers â1 and 1 for the event. The soft Hamming distances (Hamming, 1950) between this vector and the expected vectors for each class are calculated, where the soft Hamming distance between two binary strings of equal length is the sum of squares of the diï¬erences at each position of the vector. This yields a vector of numbers with length equal to the number of classes. In the simplest case we can take the class with minimum soft Hamming distance to be the resulting class. The idea is that an individual classiï¬er might make an error, but this error may be corrected by the redundancy in the combination of the classiï¬ers. For instance in BABAR many analyses have diï¬erent PID requirements on the eï¬ciency and mis-identiï¬cation rate implying diï¬erent levels of tightness in the selection. Instead of assigning the class with the minimum soft Hamming distance, a cut is applied based on the soft Hamming for the particular class and the ratios of soft Hamming distance of the particular class to those of the other classes. For example, for electron selection, we cut on Se and Se /SK , Se /SÏ , Se /Sp where Sx is the soft Hamming distance for class x. The disadvantage of the ECOC approach is in the need to build the classiï¬ers for the exhaustive matrix, which becomes daunting if the number of classes becomes large. BABAR eventually applied the ECOC approach in the evolution of its particle identiï¬cation algorithm (Chapter 5), where the results of several bagged decision tree classiï¬ers are combined. We may get an idea of the impact from Fig. 4.4.1, which compares three methods for particle identiï¬cation: a likelihood-based selector (Section 4.4.2); a selector using bagged decision trees (Section 4.4.7) with a non-exhaustive error correction matrix; and a selector using bagged decision trees with an exhaustive error correction matrix. In the case of the non-exhaustive matrix, the classiï¬ers used are one-vs-one classiï¬ers, comparing the pion with kaon hypothesis, pion with electron, etc."
311,3064,0.986,The Physics of the B Factories,"pectation. There is also no clear prediction for the flavor diagonal observable (g â 2)Î¼ . There are also other extensions of the MFV hypothesis, beside GMFV. At the practical level the GMFV is equivalent to the Next-to-Minimal Flavor Violation (NMFV) hypothesis, even though the original motivations were diï¬erent. NMFV was put forward in (Agashe, Papucci, Perez, and Pirjol, 2005) by demanding that NP contributions only roughly obey the CKM hierarchy, and in particular can have O(1) new weak phases. The consequences of spurions that transform diï¬erently under GF than the SM Yukawa coupling matrices have been worked out by Feldmann and Mannel (2007). The MFV hypothesis has also been extended to the leptonic sector (MLFV) in (Cirigliano and Grinstein, 2006; Cirigliano, Grinstein, Isidori, and Wise, 2005). In MLFV the most sensitive FCNC probe in the leptonic sector is Î¼ â eÎ³, while Ï â Î¼Î³ could be suppressed below the sensitivity of future super flavor factories. 25.2.2.5 MFV SUSY Low energy supersymmetry (SUSY), where the superpartners have â¼TeV scale masses is one of the most popular solutions to the hierarchy problem. Since this model is perturbative one can make reliable predictions. This aids the popularity of SUSY among theorists. Already its minimal incarnation â the Minimal Supersymmetric Standard Model â has the salient features of gauge coupling unification and contains a viable dark matter candidate. The âMinimalâ in the MSSM refers to the field content. Each SM particle obtains only one superpartner, and also the extension of the Higgs sector is minimal. However, the flavor structure need not be minimal. The parameters that describe the supersymmetry breaking, e.g., the squark masses and trilinear couplings can in principle carry very diï¬erent flavor structures from the one seen in the quark sector of the SM. In total there are 124 parameters in the MSSM, much more than the 19 parameters of the SM (Berger and Grossman, 2009; Dimopoulos and Sutter, 1995; Haber, 2001). Of these parameters, 110 are in the flavor sector: 30 masses, 39 real mixing angles and 41 phases. If all of the mixing angles and phases were O(1) this would lead to FCNCs that are orders of magnitude larger than the experimental bounds. The SUSY breaking does have to be non-generic and further assumptions about its structure are required in order to have an acceptable phenomenology. An attractive hypothesis is MFV, which we discussed in general terms in the previous subsections. The flavor breaking is assumed to arise only from the Yukawa interactions (in this case from the superpotential), while the SUSY breaking is flavor blind. This means that the squark masses can be written as mÌ2qL =mÌ2 (a1 1 + b1 YU YUâ  + b2 YD YDâ  b3 YD YDâ  YU YUâ  + b4 YU YUâ  YD YDâ  + Â· Â· Â· ),"
372,1496,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where uP and vP are the time derivatives of the projected baseline components, (Ë 0 .)/ and (Ä±.)/ are the coordinate offsets from the reference feature, and (Ë 0 .)/ D (Ë.)/ cos Ä±. The relative positions of the maser feature can then be found by fitting Eq. (12.59) to a series of fringe-frequency measurements at various hour angles. This technique was first employed by Moran et al. (1968) for the mapping of an OH maser. The errors in fringe-frequency measurements decrease as * 3=2 [see Eq. (A12.27)], where * is the length of an observation, but for large values of *, the differential fringe frequency (2 )f is not constant, because uR and vR are not zero. Thus, there is a limited field of view available for accurate mapping with fringefrequency measurements. This field of view can be estimated by equating the rms fringe-frequency error in Eq. (A12.27) with * times the derivative of the differential fringe frequency with respect to time. Therefore, for an eastâwest baseline, D# !e2 ($* cos $ ' where ($ is the field of view. For"
372,627,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"This formula can be used for checking that &a is not too large. Two aspects of the behavior predicted by Eq. (6.81) should be mentioned. First, if the source is near the m0 axis and at a low declination, the averaging has very little effect. This is because the ridges of the sinusoidal corrugations of the visibility function then run approximately parallel to the u0 axis, and in the transformation u0 D u cosec Ä±0 , the period of the variations in the v direction is expanded by a large factor. In comparison, the arc through which any baseline vector moves in time &a is small, and hence, the averaging has only a small effect on the visibility amplitude. Second, for a source on the l0 axis, Ra is independent of Ä±0 . In this case, the ridges of the corrugations run parallel to the v axis, and the expansion of the scale in the v direction has no effect on the sinusoidal period. For arrays that contain baselines other than eastâwest, the centers of the corresponding loci in the .u0 ; v 0 / plane are offset from the origin, as in Fig. 6.13a, and a time offset is no longer equivalent to a simple rotation of axes. However, this may not increase the smearing of the visibility, so the effect may be no worse than for an eastâwest array with baselines of similar lengths."
311,1388,0.986,The Physics of the B Factories,"This formula separates out the process independent nonperturbative quantities into F BâVâ¥ , a form factor evaluated at maximum recoil (q 2 = 0), and the light-cone distribution amplitudes (LCDA), ÏB and ÏVâ¥ , for the heavy and light mesons. This leaves the quantities T I and T II , known as hard-scattering kernels, which can be calculated perturbatively. These correspond to vertex and spectator corrections, respectively, and have been calculated to O(Î±S1 ) (Ali and Parkhomenko, 2002; Beneke, Feldmann, and Seidel, 2001; Bosch and Buchalla, 2002b; DescotesGenon and Sachrajda, 2004), and recently in some cases to O(Î±S2 ) (Ali, Pecjak, and Greub, 2008). The LCDA of light pseudoscalar and vector mesons that enter the factorization formula have been studied in detail through the use of light-cone QCD sum rules (Ball and Braun, 1999; Ball, Braun, Koike, and Tanaka, 1998; Braun and Filyanov, 1989, 1990). However, not much is known about the B meson LCDA, whose first moment enters the factorized amplitude at O(Î±S ). Because this moment also enters the factorized expression for the B â Î³ form factor, it might be possible to extract its value from measurements of decays such as B â Î³eÎ½, if the power corrections are under control. The QCDF formula introduces an important simplification in the form factor description. The B â Vâ¥ form factors at large recoil have been analyzed in SCET and are independent of the Dirac structure of the current in the heavy quark limit (Charles, Le Yaouanc, Oliver, PeÌne, and Raynal, 1999). As a consequence of this, all the form factors reduce to a single form factor up to factorizable corrections in the heavy quark and large energy limits. Field-theoretical methods such as SCET make it possible to reach a deeper understanding of the QCDF approach. The various momentum regions are represented by diï¬erent fields, and the hard-scattering kernels T I and T II can be shown to be Wilson coeï¬cients of eï¬ective field operators. Using SCET one can prove the factorization formula to all orders in Î±S and to leading order in Î/mb (Becher, Hill, and Neubert, 2005). QCD is matched on SCET in a two-step procedure that separates the âhard scale Î¼ â¼ mb and then the hard-collinear scale Î¼ â¼ Îmb from the hadronic scale Î. The vertex correction term T I involves the hard scales, whereas the spectator scattering term T II involves both the hard and the hard-collinear scales. This is why large logarithms have to be resummed, which can be done most eï¬ciently in SCET. In principle, the field-theoretical framework of SCET allows one to go beyond the leading-order result in Î/mb . However, a breakdown of factorization is expected at that order. For example, in the analysis of B â K â Î³ decays at sub-leading order, an infrared divergence is encountered in the matrix element of O8 (Kagan and Neubert, 2002). In general, power corrections involve convolutions, which turn out to be divergent. Currently, no solution to this well-analyzed problem of end-point divergences within power corrections is available (Arnesen, Ligeti, Rothstein, and Stewart, 2008; Becher, Hill, and Neubert, 2004; Beneke and Feldmann, 2004). Thus, within the QCDF/SCET approach, a general, quantita-"
311,1391,0.986,The Physics of the B Factories,"The overall uncertainty is the quadratic sum of non-perturbative (5%), parametric (3%), perturbative scale (3%) and mc interpolation ambiguity (3%) uncertainties. An additional scheme dependence has since been found (Gambino and Giordano, 2008), but it is within the perturbative uncertainty of 3% (Misiak, 2008). However, in experimental measurements, a large background from non-signal BB events at low values of photon energy limits the minimum useful EÎ³ . The final B â Xs Î³ CLEO publication (Chen et al., 2001b) reports results for EÎ³ above 2.0 GeV using 9 fbâ1 of Î¥ (4S) data and 4.4 fbâ1 of oï¬-resonance data. They made an extrapolation down to the full energy range, and quoted an inclusive branch+0.18 ing fraction of (3.21 Â± 0.43 Â± 0.27â0.10 ) Ã 10â4 , where the errors are statistical, systematic and model-dependence, respectively. The much larger data samples of the B Factories and to some extent their improved detectors have allowed for a number of significant advances in the analysis techniques, leading to large reductions in the systematic uncertainties as well as the statistical uncertainties of measured branching fractions. They have also made it possible to reduce the photon energy threshold, in one case down to 1.7 GeV, and detailed studies of the EÎ³ spectrum are used to help constrain the model-dependent extrapolation. The world-average extrapolated branching fraction now has an uncertainty comparable to that on the theoretical prediction. In the following, four measurements of the inclusive B â Xs Î³ branching fraction are described. Three of these are fully inclusive, while the fourth builds up the branching fraction as a sum of exclusive final states. The hallmark of a fully inclusive measurement is that for a signal B it requires only the detection of a high-energy photon with EÎ³ close to half the b-quark mass. Because of this the processes B â Xs Î³ and B â Xd Î³ are not separated. For branching fractions, the B â Xd Î³ contribution is easily subtracted using B(B â Xd Î³) = (|Vtd |/|Vts |)2 = 0.044 Â± 0.003 . (17.9.21) B(B â Xs Î³) As the measurement is based on the photon, the result is not much aï¬ected by uncertainties in the hadronization process of the s quark. However, the measured value of EÎ³ is subject to electromagnetic-calorimeter resolution. Also, because (for two of the three measurements) the B rest frame is not known, there is Doppler smearing due to the motion of the B in the Î¥ (4S) center-of-mass frame. Inclusiveness is not compromised by imposing requirements on the non-signal B (B) meson in the event."
311,1836,0.986,The Physics of the B Factories,"where the uncertainty is due to the experimental uncertainty in ÎÎ·b (1S) . If the Î·b (2S) width was allowed to float in the fit, a value of ÎÎ·b (2S) = (4+12 â20 ) MeV or ÎÎ·b (2S) < 24 MeV at 90% C.L. using the Feldman-Cousins approach (Feldman and Cousins, 1998) was obtained. Results are given in Tables 18.4.2 and 18.4.3. Systematic uncertainties in the Î·b (nS) parameters were evaluated due to the background fit function choice, fit range and binning choice, as well as signal shape and contributions from the experimental hb (nP ) mass uncertainties and photon energy resolution. The various contributions in quadrature to estimate the total systematic uncertainty. The eï¬ciencies used to normalize the above radiative transition yields were determined using a combination of Monte Carlo and data-driven studies (Mizuk, 2012)"
30,161,0.986,Determinants of Financial Development,"where yit is the dependent variable FD, xit is the explanatory variable POLITY2, zit is a vector of controlling variables including the logarithm of the real GDP per capita (LGDP), trade openness (OPENC), aggregate investment (CI) and the black market premium (BMP). OPENC is the logarithm of one plus the trade share, the sum of exports and imports over GDP (at current prices), divided by 100. CI is the ratio of investment to real GDP per capita (using domestic prices), divided by 100. BMP is the logarithm of one plus the black market premium divided by 100. Î´ is a parameter vector, e.g. (Î´1 , . . . Î´4 ), . Î·i is an unobserved time-invariant country-specific effect and can be regarded as capturing the combined effect of all omitted variables. Ït is the time effect. vit is the transitory disturbance term. We assume that the transient errors vit are serially uncorrelated. In system GMM estimation all xâ² s and zâ² s are assumed to be potentially correlated with Î·i and predetermined with respective to time-varying errors.70 To avoid the potential endogeneity of explanatory variables, lagged values of xi, t and zi, t are included in the regression equation, which allows feedback from the past shocks onto xi, tâ1 and zi, tâ1 while the current and future realizations of yit do not affect them. The assumption is inspired by Rodrik and Wacziarg (2005), who argue that âdemocratisations tend to follow periods of low growth rather than precede themâ. In contrast to the GMM approach, the following biascorrected Least Squares Dummy Variable (LSDV) estimation assumes all xâ² s and zâ² s to be strictly exogenous, which rules out the possibility of feedbacks from the past, current and future shocks onto xi, tâ1 and zi, tâ1 . When the Ordinary Least Square (OLS) technique is used to estimate this model, the OLS estimate of Î± is inconsistent and likely to be biased upwards since the lagged values of yit are positively correlated with the omitted fixed effects. A number of methods have been developed to deal with the presence of fixed effects in the dynamic panel data model. By using a within group operator, the LSDV method eliminates any omitted variables bias created by the unobserved individual effect and estimates the new model below by OLS:"
148,60,0.986,Anti-fragile ICT Systems (Volume 1.0),"2.8 Talebâs Four Quadrants Following Taleb [11, 12], we create a map to classify the negative impact of different failures in complex adaptive ICT systems. We again represent the impact of events in a complex adaptive ICT system by a continuous random variable with a particular PDF. Furthermore, we discriminate between two types of negative impacts, namely, local and global impacts. Some systems only permit the local impact of failures, while other systems allow local failures to propagate and create a global (systemic) impact. The PDF of the local or global impact has a thin or thick left tail. The four quadrants of the map in Fig. 2.4 represent the four possible combinations of local and global impacts and thin and thick tails. The quadrants represent four classes of complex ICT systems with very different extreme behaviors. The map shows where classical risk analysis works well and where it is of questionable use"
233,367,0.986,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"Standardisation of Sampling The most commonly used application for rarefaction is standardisation to allow comparisons to be made between datasets with differing amounts of sampling effort. Standardisation can be achieved by rarefying all datasets back to a common (typically the minimum) number of accumulation units (Sanders 1968; Gotelli and Colwell 2001). Law et al. (1998) surveyed bats in ten State Forests of the south-west slopes region of New South Wales, Australia. Survey methods were a combination of ultrasonic detectors, harp-traps, mist-nets and trip-lines. For the purposes of this demonstration, only data from the harp-traps will be used. A harp-trap is a rectangular frame, stringed vertically with nylon line, placed so as to intercept the flight path of low-flying bats (Tidemann and Woodside 1978). A bat striking the nylon lines of the trap will tumble down into a collecting bag at the bottom. Sampling effort among State Forests was variable with between 8 and 30 trapnights. Comparison of bat diversity between State Forests is therefore confounded by variation in sampling effort, as can be seen when plotting separate PD rarefaction curves for each State Forest (Fig. 3). To correct for variation in trapping effort, expected PD for each State Forest was calculated for the common value of 15 individuals, which was the minimum number recovered from a State Forest (Fig. 3). While rarefying to eight trap-nights (samples) would also be an appropriate method of standardisation, data on the bat species caught per trap-night were not available in Law et al. (1998). Standardising for sample effort changed the rank order of the sites for Phylogenetic Diversity (Table 1). A test of the rank correlation between the standardised and non-standardised PD values was relatively high but non-significant (Spearmanâs correlation coefficient, rho = 0.57, p = 0.084). Therefore, what one concludes about the relative bat diversity (and perhaps conservation importance) among these sites is dependent upon whether or not sampling effort is taken into account."
311,2090,0.986,The Physics of the B Factories,"ms 1 < mDs (19.1.70) tan Î² mc 3Ï The parameter Ï is the total relative uncertainty on fDs coming from theory and measurements. At present Ï â¼ 3% and evidence for new physics can be obtained if tan Î² < 2.1 GeV/c . In future, when precision of â¼ 1% can be reached, this condition becomes: tan Î² < 3.6 GeV/c . Unless the charged Higgs boson mass is rather low or the value of tan Î² quite large, no new physics contributions are expected and measurements of leptonic charm decays therefore provide stringent tests of lattice QCD calculations. It should be noted also that relative uncertainties coming from external parameters as the Ï mass, the Ds+ mass, the value of |Vcs |, and the Ds+ lifetime have a total contribution of 0.7%. The largest contribution is from the Ds+ lifetime which needs therefore to be more accurately measured. 19.1.7 Rare or forbidden charmed meson decays 19.1.7.1 Measurement of the Branching Fractions of the Radiative Charm Decays D0 â KÌ â0 Î³ and D0 â ÏÎ³ In the b-quark sector, radiative decay processes have provided a rich field to study the Standard Model of particle physics. These decays are dominated by short-range electroweak processes, whereas long-range contributions are suppressed. The situation is reversed in the charm sector, where radiative decays are expected to be dominated largely by non-perturbative processes, examples of which are shown schematically in Fig. 19.1.42. Long-range contributions to radiative charm decays are expected to increase the branching fractions for these modes to values of the order of 10â5 , whereas short-range interactions are predicted to yield rates at the 10â8 level. Given the expected dominance of long-range processes, radiative charm decays provide a laboratory in which these QCD-based calculations can be tested. Numerous theoretical models have been developed to describe these radiative charm decays. The two most comprehensive studies (Burdman, Golowich, Hewett, and Pakvasa, 1995; Fajfer, Prelovsek, and Singer, 1999) predict very similar amplitudes for the dominant diagrams shown in Fig. 19.1.42. The first observation of flavor changing radiative decay of charm mesons, D0 â ÏÎ³, was accomplished by Belle using 78 fbâ1 (Abe, 2004c). To reduce the combinatorial background the measurement is performed using Dâ+ â D0 Ï + decays, and the Ï meson is reconstructed in decays to K + K â . The photons are required to have an energy in excess of 450 MeV/c2 (and not yielding a Ï 0 mass with any additional photon in the event - the Ï 0 veto). Furthermore, the | cos Î¸hel | < 0.4 requirement, where Î¸hel is the angle between the D0 and the K + mesons"
29,68,0.986,"Micro-, Meso- and Macro-Connectomics of the Brain","three orders of magnitude, showing a long-tailed distribution with small numbers of strong and a large numbers of weak connections. Although the connection strength in mouse cortex varies over a narrower range than in macaque (Markov et al. 2011), the lognormal distribution found in both species indicates that the fundamental principles of cortical connectivity are evolutionarily conserved. In primates, visual information is processed in dorsal and ventral cortical streams specialized for âwhereâ an object is located or for guiding actions and âwhatâ an object is (Ungerleider and Mishkin 1982; Goodale and Milner 1992). If such streams exist in mice, how do they arise from a network with seemingly low binary specificity? One way this might be achieved is by routing the flow of information through pathways with different connection strengths. Consistent with this notion, we found that each source area of visual cortex had a unique profile of connection strengths. We assessed between-area similarities and found that the projection strengths among dorsal and ventral networks were distinct. The dorsal network consisted of areas AL, rostrolateral area (RL), anteromedial area (AM), posteromedial area (PM) and anterior area (A), whereas V1, LM, laterointermediate area (LI), postrhinal area (POR) and posterior area (P) were grouped in the ventral network (Wang et al. 2012). Although streams were revealed in the graph of cortex-wide connections, we wondered whether they were present in the 10  10 connectivity graph of visuotopically organized areas. The graph of projection strengths clearly grouped areas into dorsal (i.e., AL, RL, AM, PM, A) and ventral (i.e., V1, LM, LI, POR, P) communities in which connections within modules were twice as strong as those between modules. Within modules, the shortest pathways were always direct. By contrast, the shortest pathways between modules were often indirect, which means that the combined strength of the indirect path was stronger that the direct path. Thus, for communication between modules, the most effective path may be indirect. Interestingly, not a single short path linking the two modules travels through V1, indicating that, similar to cat and monkey (Sporns et al. 2007), V1 is not a network hub for interareal communication. Instead, judged by the number of connections, this role belongs to area LM. Although lower in the hierarchy than monkey V4, which has a similar status in the network, LM may play a critical role in integrative processing of visual information."
175,410,0.986,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications",4.5.3.4 Piecewise Linear Model There are a number of ways of modeling the piecewise linear concave beneï¬t function shown on the right side of Fig. 4.28. Several are deï¬ned in the next several sets of equations. Each method will result in the same model solution.
311,1128,0.986,The Physics of the B Factories,"tor of 1 â 2Râ¥ , where Râ¥ is the fraction of the CP -odd component. BABAR uses the previously measured value 0.233 Â± 0.010 Â± 0.005 (Aubert, 2007x). Systematic errors on the time-dependent asymmetry parameters are summarized in Table 17.6.2. The dominant sources for S are due to the uncertainties in vertex reconstruction and Ît resolutions, flavor tagging, and background in the J/Ï KL0 mode. The systematic error on C is dominated by tagside interference. For this source, Belle takes into account a cancellation between CP -even and CP -odd states, while BABAR does not. Chapter 15 discusses the main sources of systematic uncertainty on time-dependent CP asymmetry parameter measurements in detail. The Ît distributions and asymmetries obtained from the data for all modes combined are shown in Fig. 17.6.7. The values of C obtained are consistent with zero in accordance with SM expectations, and hence âÎ·f S gives essentially sin 2Ï1 . The average of the two experiments"
273,179,0.986,Report on Global Environmental Competitiveness (2013),"In which, Y is the GEC comprehensive evaluation score, Yi1 is the evaluation score of Module Indicator i, Yij2 is the evaluation score of Factor Indicator j, xijk is the non-dimensional data value of Foundation Indicator k under Factor j in Module i, wijk is the weight of this Foundation Indicator, l represents the number of Module Indicators in the GEC indicator system, m is the number of Factor Indicators in each Module Layer, and n is the number of Foundation Indicators in each Factor Layer. With the GEC model, evaluation of a countryâs environmental competitiveness becomes a simple job, because the weight of each indicator is fixed and the only thing to be done is to input the non-dimensional data value of the Foundation Indicators of the country; then the GEC score as well as the scores of each Module Indicator and Factor Indicator can be obtained. The model can also carry out comprehensive evaluation on each countryâs environmental competitiveness; all countries can be ranked, compared and analyzed according to respective comprehensive evaluation scores."
175,814,0.986,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","(d) Using the estimated model in (c), generate a 50-year synthetic streamflow record and demonstrate that the mean, variance, and show that ï¬rst autocorrelations of the synthetic flows deviate from the modeled values by no more than would be expected as a result of sampling error. 6:39 (a) Assume that one wanted to preserve the covariance matrices S0 and S1 of the flows at several site Zy using the multivariate or vector ARMA(0, 1) model Zy Ã¾ 1 Â¼ AVy  BVy1 where Vy contains n independent standard normal random variables. What is the relationship between the values of S0 and S1 and the matrices A and B? (b) Derive estimates of the matrices A, B, and C of the multivariate AR(2) model Zy Ã¾ 1 Â¼ AZy Ã¾ BZy1 Ã¾ CVy using the covariance matrices S0, S1, and S2. 6:40 Create a model for the generation of monthly flows. The generated monthly flows should have the same marginal distributions as were ï¬tted to the observed flows of record and should reproduce (i) the month-to-month correlation of the flows, (ii) the month-toseason correlation between each monthly flow and the total flow the previous season, and (iii) the month-to-year correlation between each monthly flow and the total 12-month flow in the previous year. Show how to estimate the modelâs parameters. How many parameters does your model have? How are the values of the seasonal model? How do you think this model could be improved?"
244,78,0.986,"Advancing Human Assessment : The Methodological, Psychological and Policy Contributions of Ets","where exp denotes exponentiation, nl is the sample size at test score yl, and h is a kernel smoothing bandwidth parameter determining the extent of smoothing (usually set at 1.1Nâ0.2; Ramsay 1991). The rationale of the kernel smoothing procedure is to smooth out sampling irregularities by averaging adjacent xik values, but also to track the general trends in xik by giving the largest weights to the xik values at y scores closest to yk and at y scores with relatively large conditional sample sizes, nl. As indicated in the preceding Livingston and Dorans (2004) quote, the kernel smoothing in Eqs. 2.14 and 2.15 is also applied to the conditional percentages of examinees omitting and choosing each distracter that contribute to 1â xik . Standard errors and confidence bands of the raw and kernel-smoothed versions of xik values have been described and evaluated in Lewis and Livingston (2004) and Moses et al. (2010)."
241,821,0.986,Second Assessment of Climate Change for the Baltic Sea Basin,"is targeted at the combined 95th percentile based on the estimated uncertainty ranges in Table 14.5 while allowing for a more intense forcing scenario than the SRES A1B scenario (see Table 14.1). Of particular importance in the Baltic Sea is the long-tailed uncertainty of the Antarctic Ice Sheet, dynamic contribution for which the high-end estimate of 40 cm used is similar to that of Spada et al. (2013). However, this should not be interpreted in terms of a strict conï¬dence bound given how the uncertainty ranges have been derived. At present, this high-end estimate is not considered likely from a process model perspective; however, there are several other lines of evidence that point to even greater sea-level rise being plausible. The total ice sheet contribution adopted here is 25 cm lower than the very likely upper range derived from an expert elicitation (Bamber and Aspinall 2013). Furthermore, the high-end scenario is more"
256,502,0.986,"Supercomputing Frontiers : 4Th Asian Conference, Scfa 2018, Singapore, March 26-29, 2018, Proceedings","underlap approach [15], and significant convergence degradation was observed with the latter preconditioning. In addition, if one uses a MPK with CA preconditioning, s-step halo data is transferred at once. The number of halo data communication directions are significantly increased from 6 (bidirectional in x, y, z) to 26 (including three dimensional diagonal directions), and redundant computations are needed for the halo data. In order to avoid these issues, in this work, we use the BJ preconditioning with a hybrid CA approach [3]. In the P-CACG method, dominant computational costs come from the s-step SpMVs (line 5) and the following BJ preconditioning (line 6) in the outer loop, GEMM operations for constructing the Gram matrix (line 7) in the outer loop, and three vector operations for the three term recurrence formulae (lines 22â24) in the inner loop. Here, the size of GEMM operations depends on s, and thus, their arithmetic intensity is increased with s. If one applies cache blocking optimization, coefficients of the three term recurrence formulae can be reused for s-steps, and the arithmetic intensity of three vector operations is also improved by extending s. The SpMV requires one local halo data communication per inner iteration as in the P-CG method, while the Gram matrix computation needs only one All reduce for s(s + 1) elements of Gk,kâ1 and (s + 2)(s + 1)/2 upper-triangular elements of Gk,k per outer iteration. In addition, we compute the norm of residual vector rsk = b â Axsk+1 for the convergence check, which require one All reduce per outer iteration. Therefore, the P-CACG method requires two All reduces per outer iteration."
45,265,0.986,Measurement and Control of Charged Particle Beams,"This can be seen by inspecting the raw data shown in Fig. 4.4. For more complex beam distributions, a somewhat better characterization is achieved by applying an âasymmetric Gaussianâ fit to the wire-scan measurement, in which the left and right hand sides of the measured beam profile are approximated independently by two separate Gaussians. For example, the fitting function used at the SLC [7] was (x â x)2 (4.37) f (x) = f0 + fmax exp â 2x2 (1 + Î±[sign(x â x)] where Î± represents an asymmetry factor and is zero for a perfect Gaussian2 . The Ï for the left and right hand sides of the fitted distribution are Ï = x2 (1 Â± Î±). For the ellipse reconstruction the average Ï was used. When large tails are present in the raw data this more accurately represents the beam distribution. Based on the raw data it is clear, however, that even with the more sophisticated fitting algorithm, the fit only marginally represents the actual distributions. For reasonably well âmatchedâ beams, the graphical summary display is most useful for quick evaluation of the beam. In this example the deviations between the design and measured ellipse in the graphics suggest that a closer inspection of the raw data may be warranted. The âdouble-humpsâ in the single-wire measurements are characteristic of an upstream error; a beam, if kicked transversely, will filament, i.e., it loses coherency due to the natural spread in the phase advance between particles, resulting in an increased emittance and the characteristic double humps. If a wire is mounted at 45â¦ with respect to x and y (a âu-planeâ wire), then it is also possible to measure the coupling between the horizontal and vertical plane. The (4Ã4) Î£-matrix is â Î£21 Î£beam = â"
311,1898,0.986,The Physics of the B Factories,"Ï2 < 20 (for 1 degree of freedom) and be displaced by no more than 2 cm from the nominal e+ eâ interaction region in a plane transverse to the beams. The dimuon system is combined with the highestenergy photon candidate to build an Î¥ (3S) candidate. A kinematic fit is performed to the three particles, constraining the total energy of the three particles to be within the beam-energy spread of the e+ eâ collision that should have produced the Î¥ (3S), and the constraint that the particles originate from the primary interaction region. The fit is required to satisfy Ï2 < 36 (for 6 degrees of freedom), which corresponds to a probability of rejecting good kinematic fits that is less than 10â6 . Particle identification is used in certain regions of photon energy in order to reject specific backgrounds. After the above kinematic selection, the primary background is determined from MC simulation to be e+ eâ â Î¼+ Î¼â Î³. In a photon-energy region corresponding to mA0 < 1.05 GeV/c2 , contributions from Ï â K + K â (where K + â Î¼+ Î½) and Ï â Ï + Ï â are suppressed by requiring that both tracks be positively identified as muons using particle identification. In an mA0 region corresponding to the location of the Î·b mass, events are required to have no additional photons with EÎ³ > 0.08 GeV; this suppresses radiative transitions of the Î¥ (3S) to the Î¥ (2S) through a Ïb state. The eï¬ciency of the above selection of A0 â Î¼+ Î¼â is studied using a signal MC simulation and varies between 24-44%, depending on mA0 . The signal yield is extracted from the data in the range 0.212 â¤ mA0 â¤ 9.3 GeV/c2 using a maximum-likelihood fit to the variable, (18.4.24) mR = m2Î¼Î¼ â 4m2Î¼ ."
311,1310,0.986,The Physics of the B Factories,"or Gounaris-Sakurai in the case of Ï0 â Ï + Ï â , with BlattWeisskopf centrifugal factors and angular terms (see the Isobar formalism text in Section 13.2.1). The resonance composition measured by Belle is shown in Table 17.8.4. Note that Ï1 and Ï2 states, with masses and widths allowed to vary in the fit, are introduced as an eï¬ective description of structure in the ÏÏ S-wave."
372,813,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"spacing between adjacent levels is represented by *, in units of the (unquantized) rms voltage, #, i.e., *# is the spacing between the levels in volts. We consider first the case in which the number of levels is even, as in Fig. 8.9. Any one sample that falls between the two consecutive thresholds at m*# and .m C 1/*# will be assigned a value .m C 21 /*#. The normalized trapezoidal probability distribution for the voltage in this segment of the overall probability distribution in Fig. 8.9 can be written as $ "" p.v/ D m*# < v < .mC1/*# ; (8.60) C v! mC *# &m where &m is the change in probability, over the voltage range m*# to .m C 1/*#. The extra variance that is incurred by quantizing the voltage is"
45,463,0.986,Measurement and Control of Charged Particle Beams,"8.9 Multi-Bunch Energy Compensation Two methods, known as Îf and Ît compensation have been proposed to combat multibunch phase transients in linear accelerators. Shown in Fig. 8.25 is the principle of Ît compensation [51]. Here the voltage Vk represents the voltage response of a finite bandwidth accelerating structure to a step function input pulse. The lower curve represents the beam-induced voltage Vb of the entire bunch train. By injecting the beam prior to the time the linac structure is at peak voltage, the vector sum Vk +Vb is observed to be flat over the duration of the bunch train. The projected energy spread is therefore minimized and the phase relationship between the bunches is constant. The principle of the Îf compensation is illustrated [52] in Fig. 8.26. In this design from the ATF in Japan, some fraction of the many accelerating structures are slightly detuned by Â±Î´f . The different bunches therefore obtain a different energy gain which depends on their location within the train."
30,65,0.986,Determinants of Financial Development,"In Tables 2.1 and 2.2, the BMA procedure has yielded PIPs for all candidate variables. A natural question to ask is about the structure of the models, especially the models with higher explanatory power. Table 2.3 lists the structure of the top ten models for FD in the whole sample in terms of posterior model probabilities, serving as a concrete illustration of model selection. A noteworthy point is that all these models have more than ten possible predictors with geographic variables (such as REGEAP, AREA), policy variables (such as EXPPRIM) and institutional variables (like KKM and PCI) and âotherâ variables (like GDP90, POP90, ETHPOL and EURFRAC) present in all models. However, one should be aware of the dramatic model uncertainty, reflected by less than 5% posterior model probabilities for all top ten âbestâ models, which indicates the potential importance of the BMA and Gets procedures for model selection as a systematic response to pervasive model uncertainty. Moving on one step further, OLS regressions are used to estimate some of the best performing models in Table 2.4. The best model, that is the model with highest posterior probability in Table 2.3, is presented in column 4. The âotherâ variables, like GDP90, POP90 and EURFRAC, are"
349,59,0.986,Methods for Measuring Greenhouse Gas Balances and Evaluating Mitigation Options in Smallholder Agriculture,"the entire scene is (visually) split into a few (larger) regions (or strata) that can be considered homogeneous in terms of land-cover characteristics and the physical setting of the landscape. The stratiï¬cation is usually done just after the automated image segmentation (Fig. 2.4). Of course, results from other studies can be used as well (e.g., boundaries shown in Fig. 2.2). Figure 2.5a shows the RGB composite of a WorldView-2 image of the Nyando study area, and Fig. 2.5b, the corresponding DEM. In both maps, manually drawn landscape boundaries (strata) are also shown (yellow lines). For one of the strata, Fig. 2.6a shows the available reference information obtained from ï¬eldwork and complemented through visual image interpretation. These training samples are necessary for the RF classiï¬er to âlearnâ the relationship between input features and class labels. The resulting object-based classiï¬cation is shown for this landscape unit in Fig. 2.6b. The object limits (e.g., gray lines in Fig. 2.6a) have been automatically derived using GRASS GIS. For the classiï¬cation, several algorithms are available (e.g., maximum likelihood classiï¬er, CART, kNN, etc.). Based on the authorsâ own and published experience, we exploited a widely used ensemble classiï¬er called ârandom forestâ (RF) which often yields good and robust classiï¬cation results (Gislason et al. 2006; RodriguezGaliano et al. 2012; Toscani et al. 2013). RF uses bootstrap aggregation to create different training subsets, to produce a diversity of classiï¬cation trees, each providing a unique classiï¬cation result. For example, if 500 decision trees are grown inside the RF, one will obtain 500 class labels for each object. The ï¬nal output class is obtained as the majority vote of the 500 individual labels (Breiman 2001). The proportion of votes of the winning class to the total number of trees used in the classiï¬cation is a good measure of conï¬dence; the higher the score, the more conï¬dent one can be that a class is correctly classiï¬ed. Similarly, the margin calculated as the proportion of votes for the winning class minus the proportion of votes of the second class indicates how sure the classiï¬er was in their decision. Such conï¬dence indicators are not readily obtained using visual image interpretation. RF also produces an internal unbiased estimate of the generalization error, using the so-called âout-of-bagâ (OOB) samples to provide a measure of the input featuresâ importance through random permutation. Classiï¬cation performance of the entire LULC map can be based on common statistical measures (overall accuracy (OA), producerâs accuracy (PA) and userâs accuracy (UA)) (Foody 2002) derived from the classiï¬cation error matrix, using suitable validation samples. Figure 2.7 shows the resulting LULC map of Nyando obtained with this object-based classiï¬cation approach and using VHR imagery from WorldView-2Â®."
372,719,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"7.3.9 Double-Sideband Systems The considerations up to this point have applied to single-sideband (SSB) systems. For double-sideband (DSB) systems, some differences must be considered (Thompson and DâAddario 2000). For an SSB system, the main effect of a phase error is to cause a rotation of the correlation vector, as indicated in Fig. 6.5a, resulting in an error in the correlator output phase, as considered above.7 For a DSB system, the delay error causes the components of the correlation vector resulting from the two sidebands to rotate in opposite directions in the complex plane, as shown in Fig. 6.5b, where the line AB represents the phase angle when the delay error is zero.8 The amplitude of the vector sum of the two components is proportional to cos.2&!0 ""%/, where !0 is the IF center frequency, but the phase of the correlation is not changed by a variation in the instrumental delay. Consider a case in which the geometric delay is varying rapidly enough that the delay error changes sign several times during the minimum averaging time at the correlator output. For an SSB system (Fig. 6.5a), the phase of the correlation vector swings back and forth, following the difference of the error patterns for the two antennas (the small arrows indicating variation of the vector phase in Fig. 6.5 reverse direction when the sign of the phase error changes). For a DSB system (Fig. 6.5b), the phase angles of the vectors representing the two sideband responses move in opposite senses. In both the SSB and DSB cases, components of the correlation that are normal to the vector time-average (in Fig. 6.5b, the line AB) cancel, and the magnitude of the correlation is proportional to the time average of the cosine of the phase measured with respect to the mean phase. Over an averaging period in which the SSB phase error changes sign, the loss in sensitivity is effectively the same for the SSB and DSB systems. Note, however, that in the SSB case, the loss in sensitivity occurs in the averaging, whereas in the DSB case, the loss occurs immediately in the correlation process. Thus, in the SSB case, there is an opportunity to correct for phase errors after cross-correlation, but in the DSB case, this is possible only if the sideband responses can be separated. If we are considering delay errors that are quasi-constant, or vary only slowly with time, the tolerance on the errors is more stringent in the DSB case. Such errors were more important in early interferometers with analog delay systems using coaxial cable or ultrasonic elements (see, e.g., Coe 1973), which could be temperature sensitive and difficult to calibrate accurately. In digital systems, the delays are controlled by a highly accurate master clock, and the only significant"
142,216,0.986,Subseafloor Biosphere Linked To Hydrothermal Systems : Taiga Concept,"of the inter-species comparison, the trend in genetic diversities of local populations of two limpets Bathyacmaea secunda and Lepetodrilus nux differed. Higher genetic diversity was observed in the southern populations in Bathyacmaea secunda, but lower genetic diversity was observed in the same ranges of populations in Lepetodrilus nux (Fig. 5.9b, c). Ecological features of vent faunas are necessary for more precise estimation of biological and geological histories of vent fields. The trends of diversity are also different among species, even for similar animals inhabiting areas that are close to each other (Fig. 5.10). This result indicates that the genetic diversities of the species positioned on the right-hand side develop quickly. However, those of species on the left-hand side develop slowly. In other words, the variety of genetic diversity means that we have a series of scales with several ranges of resolution. Among the analyzed species shown in Fig. 5.9, Shinkaicaris leurokolos is the species for which the quickest genetic substitution is expected: all analyzed sites showed uniformly high diversities. However, such diversity has not developed among the slowest species: Lepetodrilus nux. Among these three, the intermediate, Bathyacmaea secunda, might adequately reflect the ages of the hydrothermal vent sites, which is apparently consistent with geochronological results, even including extremely old ages (Fig. 5.5). According to the genetic diversities of Shinkaicaris leurokolos and Bathyacmaea secunda, the estimated ages of populations are on the order of 104 years, which corresponds well to the ranges inferred using a geochronological approach. Ecological information will help to interpret these results of phylogeographic analyses and will be useful to estimate the ages of hydrothermal vent sites in the Okinawa Trough. Even for the same vent sites, inhabitants show various levels"
32,658,0.986,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","The degree distribution can be well fitted by a straight line on log-log scale indicating scale-free networks with power law degree distribution with form P.k/ / k . The curves with different values of  and  can be rescaled by 2m2 to get data collapse as it is shown in Fig. 29.2b. This means that the exponent is independent from m in all cases not only for BA networks. The exponent of the degree distribution is independent of the number of nodes connected in the first step  and in each secondary step  as well, its value is D 2:895 Ë 0:038 as expected. The value of the exponent is obtained by averaging the exponents of systems at different input parameter combinations. This independence needs some explanations. Letâs see for example the  D 1 and  D 9 system. Only 10 % of the links based on purely preferential attachment and 90 % just randomly connected to the neighbors of popular nodes. How can this network be scale-free? As a matter of fact the 90 % also preferred, because sooner or later these neighbors also become popular as they popular neighbor gets more and more links. To characterize the networks from the point of view of the cliques I calculated the clustering coefficient of nodes in my undirected graphs. Local clustering coefficient C of a node is the ratio of the number of existing links between neighbors of this node and the number of possible connection between them. In a general case C is proportional to the reciprocal of the degree of node, which indicates small degree nodes are mainly members of cliques while hubs of the networks connect them together."
327,39,0.986,Bottom-Up Fabrication of Atomically Precise Graphene Nanoribbons,"k? Â¼ N Ã¾r 1 2p a , r being an integer between 1 and N. Keeping in mind that grapheneâs vanishing band gap and the massless electron behavior occur right at the K-point, the closer those 1D-cuts get to the K-point, the lower the aGNRâs band gap and the effective masses of its frontier bands will be. That is, some of the aGNRsâ attributes can be readily understood from simple geometric considerations: along the 2 2p 1D-projection, K and Kâ are located at k? Â¼ 13 2p a and k? Â¼ 3 a (Fig. 8a). It is the 3 in the denominator that accounts for the presence of three aGNR families, depending on whether their width in terms of dimer lines N is equal to 3p, 3p + 1 or 3p + 2 (p being an integer). There is always a cut going directly through the K-point for the 3p + 2 family. This family thus has the lowest band gap and effective mass, these values being higher on either of the other two families for which no cuts across the K-point are present. However, the wider the GNR, the smaller the spacing between the cuts and the closer they get to the K-point. This determines the inverse proportionality of band gap and effective mass with nanoribbon width within each of the families, although if the width increases atom by atom it would imply changes between families that could result in an increased band gap and effective mass as well. It is worth remarking here that, under the assumption of equivalent carbon atoms throughout the GNR, tight binding calculations predict the 3p and 3p + 1 families to follow a very similar trend. However, this assumption is inaccurate the closer the atoms are to the edge, most evidently for the hydrogen saturated carbon atoms. Taking into consideration bond length distortions and the associated changes in the hopping constant, important changes appear in the resulting band gap calculations, as are a clear splitting of the 3p and 3p + 1 bandgap trends, as well as the opening of a bandgap for the 3p + 2 family. These ï¬ndings coincide with more reï¬ned ab initio calculations revealing the band gaps of the three different families to order according to Eg (3p + 1) > Eg (3p) > Eg (3p + 2) [11, 71]. Besides tight-binding [11, 69, 71], several other methods have been used to quantitatively describe the width-dependent band gap in aGNRs, like for example extended HÃ¼ckel theory (EHT) [78], density functional theory (DFT) [11, 79], or the GW approximation [79]. At this point, it is important to remind that, to date, all the experimentally reported band gaps of atomically precise GNRs are from surface-supported ribbons. While less important on weakly interacting substrates, if more reactive surfaces are used or reactive functional groups are added to the GNR structure, strong GNR/substrate hybridizations may occur. Under such circumstances the band gap will be substantially affected and the GNRâs electronic properties may bear little resemblance with those of free-standing GNRs. In addition, the band gaps of GNRs have been most commonly characterized by ionizing techniques such as STS [34, 43, 45, 46] or a photoemission/inverse photoemission combination [35]. This implies on the one hand that it is not ground states but quasiparticle energies which are probed, and on the other hand that the surrounding dielectric medium (both the GNR itself as well as the substrate) can have a substantial effect on those stateâs energies through polarization-induced screening. Among the various theoretical methods mentioned above, the intrinsically present"
175,956,0.986,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","were generated independently from normal distributions with the means and variances as shown in Table 8.6. The table contains the speciï¬ed means and variances for the generated values of L, q, and z, and also the actual values of the means and variances of the 200 generated values of L, q, z and also of the 200 corresponding generated output phosphorus concentrations, P. Figure 8.18 displays the distribution of the generated values of P. One can see that given the estimated levels of uncertainty, phosphorus levels could reasonably range from below 10 to above 25. The probability of generating a value greater than 20 mg/m3 was 12.5%. The 5% to 95 percentile range was 11.1â23.4 mg/m3. In the ï¬gure, the cumulative probability curve is rough because only 200 values of the phosphorus concentration were generated, but these are clearly enough to give a good impression of the overall impact of the errors."
165,191,0.986,New Methods for Measuring and Analyzing Segregation,"An integration-promoting exchange is âdepolarizingâ because it moves the two areas involved in the exchange closer together on area proportion White since | p i â p j | is smaller after the exchange is completed. At the same time, the exchange is âdeconcentratingâ because pairwise same-group contact goes down for both Whites and Blacks in the affected areas. Again, since the residential distribution of Whites and Blacks in other areas is unchanged, the exchange reduces overall area polarization, reduces overall group concentration, reduces overall group separation, and lowers the value of S. Based on this, it is clear that the underlying logic of the separation index (S) resonates well with the exchange criterion. In contrast, the underlying logic of the dissimilarity index (D) is often at odds with the criterion. D registers integration-promoting exchanges only in the circumstance that the racial composition of the two areas involved in the exchange are on opposite sides of P, proportion White for the city overall. Integrating-promoting exchanges that involve areas with racial compositions on the same side of P have no impact on D. In addition to meeting the minimum requirements for satisfying the exchange criterion, the separation index (S) has additional properties that in my opinion are desirable for assessing whether groups live apart or together. I list them as follows.7 â¢ All else equal, an integration-promoting exchange produces a larger reduction in S when the two areas involved in the exchange are more polarized. I term this the âpolarizationâ property with polarization or dispersion being based on the initial size of | p i â p j | . Substantively, this is appealing because, assuming area size is constant, exchanges between more polarized areas reduces same group contact for larger fractions of the affected population. No surprisingly, D does not have this property. â¢ All else equal, an integration-promoting exchange produces a larger reduction in S when the two areas involved in the exchange are closer to one of the polarization boundaries of all-White or all-Black. That is, the reduction is larger when the minimum of the two values | p i â1 | and | p j â 0 | is closer to 0.0. The substantive appeal of this characteristic is similar to that for the âpolarizationâ property. Here again exchanges that involve areas that are nearer to the homogeneous âpolesâ of 0 and 1 reduce same group-contact for larger fractions of the affected population. D does not have this property. â¢ The âpolarizationâ property holds throughout the full range of area proportion White (p). Thus, in contrast to D, integration-promoting exchanges have desirable impacts on reducing S regardless of whether the two areas involved in the"
372,1455,0.986,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"12.3 Time and Motion of the Earth We now consider the effect of changes in the magnitude and direction of the Earthâs rotation vector on interferometric measurements. These changes cause variations in the apparent celestial coordinates of sources, the baseline vectors of the antennas, and universal time. The variations of the Earthâs rotation can be divided into three categories: 1. There are variations in the direction of the rotation axis, resulting mainly from precession and nutation of the spinning body. Since the direction of the axis defines the location of the pole of the celestial coordinate system, the result is a variation in the right ascension and declination of celestial objects. 2. The axis of rotation varies slightly with respect to the Earthâs surface; that is, the positions on the Earth at which this axis intersects the Earthâs surface vary. This effect is known as polar motion. Since the .X; Y; Z/ coordinate system of baseline specification introduced in Sect. 4.1 takes the direction of the Earthâs axis as the Z axis, polar motion results in a variation of the measured baseline vectors (but not of the baseline length). It also results in a variation in universal time. 3. The rate of rotation varies as a result of atmospheric and crustal effects, and this again results in variation in universal time. We briefly discuss these effects. Detailed discussions from a geophysical viewpoint can be found in Lambeck (1980)."
297,1629,0.986,The R Book,"r learning which variables are random effects; r specifying the ï¬xed and random effects in the model formula; r getting the nesting structure of the random effects right; r remembering to get library(lme4) or library(nlme) at the outset. The issues fall into two broad categories: questions about experimental design and the management of experimental error (e.g. where does most of the variation occur, and where would increased replication be most proï¬table?); and questions about hierarchical structure, and the relative magnitude of variation at different levels within the hierarchy (e.g. studies on the genetics of individuals within families, families within parishes, and parishes with counties, to discover the relative importance of genetic and phenotypic variation)."
311,2981,0.986,The Physics of the B Factories,"25.1.1 Introduction The previous sections of this book present a plethora of analyses that provide measurements of various observables, which in turn can be related to fundamental theory parameters. In particular, some observables are connected with the elements of the CKM matrix, which for three quark families is specified by four independent parameters as discussed in detail in Section 16.4. One convenient parameterization of the CKM matrix is the smallangle approximation by Wolfenstein, Eq. (16.4.4). In this approximation, following the Wolfenstein-Buras redefinition, there are four parameters, A, Î», Ï and Î·, that fully determine the CKM matrix, see Eqns (16.4.5), (16.4.6), and (16.4.8). The parameter A is of order one and is determined by Vcb , Î» is the expansion parameter and is related to the Cabibbo angle by Î» = sin Î¸c , and Ï and Î· represent the apex of the Unitarity Triangle (see Section 16.5). A non-zero value of Î· indicates CP violation in the Standard Model. These four parameters are simultaneously determined by combining various experimental results and theory parameters connecting the observables to the CKM formulation in a global CKM fit. This section gives a brief overview of CP violation in the era of the B Factories (Section 25.1.2) before describing two global fit strategies that were continually updated during that time (Section 25.1.3). Experimental and theoretical inputs required in order to perform a global fit are discussed in Sections 25.1.4 and 25.1.5, respectively. Results of SM-based global fits are given in Section 25.1.6, and concluding remarks can be found in Section 25.1.7."
219,760,0.986,Economics of Land Degradation and Improvement â A Global Assessment for Sustainable Development,"The Household Plot Level Analysis This section presents the analysis of drivers of land degradation on the household level as described in the methodology. Since soil erosion induced by water is unambiguously the major symptom of land degradation in India, as shown in Table 15.2, it is regarded as a suitable proxy for land degradation in a broader sense. Table 15.13 displays descriptive statistics of all the variables used in the analysis. The main results are depicted in Table 15.14, ï¬rst column. They show that the higher the frequency of application of organic manure, as well as chemical fertilizers, the lower the likelihood of soil erosion, given equal characteristics, where the effects are signiï¬cant at 1 %. The use of pesticides, in contrast, is found to increase the occurrence of soil erosion. The number of different crops grown within the time span of the monthly survey also signiï¬cantly (p < 0.001) drives the extent of soil erosion. The quality of drainage exposes a U-shaped influence on erosion, where a good drainage system fosters erosion and a mediocre one works against it, compared to bad quality drainage. Erosion is rather present on large ï¬elds, as shown by the positive signiï¬cant coefï¬cient of the plot area. Other variables that are negatively associated with erosion are the education dummies (relative to the category âilliterateâ) and the time of the household head devoted to work on the parcel. Interestingly, land property is positively associated with soil erosion, which might hint at a certain degree of insecurity in land tenure. The second column of Table 15.14 displays results with state ï¬xed effects, which account for some variation. While some variables display lower coefï¬cients, the main explanatory variables, namely application of manure and fertilizer, respectively, remain signiï¬cant in their explanatory power. The last two columns run a usual probit, where erosion is measured with two outcomes, âyesâ or ânoâ, regardless of the extent. The results are qualitatively similar, with the coefï¬cients for use of manure and fertilizer still on a high level, while use of pesticides does not signiï¬cantly explain erosion. The positive effect of organic manure application than the effect of fertilizer application is stronger in all four speciï¬cations. Thus, the application of manure seems to be more sustainable way in terms of land conservation compared to the utilization of chemical fertilizer or pesticides."
255,379,0.986,Railway Ecology,"The diachronic analysis of the local connectivity values shows that the impact of the HSR line ranges from a few meters to several kilometers. The impact is often located near the infrastructure but, in some sections, it may occur up to 12 km from the line. This variability is related to the landscape conï¬guration and the initial state of the connectivity of the habitat patches. Indeed, all impacted habitat patches are into components fragmented by the HSR line. The extent of disturbance therefore depends on the size of these components, with a large component increasing the distance of the impact as in the north-east of the study area. This highlights the value of a regional-scale analysis for taking into account the long-distance effect of an infrastructure on connectivity. From this perspective, graph-based methods are interesting because they include both structural and functional aspects of connectivity. Our results are consistent with several previous studies that highlighted the importance of integrating the barrier effect in addition to the direct loss of habitat in environmental impact assessment (Clauzel et al. 2013; Forman and Alexander 1998; Fu et al. 2010; Girardet et al. 2013; Liu et al. 2014). This graph-based approach provides an approximation of the potential impact but not a hard and fast measurement of the true impact. In order to validate our approach, these ï¬ndings could be conï¬rmed by ï¬eld observations to test whether the real impact of the infrastructure is similar to that predicted by the model. In June 2011, a speciï¬c ï¬eld survey was conducted in the Ognon valley to observe the tree frog presence after the construction of the HSR line. A total of 227 sites was visited with 42 presences and 185 absences. The results from this survey were compared with the connectivity changes predicted by the model. All presence points were located where there was no impact according to the model. The absence points were located in more or less affected areas, with a potential connectivity change between 0 and â90%. These survey results should be considered carefully because the Spring of 2011 was very unfavorable for the tree frog due to early drying up of water bodies. These climatic conditions could therefore explain the many points of absence of the species. Furthermore, the time delay between the construction of the HSR line and the surveys was not sufï¬cient to assess the real impact of the infrastructure. Other ï¬eld surveys should be conducted in the coming years to assess precisely the conservation status of the tree frog populations and of their habitats in the region. These surveys will also identify the causes of extinction of breeding populations, as several environmental factors may lie behind the extinction process, and be compounded to the long-distance effects of the infrastructure. The method used to identify the best locations for new amphibian crossings goes beyond the prioritization of candidate sites developed by GarcÃ­a-Feced et al. (2011). It is cumulative and so includes changes made to graph topology by adding previous links before searching for the next one. Graph modelling is used to include broad-scale connectivity as a criterion to be maximized, which is a key factor for the ecological sustainability of landscapes and for the viability of metapopulations (Opdam et al. 2006). In this study, the tested locations corresponded to the links, i.e. corridors potentially used by the tree frog, cut by the HSR line. Relying on the initial network of the species increases the likelihood of functional crossings because these links already connected habitat patches before the implementation of"
78,295,0.986,The Onlife Manifesto : Being Human in a Hyperconnected Era,"of separation between nodes: for instance, if 3 nodes are randomly rewired, the degrees of separation decrease from 5 to 3. This means that, in a circle of 6 billion (people) nodes as our world could be represented today, if random links in the network would be about 2 out of 10,000, the degree of separation turns out to be 8. But if they are 3 out of 10,000, then 5! Since the pioneering work of Stanley Milgram (1967) and, later, of Mark Granovetter (1973), the idea of small world-networks became in few years one of the key words of contemporary scientific research by fostering a large set of empirical studies on the topology of complex systems. Significant effort has been made in order to structure analytical models able to capture the nature of small worldnetworks. Here, it suffices to mention only two of these. The first small worldmodel was proposed by Duncan Watts and Steven Strogatz (1998): they suggested to randomly rewire a small fraction of the edges belonging to a low-dimensional regular lattice so as to prove that the degrees of separation in the network would exponentially decrease. Yet, contrary to random networks, the shortening of the diameter proceeded along with high clustering coefficients as in regular networks. These small world-features explain the results of Milgramâs and Granovetterâs research because short diameters of the network and high clustering coefficients quantify both the low degrees of separation between two citizens picked up randomly in such a complex network like the American society studied by Milgram in the mid 1960s, and the âstrength of weak tiesâ stressed by Granovetter in the early 1970s. The second analytical model we need to examine was defined by Albert-LÃ¡szlo BarabÃ¡si (2002): he noted that most real world networks, such as the internet, grow by continuous addition of new nodes whereas the likelihood of connecting to a node would depend upon its degree of connectivity. This sort of special attachment in a growing system explains what Watts and Strogatz apparently missed, namely, the power-law distribution of the network in a topological scale-free perspective: small world-networks in the real world are indeed characterized by few nodes with very high values and by most nodes with low connectivity. The presence of hubs or of a small fraction of nodes with a much higher degree than the average offers the key to comprehend why small world-networks can be both highly clustered and scalefree. This occurs when small, tightly interlinked clusters of nodes are connected into larger, less cohesive groups. Drawing on this research, we can deepen the notion of complexity mentioned in the introduction. Todayâs onlife kosmos can indeed be comprehended in accordance with the nature of the hubs and the degree of their connectivity in a small world network, because the emergence of spontaneous orders, e.g. peer-to-peer (P2P) file-sharing systems on the internet, often goes hand in hand with the hierarchical structure of these networks (Pagallo and Durante 2009; Glorioso et al. 2010). Significantly, in The Sciences of the Artificial (new ed. 1996), Herbert Simon insisted on this point, i.e., the notion of âhierarchyâ as the clue for grasping the architecture of complexity and, moreover, the idea of ânearly decomposable systemsâ that reconciles rigid top-down and bottom-up approaches. In the wording of Simon, âthe clusters of dense interaction in the chartâ of social interaction âwill identify a rather well-defined hierarchic structureâ ( op. cit., p. 186). Furthermore, according"
311,299,0.986,The Physics of the B Factories,"for ÎE more strongly depends on the B-meson decay mode: it is much larger for low mass ï¬nal states such as Ï + Ï â (Lees (2013b) quotes ÏâE â¼ 29 MeV) than for high mass ï¬nal states such as D(â) D(â) K (del Amo Sanchez (2011e) quotes ÏâE between 6 and 14 MeV for modes with zero or one Dâ0 meson in the ï¬nal state). The energy diï¬erence and beam substituted mass, deï¬ned in Eqs (7.1.5) and (7.1.8), exploit optimally the kinematical constraints from the Î¥ (4S) decay to two B mesons. A small correlation between the ÎE and mES variables follows from their common inputs â the beam energy, measured momentum of charged particles and energy of neutrals. The correlation from the energy measurement becomes severe if the ï¬nal state particles contain high energy photons, as shown in the top scatter plot in Figure 7.1.3. The correlation coeï¬cient is +18% for mES and ÎE in B + â K + Ï 0 . The correlation can be reduced by calculating mES after modifying the magnitude of the Ï 0 momentum but retaining its direction to constrain the reconstructed B energy to be the beam energy.27 The bottom scatter plot in Figure 7.1.3 shows that the correlation between the modiï¬ed mES and ÎE is reduced and the corresponding correlation coeï¬cient is â4% (Duh, 2012). This technique is found useful only for two-body B decays with a hard photon, Ï 0 or Î· â Î³Î³ meson in the ï¬nal state. For other B decays with soft photons only, the modiï¬ed mES has similar distribution as that of mES because the mES resolution is dominated by the beam-energy spread. Furthermore, the modiï¬cation does not artiï¬cially create an enhancement in mES for the continuum background. For ï¬nal states with heavy particles, in particular B decays to baryons, the correlation becomes strong since the beam energy spread ÏEbeam dominates in both variables. The diï¬erence between the mean beam energy used in the calculation of ÎE and mES and the true beam energy of the event is the same, hence this contribution alone would lead to 100% correlation. Therefore, in these analyses other pairs of variables are preferred. If ÎE is replaced by the invariant mass"
311,437,0.986,The Physics of the B Factories,"strongly discriminating variables, then using Fisher discriminants as a discriminating variable in a likelihood ï¬t. At the selection step, signal eï¬ciencies were often adapted to the speciï¬c signal-to-background rates for the ï¬nal state being considered; for example in (Aubert, 2007ay), the cut on the | cos Î¸S | value applied in the B + â h+ Ï 0 study was chosen to retain about â¼ 80% of signal while rejecting â¼ 65% of continuum; in contrast, a tighter selection was applied for B 0 â Ï 0 Ï 0 , as a consequence of its smaller signal-to-background rate. In the same spirit, the ï¬nal update of the B 0 â Ï + Ï â , K + Ï â study (Lees, 2013b) applied a looser cut on | cos Î¸S |, achieving close to â¼ 90% signal eï¬ciency. Owing to its larger signal purity, in this study both the signal and background parameters of the Fisher p.d.f. were extracted from the signal sample itself in the maximum-likelihood ï¬t (instead of being extrapolated from sidebands or simulation control samples), thus minimizing the corresponding systematic uncertainties. The observation of the rare B + â K + K 0 and B 0 â K 0 K 0 decays (Aubert, 2006ai) is another useful illustration of linear discriminants in BABAR; the enhancement of signal sensitivity provided by a similar Fisher discriminant was instrumental in establishing the observation of these two rare channels. Concerning nonlinear discriminants, most BABAR analyses of charmless B â hhh decays (h = Ï, K) implemented NN discriminants in line with the generic strategy discussed in Section 9.4.2; at the selection level, typical cuts on the NN value were chosen to retain some â¼ 90% of signal, while rejecting up to â¼ 75% of continuum. For Dalitz-plot analyses such as (Aubert, 2009av), non-negligible correlations between the NN and the Dalitz variables for continuum events were observed, and addressed with a dedicated parameterization; in this way, the â¼ 1.4Ï separation provided by these NN discriminants could be implemented in the likelihood function, and used in the amplitude ï¬ts."
362,306,0.986,Cloud-Based Benchmarking of Medical Image Analysis,"with higher irregularities are linked at a lower level. As shown in Fig. 10.1câe, the segmentation starts from a large scale, so that the border between the ventral cavity and the chest and abdominal wall (Fig. 10.1c) is delineated, and then, at a smaller scale the, border between the thoracic and abdominal cavity (Fig. 10.1d) is identified. At the finest scale, individual organs are segmented (Fig. 10.1e). The proposed top-down approach solves two major challenges of organ segmentation. The first one is to locate the anatomical structures within the dataset. Due to respiratory motion and anatomical variation, even for the scans with similar scanning range, the location of the same organ can still vary considerably. In the proposed multiorgan segmentation framework, the location information of the major structures is first detected with higher confidence and then passed down to the lower-level structures to initialize their segmentation. This process is similar to a multi-resolution registration approach. However, the benefit of using statistical shape models at each level is that the negative influence of anatomical and appearance variation of finer structures is eliminated to a large extent. The other major challenge that the proposed method solves is to delineate the boundary between two closely attached organs. Such delineation can be difficult in certain places where the contrast between organs is very vague or vanishes. In addition to the local features, the proposed method also utilizes the shape information of larger structures to guide the segmentation, i.e. the boundary information from higher-level structures provides extra cues to guide the segmentation of the lower-level structures. Such a hierarchical framework has proved to be very robust and performed relatively well even on non-contrast-enhanced CT image when using only region-based energy based on image intensity [16, 17]. To further improve the segmentation accuracy of the hierarchical model-based method, a model-guided edge-based energy term is proposed and combined with the region-based energy term to guide the level set evolution [19]. Unlike the conventional edge-based energy terms, which ignore the orientation of the edge-related features, the model-guided edge-based energy term uses the normal direction of the shape model to suggest the searching orientation of the local structures. This makes it possible to distinguish the black-to-white edges from white-to-black edges, which generate the same edge responses when using conventional gradient and local phase measurements. As such ambiguity often exists in the area where two organsâ borders approach each other, there is a greater chance for the segmentation region to leak to the nearby organ when using conventional edge-based energy terms. Finally, to improve segmentation speed, a novel coherent propagating level set algorithm was implemented. The new algorithm forces the contour to move monotonically according to a predicted developing trend which makes the level set functions converge faster. It also makes it possible to detect local convergence, so that the parts of the boundary that have reached their final position can be excluded in subsequent iterations, thus significantly reducing computation time [20, 22]. The proposed method was tested using the VISCERAL benchmark database, and promising results were delivered within reasonable processing time without any user intervention."
80,352,0.986,Innovations in Quantitative Risk Management (Volume 99.0),"4.1 Stage 1: Fitting the Marginal Distributions via MLE The estimation for the three model parameters in the l.g.g.d. can be challenging due to the fact that a wide range of model parameters, especially for k, can produce similar resulting density shapes (see discussions in [19]). To overcome this complication and to make the estimation efficient, it is proposed to utilise a combination of profile likelihood methods over a grid of values for k and perform profile likelihood based MLE estimation for each value of k, over the other two parameters b and u. The differentiation of the profile likelihood for a given value of k produces the system of two equations:"
231,1414,0.986,North Sea Region Climate Change Assessment,"18.3.1.3 Germany In Germany, the four federal states use three different methods for evaluating design water levels on the North Sea coast and adjacent estuaries. They have been tuned to yield similar values at the Cuxhaven gauge at the mouth of the Elbe estuary between 2010 and 2012. A matching value is achieved for the method practised in Schleswig-Holstein by adding an additional measure for the surge set-up in an estuarine mouth to the value achieved by the commonly used yearly exceedance probability of 5 Ã 10â3. Hamburg has developed a new deterministic approach in order to meet the target range. Bremen and Lower Saxony met the anticipated target value beforehand by applying the traditionally used deterministic single-value method by combining the actual mean high water level with the highest values of maximum spring elevation, storm surge set-up and the chosen safety margin for climate change effects for the determination of design water levels. Design water levels in Lower Saxony and in the Netherlands at the Ems-Dollard estuary have similar values, the surge set-up of the design water level has a yearly exceedance probability of 2.5 Ã 10â4. Tolerable wave overtopping at dykes is limited to 2 [lÃ (m s)â1] in Schleswig-Holstein and to 3 % in Bremen and Lower Saxony corresponding to an overtopping volume in the range of approximately 0.1â1.5 [lÃ (m s)â1] with a tendency to correspond to the cross-sectional areas of dykes. All four states"
29,111,0.986,"Micro-, Meso- and Macro-Connectomics of the Brain","Fig. 7 Phylogeny of the neocortical sheet. Schema showing the layout of cortical areas in different classes of mammals. This figure shows that, during phylogenesis, the positions and dimensions of conserved primary areas (colored) are conserved, which contrasts with the progressive increase of the surrounding association cortex, indicated in white. The expansion of the association cortex is thought to accommodate the increase in the number of areas, possibly via a process of genetically driven duplication of areas. This can be seen for area V2 (light blue), a second-order visual area that surrounds the primary visual area, area V1 (dark blue). Note the highly consistent location in primates of MT (green), a higher-order visual area, with respect to areas V1 and V2. Throughout the phylogenetic tree, there is a remarkable consistency between the positions of the visual areas and the primary auditory area (yellow), somatosensory area (red) and secondary somatosensory cortex (orange). Top left, representation of common mammalian ancestor; lower right, common primate ancestor (Buckner and Krienen 2013)"
341,248,0.986,Freshwater Governance for the 21st Century,"There are many definitions of scale and level in the literature, and the terms are often conflated. For the purposes of this chapter, it is useful to distinguish between these two terms. Thus the term scale refers to different ordering systems for space, time, administration and jurisdiction (Ramasar 2014); and the term level refers to points along a scale which in most cases takes on a spatial unit of analysis, for example, global, basin, state, province and local levels (Gibson et al. 2000; Cash et al. 2006)."
280,151,0.986,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"Bicyclus anynana is found from Ethiopia to South Africa (Condamin 1973) and has evolved along a range of climates, but the original lab population of Bicyclus anynana stems from Malawi, a country with strong seasonality. The arrival of the dry season in Malawi is primarily cued by decreasing temperatures, whereas the arrival of the wet season is cued by increasing temperatures (Brakefield and Reitsma 1991). Lab-rearing experiments, where photoperiod and thermoperiods were varied, confirmed that average temperature and fluctuations in night- and daytime temperature were the most important determinants of eyespot size plasticity in this species (Brakefield and Mazzotta 1995). Food plant quality, however, also affected eyespot size plasticity (Kooi 1995). Once environmental cues with significant effects on the induction of plasticity were identified, the next investigations probed how and when these cues interacted with the gene regulatory networks that differentiate the eyespot patterns to modify their output in a plastic manner. In particular, these investigations focused on the mechanisms whereby average daily temperature induced the wet and the dry seasonal forms in B. anynana. The first consideration was whether temperature only exerted its effects on wing pattern development during specific developmental windows or critical temperature-sensitive stages. Early work in this system used temperature-shift experiments to identify the critical period during eyespot development that was sensitive to rearing temperature and able to modify the final size of eyespots (Kooi and Brakefield 1999). These experiments used a variety of shifts differing in length of time that the animals were kept at each of the two alternative temperatures (17 and 27  C) and times of initiation of the shift. Kooi and Brakefield (1999) concluded that the most important period of sensitivity that led to changes in the size of two of the ventral eyespots (forewing M1 and hindwing Cu1 eyespots) was the final 5th larval instar. Furthermore, while they found that temperatures experienced during the first 24 hrs of pupal development still impacted eyespot size, they concluded that temperatures experienced during this period could not shift a WS wing pattern into a DS pattern and vice versa (Kooi and Brakefield 1999). More recent work replicated these experiments, with narrower window temperature shifts, and confirmed that the late larval period, in particular, the wandering stage of development, when the larvae stop eating and start looking for a place to pupate, was the most temperature-sensitive stage for the determination of ventral eyespot size plasticity of Cu1 ventral hindwing eyespots (Monteiro et al. 2015). These experiments also highlighted that forewing and hindwing ventral Cu1 eyespots in females responded differently to temperature. Forewing Cu1 eyespots, which are normally hidden by the hindwing when the butterfly is at rest (Fig. 5.1), were much less plastic than Cu1 hindwing eyespots, which are always exposed at rest. In addition, the size of the white center in forewing eyespots was not plastic at all (Monteiro et al. 2015). Subsequent work (Bhardwaj et al. 2017), examining plasticity in dorsal eyespots, similarly concluded that the wandering"
30,304,0.986,Determinants of Financial Development,"192 Notes 102. CO2 e is the Carbon Dioxide Equivalent, the unit of measurement used to indicate the global warming potentials defined in decision 2/CP.3 of the Marrakech Accords or as subsequently revised in accordance with Article 5 of the Kyoto Protocol. 103. Data on latitude, elevation and land area for Singapore are added to the physical factors dataset of CID. 104. This inclusion is stimulated by the works of Alesina et al. (2003) and Stulz and Williamson (2003), for example. Alesina et al. (2003) argue that the ethnic and religious fractionalizations in a country are associated with its economic success and institutional quality. Stulz and Williamson (2003) show that culture, proxied by ethnic, religious and language differences, explain why investor protection differs across countries and how investor rights are enforced among countries. 105. The inclusion is due to La Porta et al. (1998) who suggest that the legal origin of a country is helpful in explaining the extent to which investor rights are protected in it. More specifically, countries with a Common Law tradition tend to place more emphasis on private rights protection and less on the rights of the state, while countries which have adopted a Civil Law tradition do the opposite. 106. The Andrews (2005) approach is very general in the sense that the effects of common shocks, which are Ï -measurable, may differ across population units, in a discrete or continuous fashion, and may be local or global in nature. 107. The addition of the spatially lagged dependent variable results in a form of endogenity, rendering the OLS an inapplicable method for spatial lag model. To estimate the spatial lag model consistently, the Generalized 2SLS and Maximum Likelihood approach (ML) have been proposed (Kelejian and Prucha, 1998, 1999; Lee, 2003, 2007; Kelejian et al., 2004; Anselin, 2006). 108. Since the spatial error model is a special case of a regression specification with a non-spherical error variance-covariance matrix, more specifically, the offdiagonal elements are non-zero. OLS estimates remain unbiased whilst the standard errors are biased. The OLS method can therefore be applied to this model with the standard errors adjusted to allow for error correlation. The spatial error model can be consistently estimated by GMM or ML (Kelejian and Prucha, 1998, 1999; Anselin, 2006). 109. This evidence is preliminary. One might find that countries like Brazil, closer to Paraguay, have large CDM credit flows. This suggests that, apart from geographic distance, other geographic variables are also important in the process of CDM development, and so are the institutional variables and financial variables. 110. Data on the great circle distances are also from Gleditsch et al. (2001). 111. If Moranâs I is greater (smaller) than its expected value, E(I), and/or Gearcyâs C is smaller (larger) than its expected value, E(C), the overall distribution of the variable in question can be reflected by positive (negative) spatial autocorrelation. 112. In this analysis, we also explore the impacts on CDM credit flows of other geographic factors such as being landlocked, the minimum distance from one of the three capital-goods-supplying centres (New York, Rotterdam and Tokyo), mean distance to the nearest coastline or a river navigable to the"
241,965,0.986,Second Assessment of Climate Change for the Baltic Sea Basin,"Background levels assume that transformations in the landscape have reached steady state, that is their natural release is in balance with natural removal processes. Increasing numbers of studies, however, show that transformations can either decrease or increase in their efï¬ciency with changing environmental conditions. For example, Weyhenmeyer and Jeppesen (2010) reported that the efï¬ciency of nitrate removal from freshwaters varies with changes in N deposition. Since substantial fractions of substances are retained or lost in boreal catchments by a wide range of transformation processes, changes in transformation processes within the landscape will have major impacts on river export. HÃ¤gg et al. (2010) estimated that about 75 % of the anthropogenic N deposited from the atmosphere was retained within the boreal landscape before entering the sea. Retention capacity varies depending on background levels and the biogeochemical process in question. There are indications that southern parts of the boreal region surrounding the Baltic Sea are more N-saturated than northern parts (Weyhenmeyer and Jeppesen 2010), resulting in a more effective N retention capacity in northern landscapes than southern. This pattern is probably also valid for other substances and will be affected by changes in trends in atmospheric deposition."
29,116,0.986,"Micro-, Meso- and Macro-Connectomics of the Brain","as a function of a linear variable (left) and the same variable scaled logarithmically (right). the set of nodes to which a node is connected by an edge. a point used to identify an object/entity in a graph. For example, we could consider individual areas of the brain as nodes. On a finer scale, we could consider individual neurons as nodes. the number of connected edges that must be traversed to travel between two nodes in a graph. used here as a probability law for which the frequency of an event declines as a power of its size. In graph theory, a power law may be used to define the degree distribution of a graph in which the frequency of nodes with a given degree falls off as a power function of the degree. This results in many nodes with a small degree and a few nodes with a very large degree (hubs). a higher-than-expected incidence of edges between hubs than between other nodes. a topological graph with high clustering and low average path length. a graph in which the spatial positions of the nodes (and, thus, the distances between them) are defined. a graph defined solely in terms of the relations implied by its nodes and edges but with no additional attributes, such as a metric distance or spatial position, weights or any other measures. It can be represented by a simple connectivity matrix, with 0âs and 1âs for its entries, indicating non-connections or connections, respectively."
71,237,0.986,Advancing Culture of Living with Landslides: Volume 1 ISDR-ICL Sendai Partnerships 2015-2025 (Volume 1.0),"interfaces may correspond to the surfaces of existing joints, to rock anisotropy, or non-persistent joints (Perfect 1997). The range of the probability of failure is bâ3 < P (1/bi) < 1. When P (1/bi) = 1 and Df = 3 the whole block breaks, while for P (1/bi)  bâ3 the block remains intact. The model performance is summarized in Fig. 13 The fractal fragmentation model has been adapted for the case of the rockfall. First, instead of k initial volumes of unit length, the IBSD is used as input, classifying it in bins. Second, not all the blocks of the IBSD break upon impact on the ground. To consider this, a survival rate, Sr, representing the proportion of blocks that do not break is deï¬ned. The FFM has been applied to several cases inventoried in the Spanish Pyrenees. Here, the case of Vilanova de Banat is presented. This rockfall took place in November 2011, affecting a volume of about 10,000 m3 of limestone (Ruiz-Carulla et al. 2015a). The model uses as input data the size of the rockfall (the unstable volume) and the discontinuity pattern of the detached rock mass (joint set orientations and spacing) to obtain the ISBD. A Nikon D90 digital camera with a focal length of 60 mm and 12Mp resolution was used to generate the digital surface model (DSM) of the rockfall scar. The following step was to reconstruct the volume of the detached rock mass by subtracting the DSM of the scar from the available topographic map at 1:5000 scale (before the failure). Then, the joint sets and their spacing were identiï¬ed using the DSM texturized with the images and matched to the detached rock mass. Given that neither high quality photos of the source area nor a detailed digital surface model (DSM) prior to the occurrence of the rockfall were available, the volume and the IBSD obtained are subjected to a high degree of uncertainty. There is a difference between the total volume measured in the"
311,2415,0.986,The Physics of the B Factories,"19.4.1.2 Îc , Î£c families The known Îc and Î£c states are shown in Fig. 19.4.1. The lowest-lying is the Î+ c ground state, which decays weakly (see next section). The most precise measurement of the c mass was made by BABAR (Aubert, 2005a). Since c are produced copiously at the B Factories, the key challenge is control of the systematic uncertainties. These arise from eï¬ects which could change the momentum scale, principally uncertainties in the magnetic field and energy loss in material. By selecting Î+ c decay modes with low energy release (Q value), a large fraction of the reconstructed Î+ c mass comes directly from the rest masses of the final-state daughters, which are known to high precision, and only a small fraction from the measured 3momenta. Thus, the eï¬ect of the momentum uncertainty 0 + on the Î+ c is reduced. Using the modes Îc â ÎKS K 0 0 + and Î+ found c ) = (2286.46 Â± 0.14) MeV/c2 . At the time of writing, this is the most precise measurement of an open charm hadron mass and has significantly lower uncertainty than existing theoretical calculations based upon lattice QCD or advanced potential models. In order of increasing mass, the next states are the ground states Î£c (2455) and Î£c (2520). In both cases these are isotriplets, and the only kinematically allowed strong decay is Î£c â Î+ c Ï. The Î£c (2455) is one of the few charmed baryons whose angular momentum has been measured. This was accomplished with a sample of fully reâ constructed B â â Î+ decays proceeding via an inc pÏ termediate Î£c (2455)0 (Aubert, 2008aa). In this exclusive production environment where the initial state is known to have J = 0, the angular distributions for diï¬erent spin hypotheses are fully determined (see Sections 12.1 and 19.4.3 for more on the helicity formalism). In this case the helicity angle, Î¸h , is defined as the angle between the direction of the Î+ c in the Î£c rest frame and the direction of the Î£c in the B â rest frame. BABAR found that the Î£c (2455)0 is consistent with J = 1/2 and inconsistent with J = 3/2 as shown in Fig. 19.4.4, in line with the quark model predictions (Section 19.4.1.1). The lowest-lying excited states are a pair of Î+ c not far + â above the Î+ c Ï Ï threshold, the Îc (2595) and Îc (2625). These are interpreted as a doublet with the light diquark in a spin-antisymmetric state and one unit of orbital angular momentum between the diquark and the heavy quark (L = 1), so that the total j p of the light degrees of freedom is 1â . Adding in the spin-1/2 heavy quark, the total J P of the baryon is 1/2â for the Îc (2595) and 3/2â for the Îc (2625) as shown in the first two rows of Table 19.4.3. The pattern of decays seen by ARGUS (Albrecht et al., 1993b, 1997), E687 (Frabetti et al., 1994a, 1996b), and CLEO (Edwards et al., 1995) leads to the following conclusions: + â + 0 â The states decay to Î+ c Ï Ï but not to Îc Ï , so they have isospin 0 (Îc ) and not 1 (Î£c ). â The Îc (2595) decays predominantly to Î£c (2455)0 Ï + , which is a favored S-wave decay: (1â , 1/2â ) â (1+ , 1/2+ )(0â )."
29,101,0.986,"Micro-, Meso- and Macro-Connectomics of the Brain","The G29  29 graph has a density of 66 % and it does not have a SF (power law) degree distribution, neither for the in- nor the out-degrees (see Fig. 3; a SF degree distribution falls as a power law as a function of the degree). Thus, for dense networks, alternative methods are needed to detect their core-periphery structure. We introduced a novel method to detect core-periphery structures in dense graphs based on a clique distribution analysis (Ercsey-Ravasz et al. 2013). A clique is a subset of nodes that have all the possible connections between them. The largest clique in the G29  29 has ten nodes, and there are 13 such cliques of 10 in G29  29, all involving only 17 nodes, forming the core of G29  29 with a very high density of 92 %. The rest of the nodes form the periphery with a 49 % density of connections and a density of 54 % of connections between core and periphery nodes (ErcseyRavasz et al. 2013). This is a clear-cut core-periphery structure with a core of 92 % density surrounded by the rest of the graph having roughly 50 % density. The probability for seeing such a core-periphery structure in a random graph with the same number of nodes and edges is 1017, infinitesimally small. So why doesnât the rich-club measure (2) pick out this structure? The explanation lies with the second expression in Eq. (2), which shows that the normalized measure is simply the fraction of edges between the larger-than-k degree nodes and the same quantity for the randomly rewired network. Thus, this rich-club coefficient will be large only if the randomized network has a significantly reduced density between the same set of nodes. That can only happen in a sparse network and if the degree distribution is heterogeneous as well. In our network, due to its high density, even by random rewiring we cannot reduce significantly the density of connections between these particular nodes. Additionally, the networkâs degree distribution is not very heterogeneous; Table 1 and Figs. 3 and 4 show that most of the nodes are high-degree nodes. In particular, area 8l has an in-degree of 28, thus receiving connections from all the others within G29  29. There are 12 nodes with in-degree 20 or larger, meaning that 41.3 % of all nodes receive connections from at least 20/29 ï¬ 69 %"
311,1696,0.986,The Physics of the B Factories,"former fit. The statistical significance of the observation of the new Î·c decay mode is estimated to be 7.9 standard deviations. Taking into account reconstruction eï¬ciency, the branching fraction of the Î·c â ÎÎ decay is calculated +0.24 +0.09 to be B = (0.87â0.21 (stat) â0.14 (syst) Â± 0.27 (B)) Ã 10â3 , where the third uncertainty term is due to the poorly known absolute branching fractions of Î·c . This term cancels in the ratio B(Î·c â ÎÎ)/B(Î·c â ppÌ), measured to be +0.19 Â± 0.12, consistent with the theoretical expecta0.67 â0.16 tions. 18.2.2.2 Open charm decays of J CP = 1ââ charmonium states The process with a photon radiated from the initial state (ISR), e+ eâ â Î³ISR V (see Fig. 21.2.2 for the Feynman diagram), generates a state V coupled to the virtual photon, and therefore with the same quantum numbers, J P C = 1ââ . Such events represent an excellent laboratory to study exclusive decays of the vector V , with very clean signals observed in most studied final states (see Chapter 21 for a detailed description of the process). The ISR method has been successfully used to measure charmonium decays into open charm final states: their high multiplicity allows for eï¬cient reconstruction with the ISR method, while the small branching fractions of charmed mesons to modes convenient for reconstruction make them diï¬cult to detect in exclusive B decays. The B Factories have provided measurements of the branching fractions of various vector charmonium states for the first time. Here we describe only the procedure that was used to extract the branching fractions, and summarize the results. Both BABAR (Aubert, 2009n) and Belle (Abe, 2007d; Pakhlova, 2008a) have studied the processes e+ eâ â"
352,490,0.986,Interface Oral Health Science 2014,"Hierarchical information processing in the vSI was summarized in a previous review [24]. As shown in Fig. 24.2, spatial convergence of somesthetic information arising from orofacial structures proceeds across three cytoarchitectonic areas; i.e., areas 3, 1, and 2 [20, 25], in a manner established in the hand representation (see review [26]). Along this rostrocaudal stream, neuronal receptive fields (RFs) become larger and more complex so that the RFs cover functionally related portions of orofacial structures (composite RFs). The patterns of spatial convergence can be classified into three types (Fig. 24.2): bilateral convergence, intermaxillary convergence, and inter-structural convergence. Furthermore, the spatiotemporal integration also proceeds along this stream: the relative incidence of neurons exclusively responsive to light stroking stimuli (movement-specific neurons) increases moving caudally towards area 2 [27]. Of these, the majority responded with directional selectivity, that is, they responded exclusively to stimuli moving in a particular direction. Most of the movement-specific neurons had ordinally uninterrupted RFs and the remaining had composite RFs discretely covering different structures. The relative incidences of neurons with composite RFs in area 2 were significantly higher in movement-specific neurons than in other neurons, suggesting that the spatiotemporal integration for representing moving objects is accompanied by the convergence of inputs from discrete, but functionally related, oral portions. Such a hierarchical scheme in the vSI might be a prerequisite neural process for dexterous orofacial function and oral stereognosis. The spatial convergence found in awake macaque monkeys was indirectly supported in a subsequent human study using functional MRI [28]. Although neuronal RFs could not be studied in humans, the investigators inspected the degree of activation overlaps between the representations of different oral structures, such as the lips, teeth, and tongue. They showed that the overlap in the middle and caudal portions of the postcentral gyrus was significantly greater than in the rostral portion of the postcentral gyrus. The SI receives not only somesthetic inputs arising from the periphery, but also signals from the frontal lobe related to motor command. It is therefore important to determine the activity of vSI neurons during self-generated orofacial movements."
214,525,0.986,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"Integrated assessment models (7) models of the earth and human system that generally include both physical and social science models that consider demographic, political, and economic variables that affect emissions of greenhouse gases as well as the physical climate system. Usually the physical system is simpliï¬ed. Intergovernmental Panel on Climate Change (IPCC) (11) the international scientiï¬c body that conducts assessments of climate change science, impacts, and policy. http://www.ipcc.ch/. Intermediate complexity models (5) simpliï¬ed models of the climate system (also know as earth system models of intermediate complexity). These models usually represent the climate by an energy balance over large regions (like an ocean basin or an entire continent) that are tied together. They are less complicated than full earth system models, but they do try to represent or specify feedbacks, so they are more complex than simple idealized models like energy balance models. Isostatic rebound (8) the rise of land masses that were depressed by the huge weight of ice sheets during the last glacial period. Isostatic refers to the equilibrium of the earthâs crust with the mantle underneath. Isotopes (3) different forms of the same element that contain equal numbers of protons and electrons but different numbers of neutrons, and, hence, that differ in relative atomic mass but not in chemical properties. Kinetic energy (4) the energy that a body possesses as a consequence of its motion, deï¬ned as one-half the product of its mass (m) and the square of its speed (v): Â½ mv2. Latent heat (7) energy released or absorbed by changes of phases of water. Condensation and freezing release heat, while evaporation and melting require heat input. Leads (6) open water that forms between patches of sea ice, usually due to divergence (separation of ice). Legitimacy (12) valid, objective, fair, or free of bias. Limited-area models (5) models that cover only a part of the earth and have lateral boundaries, such as regional climate models. Such models must be given lateral boundary conditions. Longwave radiation (5) energy emitted at wavelengths longer than about 4 micrometers (millionths of a meter) in the infrared part of the spectrum, usually of terrestrial origin. Mean (3) the arithmetic average of a set of numbers. Deï¬ned as the total sum of all values divided by the number of values."
337,44,0.986,Understanding Society and Natural Resources : Forging New Strands of Integration Across the Social Sciences,"One well-known isomorphism is the âself-similarityâ between scales exhibited by fractal structures (Mandelbrot 1977) which may provide another approach to the problem of scaling. This self-similarity implies a regular and predictable relationship between the scale of measurement (here meaning the resolution of measurement) and the measured phenomenon. For example, the regular relationship between the measured length of a coastline and the resolution at which it is measured is a fundamental, empirically observable one. It can be summarized in the following equation: L = k â s("
285,630,0.986,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","Figure 1 displays the time curves of fixations to all four pictures displayed within the NS blocks for C (a), and N (b), and for the DS blocks for C (c) and N (d). These figures show proportions of fixations to the four pictures displayed, averaged across participants, and the 95 % confidence intervals for the fixations to the target and competitor. Of particular interest for this study are the three-way interactions between Context (C versus N) and Presentation (NS versus DS) and the terms describing the course of the curves. For the fixations to the phonological competitor, as significant emerged the three way interactions with the first term (the intercept) of the curve ( Ï2(18) = 28476, p < 0.001, the interaction with the quadratic term (the slope), ( Ï2(18) = 28184, p < 0.001), the interaction between the cubic term (rise and fall around the central inflection), ( Ï2(18) = 27632, p < 0.001), and the quartic term (curvature in the tails), ( Ï2(18) = 27651, p < 0.05). The interaction with the intercept shows that the context sentences reduced the area under the fixation curves to the competitor for NS (red lines in Fig. 1a versus b), and that this reduction was smaller for DS (red lines in Fig. 1c versus d). The interaction with the slope shows that the growth of fixations to the competitor is shallower for DS in the neutral context than it is for NS in neutral context. The interaction with the cubic term reflects that the location of the peak of fixations towards the competitor in DS is delayed for about 300 ms relative to the location of the peak for NS, and that the course of this curve is more symmetric than for NS, and this mainly for the items presented in neutral"
241,735,0.986,Second Assessment of Climate Change for the Baltic Sea Basin,"For the Baltic Sea area, statistical downscaling methods have mostly been applied to estimate future changes in hydrological variables, such as precipitation and run-off, and storm-related variables such as wind. The usual large-scale predictors are SLP and geopotential height. One particular aspect of the applications of statistical downscaling to the Baltic Sea so far is the frequent use of nonlinear statistical methods, such as weather typing, fuzzy networks and clustering algorithms, whereas for other areas linear regression methods, such as principal component regression, have generally been more frequently found in the literature. Rogutov et al. (2008) gave an example of how a standard statistical downscaling method for precipitation should look. The authors considered the whole of western Europe, but the results are also relevant for the Baltic Sea area. The method used principal components regression, which has been applied not only for downscaling purposes but also for climate reconstructions based on proxy data since mathematically the problem is very similar. Both the predictor (SLP) and the predictand (precipitation) are decomposed by a previous principal component analysis, and the leading components are retained for further analysis. This ensures that the covariance matrices that result in the regression analysis are not singular, avoiding over-ï¬tting of the statistical model. There is no clear rule to determine the optimal number of retained principal components, but the number can be approximately estimated by sensitivity calculations until the skill of the reconstructed predictand, when compared to observed data, does not grow. Linear regression methods produce predictands with the same probability distribution as the predictors. Since atmospheric circulation variables tend to be approximately normally distributed, linear statistical downscaling methods may work well to estimate changes in monthly, seasonal or annual precipitation, which also tend to be approximately normally distributed. However, this is not the case for daily precipitation. In this case, more sophisticated nonlinear methods are needed, for instance those based on classiï¬cation of weather types (see Chap. 10, Sect. 10.3). Wetterhall et al. (2009) employed a classiï¬cation scheme constructed on a fuzzy logic algorithm to estimate changes in daily precipitation over Sweden based on the output of the GCM HadAM3H driven by the SRES scenarios A2 and B2. They also employed a weather generator that takes into account the weather type and is able to replicate the serial autocorrelation of daily precipitation. The advantage of this approach is that it is in theory able to estimate changes not only in daily precipitation amount and occurrence but also in block maxima, that is, maximum 3- or 5-day precipitation. Under these two scenarios, and conditional on the GCM used, Wetterhall et al. (2009) found that precipitation in Sweden tended to increase in the twenty-ï¬rst century and that the maximum 5-day precipitation also became larger"
273,167,0.986,Report on Global Environmental Competitiveness (2013),"Foundation layer is composed of individual indicators with direct measuring capacity, directly showing the measurement of indicators in factor layer; it is the most basic layer and operation layer of GEC indicator system. The evaluation of the entire indicator system is actually carried out in this layer. As per the defined scope of pillars, there are 60 designed individual indicators, as shown in Table 4.2. GEC Evaluation Indicator System is composed of four layers, system layer, module layer, factor layer and foundation layer, which corresponds to 1 index, 5 sub-index, 16 pillars and 60 individual indicators; among these, the index, subindex and pillars are indirect synthetic indicators, while individual indicators are direct objective indicators that are measurable and therefore will use the data"
238,272,0.986,Nanoinformatics,"with values equal to the threshold were joined to give crisp isosurfaces. Figure 7.14b shows the same isosurface with the inclusion of uncertainty. Uncertainty is a function of spatial distribution of atoms. Distribution of atoms is less at distances away from the isosurface and thus no effect was observed. Near the surface, the uncertainty of the isosurface decreases. From the image, it is observed that there is an increased level of intensity as the surface is approached."
311,1718,0.986,The Physics of the B Factories,"spectroscopy. Chapters 21 and 22 describe in detail the numerous experimental results obtained at the B Factories, in ISR and two-photon physics respectively. When describing charmonium formation from cc pairs produced either in B decays or in e+ eâ annihilation, eï¬ective field theories are used. The EFT most often exploited is non-relativistic QCD (NRQCD), which assumes factorization of the production of charmonium partons (e.g. a cc pair) in the given process, and the formation of charmonium from those partons (Bodwin, Braaten, and Lepage, 1995; Caswell and Lepage, 1986; Thacker and Lepage, 1991). The former part contains a partonic level cross section generally calculated in perturbative QCD, in which the cc pair may be produced in a color singlet or color octet state (Braaten and Fleming, 1995; Cho and Leibovich, 1996a). The latter part, which describes the evolution of the cc pair with the quantum numbers of the final charmonium state, cannot be calculated in perturbation theory, and the relevant parameters are usually extracted from the data. A signature of the NRQCD approach is the universality of the long distance production matrix elements, which are assumed to be independent of the hard process of parton production. For a more extensive account of EFTs and quarkonia, see Section 18.1.4. Before presenting the experimental results it is worth emphasizing that the B Factories provide the cleanest processes for calculation of charmonium production, as ccpairs in B decays (Section 18.2.4.1) and e+ eâ annihilation (Section 18.2.4.2) are produced via weak and electromagnetic processes, which can be calculated exactly. However in some cases, the test of such predictions requires a careful treatment of the details of the experimental measurements (Section 18.2.4.3)."
311,1232,0.986,The Physics of the B Factories,"where the coeï¬cient F is not equal to one in case of SU (3) breaking, and fÏ (fK â ) is the Ï (K â ) decay constant. The factor F is estimated to be 0.9 Â± 0.6 (Beneke, Gronau, Rohrer, and Spranger, 2006). In fact it turns out that SU (3) breaking has little eï¬ect on the overall constraint obtained for Ï2 , and one can obtain a precision comparable to the isospin analysis approach even with 100% SU (3) breaking uncertainty. The decay widths ÎL in Eq. (17.7.11) can be replaced by the corresponding branching fractions multiplied by the ratio of B 0 to B Â± lifetimes. This approach provides a stringent constraint on Ï2 that can be used as a cross-check of the traditional SU (2) isospin analysis. Results of using this approach can be found in Section 17.7.6, but given that the same inputs are used for this approach and the isospin analysis (Section 17.7.7), one should take care not to combine the results obtained from the two methods when computing a global average for Ï2 . 17.7.2 Event reconstruction The reconstruction of the charmless B decays and event selection follows a similar sequence in both BABAR and Belle. First, a sample of charged tracks and photons is selected. Typically, charged tracks are required to originate from the interaction region and to be identified as pions (Chapter 5). In most of the cases an electron veto is applied. After an initial Ï 0 selection based on twophoton candidates, the Ï 0 candidates are kinematically constrained to the nominal Ï 0 mass. Tracks and Ï 0 candidates are combined to produce composite candidates (e.g. Ï, a1 (1260)), and finally, signal B candidates are admixture of the transverse polarization without signiï¬cantly aï¬ecting the overall precision on the constraint obtained for Ï2 given the data samples available at the B Factories."
222,425,0.986,Ecosystem Services For Well-Being in Deltas : integrated Assessment For Policy Analysis,"simulations to produce a range of projections which reflect the uncertainty in the parameters. The Met Office perturbed versions of HadCM3 with associated HadRM3P simulations for the 130-year period from 1971 to 2100 to create 17 ensemble results. Three members from this ensemble were selected, referred to as the Q0, Q8 and Q16 runs, respectively (see Table 15.1). The Q0 run represents exhibits a mid-range climate sensitivity to the A1B emissions forcing; Q16 has the highest climate sensitivity (i.e., it is the ensemble member that exhibits the highest global temperature response to the A1B emissions forcing); and finally, the Q8 run, although it has similar sensitivity to Q0, exhibits a different precipitation response. Specifically, unlike the other ensemble members, the Q8 run shows a mid-century decrease in precipitation (Table 15.1). The inclusion of the Q8 run therefore enables the impacts on sediment transfer processes of this possible climate response to be considered, even if the likelihood of this response can be considered to be relatively low. It was found that fluvial sediment delivery rates to the GBM delta associated with these climate data sets were all projected to increase under the influence of anthropogenic climate change, albeit with the magnitude of the increase varying across the Ganges and Brahmaputra catchments (Fig. 15.1). Of the two study basins, the Brahmaputraâs fluvial sediment load is predicted to be more sensitive to future climate change. By the middle part of the twenty-first century, model results suggest that sediment loads will increase (relative to the 1981â2000 baseline period) over a range of between 16 and 18 per cent (depending on climate model run) for the Ganges, but by between 25 and 28 per cent for the Brahmaputra. The simulated increase in river sediment supply from the two catchments Table 15.1 Overview of the change in temperature and precipitation with respect to the annual mean for the 1981â2000 baseline period under the Q0, Q8 and Q16 Met Office RCM runs for the South Asian domain used in this study (for the SRES A1B emissions scenario) (Darby et al. 2015âPublished by the Royal Society of Chemistry) Climate model run"
311,2344,0.986,The Physics of the B Factories,"The M (DÏ) spectra are fitted with contributions from the D2â , Dâ (2600) and Dâ (2760) described with relativistic BW distributions. The smooth background is modeled using an exponential function multiplied by a two-body phase-space factor dropping toward the DÏ mass threshold. The feeddown is described by convolving BW functions with a function describing the resolution and mass shift obtained from the MC simulation. The masses and widths of the D1 and D2â feeddowns are fixed to the values obtained respectively from the same M (DÏ) distribution and from the M (Dâ+ Ï â ) study described below. Finally, although not visible in the M (D+ Ï â ) mass distribution, a BW function is included to account for the broad D0â . The Dâ+ Ï â mass distribution is shown in Fig. 19.3.11 and exhibits the following features: â Prominent D10 and D2â0 peaks. â Two enhancements at 2.60 GeV/c2 and 2.75 GeV/c2 , which are denoted as Dâ (2600)0 and D(2750)0 . The angular analysis of the M (Dâ+ Ï â ) â 2.6 GeV/c2 region shows that it could not be described by a single resonance, instead two resonances with diï¬erent helicityangle distributions could be present. Thus, a new component, labeled as D(2550)0 , is included in the M (Dâ+ Ï â ) fit. The D(2550)0 parameters are obtained by requiring | cos Î¸H | > 0.75 in order to suppress the other resonances"
216,123,0.986,Advances in Production Technology,"The results show that the quality of the metamodel is dependent on the number of sampling points; the quality is improved when the number of training points is increased. As visualization technique contour plots were used, which in their entirety form the process map. The star-shaped marker, denoting the seed point of the investigation, represents the current cutting parameter settings and the arrow trajectory shows how an improvement in the cut quality is achieved. The results show that in order to minimize the cutting surface roughness in the vicinity of the seed point, the beam radius in the feed direction x should be decreased and the focal position should be increased Eppelt and Al Khawli (2014) In the special application case studied here the minimum number of sampling points with an RBFN model is already a good choice for giving an optimized working point for the laser cutting process. These metamodels have different accuracy values, but having an overview of the generated tendency can support the developer with his decision making step."
311,2074,0.986,The Physics of the B Factories,"mV = 2.5 GeV/c2 , and only experimental results corrected for the S-wave contribution are kept. The HMÏT model proposed by (Fajfer and Kamenik, 2005, 2006) generalizes the approach of Becirevic and Kaidalov and satisfies the scaling laws of obtained in the infinite mass limit as well as the known scaling at large energy of the outgoing kaon. Values from lattice QCD (Abada et al., 2003; Gill, 2002) are obtained using the quenched approximation. The relative accuracy of experimental measurements is typically 2% whereas it is around 10% for lattice QCD. Theoretical expectations agree with the general picture (A2 (0) â¤ A1 (0) < V (0)) but significant diï¬erences are observed."
311,1728,0.986,The Physics of the B Factories,"using the data sets obtained in the first year of their operation (L â¼ 20 fbâ1 ). In both collaborations the J/Ï production was studied in the full momentum interval: the region below 2 GeV/c was studied using continuum data. BABAR obtained (2.52 Â± 0.21 Â± 0.21) pb (Aubert, 2001b), while Belle obtained (1.47 Â± 0.10 Â± 0.13) pb (Abe, 2002n). The discrepancies between the two measurements are likely due to diï¬erences in the selection criteria for J/Ï events that were used to suppress contributions from the huge QED background. Corrections for the selection eï¬ciency are model dependent and may result in poorly controlled systematic uncertainty. (See also the discussion in Section 18.2.4.3 below.) While the measured cross section is not in contradiction with the NRQCD predictions (color singlet + color octet) of 1.1â1.6 pb (Yuan, Qiao, and Chao, 1997a,b), the expected sole color singlet contribution is too small to describe the data. On the other hand, the J/Ï momentum spectrum measured by Belle and BABAR does not show any indication of the sizable color octet contribution, that was expected to result in an enhancement at the maximum momentum value. BABAR and Belle also measured the J/Ï production and helicity angle distributions, which roughly agree with NRQCD expectations. Belle and BABAR performed searches for other charmonium states produced in e+ eâ annihilation. In addition to the J/Ï production study, Belle also measured +0.09 Ï(e+ eâ â Ï(2S) X) = (0.67 Â± 0.09â0.11 ) pb (Abe, 2002n) and set upper limits on the production of Ïc1 and Ïc2 . Later BABAR, using a much larger data sample, improved prompt these limits: ÏN (e+ eâ â Ïc1(2) X) < 77(79) fb at the ch â¥3 90% confidence level (Aubert, 2007at). Upper limits were set for events where the charmonium momentum exceeds 2.0 GeV/c and there are at least three additional charged tracks. These limits are consistent with NRQCD predictions. The recoil mass analyses In 2002, contrary to NRQCD expectations, Belle observed that most of the prompt J/Ï âs are accompanied"
241,315,0.986,Second Assessment of Climate Change for the Baltic Sea Basin,"Fig. 4.15 Fractional change in the frequency of very low daily minima in winter surface temperature. The fractional change in occurrence of below-10th percentile daily minimum temperature events is plotted between the winters 1964/1965â1968/1969 and 1990/1991â1994/1995. The 10th percentile is deï¬ned over the former period for each ensemble and at each grid point. In order that the existing dataset could be used, percentiles are calculated over 1961â1990. All daily values for the model were pooled together over the DecemberâFebruary (DJF) period to calculate both percentile thresholds and changes in frequency (Scaife et al. 2008)"
238,374,0.986,Nanoinformatics,"Figure 10.8 shows a cross-sectional HAADF-STEM image of the R-SPE grown Naâ2/3MnO2 ï¬lm around the interface observed from the direction of Naâ2/3MnO2||Î±-Al2O3. The stripe patterns correspond to the layered structure of Naâ2/3MnO2. It should be noted that an interfacial layer is not observed, conï¬rming that the present sample has an atomically sharp interface between the ï¬lm and substrate, contrary to the previously reported observation for a Naâ0.8CoO2 ï¬lm. Figure 10.9 summarizes the temperature (T) dependence of the electrical conductivity (Ï) for the Naâ2/3MnO2 and hydrated Naâ0.61MnO2 â 0.42H2O epitaxial ï¬lms. It should be noted that the Ï â T curves for both ï¬lms do not show a remarkable hysteresis in the heatingâcooling cycles ranging from RT to 400 K, suggesting that the absorbed water does not signiï¬cantly contribute to Ï because the surface-adsorbed water should be released at 100â150 Â°C, although a slight deviation from a straight line at â¼100 Â°C is observed in hydrated Naâ0.61MnO2 â 0.42 H2O ï¬lm. At RT, Ï of the Naâ2/3MnO2 epitaxial ï¬lm is â¼1 mS cmâ1, which is two orders of magnitude larger than that of an Î±-Na0.70MnO2.25 single crystal (â¼0.5 Î¼S cmâ1). In contrast, Ï of the Naâ0.61MnO2 â 0.42H2O ï¬lm is â¼0.1 mS cmâ1, which is comparable to that of the NaxMnO2 Î nH2O ceramic (â¼0.05 mS cmâ1). In both cases, Ï increases exponentially with temperature because electron hopping becomes"
233,398,0.986,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"Jack-Knife in Conservation The use of a meta-criterion to deï¬ne an optimal parameter value has been used widely in phylogenetic analysis, i.e. the incongruence length difference test to deï¬ne the ts/tv/gap costs (Wheeler 1995) or jack-knife frequencies to evaluate whether concavity parsimony outperforms linear parsimony (Goloboff et al. 2008). In conservation biology, there must be a measure of the conï¬dence and robustness of the results. A sensitivity analysis, deleting at random part of the information, helps to understand the support of the data as the persistence of a given area in the ranking. Therefore, jack-knife is the appropriate tool to explore the behavior of the results to perturbations in the data set (Holmes 2003). In a conservation phylogenetic based analysis, there are three different items to evaluate, as we have three input parameters: the topology, the species in a given topology, and the distribution of a species."
256,546,0.986,"Supercomputing Frontiers : 4Th Asian Conference, Scfa 2018, Singapore, March 26-29, 2018, Proceedings","and the 216h matrix was generated from a problem with two spherical objects. In addition, a human 1x1 matrix was generated from a problem with a single human-shaped object. The sizes of the low-rank sub-matrices and small dense sub-matrix of each target matrix are shown in Fig. 12, where the two left graphs of each matrix show the size of the low-rank sub-matrices and the right shows the size of the small dense sub-matrix. With the 10ts and 100ts matrices, the size of the approximate matrices ndt and ndl was less than approximately 200 (some were close to 700). Note that all ranks kt were very small (the largest was 23). With the small dense matrices, all matrix lengths were less than 100, and many were less than 30. With the 216h and human 1x1 matrices, the aspect ratio of the small dense matrices was similar to that of the 10ts and 100ts matrices. With the approximate matrices, although kt was greater than that of the 10ts and 100ts matrices, the aspect ratio was similar. However, although nearly all ndt and ndl lengths were less than 1000, a few matrices had ndt and ndl lengths that were greater than 5000. Note that the sizes of these matrices depend on the target matrix. Moreover, the size is controlled by the matrix assembling algorithm and HACApK parameters. The above sizes were generated using current usual HACApK parameter settings. It is expected that optimizing the matrix size will affect HMVM performance, and this will be the focus of future work."
372,1208,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"10.2.6 Wide-Field Imaging To take full advantage of large new instruments with wide bandwidths, high sensitivity, and full polarization responses, it is necessary to measure the radio sky down to the level of the background radiation from the Epoch of Reionization (EoR) and to be able to separate out components from individual radio sources that overlie the background. The width of the synthesized field may be much greater than a few degrees, so the image is no longer the Fourier transform of the visibility function. The basic requirement for such an analysis is an equation for the visibility values that would be measured for a given brightness distribution, taking account of all details of the locations and characteristics of the individual antennas, the path of the incoming radiation through the Earthâs atmosphere including the ionosphere, the atmospheric transmission, etc. This is the interferometer measurement equation introduced in Sect. 4.8. In its basic form, it describes the response of a single pair of antennas and is thus applicable to any specified system of antennas and any brightness distribution, to provide values of the visibility for each antenna pair. It includes direction-dependent effects such as the primary beam patterns of the antennas, polarization effects that vary with the alignment of the polarization of the source relative to that of the antennas, and the baselines of the antenna pairs. These must be accounted for without small-field or other approximations. Direction-independent effects such as large-scale propagation in the atmosphere and the ionosphere, and the response of the receiving system, can also be included. The reverse operation, i.e., the calculation of the optimum estimate of the image from the measured visibility values, is less simple. Taking the Fourier transform of the observed visibility function usually produces a brightness function with physically distorted features such as negative brightness values in some places. However, starting with a simple but physically realistic model for the brightness, the measurement equation can accurately provide the corresponding visibility values that would be observed. By comparing these with the observed values, it is possible to adjust the brightness model toward the observed distribution and, by iterative repetition of this process, to arrive at an image that agrees with the visibility"
297,1637,0.985,The R Book,"In aov, the effect size for treatment i is deï¬ned as yÌi â Âµ, where Âµ is the overall mean. In mixed-effects models, however, correlation between the pseudoreplicates within a group causes what is called shrinkage. The best linear unbiased predictors (BLUPs, denoted by ai ) are smaller than the effect sizes ( yÌi â Âµ), and are given by"
372,206,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"2.4 Two-Dimensional Synthesis Synthesis of an image of a source in two dimensions on the sky requires measurement of the two-dimensional spatial frequency spectrum in the .u; v/ plane, where v is the northâsouth component as shown in Fig. 2.7a. Similarly, it is necessary to define a two-dimensional coordinate system .l; m/ on the sky. The .l; m/ origin is the reference position, or phase reference position, introduced in the last section. In considering functions in one dimension in the earlier part of this chapter, it was possible to define l in Eq. (2.3) as the sine of an angle. In two-dimensional analysis, l and m are defined as the cosines of the angles between the direction .l; m/ and the u and v axes, respectively, as shown in Fig. 2.7c. If the angle between the direction .l; m/ and the w axis is small, l and m can be considered as the components of this angle measured in radians in the eastâwest and northâsouth directions, respectively. For a source near the celestial equator, measuring the visibility as a function of u and v requires observing with a two-dimensional array of interferometers, that is, an array in which the baselines between pairs of antennas contain components in the northâsouth as well as the eastâwest directions. Although we have considered only eastâwest baselines, the results derived in terms of angles measured with respect to a plane that is normal to the baseline hold for any baseline direction. A source at a high declination (near the celestial pole) can be imaged in two dimensions with either one- or two-dimensional arrays, as shown in Fig. 1.15 and"
311,268,0.985,The Physics of the B Factories,"In some applications, such as for Dâ from B decays or the reconstruction of the associated B vertex for Ît reconstruction in Belle, the beamspot is used as a constraint on a decay vertex. In this case the size of the beamspot must be increased with the eï¬ective width of the decay length distribution of the (mother) particle, schematically, VIP,tot = VIP + Vï¬ight ."
311,867,0.985,The Physics of the B Factories,"with an estimate |Apeng | < 0.1|Atree |. It is important to note that while the decay rate is hardly changed by including the penguin contributions they are essential for the observation of direct CP -violating asymmetries (see Section 16.6). A phenomenological approach to predict the branching fractions of hadronic B decays that incorporates factorization is followed in Bauer, Stech, and Wirbel (1987) (BSW) and Neubert and Stech (1998) (NS). In this approach the QCD eï¬ects and Wilson coeï¬cients are captured by two phenomenological parameters, a1 and a2 . Here a1 represents the factor for decay modes that proceed via Type I (color-favored) amplitudes while a2 is the corresponding factor for Type II (color-suppressed) amplitudes. Decay amplitudes that have contributions from both Type I and II amplitudes (Type III) contain a linear combination of a1 and a2 . The values of a1 and a2 are determined from fits to measured B decay rates. For B meson decays the relative phase between a1 and a2 turns out to be positive, which implies constructive interference in the Type III decays. These constants, once determined, are assumed to apply universally to all two-body hadronic B final states. Table 17.3.1 gives predictions from this model for several Type I, II, and III B decay modes as well as the current PDG (Beringer et al., 2012) values (dominated by BABAR and Belle results) for the corresponding branching fractions. The model reproduces well the Type I (colorfavored) measurements as well as the Type III where the a1 term dominates the amplitude. Not surprisingly, the Type II predictions diï¬er considerably for some of the decay modes. In particular, the NS model predictions for the K (â) Ï â² diï¬er by a factor of two from the experimental measurements. For the KÏ â² modes, the prediction is half the measurement while for the K â Ï â² modes the prediction is twice the measurement. A generalization of factorization can indeed be rigorously derived from the first principles of QCD for the color-allowed amplitude of final states with one charmed meson (Beneke, Buchalla, Neubert, and Sachrajda, 2000). The physical picture is that of color transparency (Bjorken,"
311,2773,0.985,The Physics of the B Factories,"Table 22.2.1. Summary of partial wave analyses in the energy region below 2.4 GeV. If two values are given for the two-photon partial decay width ÎÎ³Î³ , the upper value is the one for the spin-helicity assignment (J, Î») = (2, 2) and the lower one for (0, 0). B is the branching fraction of the resonance to the corresponding decay mode otherwise explicitly noted. For the Ï 0 Ï 0 mode, we provide the value B(f2 â Î³Î³) instead of ÎÎ³Î³ since the total width is known for f2 (1270). The resonances f0 (Y ), a0 (X) and f2 (X) correspond to signals of new or unidentiï¬ed resonances found. In the K + K â mode, fJ /f0 /a2 and fJ /f2 mean that there are ambiguities in the signal assignment. Quoted upper limits are at 90% conï¬dence level. (2, 2) Reference Resonance Mass (MeV/c ) Width (MeV) ÎÎ³Î³ (eV), (J, Î») = (0, 0) Ï+ Ïâ"
8,909,0.985,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","is quite sensitive to the choice of the strange quark mass parameter and the coupling constant Ës and both must, however, be chosen consistently. Also included in Fig. 31.3a,b are our results for gluon conversion into light quarkâantiquark pairs. The shortness of for this process indicates that gluons and light quarks reach chemical equilibrium during the beginning stage of the plasma state, even if the quark/antiquark (i.e., baryon/meson) ratio was quite different in the prior hadronic compression phase. The evolution of the density of strange quarks in Eq. (31.15) in the plasma state is shown in Fig. 31.4 for temperatures T D 300; 200; 160 MeV. The saturation of the abundance is clearly visible for T  160 MeV. To obtain the measurable abundance of strange quarks, the corresponding values reached after the typical lifetime of the plasma state, 2  10 23 s, can be read off in Fig. 31.4 as a function of temperature. The strangeness abundance shows a pronounced threshold behaviour at T  120â 160 MeV. I thus conclude that strangeness abundance saturates in sufficiently excited quarkâgluon plasma (T > 160 MeV, "" > 1 GeV/fm3 )."
375,113,0.985,Musical Haptics,"4.3.5 Discussion The results presented in the previous section show that sensitivity to key vibrations is highest in the lowest range and decreases toward higher pitches. Vibrations are clearly detected in many cases where the vibration acceleration signals hardly reached typical thresholds found in the literature for sinusoidal stimuli. The literature on the detection of complex stimuli provides support to our results, although it does not explain them completely. As already discussed in Sect. 4.2.4, Wyse et al. [61] report RMS acceleration threshold values at 250 Hz corresponding to 80 dB, a value compatible with our results. However, the characteristics of those stimuli may have occasionally produced significant energy at lower frequencies, causing the thresholds to lower once they were presented to the whole hand. The pianist receives the initial transient when the hammer hits the string; then, the vibration energy promptly decreases and its partials fade each with its own decay curve. The initial peak may produce an enhancement effect similar to those measured by Verrillo and Gescheider limited to sinusoids [56] and hence contribute to sensitivity. As discussed earlier, the P-channel is sensitive to the signal energy, while is not able to recognize complex waveforms. Loudness summation instead occurs when vibration stimulates both the Pacinian and non-Pacinian (NP) channels, lowering the thresholds accordingly [7, 37, 56]. In our experiment, summation effects were likely to occur when the A0 key and, possibly, the A1 key were pressed. From A3 on, only the P-channel became responsible for vibration perception. Figure 4.3 seems to confirm these conclusions, since they show a pronounced drop in sensitivity between A1 and A3 in both parts of Experiment 2. As Fig. 4.6 demonstrates, this drop is only partially motivated by a proportional attenuation of the vibration energy in the grand piano, while it is not motivated at all in the upright piano. Hence, it is reasonable to conclude that the NP-channel played a perceptual role until A3. Beyond that pitch, loudness summation effects ceased. In analogy with Experiment 1, the results of this experiment also suggest the occurrence of spatial summation effects [10] when a cluster of notes, whose fundamentals overlap with the tactile band, is played instead of single notes. As Fig. 4.3 (bottom panel) shows, playing the cluster in the fourth octave boosted the detection in that octave, whereas the same effect did not occur in the fifth octave. Unlike Experiment 1, this summation originates from multifinger interaction rather than varying contact areas in single-finger interaction. This evidence opens an interesting question about the interaction of complex vibrations reaching the fingers simultaneously. Measurements of cutaneous vibration propagation patterns in the hand resulting from finger tapping show, however, an increase in both intensity and propagation distance with the number of fingers involved [49], which may partially explain the increased sensitivity we observed."
311,1209,0.985,The Physics of the B Factories,"As seen in Fig. 17.6.19, time-reversal symmetry is clearly violated in all four transition comparisons. The significance of the observed T violation is obtained from the log-likelihood value ln L. The diï¬erence 2Î ln L between the best fit and the fit without T violation, including systematic errors, is 226 with 8 d.o.f., which corresponds, assuming Gaussian errors, to 14Ï. Using the same procedure for the CP T -symmetry diï¬erences such as in Eq. (17.6.26), no CP T violation is observed. The diï¬erence 2Î ln L between the values for the best fit and the fit with CP T symmetry is 5, equivalent to 0.3Ï. The analysis also determines four CP asymmetries; the results are compatible with those obtained from the standard CP violation analysis based on the same CP final states (Aubert, 2009z); the observed significance of CP violation is equivalent to 17Ï. This is larger than 14Ï for T violation since the comparison between two (âÂ± , ccKS ) rates has a higher statistical and systematic significance than the comparison of the rates (âÂ± , ccKS ) and (âÂ± , ccKL ). In the Standard Model, the eight coeï¬cients SÎ±,Î² measurements of sin 2Ï1 . Hence the four measured T Â± â ÎSCP violating asymmetries, ÎSTÂ± and ÎSCP T , can be seen as four measurements of 2 sin 2Ï1 . The results in Taâ¦ ble 17.6.16 lead to a mean value Ï1 = (21.8 Â± 2.0) , which is of course completely correlated with the Ï1 value obtained from the CP -asymmetry measurements discussed in Sections 17.6.3 and 17.6.10. In conclusion, the BABAR experiment (Lees, 2012m) has demonstrated with a large significance of 14Ï that detailed balance and therefore time-reversal symmetry are violated. In b â ccs decays, T and CP symmetry breakings are seen in two diï¬erent observations, are time dependent with only a sin Îmt term, are of order O(10â1 ), and are induced by the interference between qA and pA, i.e. the interference between decay and mixing. All these properties are diï¬erent from those of the earlier observed flavor mixing asymmetry in K 0 â K 0 transitions, where CP and T transformations lead to the same observation, the asymmetry is time independent, is of order O(10â3 ), and is produced by the interference of absorptive (Î12 ) and dispersive (M12 ) contributions to mixing."
71,73,0.985,Advancing Culture of Living with Landslides: Volume 1 ISDR-ICL Sendai Partnerships 2015-2025 (Volume 1.0),"flood plain and small lagoon. The subsequent river erosion produced the present morphology for the part where the rupestrian settlement is located (Delmonaco and Margottini 2014). The cliff and niches are composed of alternating conglomerate and siltstone (yellow at the bottom and red in the middle of the cliff); the conglomerate, with different sized pebbles (coarse, mid and ï¬ne-grained), is the predominant material in the cliff and exhibits a moderate cohesion. The siltstone is interlayered with the conglomerate strata. The mean value for the uniaxial compressive strength of the conglomerate is 2.99 MPa, whereas the value for the uniaxial compressive strength of the siltstone is 6.91 MPa (Feker 2006). Point load data10 show that the average UCS values are 11.0 MPa for siltstone and 5.6 MPa for conglomerate. Accordingly, the siltstone is apparently stronger than siltstone. A simple test with immersion in water, clearly shows a potential and relevant slaking attitude of the material, clearly not visible on site because of the low rainfall rate in the area (annual average rainfall equal to 162.56 mm) (Margottini 2014a, b). The mechanism for the swelling is likely dependent on the presence of a cement carbonate in the conglomerate and the absence of such permanent cement into the sandstone, as revealed by microscopic thin section and x-ray diffraction (Fig. 11b, c). As a general conclusion we note that relevant geomechanical parameters also depend on the local environment"
372,412,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"This equation is known in radio engineering as the Ruze formula (Ruze 1966) and in some other branches of astronomy as the Strehl ratio. As an example, if #=% D 1=20, the aperture efficiency, A=A0 , is 0.67. In the case of antennas with multiple reflecting surfaces, the rms deviations can be combined in the usual rootsum-squared manner. Secondary reflectors, such as a Cassegrain subreflector, are smaller than the main reflector, and for smaller surfaces, the rms deviation is usually correspondingly smaller. The surface adjustment of the 12-m-diameter antennas"
372,1791,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"14.4.3 Refractive Scattering The realization by Sieber (1982) that the characteristic periods of amplitude scintillations of pulsars, on timescales of days to months, were correlated with their dispersion measures led Rickett et al. (1984) to the identification of another important scale length in the turbulent interstellar medium, the refractive scale dref . Refractive scattering is important in the strong scattering regime (d0 < dFresnel D (R), where d0 is the diffractive scale size defined by D) .d0 / D 1. The refractive scale is the size of the diffractive scattering disk, which is the projection of the cone of scattered radiation on the scattering screen, located a distance R from the observer. The diameter of the diffractive scattering disk is R&s : The scattering disk represents the maximum extent on the screen from which radiation can reach the observer. With a power-law distribution of irregularities, it is the irregularities at the maximum allowed scale that have the largest amplitude and are the most influential. Thus, the refractive scale is dref ' R&s : Since &s ' (=d0 ; we can write"
311,2673,0.985,The Physics of the B Factories,"Among the most interesting studies of this final state performed by BABAR is the extraction of the relatively small contributions of the ÏÏ + Ï â and ÏÏ 0 Ï 0 (Ï â K + K â ) intermediate states (Aubert, 2006c, 2007bc). The original motivation was the search for decays of the then recently-discovered vector meson Y (4260). As discussed in Section 18.3, the Y (4260) was discovered by BABAR in the process e+ eâ â Î³ISR Y (4260) â Î³ISR J/Ï Ï + Ï â , but was not seen to decay to D(â) D(â) , although this was expected for a wide conventional charmonium state with a mass well above the DD production threshold. A certain exotic-structure model for the Y (4260) predicted a large branching fraction for the decay into ÏÏÏ (Zhu, 2005). Since the Ï resonance is relatively narrow, a clean sample of ÏÏÏ events can be easily separated. The scatter plot of the reconstructed masses, m(Ï + Ï â ) versus m(K +K â ), for selected events in a data sample corresponding to 232 fbâ1 is shown in Fig. 21.3.13(a) (Aubert, 2006c): a clear Ï â K +K â vertical band is visible, as well as an accumulation of events indicating correlated production of the Ï and f0 (980) â ÏÏ resonances. A wide horizontal band corresponding to Ï0 â Ï + Ï â production is also seen. The invariant mass distribution of the ÏÏ system in ÏÏÏ events is obtained using the condition |m(K +K â ) â mÏ | < 10 MeV/c2 , where mÏ is the nominal Ï-mass. The background from true K +K â ÏÏ events with non-resonant K +K â pair is subtracted using the Ï mass sidebands 10 < |m(K +K â ) â mÏ | < 20 MeV/c2 ; other backgrounds are subtracted based on MC simulation. The final mass spectrum for ÏÏ pairs associated with Ï production is shown in Fig. 21.3.13(b)(Aubert, 2007bc). Besides the clear f0 (980) signal, and a concentration consistent with the f2 (1270) resonance, a broad bump at lower mass values is observed, which can be interpreted as the controversial f0 (600) scalar meson. The e+ eâ â ÏÏÏ mass spectrum is measured in 25 MeV/c2 wide bins by extracting the number of reconstructed Ï â K +K â decays from a fit to the K +K â mass spectrum. The corresponding cross section is then obtained applying Eq. (21.2.12) and taking into account the Ï â K +K â branching fraction. With an analogous procedure, but requiring in addition that 0.85 < m(ÏÏ) < 1.1 GeV/c2 , a 90% pure sample of Ïf0 (980) is selected, and, assuming a decay rate B(f0 (980) â Ï + Ï â ) = 2/3,"
283,66,0.985,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","Therefore, the practical soft decision performance of a binary code lies between the upper and lower Union bound. It will be instructive to observe the union bound performance for actual codes using their computed weight distributions as the SNR per bit NEb0 increases. By allowing NEb0 to become large (and Ps to decrease) simulations for several codes suggest that at a certain intersection value of NEb0 the upper bound equals the lower bound. Consider Figs. 2.3, 2.4 and 2.5 which show the frame error rate against the SNR per bit for three types of codes. The upper bounds in the figures are obtained using the complete weight distribution of the codes with Eq. (2.5). The lower bounds are obtained using only the number of codewords of minimum weight of the codes with Eq. (2.8). It can be observed that as NEb0 becomes large, the upper bound meets and equals the lower bound. The significance of this observation is that for NEb0 values above the point where the two bounds intersect the performance of the codes under soft decision can be completely determined by the lower bound (or the upper bound). In this region where the bounds agree, when errors occur they do so because the received sequence is closer to codewords a distance d away from the correct codeword. The actual performance of the codes before this region is somewhere between the upper and lower bounds. As we have seen earlier, the two bounds agree when the sum in (2.9) approaches 0. It may be useful to consider an approximation of the complementary error function (erfc), erfc(x) < eâx"
103,410,0.985,Solar Particle Radiation Storms Forecasting and Analysis : The Hesperia Horizon 2020 Project and Beyond (Volume 444.0),"10.4.2 Inversion of NM Data to the Border of the Earthâs Magnetosphere In general, the standard analysis of GLE events is based on an inverse problem, where data by the worldwide network of NMs is used to determine the spectral and angular characteristics of SEPs near Earth causing GLEs for selected times (Shea and Smart 1982; Mishev et al. 2014). Analysis of the characteristics of the primary solar particles causing GLEs from ground-based data records is a serious challenge (BÃ¼tikofer and FlÃ¼ckiger 2013). Data from stations with different cutoff rigidities (geomagnetic latitudes) provide information necessary to determine the spectral characteristics. Responses of stations over a wide range of geographical locations are required to determine the axis of symmetry. Data are fitted to directional distributions that are rotationally symmetric about one direction in space. This, in principle, is close but not exactly the direction of the instantaneous magnetic field measured close to the Earth but outside the Earthâs magnetosphere (Bieber et al. 2013). Therefore, axis-symmetry is assumed, but the direction of the axis of symmetry is optimized to fit the data. The PADs of relativistic solar protons in space are assumed to follow a given functionality. A variety of functions has been used in the literature. These include a linear form, an exponential plus a constant, a parabola, two Gaussians, and two exponentials plus constant. The latter three are expected to provide better fits to bidirectional fluxes, if present (Bieber et al. 2013; Mishev et al. 2014). Similarly, the spectra of relativistic solar protons during a GLE are assumed to follow a power law in rigidity, or energy with extensions that describe the softening of the spectrum at higher energies by multiplying a power law with an exponential cutoff."
36,118,0.985,Bats in the Anthropocene : Conservation of Bats in a Changing World,"Recent studies provide evidence for widespread scale dependence in associations between landscape metrics and bat responses at the assemblage, population, ensemble, and species levels (Gorresen and Willig 2004; Meyer and Kalko 2008a; Pinto and Keitt 2008; Klingbeil and Willig 2009, 2010; Henry et al. 2010; Cisneros et al. 2015). Pinto and Keitt (2008) quantified forest cover at a range of scales (buffers with radii from 50 to 2000 m) and found positive associations with bat abundance, whereby the scale that elicited the strongest response was species specific. Differential species responses to forest cover in this case were best explained by interspecific variation in diet, body size, and home range size. Similarly, multiple species- and ensemble-specific abundance responses of phyllostomid bats to landscape characteristics at multiple focal scales (buffers with 1, 3, and 5 km radii) have been reported from moderately fragmented, lowland Amazonian forest (Klingbeil and Willig 2009) and highly fragmented Atlantic forest in Paraguay (Gorresen and Willig 2004). In both studies, species were demonstrated to interact with their environment simultaneously at a range of spatial scales. In the Amazon, a change in the focal scale of response occurred between dry and wet seasons, a finding which is likely linked to seasonal differences in food abundance and diversity as well as energetic constraints associated with reproduction (Klingbeil and Willig 2010; Cisneros et al. 2015). Scale dependence in response patterns has also been observed in landscapes with an aquatic matrix (Meyer and Kalko 2008a; Henry et al. 2010), suggesting that scale effects are ubiquitous and operate in fragmented landscapes across a broad range of matrix types. Overall, such findings emphasize that multiscale approaches to determining the effects of landscape structure on tropical bats are essential. In agreement with recent findings for tropical birds (Banks-Leite et al. 2013), the available evidence suggests, however, that the extremely idiosyncratic responses of tropical bats to landscape structure make it difficult to identify any particular landscape predictor or spatial scale that performs best at predicting responses at the assemblage level. Despite the general importance of a landscape-level perspective in the study of habitat fragmentation, patch characteristics remain important for patch-dependent species (Driscoll et al. 2013). However, fragmentation studies on tropical bats that have jointly assessed the relative contribution of patch- and landscape-scale variables for explaining response patterns are scarce. Meyer and Kalko (2008a) found that the relative importance of local- versus landscape-scale characteristics in explaining species richness and compositional patterns of phyllostomids on Panamanian land-bridge islands varied with spatial scale. At the patch scale, isolation distance from the mainland was the strongest predictor, whereas the proportion of forest cover in the surrounding landscape was the most prominent descriptor explaining variation in assemblage attributes at larger scales. Although the importance of spatial scale and spatial variation in matrix quality have received some attention in the bat fragmentation literature, we know little about how species responses to fragmentation vary over time or how they are mediated by changes to the matrix. Across many human-modified landscapes in the tropics, secondary forest regrowth may reclaim once deforested"
373,52,0.985,Introduction to Data Science,"Frequency analysis Frequency analysis is a part of descriptive statistics. In statistics, frequency is the number of times an event occurs. It deals with the number of occurrences (frequency) and analyzes measures such as: â¢ Measures of central tendency like mean, median, and mode; â¢ Measures of dispersion such as standard deviation, variance (and range); â¢ Percentile value, which shows the value shows what percent of values in a data set fall below a certain percent; â¢ Skewness and kurtosis. It is essential for the analysis and interpretation of any data at a glance. Measure of central tendency"
285,652,0.985,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","It is assumed that forward masking occurs during auditory neural processing, and that it operates on peripheral auditory output (i.e. neural firing in the auditory nerve). Since the compression component of the peripheral processing is normal in both the 80 year 100 % condition and the NH â 30 dB condition, the level difference between syllable 2 and syllables 1 and 3 should be comparable in the peripheral output of any specific condition. In this case, ÎSRT provides a measure of the more central masking that occurs during neural processing. When the compression is cancelled, the level difference in the peripheral output for syllable 2 is greater in the dip conditions. If the central masking is constant, independent of the auditory representation, the reduction of the output level for syllable 2 should be reflected in the amount of masking, i.e. ÎSRT. This is like the description of the growth of masking by Moore (2012), where the growth of masking in a compressive system is observed to be slower than in a non-compressive system. Moore was describing experiments"
311,3039,0.985,The Physics of the B Factories,"valid to arbitrary high energies but need to be supplemented with extra fields and/or interactions. One of the more interesting realizations is the Littlest Higgs model with T parity. In it the SM fields are supplemented by a new heavy top quark (T+ ), a triplet of heavy scalars (Î¦) as well as a new set of heavy gauge bosons WH , ZH , AH . It has an interesting non-MFV flavor structure with only 10 new parameters in the quark sector. As a result there are still correlations between FCNC processes in the down and up-quark sectors. The constraints from B â Xs Î³ are easily satisfied, while significant eï¬ects would be expected in Bs mixing and in K â ÏÎ½ Î½Ì and KL â Ï 0 â+ ââ decays. 25.2.2 Detailed description of NP models We now give a more detailed description of the models, focusing especially on the impact the B Factory observables had on constraining the models. We highlight the following observables in particular, sin 2Ï1 , B â Xs Î³, B â Ï Î½, D0 â D0 mixing, B â ÏKS0 , all of which are listed in Table 25.2.2. We also include the anomalous moment of the muon, (g â 2)Î¼ , as another important observable. Even though it was not measured at the B Factories, the ISR results obtained at the B Factories provide crucial inputs to the SM predictions (see Section 21.3.4 for detailed discussions on the impact of the B Factories ISR result on the muon g â 2). For ease of comparison a star system is used in Table 25.2.2, where more stars mean that the generic predictions of the model agree better with the observations (from 1 to 3 stars)."
280,17,0.985,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"The color patterns of butterflies are organized as a set of three-symmetry systems (SÃ¼ffert 1929; Schwanwitsch 1924, 1929; Nijhout 1991). The basal symmetry system is often absent or represented only by its distal band. The central symmetry system runs in the middle region of the wing and is centered on the discal spot. The border symmetry system runs along the distal region of the wing usually paralleling the wing margin (Fig. 1.2). The most complex patterns are typically found in the border symmetry system. The principal elements of the border symmetry system are the border ocelli or eyespots. Although the canonical morphology of an ocellus is a set of concentric circles of contrasting pigments with a well-defined central spot called the focus (Nijhout 1980), circular elements are actually quite uncommon within the larger diversity of butterfly color patterns. More often the shape of the âocellusâ deviates significantly from the circular (heart shaped, dagger shaped, bar shaped) and is often hardly recognizable as homologous to a circular element (Nijhout 1990, 1991). The proximal and distal bands of the border symmetry system have very different characters. The proximal band, when present, is typically arc shaped, or nearly straight. The distal bands are almost always present and have an exceptionally diverse array of shapes. Because its development and evolution are quite independent of that of the border ocelli, this element has been given a special name: the parafocal element (Nijhout 1990). SÃ¼ffert (1929) recognized this as the distal band of the border symmetry system but did not give it a special name, and Schwanwitsch (1924) thought it was actually part of the submarginal band system. The results given below in this paper support SÃ¼ffertâs interpretation, as does the recent work of Otaki and colleagues (Dhungel and Otaki 2009; Otaki 2009, 2011)."
241,634,0.985,Second Assessment of Climate Change for the Baltic Sea Basin,"10.3.2.4 Weather Generators Stochastic weather generators are statistical models producing high-resolution local-scale time series of a suite of elements such as temperature and precipitation among others, whose large-scale statistics follow the required criteria (Richardson 1981; Wilks and Wilby 1999; Olsson et al. 2009; Willems and Vrac 2011). Among many applications, they can serve as a computationally effective tool to produce site-speciï¬c data sets at the required time resolution (Semenov et al. 1998). The distribution used is usually different for different climate variables. For temperature, the normal distribution is the most popular (Semenov et al. 1998). More complicated is the generation of precipitation data, and different functions are used. Among the most popular are the Markov chain, the semi-empirical, and the NeymanâScott rectangular pulse (NSRP) weather generator. In the Markov chain generator, precipitation occurrence and totals are produced separately (Sunyer et al. 2012). Two states are possible: wet or dry days. The amount of precipitation on a rainy (wet) day is most often generated using a gamma or exponential distribution (Benestad 2007). In the semi-empirical generator, a few distributions can be deï¬ned, for instance for wet and dry spell lengths and precipitation amount. In the NSRP weather generator, Kilsby et al. (2007) proposed four different steps. A storm origin is described by the Poisson process. Separate rain cells within a storm are separated by time intervals taken from exponential distribution. The duration and intensity of each rain cell are also described by exponential distributions, and their sum gives a rainfall total. Weather generators can be used when the observation records are relatively short. They can also supply many weather ârealisationsâ having the same overall statistics. A wide suite of statistics can be used to ï¬t the model: mean, variance, skewness, autocorrelation, and many others. Weather generators can also serve to produce data for locations where there is information about the statistical distribution and time structure. For places with only short records of high-temporal-resolution data but longer series with data of low resolution, it is possible to use information from the longer records to make inferences about the distributions, and it is in principle possible to produce projections for temporal scales higher than those usually produced by RCMs (6 h). 10.3.2.5 Randomisation Models generally underestimate the local-scale variance. To resolve this, Karl et al. (1990) proposed the use of a scaling factor to ensure that the variance of the projected surface values will match the observed variance. But this could"
311,1536,0.985,The Physics of the B Factories,"Table 17.10.3. Summary of measurements of B â D(â) Ï Î½. NBB : number of BB pairs in the data sample used for the analysis, B: branching fraction (the ï¬rst error is statistical, the second systematic, and the third due to the branching fraction uncertainty in the normalization mode), Î£: signiï¬cance of the signal including systematic, R(D(â) ): the ratio B(B â D(â) Ï Î½)/B(B â D(â) âÎ½). Experiment"
311,3049,0.985,The Physics of the B Factories,"We expect a significant contribution from the loop induced diagrams with a charged Higgs and a top quark in the loop such as Bd â B d oscillation and the decay B â Xs Î³. A large enhancement of the charged Higgs contribution is possible because of the first term in Eq. (25.2.6) with ui = t due to the large top quark mass. While the experimental measurements of the Bd â B d oscillation frequency can constrain part of the parameter space, in particular for small tan Î², the constraint from B â Xs Î³ is generically more important. At LO the Wilson coeï¬cient relevant for B â Xs Î³ including the NP contributions is (Ciuchini, Degrassi, Gambino, and Giudice, 1998b; Grinstein, Springer, and Wise, 1990; Hou and Willey, 1988) m2W Â±"
165,431,0.985,New Methods for Measuring and Analyzing Segregation,"In this chapter I introduce a new approach for addressing the problem of index bias at the point of measurement. Specifically, I introduce new formulations of popular indices of uneven distribution that are free of bias and take expected values of zero when individuals and households are randomly assigned to residential locations. I accomplish this task by drawing on the difference of means formulations of segregation indices introduced in earlier chapters to first identify and then eliminate the root source of bias in standard versions of popular indices of uneven distribution. The crucial insight from the difference of means formulation is that the values for all popular indices of uneven distribution can be seen as resting on person-specific scores for pairwise group contact (p). Close consideration reveals that the source of index bias is found in these group contact scores. Happily, a surprisingly simple refinement in the calculation of these scores eliminates index bias. I review the root problem and its solution in more detail in the body of this chapter but offer a brief preview the essence of the problem and the solution here. To begin, recall that the difference of means framework establishes that all popular indices of uneven distribution can be formulated in terms of group differences in scaled residential exposure or contact. More specifically, the score for a particular index of uneven distribution can be obtained by calculating the difference of group means on individual residential outcomes (y) scored using an index-specific scaling function y = f ( p ) . The input to the scaling function, âpâ, is the individualâs level of pairwise contact with the reference group in the comparison. The value of p is calculated from the area population counts for the two groups in the segregation comparison based on p i = n1i / ( n1i + n 2 i ) . This approach to calculating the value of p introduces inherent upward bias in group differences on scores for p and also group differences on scores of y. The source of bias is simple; the count terms (i.e., n1i and n2i) used in the calculation of group contact (pi) include the individual in question. The score for contact thus combines two components of contact â contact with self and contact with neighbors. For any individual the component of contact that derives from contact Â© The Author(s) 2017 M. Fossett, New Methods for Measuring and Analyzing Segregation, The Springer Series on Demographic Methods and Population Analysis 42, DOI 10.1007/978-3-319-41304-4_15"
375,64,0.985,Musical Haptics,"the presence of water in the physics of the contact owing to the fact that keratin is the building material of the stratum corneum. Keratin is akin to hydrophobic polymers with the effect that traction increases with the presence of water despite the reduction of the interfacial shear strength. This is true up to a point where, in fact, excess of water hydrodynamically decreases friction in competition with the former effect. A third complicating factor is that the presence of water plasticises the stratum corneum with the consequence of dramatically increasing the effective contact area, which is a phenomenon that occurs at the molecular level [19]. A fourth factor is the very large effect of time on the frictional dynamics. In fact, all these four factors dominate the generation of traction as opposed to the normal gripping load, in direct opposition to the simplistic friction models adopted in the greatest majority of neuroscience and robotic studies [1]. Furthermore, this physics depends completely on the counter surface interacting with the fingers, where the material properties, the roughness of the surface and its structural nature (say wood) interact with the physiology of sudation (perspiration) through an autonomic function performed by the brain [2]."
311,1178,0.985,The Physics of the B Factories,"in the time-dependent interference terms (Eq. (12.1.9)), is important both to solve the two-fold ambiguity in 2Ï1 and to test the consistency of this determination with the more precise value from other b â ccs decays. The decay of a pseudo-scalar to vector-vector final state can be described with three angles defined in the transversity basis (Dunietz, Quinn, Snyder, Toki, and Lipkin, 1991), where the three amplitudes, A0 , A , and Aâ¥ have well-defined CP eigenvalues. The amplitudes are determined by a time-integrated angular analysis of B 0 â"
372,381,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"4.8 The Interferometer Measurement Equation The set of equations for the visibility values that would be measured for a given brightness distributionâtaking account of all details of the locations and characteristics of the individual antennas, the path of the incoming radiation through the Earthâs atmosphere including the ionosphere, the atmospheric transmission, etc.âis commonly referred to as the measurement equation or the interferometer measurement equation. For any specified brightness distribution and any system of antennas, the measurement equation provides accurate values of the visibility that would be observed. The reverse operation, i.e., the calculation of the optimum estimate of the brightness distribution from the measured visibility values, is more complicated. Taking the Fourier transform of the observed visibility function usually produces a brightness function with physically distorted features such as negative brightness values in some places. However, starting with a physically realistic model for the brightness, the measurement equation can accurately provide the corresponding visibility values that would be observed. This provides a basis for derivation of realistic brightness distributions that represent the observed visibilities, using an iterative procedure. The formulation of the interferometer measurement equation is based on the analysis of Hamaker et al. (1996) and further developed by Rau et al. (2009), Smirnov (2011a,b,c,d), and others. It traces the variations of the signals from a source to the output of the receiving system. Direction-dependent effects include the direction of propagation of the signals, the primary beams of the antennas, polarization effects that vary with the alignment of the polarization of the source relative to that of the antennas, and also the effects of the ionosphere and troposphere. Directionindependent effects include the gains of the signal paths from the outputs of the antennas to the correlator. It is necessary to take account of all these various effects to calculate accurately the visibility values corresponding to the source model. Several of these effects are dependent upon the types of the interferometer antennas and the observing frequencies, so the details of the measurement equation are to some extent specific to each particular instrument to which it is applied."
372,556,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where )u and )` represent the corresponding expressions in square brackets in Eq. (6.28). The responses considered above represent the normal output of the interferometer, which we call condition 1. The expression for the imaginary output of the correlator is obtained by replacing $G by $G $ ""=2. Consider a second condition in which a ""=2 phase shift is introduced into the first LO signal of antenna m, so that #mn becomes #mn $ ""=2. The correlator outputs for the two conditions are obtained from Eqs. (6.28) and (6.29): condition 1"
200,170,0.985,"Earthquakes, Tsunamis and Nuclear Risks","order of 10 7/year. Figure 9.5 shows the CDF by the snow hazard category, in which the dominant snow hazard category was a combination of 1â2 m/day of snowfall speed and 0.5â0.75 day of snowfall duration (1â1.5 m of snow depth). The dominant sequence was that the personnel failed the door opening on the roof after the 1st awareness of the snow removal necessity, resulting in the loss of decay heat removal system due to snow. Importance and sensitivity analyses indicated a high risk contribution to secure the access routes. Looking at Fig. 9.2, the dominant snowfall speed of 1â2 m/day is approximately 10 2/year of annual access probability (at 1 m/day of snowfall), and the dominant snow depth of 1â1.5 m is approximately 10 1/year (at 1 m of snow depth). Such frequencies are not so low that we are aware of the importance of relatively high frequent hazard through this study. The PRA results would be served for the development of safety measures and accident management. In general, although careful attention may be often paid to extremely low-frequency events bringing high consequence, significant hazard intensity could be clarified through PRA studies. The event tree methodology is well known as a classical manner for the PRA; however, it is difficult to express time-dependent event sequences including recovery. Therefore, a new assessment technique was also being developed for the event sequence evaluation based on a continuous Markov chain Monte Carlo method with plant dynamics analysis [14]."
311,2241,0.985,The Physics of the B Factories,"One first uses the diï¬erence of the two measurements in Eq. (19.2.74) to determine AÏÇ« s . After correcting for AÏÇ« s , one corrects for AFB from Eq. (19.2.73) as explained above. An additional complication arises due to the fact that AÏÇ« s , which at least partially arises from the diï¬erence of charged particle interactions in the detector material, depends on the momentum and the laboratory polar angle of the pion. Hence AÏÇ« s is examined as a function of (pÏs , cos Î¸Ï ) which denote the magnitude of the momentum and the cosine of the polar angle of the slow pion, respectively. The asymmetry values for various decays are thus calculated in bins of (pÏs , cos Î¸Ï ), as well as in bins of the D meson polar angle, as explained below. Graphically the method of correction is illustrated in Fig. 19.2.27. A similar method has been used in the analysis of D0 â KS0 P 0 (P 0 = Ï 0 , Î·, or Î· â² ) decays (Ko, 2011). An attentive reader may wonder why the treatment of the"
311,2085,0.985,The Physics of the B Factories,"associated with Dsâ â Î¼â Î½ Î¼ decays are removed by requiring m2r > 0.5 GeV2/c4 . The Eextra distribution is used to extract the yield of signal events as illustrated in Figure 19.1.41 for the BABAR (del Amo Sanchez, 2010g) and Belle (Zupanc, 2013b) analysis. These analyses have a 100 times lower tagging eï¬ciency than CLEO-c data collected at threshold. Yet this small eï¬ciency is compensated by the much higher registered integrated luminosity. Measured statistics for the Dsââ tag and the signals are given in Table 19.1.21. From the obtained Ds â âÎ½â signal yields the branching fractions for individual modes are calculated and are given in Table 19.1.22. 19.1.6.3 Measured and Expected fDs Values Using Eq. (19.1.69), measured branching fractions of Ds leptonic decays are used to extract the value of the deexpt.,SM cay constant fD . Results obtained by the diï¬erent experiments are compared in Table 19.1.23. While measurements at CLEO-c are statistically limited, systematic uncertainties related to the background control dominate the methods developed at B factories which have a total combined accuracy similar to CLEO-c. Having two results with diï¬erent systematics and similar uncertainty is important to have confidence in the final result as it was al+ ready illustrated for the measurement of fK (0) in charm semileptonic decays. The averaged value of all measurements is fDs = (257.5 Â± 4.6) MeV. New physics can change Eq. (19.1.69) and the value expt.,SM of fD extracted previously from data, in the Stans dard Model framework, may diï¬er from QCD expectations. Several LQCD collaborations using an unquenched"
311,2572,0.985,The Physics of the B Factories,"LEP experiments are shown in Fig. 20.6.7 for the vector current, and in Fig. 20.6.8, for the axial-vector current. The statistical errors in the high mass region above 2 GeV2/c4 , where perturbative QCD plays an important role, are large. Mass spectra data can contribute significantly to the improvement of our knowledge of this region. See Boito et al. (2012) for a recent discussion of the importance of the new data in this region."
311,1491,0.985,The Physics of the B Factories,"eled based on (Ali, Hiller, Handoko, and Morozumi, 1997; Ali, Lunghi, Greub, and Hiller, 2002; Kruger and Sehgal, 1996) in which the J/Ï and Ï(2S) regions are interpoB(B â Ïâ+ ââ ) â¡ B(B + â Ï + â+ ââ ) (17.9.73) lated as if these contributions do not exist. The model+ dependent eï¬ects give a total systematic error of 20%, still B(B 0 â Ï 0 â+ ââ ) , ÏB 0 slightly smaller than the statistical error. The resulting inclusive branching fractions are given in Table 17.9.14. In they set an upper limit of: Figure 17.9.15 Belleâs results on the diï¬erential branch+ â ing fractions for B â Xs â â as functions of MXs and B(B â Ïâ+ ââ ) < 9.1 Ã 10â8 . (17.9.74) q 2 are shown. With more events the measurements could be broken into bins of di-lepton mass squared, and the Belle (Wei, 2008a) has analyzed 657M BB and also finds forward-backward asymmetry AFB could be studied. a few candidate events. A fit gives a small excess with a The results for the inclusive b â sâ+ ââ branching frac- significance of 1.2Ï. They set an upper limit of: tion are consistent with the SM prediction (e.g., B(B â Xs Î¼+ Î¼â ) = (4.20 Â± 0.70) Ã 10â6 assuming interpolation B(B â Ïâ+ ââ ) < 6.2 Ã 10â8 . (17.9.75) over the veto region (Ali, Lunghi, Greub, and Hiller, 2002)). They can be interpreted as giving a preference to a neg- The SM prediction of 2Ã10â8 is not far below these limits, ative sign for Wilson coeï¬cient C7 , where the inclusive and the backgrounds are quite manageable, and in fact the b â sÎ³ branching fraction only determines the magnitude first observation for the charged mode B + â Ï + Î¼+ Î¼â has been recently reported by LHCb (Aaij et al., 2012e) with of C7 and not the sign. Belle has reported an unpublished preliminary result a branching fraction consistent with the SM. with 657M BB, with several improvements in the analysis (Iijima, 2010). The largest improvement is that the Xs mass range is divided into the K, K â and high mass 17.9.8 Electroweak penguin decays b â s(d)Î½Î½ range, and the high mass range alone has been measured with 3Ï significance. In addition, several new background The b â sÎ½Î½ decays are described by an electroweak pen0 sources have been identified. They include a semileptonic guin diagram including a Z boson or a W W box diagram, Wilson coeï¬cients vector axialB decay background that peaks in the mES distribution + â vector parts Unlike there is no due to one additional misidentified lepton that compensates the missing neutrino, and contributions from higher contribution from a virtual photon penguin diagram (C7 ). cc resonances that were disregarded in the previous anal- There is also no contribution from cc resonances. Measuryses. Although the preliminary results have been used in ing the branching fractions of these decays provides a powthe HFAG averages and also in some literature, we do not erful test of new physics complementary to other rare B decays (Altmannshofer, Buras, Straub, and Wick, 2009). include them in Table 17.9.14. The experimental challenge is to identify a B decay to In addition, BABAR has submitted for publication an and two missing neutrinos. This is similar updated measurement of B â Xs â+ ââ based on 471M an Xâs system BB events (Lees, 2014). Along with the results in the to B â Ï Î½ decays, which have been first observed at the B Factories. All that has to be done is to replace the The more recent theory publications used for Eqs 17.9.70 observable Ï decay products (Ï, Ï, e, Î¼), with a K or K â meson. However, the SM predictions for B â KÎ½Î½ or and 17.9.71 do not quote values for the full q 2 range."
311,576,0.985,The Physics of the B Factories,"are the corresponding momenta. The region of kinematically allowed phase space described by these constraints is shown in Fig. 13.1.1. The points on the boundary of the phase space correspond to the conï¬gurations where the ï¬nal state particles are collinear. In particular, three extreme points where m2ab , m2bc , or m2ac are maximal, correspond to the conï¬gurations with one of the particles produced at rest (in the frame of the decaying particle)."
311,2215,0.985,The Physics of the B Factories,"WS events can thus be determined using a much larger sample of kinematically equal RS decays in accordance with blind analysis principles described in Chapter 14. The selection criteria include the requirements on minimum invariant mass and momentum of the kaon and lepton system, which suppress the backgrounds from various B meson decays. Two important sources of background are D0 â K + K â and Ï + Ï â decays, which in case of misidentification of one or both final state mesons produce a peak in the ÎM distribution. These are eï¬ectively suppressed by calculating the invariant mass of the Kâ system, assigning an appropriate mass to the kaon and lepton candidate,133 and requiring that the resulting value is not consistent with the nominal D0 mass. The requirement on the magnitude of the CM momentum of the Kâ system, pâ (Kâ) > 2.0 GeV/c2 , improves the resolution on ÎM and rejects a majority of D0 mesons produced in B meson decays. Events with Î³ â e+ eâ conversions are also a source of background for electron and slow pion candidates. These are suppressed by calculating the invariant mass of the electron and the pion candidate, assigning the electron mass to both tracks, and requiring the result to be larger than 140 MeV/c2 . Apart from the aforementioned backgrounds and genuine signal there are other D meson decays with ÎM in the signal region. Despite the fact that the measurement method is not specifically aiming to reconstruct those decays the charge correlation between the lepton and the Ïs is the same as in the signal decays and hence they carry similar information on the possible mixing parameters. For this reason they are referred to as the associated signal:"
105,52,0.985,"Computer and information Sciences : 31St international Symposium, Iscis 2016, KrakÃ³w, Poland, October 27â28, 2016, Proceedings (Volume 659.0)","settings where the parameter took that specific value. It can be seen that a population size of 5 has the highest effect in each case during the 10 nominal minutes run time. Similarly, the intensity of mutation parameter value of 0.2 performs well at each time. For the tour size parameter, 5 has the highest effect throughout the search except at one point: at 10 nominal minutes, the tour size of 4 had a score of 19.58 while tour size 5 had a score of 19.48, giving very similar results. The best value for the depth of search parameter changes during the execution; however, it is always one of the values 0.6, 0.8 or 1.0. 0.6 for depth of search is predicted to be the best parameter value for a shorter run time. The analysis of variance (ANOVA) is commonly applied to the results in the Taguchi method to determine the percentage contribution of each factor [16]. This analysis helps the decision makers to identify which of the factors need more control. Table 1 shows the percentage contribution of each factor. It can be seen that intensity of mutation and population size parameters have Table 1. The percentage contribution of each parameter obtained from the Anova test for 6 problem domains par. \n.t.b. 1 (min.)"
311,567,0.985,The Physics of the B Factories,"12.3 Analysis details 12.3.1 Generators In order to perform an angular analysis it is important to have simulated data with the correct angular distributions. This allows one to calculate, for example, the correct eï¬ciencies on the signal (see the next subsection) and to study how well (with how much bias) the angular ï¬ts described in Section 12.4 can extract the ï¬tted parameters. Here is a brief explanation of how this is achieved in the EvtGen event generator (Lange, 2001) introduced in Chapter 3. The crucial point is that decay amplitudes, and not probabilities, are used for each step in the generation of a decay chain. This allows one to include all angular correlations in the entire decay chain. Each particle is described according to the value of its spin and mass by an object with the corresponding number of degrees of freedom. Each decay in the decay chain is handled by a speciï¬c model taking into account the spin of the initial and ï¬nal state particles. Relevant parameters can be given as arguments to the decay model. For example in the case of the model describing the decay of a scalar to two vector mesons, the six arguments are the magnitude and the phase of the three helicity amplitudes."
175,1444,0.985,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","Water distribution systems include pumping stations, distribution storage, and distribution piping. The hydraulic performance of each component in the distribution network depends upon the performance of other components. Of interest to designers are both the flows and their pressures throughout the network. The energy at any point within a network of pipes is often represented in three parts: the pressure head, p/Î³, the elevation head, Z, and the velocity head, V2/2g. (A more precise representation includes a kinetic energy correction factor, but that factor is small and can be ignored.) For open-channel flows, the elevation head is the distance from some datum to the top of the water surface. For pressure pipe flow, the elevation head is the distance from some datum to the"
63,153,0.985,"Managing Elevated Risk : Global Liquidity, Capital Flows, and Macroprudential Policy - An Asian Perspective","5.4 Appendix The presence of feedback influences in a network model requires a large matrixâknown as a super-matrixâthat contains a set of submatrixes. The super-matrix captures the influence of elements in a network on other elements in that network. Denoting a cluster by Ch, h = 1, â¦ m, and assuming that it has nh elements eh1, eh2, eh3 â¦, ehmh, and laying out all clusters and all elements in each cluster both vertically (on the left) and horizontally (at the top), we have the super-matrix in Fig. 5.8. The typical entry of this super-matrix is in Fig. 5.9. The entries of submatrixes in Wij are the ratio scales derived from paired comparisons performed on the elements within the clusters themselves,"
297,748,0.985,The R Book,"The origin of the Weibull distribution is in weakest link analysis. If there are r links in a chain, and the strengths of each link Zi are independently distributed on (0, â), then the distribution of weakest link V = min(Zj ) approaches the Weibull distribution as the number of links increases. The Weibull is a two-paramter model that has the exponential distribution as a special case. Its value in demographic studies and survival analysis is that it allows for the death rate to increase or to decrease with age, so that all three types of survivorship curve can be analysed (as explained on p. 872). The density,"
80,803,0.985,Innovations in Quantitative Risk Management (Volume 99.0),"Then choose ÏÎ¹, j (x) = 1 SÎ¹, j (x), the indicator function of slab SÎ¹, j . We remark that this approach corresponds to discretizing the constraints of the Marginal Problem described in Sect. 3, but not to discretizing the probability measures over which we maximize the aggregated risk. While the number of test functions is nm and thus linear in the problem dimension, the number of polyhedra to consider is exponentially large, as all intersections of the ÎÎ¹,j ="
84,378,0.985,Eye Tracking Methodology,"where pi is the simple (observed) probability of viewing the ith AOI, pi j is the conditional probability of viewing the jth AOI given the previous viewing of the ith AOI, and n is the number of AOIs. Ht , or entropy, provides a measure of statistical dependency in the spatial pattern of fixations represented by the transition matrix, and may be used to compare one matrix to another."
283,703,0.985,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","it is possible to maximise the run of the consecutive ones in U (z) by varying the coefficients of es (x). It is therefore important that all possible non-zero values of eCs,0 for all s â M are included to guarantee that codes with the highest possible minimum Hamming distance are found. Table 12.3 outlines some examples of [n, k, d]2m cyclic LDPC codes. The nonbinary algebraic LDPC codes in this table perform well under iterative decoding as shown in Fig. 12.4 assuming binary antipodal signalling and the AWGN channel. The RVCM algorithm is employed in the iterative decoder. The FER performance of these non-binary codes is compared to the offset sphere packing lower bound in Fig. 12.4. As mentioned in Sect. 12.1.2, there is an inverse relationship between the convergence of the iterative decoder and the minimum Hamming distance of a code. The algebraic LDPC codes, which have higher minimum Hamming distances compared to irregular LDPC codes, do not converge well at long block lengths. It appears that"
271,267,0.985,Communicative Figurations : Transforming Communications in Times of Deep Mediatization,"classrooms but included entire grade levels (Friemel and Knecht 2009). Denser parts of the network suggest the existence of cliques of friends. Figures 8.4, 8.5, 8.6 show the respective networks for communication about TV, YouTube and video games. In these networks, the intensity of the greyscale indicates the frequency of use of the medium in question; for example, the darker a node in Fig. 8.4 the more frequently that person watches TV. A visual exploration of each of these communication networks supports the results from correlation analysis above (cf. Table 8.2). In all three networks, darker coloured nodes tend to be linked by more"
26,183,0.985,Cognitive Supervision for Robot-Assisted Minimally Invasive Laser Surgery,"5.5.3 Ablation Assessment In order to analyze the characteristics of the ablations, samples are examined under a microscope and differences among the profiles are analyzed. Repeatability of the ablation process can be studied defining a metric to compare among them. Ablation profiles are segmented, measuring the values for total width as well as depth in three different points of the crater (25, 50 and 75 % of the total width). Such metrics are described in Fig. 5.12. 1 A proof is given in Appendix C."
352,179,0.985,Interface Oral Health Science 2014,"Conventionally, magnetic resonance imaging (MRI) is used for the diagnosis of various diseases. However, when metallic orthopedic devices are implanted in the human body, deficits and distortions are formed in the images of organs and tissues around the implant (artifact), hindering the exact diagnosis performed using a MRI. The artifacts formed by the metallic materials could be ascribed mainly to the difference in magnetic susceptibilities between living tissues and metallic materials [2]. The magnetic susceptibility of living tissues reveals diamagnetism, while that of water being 9  10 6 cm3g 1 [40]. On the other hand, Ti, being paramagnetic, has the magnetic susceptibility of 3.2  10 6 cm3g 1 [41]. This magnetic susceptibility of Ti is much lower than that of ferromagnetic iron (Fe) and Co, but still higher than that of water. Therefore, Zr, which is a congener of Ti, also exhibits a smaller magnetic susceptibility of 1.3  10 6 cm3g 1, [41]. This property has gained significant attention, and it forms the genesis for the recent developments of Zr alloys, such as ZrâNb [41, 42] and ZrâMo [43, 44] alloys. The dependence of magnetic susceptibility on the Nb content in ZrâNb alloys is shown in Fig. 7.9 [41]. As is seen, the magnetic susceptibility of ZrâNb alloys varies as a function of Nb content, showing a local minimum for the Nb content of 3â9 mass% [41]. As Zr undergoes allotropic transformation similar to Ti, the concept of the phase stability in Zr alloys is similar to that of Ti alloys. Therefore, the phase stability of Zr alloys depends on the chemical composition. The allotropic transformation results in the formation of some intermediate phases, such as non-equilibrium Î±0 and Ï phases, in addition to the equilibrium Î± and Î² phases [41â43]. In ZrâNb alloys, the magnetic susceptibility of Zrâ(3â9)Nb alloys reveals local minimum with Zrâ3Nb, Zrâ6Nb, and Zrâ9Nb alloys consisting of single Î±0 phase, (Î±0 + Ï + Î²) phases, and (Ï + Î²) phases, respectively. However, given the volume fractions of each phase, the Ï phase is considered to have the lowest magnetic susceptibilities among these phases"
78,294,0.985,The Onlife Manifesto : Being Human in a Hyperconnected Era,"4 The Topology of Onlife Networks Several spontaneous orders on the internet present the topological features of scale free-networks and âsmall worlds.â To grasp how the complexity of such topological properties affect any political planning, have a look at Fig. 3 with the key parameters of every network, namely (i) its nodes, (ii) the average distance between nodes or diameter of the network, and (iii) its clustering coefficients. This allows us to single out three models. The first one is represented by a regular network in which all of the nodes have the same number of links: this network has high clustering coefficients but a long diameter since the degree of separation between nodes is high. The second model is a random network with opposite features: it presents low clustering coefficients but a very short diameter. The explanation is that random links exponentially reduce the degree of separation between nodes in the network. The third model is a small world-network: its peculiarity depends on the apparent deviation from the properties of both regular and random networks. Like regular networks, small world-networks present high clustering coefficients, but they also share with random networks a short characteristic path length, i.e., the nodes of the network need few steps in order to reach each other. As you can see, in light of Fig. 3, in the regular network there are 20 nodes, each of which has 4 links, so that the blue node (the brighter one on the left) would need at least 5 steps to reach the red one (the brighter on the right). What is striking with a small-world network is how random links exponentially reduce the degree"
311,2036,0.985,The Physics of the B Factories,"P -wave/S-wave ratio in the Ï(1020) region The decay mode Ds+ â Ï(1020)Ï + is used often as the normalizing mode for Ds+ decay branching fractions, typically by selecting a K + K â invariant mass region around the Ï(1020) peak. The observation of a significant S-wave contribution in the threshold region means that this contribution must be taken into account in such a procedure. BABAR has estimated the P -wave/S-wave ratio in an almost model-independent â Integrating  the distributions of 4Ïpq â² Y00 and 5Ïpq â² Y20 (Fig. 19.1.32), where p is the K + momentum in the K + K â rest frame, and q â² is the momentum of the bachelor Ï + in the Ds+ in a region around the Ï(1020) peak yields"
330,186,0.985,Dynamics of Long-Life Assets : From Technology Adaptation to Upgrading the Business Model,"â Consequential LCA studies the change in environmental impacts related to a change in the life cycle. The result describes the consequences of actions within the life cycle, allocation is avoided through system expansion, and marginal data is used in the calculations. The selection between attributional and consequential approach should be made in the goal and scope deï¬nition phase depending on the purpose of the study. The unit processes within the life cycle can be grouped according to the life cycle steps (e.g. energy production, transportation) or other coding (raw material supply, own processes, end-of-life), and the results can be studied transparently (ISO 14040:2006; ISO 14044:2006). Figure 3 shows an example of a life cycle, presenting the life cycle steps of a ï¬bre product and the types of input and output flows related to each life cycle step. Life cycle inventory (stage 2) calculations require vast amounts of highly speciï¬c data. The procedures related to LCI are shown in Fig. 4. Data can be collected from the production sites within the value chain, or it may be obtained from other sources, e.g. public databases. The LCA standards set speciï¬c requirements for e.g. time-related coverage, geographical coverage, technology coverage, precision, completeness and representativeness of the data. In addition, uncertainty and sensitivity of assumptions can be demonstrated via sensitivity analyses. The results of LCA are represented per functional unit, which describes the need that is fulï¬lled with the product or service. Typical functional units are numbers of product (e.g. one car or a book) or amounts of product (e.g. 1000 kg paper or 1 l of diesel)."
222,319,0.985,Ecosystem Services For Well-Being in Deltas : integrated Assessment For Policy Analysis,"11.1 Introduction The combination of a highly variable climate, a large and increasing population and a high reliance on water dependent sectors such as agriculture means that South Asia could be particularly at risk to future climate change and variability. Changes in the climate occur in a variety of ways, including changes in mean temperature, changes in extreme daily maximum temperatures and changes in the length, frequency or magnitude of heatwaves. Similarly, increases or decreases in the frequency and magnitude of precipitation could affect water availability or change the characteristics of floods or droughts. For a monsoon climate, such as South Asia, changes in seasonality could also have a large impact on the quantities and timings of water availability, a critical factor in agrarian societies. Climate change and variability is therefore a key underpinning input into many of the biophysical models which form part of this research. The focus in this chapter is on climate projections J. Caesar (*) â¢ T. Janes Met Office Hadley Centre for Climate Science and Services, Exeter, Devon, UK Â© The Author(s) 2018 R. J. Nicholls et al. (eds.), Ecosystem Services for Well-Being in Deltas, https://doi.org/10.1007/978-3-319-71093-8_11"
311,3042,0.985,The Physics of the B Factories,"then depend only one complex parameter, Î»tâ² (or equivalently Î»t by using the unitarity relation), and the There is no compelling theoretical reason for having only mass of tâ² . The experimental world averages, Îmd = three generations of fermions. It is thus important to search (0.510 Â± 0.004) psâ1 and SbâccÌs = 0.677 Â± 0.020 (see for additional heavier quarks. The simplest possibility is Sections 17.5.2 and 17.6 for the experimental extraca sequential 4th generation, where new heavy quarks have tion of these values), are consistent with the SM prethe same quantum numbers as in the SM â the left-handed dictions within errors, ÎmSM = (0.51 Â± 0.12) psâ1 and t and b form an SU (2)L doublet, while the right-handed SbâccÌs = 0.74 Â± 0.09. Fixing the mass of tâ² to 600 GeV tâ² and bâ² are singlets (for a review see (Frampton, Hung, the constraints on Î»tâ² are shown in Fig. 25.2.2. The dashed and Sher, 2000)). In this model the 3 Ã 3 CKM matrix is line is the constraint from the mass diï¬erence Îmd , which no longer unitary since it is only a part of the full 4 Ã 4 was known before the B Factories but with much larger matrix uncertainty. The sin 2Ï1 measurement (solid line) gives a Vud Vus Vub Vubâ² strong constraint |Î»tâ² | < 0.005. Combined constraints exâ Vcd Vcs Vcb Vcbâ² â (25.2.2) clude the colored regions. â V V V V â² â . The next example is the B â Xs Î³ branching ratio, Vtâ² d Vtâ² s Vtâ² b Vtâ² bâ² which also receives a contribution from a tâ² quark running This means that the global fits of the CKM unitarity must in the loop. The experimental measurements agree with be supplemented with 3 Ã 3 unitarity relaxed, see, e.g., the SM prediction from up, charm and top quarks running (Bobrowski, Lenz, Riedl, and Rohrwild, 2009). in the loop. However, as in the case of B 0 â B 0 mixing, The heavy quarks can contribute to any loop type di- the top contribution in the 4th generation scenario can agrams. In particular, since the loop function of the box diï¬er from the SM due to a diï¬erent Vtbâ Vts obtained from and the penguin diagrams grow with the mass of the heavy 4 Ã 4 CKM fits. This can then leave some more space quark in the loop, these contributions can be large. There- for the non-zero tâ² contribution. The obtained constraint fore, the precise measurements obtained by the B Facto- on Vtââ² b Vtâ² s is not so strong since the branching ratio is ries lead to very strong constraints on the fourth row and dominated by the tree level O2 contributions which are column of the enlarged 4 Ã 4 quark mixing matrix. proportional to Vcbâ Vcs . The B â Xs Î³ branching ratio"
297,1787,0.985,The R Book,"The model is ï¬tted using binary recursive partitioning, whereby the data are successively split along coordinate axes of the explanatory variables so that, at any node, the split which maximally distinguishes the response variable in the left and the right branches is selected. Splitting continues until nodes are pure or the data are too sparse (fewer than six cases, by default; see Breiman et al., 1984). Each explanatory variable is assessed in turn, and the variable explaining the greatest amount of the deviance in y is selected. Deviance is calculated on the basis of a threshold in the explanatory variable; this threshold produces two mean values for the response (one mean above the threshold, the other below the threshold). low <- (Industry<748) tapply(Pollution,low,mean) FALSE 67.00000 24.91667 plot(Industry,Pollution,pch=16) abline(v=748,lty=2) lines(c(0,748),c(24.92,24.92)) lines(c(748,max(Industry)),c(67,67))"
145,110,0.985,"Evolution, Monitoring and Predicting Models of Rockburst : Precursor Information for Rock Failure","This chapter uses the deï¬ned Li-Yorke criterion for cracks evolution, chaotic characteristics is derived assuming without any loading condition, the sandstone CT number for initial and with the increasing of stress cracks generated which can cause the new crack energy release. Therefore, the change process of describing the density of sandstone can be used to use trigger-growth-triggered cracks chain growth model. The density of microcrack was assumed, the generalized driving force was proportional to the production of cracks, and the CT number ni+1 of sandstone can be expressed as: when the loading went to the stage of i + 1. ni Ã¾ 1 Â¼ F1 Ã°rÃ  ni"
298,59,0.985,The Globalization of Science Curricula,"Having outlined our statistical approach and the rationale behind it we now give a more thorough account of the statistical techniques used in this investigation. Cluster analysis is a statistical technique that allows the grouping of a set of observations according to certain characteristics, in such a way that observations in the same cluster are more similar to each other than to observations in other groups. In the context of our investigation, the observations that are clustered are countries, and the variables used to group them are the responses given to the TIMSS curriculum questionnaire items on the science topics that are included in countriesâ intended science curricula. Our analysis aims to investigate the existence of convergence in science curricula, with convergence signaled by the tendency of a group to expand at the expense of others. When running the analysis on the TIMSS dataset we chose to implement a two-step cluster analysis. The ï¬rst step of this procedure consists of pre-clustering the records into small sub-clusters that are then consolidated into a smaller number of groups in the second step. The advantage of this methodology over other clustering algorithms is that, when the number of groups is unknown, the algorithm automatically returns the optimal number of clusters based on the Bayesian information criterion (BIC).2 Observations are grouped together based on a distance measure that, in this case, reflects how different countries are from each other in terms of science topics covered in their curricula. In order to cluster the observations, we adopt the log-likelihood criterion that is appropriate when grouping observations using continuous as well as categorical variables. This distance measure works best when all variables are independent and categorical variables have a multinomial distribution.3 The log-likelihood is a probability-based distance. The distance between two clusters is related to the decrease in log-likelihood as they are combined.4 Our original intention was to perform a latent class analysis on the data. We performed a cluster analysis followed by a discriminant analysis due to the reduced size of the sample. Given the large number of parameters estimated, latent class analysis requires a considerable number of observations. (In statistics, a model cannot be identiï¬ed unless the sample size is one more than the number of predictors and, as a rule of thumb, we need around ten observations per parameter to estimate a model with reasonable precision.)"
45,564,0.985,Measurement and Control of Charged Particle Beams,"A conceptual illustration is given in Fig. 10.2. At other locations in the ring, the measurable polarization components are obtained by performing a second projection using PnÌ0 (s) where nÌ0 (s) is the stable spin direction at the point of interest."
311,1378,0.985,The Physics of the B Factories,"have very small coeï¬cients C3 , . . . , C6 and hence can safely be neglected. The electromagnetic penguin with C7 (mb ) â â0.3, and the chromomagnetic penguin with C8 (mb ) â â0.15, play a significant role in both b â s(d)Î³ and b â s(d)â+ ââ . Finally the vector and axial-vector contributions to b â s(d)â+ ââ have C9 (mb ) â 4, C10 (mb ) â â4. There are three principal calculational steps that lead to the LL (NNLL) result within the eï¬ective field theory approach: 1. At the scale Î¼ = mW the full SM theory is matched with the eï¬ective theory. This means that the calculation of the amplitude in the full SM is expanded in inverse powers of the large masses (mW , mZ , mt ) and the result is compared to the corresponding amplitude in the eï¬ective theory. In this way the Wilson coeï¬cients Ci (mW ) are extracted by comparison. At the high scale Î¼ = mW the Ci pick up only small QCD corrections, which can be calculated within fixedorder perturbation theory. In the LL (NNLL) calculation, the matching has to be worked out at the O(Î±S0 ) [O(Î±S2 )] level. 2. The evolution of these Wilson coeï¬cients from Î¼ = mW down to Î¼ â mb must then be performed with the help of the renormalization group. In this way the large logarithms (Eq. 17.9.1) are shifted from the matrix elements of the operators into the Wilson coeï¬cients, and the matrix elements of the operators evaluated at the low scale mb are free of these large logarithms. For the LL (NNLL) calculation, this renormalization step has to be performed up to order Î±S1 (Î±S3 ). 3. To LL (NNLL) precision, the corrections to the matrix elements of the operators sÎ³|Oi (Î¼)|b at the scale Î¼ â mb must be calculated to order Î±S0 (Î±S2 ) precision. While the Wilson coeï¬cients Ci enter both inclusive and exclusive processes and can be calculated with perturbative methods, the calculational approaches to the matrix elements of the operators diï¬er in the two cases. In inclusive modes, one can use quark-hadron duality in order to derive a well-defined heavy mass expansion of the decay rates in powers of ÎQCD /mb (Heavy Quark Expansion, HQE)84 (Bigi, Blok, Shifman, Uraltsev, and Vainshtein, 1992; Bigi, Uraltsev, and Vainshtein, 1992; Chay, Georgi, and Grinstein, 1990; Manohar and Wise, 1994). In"
280,152,0.985,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"stage is the most temperature-sensitive stage for male eyespots (female eyespots are not plastic). In summary, eyespot size is primarily sensitive to temperature during the wandering stages of development, but size of Cu1 serial homologous eyespots on ventral forewings and hindwings does not respond to temperature in the same way. Most examples of phenotypic plasticity known from insects seem to rely on a hormonal signal to translate variable environments into variable phenotypes (Nijhout 1999, Beldade et al. 2011). This prompted the search for the hormones responsible for the variation in wing pattern across B. anynana seasonal forms. Previous work on two different butterflies, the map butterfly Araschnia levana and the buckeye Junonia coenia, had discovered that differences in the presence and absence of a peak of the molting hormone, 20-hydroxyecdysone (20E), during the early pupal stage explained the different seasonal forms (spring and summer forms) of these butterflies, displaying different wing colors in response to day length (an important environmental cue used for regulating plasticity in these systems) (Koch and Buckmann 1987; Nijhout 1980; Rountree and Nijhout 1995). 20E became, thus, a candidate hormone to be investigated in connection with eyespot size plasticity in B. anynana. Surprisingly, early work surrounding investigations into the physiological basis of eyespot size plasticity decided not to investigate physiological differences between the seasonal forms but instead focus on physiological differences observed between lines reared at the intermediate temperature of 20  C, whose eyespots had been artificially selected to mimic the dry and wet season forms (Brakefield et al. 1998; Koch et al. 1996). In addition, titers of 20E were measured in individuals of these WS and DS form âgenetic mimicsâ at different stages of development focusing primarily in the early pupal stages, as no differences were observed between these mimics during the wandering stages (Koch et al. 1996). Titers of 20E measured in the early pupal stage showed small differences between the seasonal form genetic mimics, and 20E injections into the dry season form mimic, which had a natural slower increase of 20E during the pupal stage, showed small (albeit significant) increases in eyespot size toward the phenotype of wet season forms (Koch et al. 1996). Later work, however, showed that these 20E titer differences observed between WS and DS form genetic mimics could more readily explain variation in pupal stage duration than eyespot size differences (Oostra et al. 2011). Recent work finally measured 20E hemolymph titers in late larvae of temperature-induced WS and DS forms and discovered that levels of 20E differed significantly between the seasonal forms during the wandering stage of development (Monteiro et al. 2015). This is important because this stage of larval development is contained within the 5th and final larval stage, previously identified as the temperature-sensitive period for induction of eyespot size plasticity (Kooi and Brakefield 1999; Monteiro et al. 2015). Levels were higher in WS forms relative to DS forms, indicating a positive correlation between 20E and eyespot size. To test whether these different levels in 20E were causing the variable wing phenotypes, hormone injections and hormone receptor manipulations were both"
151,119,0.985,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"Where: Y is the upper reported molar yield of 0.3, i.e., 30% and k2 is the transformation rate constant of CPYO. It is likely that Y is larger than is stated above because other transformation products are at lesser yields. Integration of this function gives (19): M2 ="
231,571,0.985,North Sea Region Climate Change Assessment,"typically within the range of natural variability and can even have opposite signs for different scenarios either simulated by different climate models or for different future periods. The projected changes in future climate presented here for the North Sea region have typically been extracted from geographical distributions for either the entire globe, when scenario simulations with GCMs are considered, or for Europe, when scenario simulations with RCMs are used. In some of the respective studies, however, different parts of Europe were considered separately, typically distinguishing between northern and southern Europe. With the multi-model ensemble of regional climate simulations for Europe, which have become available through CORDEX (Jacob et al. 2014), such studies with a special focus on the North Sea region could become available in the near future. The studies considered here vary widely in the choice of underlying scenarios for anthropogenic climate forcing, namely the different SRES scenarios and RCP scenarios. There is, however, a tendency to focus on the SRES A1B scenario in previous studies and the RCP4.5 and RCP8.5 scenarios, respectively, in the most recent studies. Also, the studies vary considerably in the time periods chosen, both for the present-day and future climate conditions, which can make it difï¬cult to directly compare the magnitude of corresponding projected changes between studies. In particular, some studies focus on projections to the middle of the 21st century instead of the end of the 21st century, while some consider projections for both periods. This chapter mostly reports on changes projected at the end of the 21st century. This is mainly because for most of the forcing scenarios the projected changes are stronger at the end of the century, which means there is a higher probability of the projected regional changes exceeding the range of internal variability at that point. Moreover, the differences between RCP scenarios, in particular between the RCP2.6 and RCP4.5 scenarios, develop during the latter half of the century. Several factors contribute to the uncertainties in the projected changes, that is, the uncertainty in the climate forcing due to different scenarios, the model uncertainty associated with different climate models, and the uncertainty due to the natural variability of the climate system. By coordinating the simulation of future climate scenarios by different research groups in initiatives such as CMIP3, CMIP5 or CORDEX or in the ENSEMBLES project, the importance of some of these sources of uncertainty can be quantiï¬ed, ultimately leading to estimates of the likelihood at which certain climatic changes can be expected to occur. With the increase in computer power, climate models have been improved in several respects. In particular, components such as vegetation and marine biogeochemical cycles have been added to coupled climate models leading to the development of earth system models (ESMs) and the horizontal and vertical resolutions of both global and regional"
93,390,0.985,Nordic Mediation Research,"there is a decrease of 0.07% in the childrenâs experience of speaking freely. The horizontal line in Fig. 2 gives a graphic presentation of this.12 Table 2 below show the results of bivariate multilevel analysis where the correlation between the childrenâs assessments and each of the three predictorsâtype of mediation, conï¬ict level and worrisome conditionsâare assessed one by one. There is signiï¬cant correlation between conï¬ict level and the question âWere you able to say what you wanted to the mediator?â (t Â¼ 2.90; p < 0.05). There is a negative correlation between these two variables, which means that an increase in conï¬ict level gives a lower score on the experience variable, although the correlation is very small. There was no other signiï¬cant correlation between the variables, and the effects were generally quite small. In other words, the conï¬ict level in the family only affected the childrenâs experience of being able to say what they wanted in conversation with the mediator to a small degree. Through multivariate analysis where all the predictors (type of mediation, conï¬ict level and worrisome conditions) are assessed simultaneously, we were able to estimate the unique contribution of each variable to explain the childrenâs"
311,1658,0.985,The Physics of the B Factories,"â VS (r) S . (18.1.10) The singlet potential VS (r) is a series in the expansion in the inverse of the quark masses; the terms up to 1/m2 have been calculated long ago (Brambilla, Pineda, Soto, and Vairo, 2001; Pineda and Vairo, 2001). They involve NRQCD matching coeï¬cients (containing the contribution from the hard scale) and low-energy non-perturbative parts given in terms of static Wilson loops and field strength insertions in the static Wilson loop (containing the contribution from the soft scale). In this regime, from pNRQCD we recover the quark potential singlet model. However, here the potentials are obtained from QCD by non-perturbative matching and they often appear to have a diï¬erent form with respect to phenomenological potential models. Their evaluation requires calculations on the lattice or in QCD vacuum models. Recent progress includes new precise lattice calculations of these potentials (Koma, Koma, and Wittig, 2008; Koma and Koma, 2010). Using these potentials, all the masses for heavy quarkonia away from threshold can be obtained by the solution of the SchroÌdinger equation. A trivial example of application of this method is the mass of the hc . The lattice data show a vanishing longrange component of the spin-spin potential so that the potential appears to be entirely dominated by its shortrange, delta-like part. This suggests that the 1P1 state should be close to the center-of-gravity of the 3PJ system. Indeed, the measurements show consistency between data and this expected value (see experimental results in Table 18.2.1). LpNRQCD ="
372,421,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"in which the visibility data are required for this transformation. The discrete Fourier transform (DFT) is very widely used in synthesis imaging because of the computational advantages of the fast Fourier transform (FFT) algorithm [see, e.g., Brigham (1988)]. The basic properties of the DFT in one dimension are described in Appendix 8.4. In two dimensions, the functions V.u; v/ and I.l; m/ are expressed as rectangular matrices of sampled values at uniform increments in the two variables involved. The rectangular grid points on which the intensity is obtained provide a convenient form for further data processing. The two-dimensional form of the discrete transform for a Fourier pair f and g is defined by f . p; q/ D"
359,122,0.985,"Micro-, Meso- and Macro-Dynamics of the Brain","suggest that the oblique effect originates in higher order cortices (Nasr and Tootell 2012; Liang et al. 2007; Shen et al. 2008), as the effect is stronger here compared to early sensory cortex (Shen et al. 2008; MuÌller et al. 2000), and the effect in early cortex is selectively abolished by temporal inactivation of higher order cortex (Shen et al. 2008). Grid cells are typically aligned close to parallel to the cardinal axes of the environment. Recently, it was shown that grid representations are not limited to navigational space in that a grid map of visual space was demonstrated in the entorhinal cortex of monkeys (Killian et al. 2012). Although highly speculative, it is interesting to ponder the possibilities for similar mechanisms at play in embedding internal representations into external reference frames in the visual domain as in the spatial domain. Although not very many examples were given by Killian et al. (2012), there seems to also be a trend for grids to align with a slight offset to cardinal axes (see their Fig. 1). Further, using optical imaging in area MT (which shows movement and orientation selectivity for stimuli) in the visual system, Xu et al. (2006) showed frequency plots of activation over the range of possible stimulus orientations. In these plots, there are quite distinct peaks with bimodal offsets from the cardinal axes (Fig. 7). Upon further inspection, these offsets are very close to 7.5 , which is the exact peak we observed in the alignment offset in grid cells. This finding points to a possible, albeit suppositional, link between visual"
372,1362,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"should be at least 1.5 times the spacing for which the visibility value is required. Note, however, that since the cost of an antenna scales approximately as d2:7 (see Sect. 5.7.2.2), the expected cost of an antenna of diameter 1:5d is roughly 4.4 times that of an antenna of diameter d. The process of merging total power and interferometric data is sometimes called âfeathering.â Another possibility for covering the missing spatial frequencies is the use of one or more pairs of smaller antennas, say, d=2 in diameter, with spacing about 0:7d. A pair of antennas of diameter d=2 have one-quarter of the area, and consequently onequarter of the sensitivity to fine structure, of a pair of the standard antennas. Since the beam of the smaller antenna has four times the solid angle of a standard antenna, it will require one-quarter of the number of pointing directions, and the integration time for each one can be four times as long. Cornwell et al. (1993) present evidence that, for mosaicking, it is possible to obtain satisfactory performance with a homogeneous array, that is, one in which all antennas are the same size. This requires total-power observation as well as interferometry with some antennas spaced as closely as possible. The deconvolution steps in the data reduction help to fill in remaining .u; v/ gaps. At frequencies of several hundred gigahertz, where antenna beams are of minuteof-arc order, images of objects of order one degree in size require numbers of pointings in the range 102 â104 . Any given pointing cannot be quickly repeated, so dependence on Earth rotation to fill in small gaps in the .u; v/ coverage may not be practicable. Thus, arrays designed for mosaicking of large objects require good instantaneous .u; v/ coverage. At such high frequencies, it is also desirable to avoid high zenith angles to minimize atmospheric effects. An alternative to tracking discrete pointing centers is to sweep the beams over the area of sky under investigation in a raster scan motion. This technique has been referred to as âon-the-flyâ mosaicking. It has several advantages: â¢ The uniformity of the .u; v/ coverage for all points in the field is maximized, which results in uniformity of the synthesized beam across the resulting image and thereby simplifies the image processing. â¢ Each point in the field is observed many times in as rapid succession as possible, so some advantage can be taken of Earth rotation to fill in the .u; v/ coverage. â¢ If total-power measurements are made, the scanning motion of the beam can be used to remove atmospheric effects in a similar way to the use of beam switching in large single-dish telescopes. â¢ Waste of observing time during moves of the antennas from one pointing center to another is eliminated. With on-the-fly observing, the real-time integration at the correlator output must be somewhat less than the time taken for the beam to scan over any point in the field, and thus a large number of visibility data, each with a separate pointing position, are generated."
175,914,0.985,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","If a model calibration or âidentiï¬cationâ exercise ï¬nds the âbestâ values of the parameters to be outside reasonable ranges of values based on scientiï¬c knowledge, then the model structure or algorithm might be in error. Assuming the algorithms used to solve the models are correct and observed measurements of system performance vary for the same model inputs, as shown in Fig. 8.9, it can be assumed that the model structure does not capture all the processes that are taking place and that impact the value of the performance measures. This is often the case when relatively simple and low-resolution models are used to estimate the hydrological and ecological impacts of water and land management policies. However, even large and complex models can fail to include or adequately describe important phenomena. In the presence of informational uncertainties, there may be considerable uncertainty about the values of the âbestâ parameters during calibration. This problem becomes even more pronounced with increases in model complexity. An example: Consider the prediction of a pollutant concentration at some site downstream of a pollutant discharge site. Given a streamflow Q (in units of 1000 m3/day), the distance between the discharge site and the monitoring site, X (m), the"
142,1002,0.985,Subseafloor Biosphere Linked To Hydrothermal Systems : Taiga Concept,"The genetic distances estimated based on nucleotide sequences of the COI gene from the six individuals of Alviniconcha and three of Ifremeria nautilei, were approximately 1.9 times larger than that based on a single species for each genus (0.44) (Johnson et al. 2010), which suggests the saturation of genetic distance due to multiple substitutions at the same sites since the split between Alviniconcha and Ifremeria. The molecular evolutionary rate calculated based on such a more reasonable estimation is expected to enable reconstruction of reliable population history of Alviniconcha hessleri. Population dynamics of hydrothermal-vent endemic species such as A. hessleri are thought to be controlled primarily by fluctuating hydrothermal activity. A decline in the activity results in the reduction of population size which accelerates divergence between local populations. In contrast, recovery of the activity causes an expansion of population size, fusion of isolated populations, and"
164,137,0.985,"Marginality : Addressing the Nexus of Poverty, Exclusion and Ecology","resources, and climateâ; âPublic domain and institutionsâ; and âDemography.â For the purpose of this mapping exercise, single indicators were used to represent each of the spheres. Here the spheres âLandscape design, land use, and locationâ and âInfrastructureâ are both captured by the single indicator âaccessibilityâ, and the sphere âBehavior and quality of lifeâ is represented by stunting. For each dimension a cut-off point along a range of indicator values was used to define the threshold below which an area was considered to be marginal. Indicator layers for each of the different dimensions of marginality were overlaid to find the areas where multiple layers of marginality overlap. We defined a âmarginality hotspotâ as an area in which at least three dimensions of marginality overlapped. The maps were based on national and sub-national data published by the World Bank, the Food and Agriculture Organization of the United Nations (FAO), Harvest Choice, and others. Table 5.1 provides a detailed overview of the data used, the data sources, and the cut-off points chosen for each indicator."
372,430,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"phased-array response causes the zero level to be shifted in the positive direction by an amount equal to 1=na of the peak level, as indicated by the broken line in Fig. 5.4c. The points that represent zeros in the phased-array response become the peaks of negative sidelobes. Thus, in the response of the correlator array, the positive values are decreased by a factor .na "" 1/=na relative to those of the phased array. In the negative direction, the response extends to a level of ""1=.na "" 1/ of the positive peak but no further since this level corresponds to the zero level of the phased array. Kogan (1999) pointed out this limitation on the magnitude of the negative sidelobes of a correlator array and also noted that this limit depends not on the configuration of the individual antennas but only on their number. Neither of these conclusions applies to the positive sidelobes. This result is strictly true only for snapshot observations [i.e., those in which the .u; v/ coverage is not significantly increased by Earth rotation] and for uniform weighting of the correlator outputs. Finally, consider some characteristics of a phased array as in Fig. 5.4a. The power combiner is a passive network, for example, the branched transmission line in Fig. 1.13a. If a correlated waveform of power P is applied to each combiner input, then the output power is na P. In terms of the voltage V at each input, a fraction 1= na of each voltage combines additively to produce an output of na V, or na P in power. Now if the input waveforms are uncorrelated, again each contributes V= na in voltage but the resulting powers combine additively (i.e., as the sum of the squared voltages), so in this case, the power at the output is equal to the power P at one input. Each input then contributes only 1=na of its power to the output, and the remaining power is dissipated in the terminating impedances of the combiner inputs (i.e., radiated from the antennas if they are directly connected to the combiner). The signals from an unresolved source received in the main beam of the array are fully correlated, but the noise contributions from amplifiers at the antennas are uncorrelated. Thus, if there are no losses in the transmission lines or the combiner, the same signal-to-noise ratio at the detector is obtained by inserting an amplifier at the output of each antenna, or a single amplifier at the output of the combiner. However, such losses are often significant, so generally it is advantageous to use amplifiers at the antennas. Note that if half of the antennas in a phased array are pointed at a radio source and the others at blank sky, the signal power at the combiner output is one-quarter of that with all antennas pointed at the source."
311,2073,0.985,The Physics of the B Factories,"with R1(2) (qmax ) = 1. Corrections to these expressions correspond to expansions in 1/m, where m can take the values of the masses of the two quarks involved in the weak transition, and in the strong coupling constant, from perturbative QCD. It is an unambiguous prediction of HQET that R1 (q 2 ) > 1 as both the QCD and 1/m corrections are positive. For R2 (q 2 ), QCD corrections are small and 1/m corrections seem to decrease the value of the ratio. HQET relates the form factors in semileptonic decays to pseudoscalar and vector particles as expressed in Equation (19.1.64). According to (Amundson and Rosner, 1993), QCD corrections alone cannot explain the ratio B(D â K â e+ Î½e )/B(D â K â e+ Î½e ) = 0.62 Â± 0.02 as they have rather similar eï¬ects on all form factors. In previous considerations it is assumed that the charm and also the strange quark behave as a heavy quark in c â sâ+ Î½â decays. It is also possible to relate form factors in B and D semileptonic decays to light hadrons (Isgur and Wise, 1990a). Results on absolute values and on ratios of hadronic form factors evaluated at q 2 = 0 are compared in Table 19.1.19. In this table, BABAR measurements are quoted for fixed values of the pole masses mV = 2.1 GeV/c2 and"
311,1025,0.985,The Physics of the B Factories,"tions to the systematic uncertainties in the measured lifetimes come from the modeling of the signal Ît resolution function (0.009 â 0.014 ps) and the background Ît distribution (0.005 â 0.012 ps), the alignment of the vertex detector (0.008 ps), the knowledge of the z scale of the detector (0.008 ps), and limited statistics of the MC simulation (0.007 â 0.009 ps). The dominant contributions to the systematic error in rÏ come from limited MC statistics (0.005 â 0.006), uncertainties in the background Ît distributions (0.005 â 0.011), and the signal Ît resolution function (0.006 â 0.008). In another analysis BABAR uses events in which Brec is reconstructed in the semileptonic decay B 0 â Dââ l+ Î½ (l = e, Î¼) to determine the B 0 lifetime (Aubert, 2003m). The B yield is larger than for the hadronic final state analysis due to the large B semileptonic branching fraction. They reconstruct 680 B/ fbâ1 . Due to the missing neutrino the background level is higher than in the sample of fully-reconstructed hadronic B decays. The combinatorial Dââ background is about 18% and the sum of the backgrounds from events where the Dââ and the lepton come from diï¬erent B decays, events with a fake lepton candidate and events from continuum cc â Dââ X processes add up to 5 â 8% depending on the lepton flavor. In this analysis, BABAR simultaneously fits for ÏB 0 and the B 0 â B 0 mixing frequency Îmd (see also Section 17.5.2). Because of the diï¬erent Ît distributions for mixed (B 0B 0 or B 0B 0 ) and unmixed (B 0B 0 ) events, separately fitting the two Ît distributions enhances the sensitivity to the common signal Ît resolution function. As a result the uncertainty of ÏB 0 is reduced by approximately 15%. BABAR measures the B 0 lifetime to be ÏB 0 = +0.024 (1.523â0.023 Â± 0.022) ps. The dominant systematic error sources are the same as for the analyses of the hadronic final states and similar in size. A large additional systematic uncertainty in the ÏB 0 measurement comes from the limited statistical precision in determining the bias due to the background modeling. By comparing the fitted ÏB 0 in simulated events, BABAR observes a shift of (0.022 Â± 0.009) ps between a signal-only sample and a signal-plus-background sample. The measured B 0 lifetime is corrected for the observed bias from the fit to the MC sample with background; the full statistical uncertainty in ÏB 0 from this fit (Â±0.018 ps) is assigned as systematic uncertainty. Belle also performs a measurement of the B lifetimes and their ratio in a larger sample of 140 fbâ1 (Abe, 2005c). In this analysis they reconstruct B 0 and B + candidates in the same hadronic decay modes as in their previous analysis. In addition they reconstruct B 0 candidates in the semileptonic decay B 0 â Dââ l+ Î½. Using a fit to the Ît distributions of the signal candidates, they determine the B 0 and B + lifetimes and the B 0 â B 0 mixing frequency Îmd simultaneously. The analysis of the neutral B decays is described in more detail in Section 17.5.2. Belle measures ÏB 0 = (1.534 Â± 0.008 Â± 0.010) ps, ÏB + = (1.635 Â± 0.011 Â± 0.011) ps and ÏB + /ÏB 0 = 1.066 Â± 0.008 Â± 0.008. The largest contributions to the systematic uncertainties in the measured lifetimes come from uncertainties in the"
8,722,0.985,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","The density of states .E; V/ is defined by this identity; .E; V/dE is the number of energy levels in the interval fE; dEg. Equation (26.19) states that the partition function is the Laplace transform of the density of states. From Z.T; V; : : :/ all interesting thermodynamic quantities can be derived by logarithmic differentiation, as in Eq. (26.18). Apart from T and V, the partition function may depend on further variables like chemical potentials (one for each conservation law), external fields, etc."
281,12,0.985,Stochastics of Environmental and Financial Economics (Volume 138.0),"Here t denotes time while x gives the position in d-dimensional Euclidean space. Further, A(x, t) and D(x, t) are subsets of Rd Ã R and are termed ambit sets, g and q are deterministic weight functions, and L denotes a LÃ©vy basis (i.e. an independently scattered and infinitely divisible random measure). Further, Ï and Ï are stochastic fields representing aspects of the volatility/intermittency. In Ambit Stochastics the models of the volatility/intermittency fields Ï and Ï are usually themselves specified as ambit fields. We shall refer to Ï as the amplitude volatility component. Figure 1 shows a sketch of the concepts. The development of Y along a curve in space-time is termed an ambit process. As will be exemplified below, ambit processes are not in general semimartingales, even in the purely temporal case, i.e. where there is no spatial component x."
32,95,0.985,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","Remark 3 In the model of Kuroda et al. [7], since they assume that the exponent of inter-event time distribution is a constant Ë.m/ D Ë, they need to put a limitation on the size of hidden order: m  mmax where mmax is a positive number. Then, they derive finite number of fractional Brownian motions. And the maximum Hurst exponents Hmax is attained by the largest size mmax of hidden orders. In our model, we set no limitation on the size m of hidden orders. As a result, we derive countably infinite number of fractional Brownian motions. On the other hand in the empirical study the size of the hidden order has some limitation, and the upper bound of the size possibly depends on markets or stocks. A finite number of fractional Brownian motions appears in the case. If exponents Ë.m/ is increasing in size m [15], then"
241,1415,0.985,Second Assessment of Climate Change for the Baltic Sea Basin,"net radiation energy at the surface is used for vertical heat transfer than for evapotranspiration (see also Bonan 2008). However, woodland can also sustain its evapotranspiration rate when water availability is low, which leads to lower Bowen ratios than in grassland in dry conditions; that is, grassland becomes the source of excess heating instead of woodland. There are still important problems in relation to how ALCCs are explored in numerical experiments using climate models. Pielke et al. (2011) concluded that most studies were based on only one or two models, which did not reflect the uncertainty between models in their responses to increased CO2 levels or in the strength of their landâatmosphere interactions, an uncertainty that is evident in the LUCID multi-model study of Pitman et al. (2009). That study mainly focused on the Northern Hemisphere summer season, and the key result was a statistically signiï¬cant impact of ALCC on the simulated latent heat flux and air temperature over the regions where anthropogenic land cover changed, but the direction of the change in summer temperature was inconsistent among the models. In terms of rainfall, four of the coupled atmosphereâland models used showed a signiï¬cant impact on rainfall over regions with ALCC, while three models did not show impacts greater than the expected random variability of model outputs (Seneviratne et al. 2010). In their review, Pitman et al. (2009) did not ï¬nd statistically signiï¬cant impacts of ALCC on latent heat flux, temperature or rainfall remote from the actual ALCC, that is no teleconnections. The authors also suggested that robust conclusions on the effects of ALCCs on climate can only be drawn from multi-model experiments. Studies based on a single model only provide indications of possible feedback mechanisms and their implications. The goal of the most recently published part of the LUCID project (de Noblet-DucoudrÃ© et al. 2012) was to provide a detailed examination of why the LSMs diverge in their response to ALCC. For this purpose, the authors used seven atmosphereâland models with a common experimental design. For the vegetation distribution, each model used as a starting point the same distribution of crop and pasture, at a resolution of 0.5Â° Ã 0.5Â°, as extracted from Ramankutty and Foley (1999), combined with the pasture areas from Klein Goldewijk et al. (2011) (Fig. 25.8). However, as there are between-model differences in (i) the way land information was represented, (ii) sources of information to describe present-day and potential vegetation, and (iii) strategies to implement ALCC in the model, the resulting land-cover distribution (including natural vegetation) used in each model differed (de Noblet-DucoudrÃ© et al. 2012; Fig. 25.8). Although the areas covered by crops increased from AD 1870 to AD 1992 in all land-cover data sets used, the increase varied; all LSMs describe temperate deforestation, but at"
311,1577,0.985,The Physics of the B Factories,"third-generation quarks (Hou, Nagashima, and Soddu, 2005). In particular for B 0 â Î+ c â , which violates both lepton number and baryon number, the branching fraction is estimated to be less than 4 Ã 10â29 . Although this is far beyond any expected experimental sensitivity, searches have still been performed to the precision permitted by current data samples. BABAR has performed a search for the decays B 0 â + â Îc â , B â â Îââ , and the B â L violating mode B â â Îââ , where the lepton is a muon or an electron (del Amo Sanchez, 2011l). This is the first experimental search for these decays, and any positive signal would be evidence of new physics. B-meson candidates are formed by combining a Î+ c , Î or Î candidate with an identified muon or electron. The c candidates are reconstructed in the decay mode Îc â pK â Ï + , which has a branching fraction of about 5%. The Î candidates are reconstructed in the decay Î â pÏ â , which has a branching fraction of about 64%. The final state hadron (p, K, Ï) and lepton (Î¼, e) candidates are all required to be consistent with the candidate particle hypothesis according to PID criteria based on dE/dx , DIRC, EMC and IFR information. The 4-momenta of photons that are consistent with bremsstrahlung radiation from the electron candidate are added to that of the electron. â + invaric candidates are required to have pK Ï ant mass within Â±15 MeV/c2 of the nominal Î+ c mass. Similarly, Î candidates must have pÏ â mass within Â±4 MeV/c2 of the nominal Î mass. The final state tracks which form the decay daughters of the the Î+ c (Î) are constrained to a common spatial vertex, and their invariant mass is constrained to the Î+ c (Î) mass. This has the eï¬ect of improving the 4-momentum resolution for true B â Î(c) â candidates. The baryon and lepton candidates are also constrained to originate from a common vertex. As the Î has cÏ = 7.89 cm, the purity of the Îcandidate sample is further improved by selecting candidates for which the reconstructed decay point of the Î candidate is at least 0.2 cm from the reconstructed decay point of the B candidate in the plane perpendicular to the e+ eâ beams. Particle mis-ID backgrounds from e+ eâ â e+ eâ Î³ events in which the photon converts to an e+ eâ pair are eliminated by requiring that there are more than four tracks in the events. B-meson candidates are selected within the kinematic region |ÎE| < 0.2 GeV and 5.2 < mES < 5.3 GeV/c2 are fitted to extract the signal yield. The signal yield is extracted using an unbinned extended ML fit in which the total p.d.f. is a sum of p.d.f.s for signal and background. The signal and background p.d.f.s are each a product of p.d.f.s describing the dependence on mES and ÎE. For the modes, additional discriminating power is gained c â from a three dimensional p.d.f., where the output from a neural network discriminator is used as the third variable. No significant signal is observed for any of the decay modes, and branching fraction upper limits are determined, ranging from 5.2 Ã 10â6 to 3.2 Ã 10â8 at the 90% C.L. (see Table 17.11.2). Less stringent limits are ob-"
311,1407,0.985,The Physics of the B Factories,"where the errors are statistical and systematic, respectively. The photon spectrum is shown in Figure 17.9.6. The systematic error from hadronization already dominated with only 82 fbâ1 , and adding statistics did not reduce the overall error in the branching fraction with five times more data. However, this analysis is an important cross-check of the other inclusive analyses, where the systematic error is dominated by a diï¬erent source, the background from other B decays. And as can be seen in Figure 17.9.6, the spectrum measurement has been significantly improved with more data. A key point to note about this method is that it is the only one that distinguishes b â sÎ³ from b â dÎ³, and hence is the only method used to measure inclusive b â dÎ³ (see Section 17.9.4). The method also determines the flavor and charge of the b â sÎ³ decay, allowing measurements of direct CP and isospin asymmetries in inclusive b â sÎ³ decays. These asymmetry measurements are not expected to be sensitive to the hadronization of the Xs ."
70,884,0.985,Optics in Our Time,"longitudinal, and time coordinates of the photodetection event of DA or DB. Note, in the Glauber-Scully theory [25, 36], the quantum expectation and classical ensemble average are evaluated separately, which allows us to examining the two-photon interference picture before ensemble averaging. The ï¬eld at each space-time point is the result of a superposition among a large number of subï¬elds propagated from a large number of independent, randomly distributed and randomly radiating sub-sources of the entire chaotic-thermal source, ^ Ã°Ã Ã°~ Ïj , z j , t j Ã Â¼"
372,1441,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where te D 2""=!e is the period of the Earthâs rotation. Equation (12.28) describes the relative precision of delay and fringe-frequency measurements. In practice, measurements of delay are generally more accurate because of the noise imposed by the atmosphere. Measurements of fringe frequency are sensitive to the time derivative of atmospheric path length, and in a turbulent atmosphere, this derivative can be large, while the average path length is relatively constant. Note that fringefrequency and delay measurements are complementary. For example, with a VLBI system of known baseline and instrumental parameters, the position of a source can be found from a single observation using the delay and fringe frequency because these quantities constrain the source position in approximately orthogonal directions. The earliest analyses of fringe-frequency and delay measurements to determine source positions and baselines were made by Cohen and Shaffer (1971) and Hinteregger et al. (1972). The accuracy with which group delay can be used to measure a source position is proportional to the reciprocal of the bandwidth 1=(). Similarly, the accuracy with which phase can be used to measure a source position is proportional to the reciprocal of the observing frequency 1=). Since the proportionality constants are approximately the same, the relative accuracy of these techniques is )=(). This ratio of the observing frequency to the bandwidth, including effects of bandwidth synthesis, is commonly one to two orders of magnitude. On the other hand, the antenna spacings used in VLBI are one to two orders of magnitude greater than those used in connected-element systems. Thus, the accuracy of source positions estimated from group delay measurements with VLBI systems is comparable to the accuracy of those estimated from fringe phase measurements on connected-element systems having much shorter baselines. VLBI position measurements using phase referencing, as described below, are the most accurate of radio methods. The ultimate limitations on ground-based interferometry are imposed by the atmosphere. Dual-frequency-band measurements effectively remove ionospheric phase noise (see Sect. 14.1.3). The rms phase noise of the troposphere increases about as d5=6 , where d is the projected baseline length, for baselines shorter than a few kilometers [see Eq. (13.101) and Table 13.3]. In this regime, measurement"
65,44,0.985,Handbook of Ocean Wave Energy,"1. The PTO of a wave-activated body is the most efï¬cient when its motion is restricted to only one degree of freedom. Otherwise, the wave-activated body will always chose to move in the direction of the least resistance and thereby avoid PTO interaction. Furthermore, limiting its motions to one degree of freedom; â¢ Reduces the complexity of the PTO system and the possible amount of load cases. â¢ Optimises its efï¬ciency and facilitates its control as the exact motion of the wave activated body is known. 2. PTO systems for WECs are normally required to convert a slow oscillating movement combined with high forces (induced by the nature of the waves) to a fast rotation in one direction (required by an electrical motor). Thereby, there is a wide range of different types of PTO systems, which all present advantages and inconveniences in term of efï¬ciency, control, complexity and cost (see Chap. 8). Indicative values of efï¬ciencies for these different types of PTOs (from absorbed wave energy to generator) are [24] (Table 1.4). Note that other aspects of the PTO system can be as well of high importance, such as the ability to; Table 1.4 Overview of the indicative efï¬ciency for different PTO systems (see more on Chap. 8)"
30,68,0.985,Determinants of Financial Development,"found significant in every model. The regional dummy REGEAP is significant in all relevant models, showing that the East Asian and Pacific countries are positively associated with higher FD. While AREA is significant in Models 2 and 4, the standardized coefficient for it is rather small. For the policy variables, EXPPRIM is significant in Models 3 and 4, but not for Model 1. SDPI is significant in all relevant models, but the standardized coefficient for it is negligible. Three institutional variables, CIVLEG, KKM and PCI, are found to be significantly associated with FD in all relevant models. The effects of KKM and PCI on FD are very strong, as shown by the standardized coefficients in the lower section of the table: a one standard deviation change in KKM translates into a more than 0.5 standard deviation of the FD measure, and even stronger effects for PCI. In sum, on the one hand, the analyses above further confirm that institutions, policy and geography, taken as a group, jointly explain a substantial proportion of the variation in FD. On the other hand, the above analyses show that, in comparison to policy and geography, institutions could play a fundamental role in the process of financial development. When taken individually, at least CIVLEG, KKM, PCI, GDP90, POP90 and EURFRAC are found to have a significant influence on financial development. This finding explicitly suggests that, in addition to initial GDP and initial population, the legal origin22 and institutional quality are the most fundamental determinants of financial development in a country."
238,418,0.985,Nanoinformatics,"11.3.1 Oxygen Permeation Figure 11.2 shows the effect of the steady-state PO2 in the upper chamber on the oxygen permeability constants of non-doped and RE-doped samples [16, 17, 19, 23]. PO2 in the lower chamber was held constant at approximately 1 Pa. When a dÂµO is formed by the combination of PO2 less than 10â3 Pa and PO2 of ca. 1 Pa (low PO2 region), the oxygen permeability constants decreased with an increase in PO2 for all the samples. The oxygen permeability constants for the Hf-doped sample were comparable to those for the non-doped sample, whereas those for the Lu- and Y-doped samples were approximately one-third of those for the other samples. In the low PO2 region, all curve slopes corresponded to similar power constants of n = â1/6. For all the samples exposed to the low PO2 region, GB grooves were observed on both surfaces with a similar morphology to that formed by conventional thermal etching. The absence of GB ridges on the higher-PO2 (PO2(hi)) surface suggests that aluminum migration played a small role in oxygen permeation. Therefore, the power"
71,160,0.985,Advancing Culture of Living with Landslides: Volume 1 ISDR-ICL Sendai Partnerships 2015-2025 (Volume 1.0),"wedge-shaped form. This fact results in a higher difference between the experimental and the numerical LGWs. On the other hand, the numerical results of 2LCMFlow model are in a better agreement with the experimental measurements of test 104 with an unconï¬ned mass of sand. The computational error for this case is less than 5%. Comparison between the numerical results of LS3D and 2LCMFlow models in Fig. 9 demonstrates that with considering a rigid landslide the maximum generated wave is commonly overestimated. The maximum wave height of test 104, Hmax Â¼ apm Ã¾ anm Â¼ 0:11m, is overstimated by the LS3D model with a relative error of about 16% while the 2LCMFlow model predicted Hmax as 0.1094 m which has a relative error of around 0.51% with the equivalent experimental measurment. With comparing the numerical results of LS3D and 2LCMFlow models with the equivalent experimental measurements at higher times, the ability of each model in simulating the wave dispersion will be evaluated. Figure 8b reveals up to 8% time phase difference between the numerical results of LS3D and the experimental data of test 5. This time difference which is probably caused by the combination effects of numerical dispersion and depth-averaged modelling of real three-dimensional experiments gradually makes the numerical LGWs far from the experimental LGWs. The LGWs predicted by LS3D model disperse faster than the experimental LGWs. 2LCMFlow solves the non-dispersive Euler equations. As a result, it can be observed in Fig. 9 that the LGWs estimated by 2LCMFlow moves ahead the experimental LGWs of test 104. It means that the numerical LGWs are dispersing and descending slower than the equivalent experimental LGWs. Predicting the topographic changes of the bottom and observing the simultaneous interactions between water surface fluctuations and landslide deformations are among the advantages of considering the rheological behaviour of landslide in LGW modelling. Figure 8b shows the landslide proï¬les and the water surface elevations predicted by 2LCMFlow model 0.4 and 1.7 s after releasing the sliding mass. Landslide deposit is also illustrated in Fig. 8a. As it can be observed in Fig. 8b at t = 0.4 s, the impacts of the sliding mass within its intrusion into the water generate a series of wave crests which move towards the runup surface. Landslide moves and elongates along the sliding surface until it is completely beneath the water surface. Afterwards, as it is shown in Fig. 8b at t = 1.7 s, a wave trough starts to shape due to the motion of the submerged landslide. These interactions between the water body and the landslide stops when the sliding mass comes to rest and forms its ï¬nal deposit as shown in Fig. 8c."
391,375,0.985,Ocean-Atmosphere Interactions of Gases and Particles,"The diurnal warm layer enhances the flux of CO2 in regions where the ocean acts as a CO2 source and therefore reduces the oceanic carbon uptake (Olsen et al. 2004). An experiment was conducted off Marthaâs Vineyard in 2002 (McNeil et al. 2006) in which in situ profiles of temperature/salinity were acquired with a seabird microcat, and pCO2 with water pumped from depth to an equilibrator, where a sample of the headspace was analysed with a LI-COR LI-7000 IRGA. The profile data and subsequent pCO2/temperature relationship is shown in Fig. 2.8. There is a relatively strong diurnal warm layer, with a ÎT of about 0.6  C over the upper 4.5 m. There is a corresponding pCO2 gradient of about 12 Î¼atm and the best fit provides (âpCO2/âT)/pCO2 Â¼ 0.062  C 1, compared to the value from Takahashi et al. (1993) of 0.0423  C 1. For a global net flux of CO2 from the atmosphere to ocean, atmospheric pCO2 levels are about 2 % (approximately 6 Î¼atm) greater than the oceanic partial pressure for CO2, making measurements susceptible to small biases in the determination of the correct partial pressure difference (McGillis and Wanninkhof 2006). Therefore the diurnal warm layer can mask the true air-sea fluxes when calculated with measurements of ÎpCO2 when the water concentration is measured several meters below the sea surface. Further work is required to understand the dependency of the ocean carbon system on temperature so that the most appropriate value for the change in"
311,913,0.985,The Physics of the B Factories,17.3.5.5 W â cs Measured branching fractions for these modes are given in Tables 17.3.13 and 17.3.14. The measured branching fractions of the J/ÏK and J/ÏK â modes are compared in Table 17.3.1 with the predictions from the NS model. All of the J/ÏK measurements are higher than the predictions from the model while for the K â modes the situation is reversed. The mea-
165,422,0.985,New Methods for Measuring and Analyzing Segregation,"Statistical theory provides a different and potentially defensible rationale for case weighting when performing statistical analyses of variation in segregation across cities and communities. It is that the dependent variable (i.e., the index score) exhibits differential variability across cities. The relevant statistical issue is heteroskedasticity â a violation of the ordinary least squares (OLS) regression assumption that error variance is constant across cases. This issue is distinct and separate from index bias. Index bias is systematic with regard to the direction of its impact on index scores; biased cases have consistently inflated values for index scores. In contrast, heteroskedasticity does not involve bias; it involves greater volatility in scores around the model-predicted average and the volatility reflects scores that are below the predicted average as well as scores that are above the predicted average. When heteroskedasticity is present, estimates of means and regression coefficients are unbiased but significance tests in OLS regression may be questioned because the assumptions underlying the tests are not met. One strategy for dealing with heteroskedasticity in aggregate-level regressions is to perform weighted least squares (WLS) regression using case weights (w) that are proportional to the inverse of each caseâs expected error variance (Hanushek and Jackson 1977). Statistical theory indicates the appropriate weight (w) would be the reciprocal of the expected error variance of D. This can be calculated directly.9 But some might view absolute size of the minority population as a potentially acceptable proxy and defend weighting cases by population size on this count. This would perhaps be justified if variation in index scores was greater when minority population size is small. But empirical analysis suggests this is not the case. This is due to two reasons, one simple and one complex. I explored the issue by examining the empirical associations among three variables â the score for D, predisposition for bias measured by E[D], and minority population size â using the data set and measures introduced and described in the previous section. The simple D is the White-Black difference of proportions ( p1 - p2 ) residing in areas where proportion White for the area equals or exceeds that for the city as a whole. The expected variance ( s Â² ) of a difference of proportions is obtained by squaring the standard error of the difference of proportions â s ( p1 - p2 ) â given by p1q1 n1 + p2 q 2 n 2 or, alternatively, pq (1 n1 +1 n 2 ) if using the"
311,2895,0.985,The Physics of the B Factories,"The Bs0 decay branching fractions measured at the Î¥ (5S) are summarized in Table 23.4.1. The branching fractions of the corresponding B 0 decays are also shown for comparison. The results obtained demonstrate that the Bs0 and B 0 meson branching fractions are consistent within uncertainties. The Bs0 branching fractions with Î· and Î· â² mesons in the final state are expected to be about 1/3 of the corresponding B 0 branching fractions with a K 0 , because the Î· (â²) meson is assumed to be approximately one third ssÌ. The results indicate that the larger mass of the s-quark, compared with the mass of the d-quark, does not result in significant rearrangement of the intrinsic structure of the B meson, consisting of both heavy and light quarks. It should also be noted that Belle did not find any significant deviations of the experimentally measured Bs0 branching fractions from the corresponding theoretical predictions."
12,143,0.985,Self-Assembled Molecules â New Kind of Protein Ligands : Supramolecular Ligands,"The N-terminal fragment of the light chain V domain has been identified as highly unstable on the basis of experimental results [21]. This conclusion is supported by molecular dynamics simulations involving B-J proteins. Eliminating the N-terminal fragment results in a significant decrease in RD values, proving that the fragment contributes to destabilization of the domain and disrupts its hydrophobic core (Table 5.5). The disagreement between O and T distribution as observed in N-terminal fragment is visualized in Fig. 5.5. The status of observed hydrophobicity distribution of position 5 and fragment 11â14 can be even treated as opposite one versus the expected hydrophobicity. RD value for this fragment is equal to 0.604."
376,284,0.985,"Rangeland Systems : Processes, Management and Challenges","Landscapes consist of variable patterns and processes that are dynamic in space and time and lead to complexity that is an essential characteristic of rangelands. Spatial heterogeneity refers to how an ecosystem propertyânutrients, vegetation type, or amount of coverâvaries among points within the landscape. Temporal heterogeneity is similar but refers to variability at one point in space over time. When we consider heterogeneity we often consider spatial and temporal heterogeneity separately for statistical and logistical reasons, but in nature they are largely inseparable. For example, if temporal heterogeneity differs between two locations, the locations are also spatially heterogeneous (Kolasa and Rollo 1991). Furthermore, when patch types change positions within the landscapeâwhich often occurs at some spatial and temporal scale in nature because ecosystems are not staticâthen heterogeneity is changing over both space and time. A third scenario is the shifting mosaic, in which a specific set of patch types shift across space over time, such that the same type of patch occurs in each time step but never in the same space in consecutive time steps. In such cases, spatial heterogeneity within the landscape is conserved over time. Although the pattern of bison following burned areas of the pre-European North American Great Plains has become a model for the shifting mosaic, the phenomenon has been repeatedly shown to drive the conservation of patternâprocess relationships and the functioning of rangeland ecosystems (Fuhlendorf et al. 2012). Experimental and statistical norms limit our ability to understand landscapes that are highly dynamic in space and time and overcoming these norms is an important challenge to producing usable science on rangelands."
241,669,0.985,Second Assessment of Climate Change for the Baltic Sea Basin,"As RCMs can resolve mesoscale atmospheric features explicitly, they do add small-scale structures to the largescale circulation provided by the driving model (Feser 2006). This explicit treatment of small-scale atmospheric features leads, for many variables, to an AV with respect to the driving model. This is particularly the case for the simulation of precipitation, which also depends strongly on topography and landâsea contrast, which are better represented at the increased resolution of the RCM. Consequently, the simulated mean precipitation patterns as well as the extreme values are enhanced, especially for complex terrain (e.g. Christensen and Christensen 2001; Feldmann et al. 2008; Suklitsch et al. 2008). For the Baltic Sea region, Walther et al. (2013) demonstrated the improved simulation of the daily precipitation cycle for spring and summer with increasing RCM resolution; an example for a station in central southern Sweden is displayed in Fig. 10.10. Winterfeldt et al. (2011) analysed AV in dynamically downscaled wind speed ï¬elds. They used the Brier skill score (BSS) to detect the AV of the regionally modelled (with spectrally nudged-REMO) wind in comparison with the global reanalysis (NCEP). As seen in Fig. 10.11, the RCM provides AV along the coasts and in narrow bays and straits, in places with complex coastlines or topography. Over open seas and oceans, as well as the interior of Baltic Sea, the BSS is negative, indicating that in these regions, dynamical downscaling does not add value. Feser (2006) analysed AV in the case of SLP and 2 m air temperature provided by the REMO RCM in comparison with NCEP reanalysis data. Spatial ï¬lters were used to separate the data into two domains: that represented best at the large-scale and that represented well by the REMO model (Fig. 10.12). The effect of spectral nudging was also analysed. For SLP, no AV is provided by RCM simulation without nudging. The small improvement was obtained when spectral nudging was applied. For 2 m air temperature, Fig. 10.11 Brier skill score using QuikSCAT level 2B12 as the source of ground-truth data, global reanalysis (NCEP reanalysis) as the reference forecast, and a regional model (spectrally nudged-REMO) as the forecast, after Winterfeldt et al. (2011)"
311,3066,0.985,The Physics of the B Factories,"one could potentially also miss a signal of SUSY particles. In this section we follow a more model independent approach â the mass insertion approximation (MIA) (Hall, KosteleckyÌ, and Raby, 1986). In this approach the flavor oï¬-diagonal part of the squark mass matrix in the SuperCKM basis is parameterized by the mass insertion paramq eter, (Î´AB )ij , where A, B denote the chirality (L, R) and q indicates the (u, d) type. Assuming that the oï¬-diagonal elements are smaller than the diagonal ones, the sfermion propagator can be expanded as qÌAi qÌBj  = i(k 2 1 â mÌ2 1 â mÌ2 Î´AB"
311,2936,0.985,The Physics of the B Factories,"24.1.3.1 Collins fragmentation function The most prominent transverse spin dependent fragmentation function is the Collins fragmentation function, (z, Phâ¥ ), which Collins (1993) initially suggested as a possible way to explain the surprisingly large single spin asymmetries seen by the E704 experiment (Adams et al., 1991) in transversely polarized p p collisions. The Collins fragmentation function translates the transverse spin of a quark Sq with momentum k into the azimuthal yield of final state hadrons with transverse momentum Phâ¥ and fractional energy z. According to the Trento conventions (Bacchetta, DâAlesio, Diehl, and Miller, 2004) the overall number density for finding a hadron h (with mass Mh ) produced in the process q â â hX can be defined as: (kÌ Ã Phâ¥ ) Â· Sq (24.1.12) where the first term is the unpolarized fragmentation function described in Section 24.1.2.1 before integration of the transverse momentum. The second term contains the Collins function H1,q (z, Phâ¥ ) and correlates the spin orientation of the quark Sq , its momentum kÌ and the transverse momentum of the hadron Phâ¥ . Upon flipping the quark spin this product changes sign and thus generates a single spin asymmetry proportional to a cos Ï modulation of the azimuthal angle spanned by the vectors kÌ, Phâ¥ and Sq . In an unpolarized case the second term vanishes and the conventional, unpolarized fragmentation function definition is recovered. The existence of the spin dependent term was found by the HERMES experiment when single spin asymmetries attributed to a convolution of quark transversity and the Collins function turned out to be nonzero (Airapetian et al., 2005). However, in order to explicitly measure it, e+ eâ annihilation is used. A first study was performed on DELPHI data by Efremov, Smirnova, and Tkachev (1999), before the large amount of data accumulated by the B Factories became available. The method which is used in both B Factory experiments closely follows the prescription given by Boer (2009). The experimental selection requirements are also similar in the measurements Abe (2006a), Seidl (2008), and Garzia (2013). A product of Collins functions is accessed by selecting two charged pions in opposite hemispheres where the hemisphere is defined by the thrust axis (see Chapter 9). Experimentally this corresponds to the process e+ eâ â (Ï Â± )(Ï Â±,â )X where the brackets indicate the diï¬erent hemispheres and X is the remainder of the final state. To select two-jet like events a minimum thrust value of 0.8 is required with the thrust axis being well within the barrel acceptance (cos Î¸T < 0.75). Further selection criteria require the charged particles to be in the barrel part of the detector as well (for example, in Seidl, 2008, â0.6 < cos(Î¸lab ) < â 0.9 is used) with minimum fractional energies z = 2Eh / s > 0.2. The azimuthal asymmetries are observed in a cos(Ï1 + Ï2 ) modulation in the normalized two-hadron yields, R = N (Ï1 + Ï2 )/N12 , where Ï1,2 are the azimuthal angles defined in the CM by"
217,697,0.985,Finite Difference Computing With Pdes : a Modern Software Approach,"the disadvantage of the very simple Jacobi iteration method: the number of iterations increases with the number of unknowns, keeping the tolerance fixed, but the tolerance should also be lowered to avoid the iteration error to dominate the total error. A small adjustment of the Jacobi method, as described in Sect. 3.6.12, provides a better method."
165,220,0.985,New Methods for Measuring and Analyzing Segregation,"because the maximum departure of S from D occurs when one group is dispersed widely across areas where it is over-represented, thus resulting in small departures of pi from P in these areas. This is demographically more feasible when one group is small in comparison to the other and it is less feasible when groups are equal in size. Elsewhere I establish that the D â SMin relationship when groups are equal in size is S = DÂ² (Fossett 2015). This relationship is reflected in the curve that is closest to the diagonal. This curve documents that the absolute and relative magnitude of the possible D-S difference can be substantial even when it is at its minimum. The D-S difference when groups are equal in size reaches a maximum of 25 points when D is 50 and it is 20 points or more when D is in the range 28â72. In relative terms, the value of S can be up to 20 % lower than the value of D when D is 80; up to 30 % lower when D is 70; up to 40 % lower when D is 60; up to 50 % lower when D is 50; and so on. The D â SMin curves plotted at selected values of P depart further from the diagonal as the racial composition of the city becomes progressively more imbalanced. Since most White-Minority segregation comparisons in empirical studies involve groups that differ greatly in size, these curves are highly relevant. They document that potential D-S differences can be very large in both absolute and relative terms under combinations of D and P that are common in âreal worldâ settings. When P is 85, the D â SMin difference exceeds 25 when D is in the range of 30â93 and it exceeds 40 when D is in the range of 56â83. In relative terms, the value of S can be up to 50 % lower than the value D when D â¤ 82 and 70 % lower or more when D â¤ 58 . The potential D-S differences are even more dramatic when P is 95 or higher. For example, when P is 95, the D â SMin difference exceeds 25 when D is in the range of 28â98 and it exceeds 40 when D is in the range of 44â96. In relative terms, the value of S can be up to 50 % lower than the value D when D â¤ 94 and 70 % lower or more when D â¤ 84. Importantly, group size differentials of this magnitude are common in empirical studies of segregation in US cities. For example, they are typical of White-Asian comparisons in most cities and they are typical of White-Latino comparisons in the ânew destinationâ communities of the Midwest, South, and Northeast. The potential for D-S differences to be very large in these situations is clearly revealed in Fig. 7.4. The patterns seen here provide compelling evidence that the prevailing practice of examining only D in empirical studies of segregation should be reconsidered. The curves in the figure document that the level of group separation and area racial polarization as measured by S can vary widely across cities that are identical in terms of group displacement from even distribution (D) and relative group size (P). Figure 7.5 makes the same point but from the vantage point of the separation index (S) instead of the dissimilarity index (D). Here the diagonal depicts the values of D plotted by S when displacement from even distribution is maximally concentrated (SMax). The curves in the figure depict the values of D plotted by S when displacement from even distribution is maximally dispersed. The implication of these curves is straightforward. If one is interested in group separation as measured by S, D is an unreliable indicator because D can take very high values when groups are not residentially separated. This occurs when group displacement from even"
311,276,0.985,The Physics of the B Factories,"where Î³ and Î² are the boost parameters from the Î¥ (4S) frame to the lab frame and Î¸ and Î± = pâB c/mB c2 are the polar angle and boost factor of the B in the Î¥ (4S) frame. Since no tracks originate from the production vertex, the sensitivity to the decay time diï¬erence of the B mesons comes mainly through the diï¬erence in the z positions of the decay vertices. As the polar angles of the two B mesons are exactly opposite, the diï¬erence in the z positions can be expressed as z1 âz2 = Î³Î² 1 + Î±2 c(t1 ât2 )+Î³Î± cos Î¸c(t1 +t2 ). (6.5.3) If the small parameter Î± â 0.06 is ignored, one obtains the well known approximation Ît = Îz/Î³Î²c."
157,106,0.985,"The Economy as a Complex Spatial System : Macro, Meso and Micro Perspectives","and vary L as in Commendatore et al. (2017). Figure 1 summarizes the properties of the long-run equilibria. In this Figure we plotted the bifurcation curves L = BTBS , L = BTCP , L = TIS , L = FIA and L = P FBS in the (E, L)-parameter plane for the values of the other parameters as in (24) and (25). Crossing these curves the properties of the equilibria change. In Fig. 1, the blue region is related to the coexisting attracting equilibria IS and IAi , i = 1, 2, 3, while for the parameter values belonging to the yellow region the attracting equilibrium IS coexists with the attracting equilibria CPi . Note that the curve FIA is meaningful for E > E1 and that the curve BTCP intersects the curve TIS at E = 2b c =: E2 . We now study how the properties of the equilibria of the dynamic system Z changes for a fixed E < E2 , for example, for E = 1.5, by increasing L:10 â¢ For 1 < L < BTBS there are three coexisting attracting CP equilibria, and crossing L = BTBS (when a transverse âborder transcritical bifurcationâ of BSi , i = 1, 2, 3, occurs), the fixed points BSi become repelling and the saddle fixed points IAâ²i are born. Figure 2a (where the curves â¦1 and â¦2 given in (14) are also shown) depicts the case that holds for BTBS < L < TIS . In this Figure the basins of coexisting attracting CP equilibria â representing the set of initial conditions leading to a long-run equilibrium â are coloured red, blue and green (for CP0 , CP1 and CP2 , respectively). For the interval 1 < L < TIS , the size of invariant local demand, represented by the number of immobile workers (L), is âsmallâ, compared with the size of demand that potentially could shift, represented by the number of entrepreneurs (E). A small initial advantage of one region â i.e. an initial distribution of entrepreneurs in that"
153,64,0.985,Solving Pdes in Python : The Fenics Tutorial I,"In essence, these two lines specify the PDE to be solved. Note the very close correspondence between the Python syntax and the mathematical formulas âu Â· âv dx and f v dx. This is a key strength of FEniCS: the formulas in the variational formulation translate directly to very similar Python code, a feature that makes it easy to specify and solve complicated PDE problems. The language used to express weak forms is called UFL (Unified Form Language) [1, 26] and is an integral part of FEniCS. Expressing inner products The inner product â¦ âu Â· âv dx can be expressed in various ways in FEniCS. Above, we have used the notation dot(grad(u), grad(v))*dx. The dot product in FEniCS/UFL computes the sum (contraction) over the last index of the first factor and the first index of the second factor. In this case, both factors are tensors of rank one (vectors) and so the sum is just over the one single index of both âu and âv. To compute an inner product of matrices (with two indices), one must instead of dot use the function inner. For vectors, dot and inner are equivalent."
311,1516,0.985,The Physics of the B Factories,"Table 17.10.1. Summary table for the B + â Ï + Î½ analyses. The number of BB pairs in the data sample (NBB ), the signal yield (Nsig ), the signiï¬cance (Î£sig ), the detection eï¬ciency (Ç«sig ), and the branching ratio (B) are shown for each of the hadronictag and semileptonic-tag analyses. The combined results reported by Belle and BABAR for B are also shown, where the errors are the sum in quadrature of the statistical and systematic uncertainties. Experiment Belle"
233,612,0.985,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"In contrast, MPD is independent of species richness, and high values of MPD indicate the presence of distantly-related species co-occurring in a particular area. The balanced nature of the phylogenetic tree partly explains the low variation in Mean Phylogenetic Diversity (MPD) across Madagascar (Fig. 2c). This measure provides additional insight into the distribution of phylogenetic diversity, being of particular interest in those areas with low species richness and low values of PD. In this case, high values of MPD could indicate areas in which ecological convergence has occurred in separate lineages of Sarcolaenaceae. We note, however, the absence of areas that concurrently exhibit low species diversity and high values of PD, although some areas do have low species diversity and low PD but high MPD. The most important areas for conserving PD in Sarcolaenaceae are concentrated in the central-northern portion of Madagascar âs Eastern region, including and adjacent to the eastern edge of the Ankeniheny Zahamena Forest Corridor (Figs. 3 and 4b). However, as this area does not include any representatives of Xerochlamys and Mediusella, and because the distributions of these two genera do not overlap, the ideal strategy for protecting all lineages of Sarcolaenaceae and to maximize conservation of PD for this family, would be to include two additional protected areas: the Bongolava Forest Corridor in the northwest (Fig. 4g) and the Itremo Massif (Fig. 4e). Taken together, these three regions contain 84.9 % of the PD of Sarcolaenaceae. It is also of critical importance to consider preserving sites with high MPD values because (1) they harbor distantly related species that do not share the same evolutionary history, (2) might be impacted by different threats, and (3) require different conservation procedures. Buerki and colleagues (2015) recently suggested that the current distribution of MPD in endemic Malagasy legumes could be explained by a range of factors, such as the role of watersheds and dispersal corridors during past climatic changes, as well as by the evolutionary history of the groupâs most important dispersers, viz. extant and extinct lemurs. They conclude by advocating that a sound conservation plan should incorporate, in addition to the traditional biodiversity measures (species richness, PD and MPD), a detailed investigation of the biotic and abiotic factor that play (or have played) a role in the dynamics of each ecosystem. The trends observed in the PD of Sarcolaenaceae differ signiï¬cantly from those observed in Malagasy legumes by Buerki et al. (2015), where high values of species richness and PD are found in the subhumid highlands and lower values in humid eastern forests. However, the Bongolava Forest Corridor (Fig. 4g) and Midongy du Sud (Fig. 4c) are two sites where MPD values are high for Sarcolaenaceae that were regarded by WilmÃ© et al. (2006) and Buerki et al. (2015) as low- and high-elevation watersheds, respectively, and considered by them to represent potential refugia during the Quaternary climatic shifts. The list of important areas for conserving Sarcolaenaceae would thus also include the Bongolava Forest Corridor, Midongy du Sud, along with Makira and Masoala in the northeast and the eco-geographically diverse Behara-Tranomaro-Andohahela-Tsotongambarika area in the southeast, which spans a sharp ecotone from humid forest in the east to subarid ticket in the west (Fig. 4)."
253,702,0.985,"Autonomous Driving : Technical, Legal and Social Aspects","20.3.2 State Uncertainty The state uncertainty of a detected object is described, in accordance with Bayesâ theorem, by means of a probability density function which can be used to determine the most probable total and individual state and also, with a certain probability, possible variations from this. In the case of a multi-dimensional, normally distributed probability density function, the state uncertainty is completely represented by a covariance matrix. In estimating static variables, such as the vehicle dimensions, their state uncertainty can be reduced progressively by means of repeated measurements. The estimated value based on the available measurements converges with the true values, as long as there is no systematic sensor error, e.g. in the form of an offset. For the estimation of dynamic, time-changeable states such as the object position or the object speed, due to the movement of the object between the measuring times, there is no convergence with a true value. Therefore, for the evaluation of the quality of the state estimation, it is stipulated that the mean error is zero and the uncertainty as low as possible."
32,212,0.985,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","Therefore the robustness of the entire system is a function of mE, not the bare E. And because NE D 1 means that the average number of extinctions balances with the number of inclusions in the long time average, that corresponds to the transition point of the growth behavior. In other words, the following self-consistent condition should be satisfied for the critical number of interactions per species: m E.m / D 1. Let us now focus on the relevant parameter in the argument p above, mE. We find that the decrease of E is slower than 1=m (roughly  1= m). Therefore mE is a sub-linearly increasing function of m, and it crosses the critical value 1 around m D 13. This means that the mean-filed treatment can explain the transition in the growth behavior of the system. In addition, this theory give us the simple understanding of the transition mechanism. It originates from the balance of the two effects: although having more interactions makes each species robust against the disturbances (addition and extinction of the species relating to that species), it also increases the impact of the loss of a species. In consistent with this success in explaining the transition by the mean-field analysis, we can find essentially same phase diagram in slightly modified models, such as the model with giving a randomly distributed degrees for the newly added species, the one with different distribution functions for the link weights, and so on [11]. In the classical diversity-stability relation based on the linear stability of dynamical systems, an intrinsic stability is assumed for each element to ensure the stability of each element when that has no interactions. For the system to remain stable, each element may have essentially only one interaction that is not weak comparing to the given intrinsic stability [2]. In the present mechanism, we do not assume any kind of intrinsic stability to the elements: an element with no interaction immediately goes extinct. Even so, the system with 10 or more interactions per element can grow. In this sense, the condition we have identified is very realistic. Indeed in the real systems, it is quite often to find moderately sparse networks: the average degree is in the order of 10, not order of 1, and that seems not dependent on the system size. This novel relation between the connection in the system and its robustness might be a origin of this."
281,625,0.985,Stochastics of Environmental and Financial Economics (Volume 138.0),"Markov processes for which the logarithm of the characteristic function of the process is affine with respect to the initial state. Affine processes on the canonical state space m Ã Rn have been investigated in [3, 5, 13, 14]. Based on the exponential-affine structure of the JCIR, we are able to compute its characteristic function explicitly. Moreover, this enables us to represent the distribution of the JCIR as the convolution of two distributions. The first distribution is known and coincides with the distribution of the CIR model. However, the second distribution is more complicated. We will give a sufficient condition such that the second distribution is singular at the point 0. In this way we derive a lower bound estimate of the transition densities of the JCIR. The other problem we consider in this paper is the exponential ergodicity of the JCIR. According to the main results of [10] (see also [12]), the JCIR has a unique invariant probability measure Ï , given that some integrability condition on the LÃ©vy measure of (Jt , t â¥ 0) is satisfied. Under some sharper assumptions we show in this paper that the convergence of the law of the JCIR process to its invariant probability measure under the total variation norm is exponentially fast, which is called the exponential ergodicity. Our method is the same as in [9], namely we show the existence of a Forster-Lyapunov function and then apply the general framework of [16â18] to get the exponential ergodicity. The remainder of this paper is organized as follows. In Sect. 2 we collect some key facts on the JCIR and in particular derive its characteristic function. In Sect. 3 we study the characteristic function of the JCIR and prove a lower bound of its transition densities. In Sect. 4 we show the existence of a Forster-Lyapunov function and the exponential ergodicity for the JCIR."
165,202,0.985,New Methods for Measuring and Analyzing Segregation,"Stated another way, S will take higher values when the population residing in nonparity areas is concentrated to form racially polarized areas and S will take lower values when the population residing in non-parity areas is dispersed widely to form areas that are similar on racial composition instead of being polarized. The practical consequence for D-S comparisons is this. At a given level of displacement as measured by the dissimilarity index (D), the value of the separation index (S) can vary independently and by substantial amounts depending on whether group distributions both between âabove-parityâ areas and âotherâ areas and within ânon-parityâ areas tend toward maximum area racial polarization or minimum area racial polarization. The former concentrates both groups in homogeneous areas and maximizes same-group contact and group separation. The latter disperses both groups across less homogeneous areas and minimizes same-group contact and group separation. Ultimately, as I show below, this leads to the following conclusion about the relationship between D and S. At a given level of displacement (D), the value of the separation index (S) can vary substantially depending on whether group distributions within ânon-parityâ areas tend toward concentration or dispersion. When concentration within non-parity areas is at its maximum, the value of S will equal the value of D. But when concentration is at its minimum â that is, when groups are maximally dispersed across non-parity areas, the value of S will be lower, sometimes much lower, than the value of D. Intuitively, one can get to these two alternative outcomes via simple steps as follows. At a given level of displacement, implement as many segregation-promoting exchanges as possible within non-parity areas. If such exchanges can be made, group residential distributions will shift toward the pattern of âprototypical segregationâ and the value of S will increase. The value of D will not change so the D-S disparity will decrease. Ultimately, the value of S will rise until it reaches the value of D and D-S disparity will be zero. Alternatively, implement as many integration-promoting exchanges as possible within non-parity areas. If such exchanges can be made, group residential distributions will shift toward the pattern of âdispersed displacementâ and the value of S will decrease. The value of D will not change so D-S disparity will increase. Ultimately, S will fall until it reaches its minimum possible level and the D-S disparity reaches its maximum. At the conclusion of the process, S will take a value substantially below the value of D."
32,208,0.985,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","Where G.; x/ denotes the normal distribution with its deviation . After settling to the system, the species will experience either obtaining a new link from a newly introduced species or loosing a link during the extinction of interacting species. Those processes change the fitness of the species, and hence the distribution function. This change in FDF is found to be the one step of random walk with negative drift whose strength is proportional to 1=m. Therefore, writing this process by an operator DO , the (not normalized) FDF of species those have been experienced a loss or addition of one incoming link can be calculated from F0 as, O 0 .m; x/; F1 .m; x/ D EO DF"
255,371,0.985,Railway Ecology,"Landscape Graph Construction The nodes of the graphs corresponding to the habitat patches were deï¬ned as the aquatic habitat units (breeding ponds) adjacent to an area of potential terrestrial habitat. The quality of the habitat patches, i.e. capacity, was deï¬ned as the amount of terrestrial habitat and suitable elements around ponds. The links between nodes were deï¬ned in cost distance because the ability of the tree frog to disperse depends greatly on the surrounding matrix. The ecological literature and expert opinions were thus used to classify each landscape category according to its resistance to movement (Table 13.1). Radio tracking experiments (Pellet et al. 2004; Vos and Stumpel 1996) have concluded that wooded grasslands and linear elements like hedgerows or forest edges facilitate movement and often provided the speciesâ terrestrial habitat. Rivers and wetlands are less favorable because their permeability depends on the density of vegetation. Conversely, grasslands, roads and railways tend to constrain movement. Finally, the cores of forest patches, bare ground, buildings and motorways are considered highly impassable and are mostly avoided by the tree frog. Clauzel et al. (2013) performed several tests by varying the cost values and the number of classes to ï¬nd the model that best explained the occurrence of the tree frog in the Franche-ComtÃ© region. The results showed that highly contrasting values between favorable and unfavorable landscape categories were the most relevant. Consequently, in the present study (Table 13.1), aquatic and terrestrial habitats were assigned a cost of 1. Suitable elements such as wetlands or wooded grasslands were assigned a low cost (10), unfavorable landscape elements a cost of 100, and highly unfavorable elements a high cost (1000). In the second landscape map including the HSR line, a cost of 10,000 was assigned to the infrastructure so as to remove any links crossing it, i.e. considering it as an absolute barrier. From the two landscape maps, without and with the HSR line, two graphs were built and thresholded at a distance of 2500 m corresponding approximately to the dispersal distance for the tree frog. This distance was selected in line with the results by Clauzel et al. (2013), where several maximum distances were tested. The model using the distance of 2500 m proved the most relevant. Consequently, only links shorter than this distance were kept."
241,1482,0.985,Second Assessment of Climate Change for the Baltic Sea Basin,"assimilation techniques. Global reanalyses suffer from changing coverage and biases in the observing systems Recruitment The number of new juvenile ï¬sh reaching a size/age where they represent a viable target for ï¬shery for a given species Redï¬eld ratio Atomic ratio of carbon, nitrogen and phosphorus found in phytoplankton and throughout the deep oceans. This empirically developed stoichiometric ratio is found to be C:N:P = 106:16:1. It is of great importance for biogeochemical and ecological questions Redox (Bio-) Chemical reaction by which one partner acts as electron donor (oxidation) and another as electron acceptor (reduction) Redoxcline Water layer where the highest activity of biogeochemical redox reactions is observed Remote sensing Environmental surveying by satellite sensors from the Earthâs orbit Resilience Capacity of a system to respond to a perturbation or disturbance by resisting damage and recovering quickly Resolution In climate models, the physical distance (metres or degrees) between each point on the grid used to compute the equations. Temporal resolution refers to the time step or time elapsed between each computation of the equations Return value The highest (or alternatively lowest) value of a given variable, on average occurring once in a given period of time (e.g. in 10 years return period) Rhizodeposition Release of plant material or exudates by the root to the root-surrounding soil (rhizosphere) acts as a food source for a ecological community of microorganisms Rossby waves Subset of inertial waves in the atmosphere and oceans that largely owe their properties to EarthÂ´s rotation Rotifers Taxonomic group of microscopic zooplankton (wheel animals) Runoff Part of precipitation that does not evaporate and is not transpired but flows through the ground or over the ground surface and returns to bodies of water. Describes general, long-term and/or regional processes and is typically given as litres per second per square kilometre (l sâ1 kmâ2), allowing comparisons between rivers of different sizes, or millimetres per year (mm yrâ1), allowing comparisons with precipitation and evaporation Satellite altimetry Sea surface height measured by satellite sensors"
311,671,0.985,The Physics of the B Factories,"The physics parameters ÏB 0 and Îmd are required inputs for time-dependent measurements. During the ML ï¬tting procedure used to extract S and C from data, the B 0 lifetime and mixing frequency are ï¬xed to their nominal values. The uncertainty on the measured values of ÏB 0 and Îmd are propagated through the ï¬tting procedure, assuming that they are uncorrelated, and the resulting variation of S and C from the nominal ï¬tted values is assigned as an uncertainty. This source of uncertainty is found to be at most a few per-mille."
311,1818,0.985,The Physics of the B Factories,"where Etr is the total energy of the transition particles in the parent resonance rest frame and m2tr is the total invariant mass-squared of the transition particles. In many instances, the parent resonance is produced at rest in the center-of-mass (CM) frame of the collider, and when this is true, the above calculation holds exactly. When this is not true, the calculated recoil mass using the transition particlesâ four-momentum in the CM frame will be shifted due to the presence, for instance, of another intermediate resonance before the transition particles are produced. For instance, if the transition particles are produced in the second of two transitions, e.g. M1 â (undetected)+M2 followed by M2 â (trans)+M3 , then Lorentz boosting into the CM frame - the frame of the original parent resonance - will yield a shifted recoil mass. Most often, sequential transitions in which a shifted recoil mass is observed are transitions in which each of states 1, 2 and 3 is a bottomonium state, and as such the emitted transition particles have small masses and momenta. In such cases transition particle energies and momenta are small compared to the masses M1 , M2 , and M3 . In these cases the amount of the shift is nearly equal to M1 â M2 . When one calculates the recoil mass assuming the transition particles, which were emitted in the second transition, were emitted from the initial state, then the shifted recoil mass obtained is given by Mshifted = (P (M1 ) â P (tr))2 ."
372,307,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where !e D dH=dt D 7:29115 "" 10!15 rad s!1 D !e is the rotation velocity of the Earth with respect to the fixed stars: for greater accuracy, see Seidelmann (1992). The sign of dw=dt indicates whether the phase is increasing or decreasing with time. The result shown above applies to the case in which the signals suffer no time-varying instrumental phase changes between the antennas and the correlator inputs. In an array in which the antennas track a source, time delays to compensate for the space path differences w are applied to maintain correlation of the signals. If an exact compensating delay were introduced in the radio frequency section of the receivers, the relative phases of the signals at the correlator input would remain constant, and the correlator output would show no fringes. However, except in some low-frequency systems like LOFAR (de Vos et al. 2009), the compensating delays are usually introduced at an intermediate frequency, of which the band center ""d is much less than the observing frequency ""0 . The adjustment of the compensating delay introduces a rate of phase change 2#""d .dw=dt/=""0 D !!e u.cos Ä±/""d =""0 : The resulting fringe frequency at the correlator output is D !!e u cos Ä± 1 # ""f D"
365,1002,0.985,Climate Smart Agriculture : Building Resilience To Climate Change,"We control for ethnic group due to higher rates of poverty that are expected to affect the adoption of new technologies. Kinh households (the dominant ethnic group in Nam) represent only 17% of our sample, Thai minority is 54% (mostly located in Dien Bien), and Hâmong is 13% (mostly located in Yen Bai). In terms of institutions, 43% of households in the sample have received advice on MT and 72% on FDP and/or SIP. Seventy-four per cent of households have a member that belongs to a farmer union. Only 3% of the households received any support for fertilizers; 14% received seed support (more than 1/3 in Dien Bien); and 43% had access to formal credit, with the highest concentration in Dien Bien (73%). Also, distribution of wealth indicators differ across provinces. Dien Bien has the highest percentage of households with income sources other than agriculture and livestock measured by Tropical Livestock Units (TLU), whereas Son La has the highest asset index.17 Table 7 reports the results of the analysis on the determinants of adoption of MT in maize systems (columns A and B), and FDP and SIP in rice systems (columns C to F), using probit specifications as per Eq. (11). We estimate two different specifications for each model: one includes the long-term coefficients of variation (LT CV) of climatic variables (columns A, C and D), and the other includes also long-term averages (LT AVG) (columns B, E and F). These variables capture the potential impact of long-term average values of climatic variables that cannot be obtained from the standardized value of variation using CV. Results from columns A and B suggest: (i) households that operate plots on higher slopes are significantly more likely to adopt MT; (ii) none of the household socio-economic characteristics significantly affects adoption, suggesting adoption is very much driven by agronomic indicators; (iii) extension advice is significantly and positively correlated with higher probability of adoption as expected; (iv) a positive relation between the share of households adopting MT and the relative diffusion of MT in the same communes is a sign of positive spillovers of effective adoption; (v) access to formal credit significantly and positively affects adoption, which is especially important for ethnic minorities with limited access to credit (and extension) compared to the Kinh majority (Do and Nguyen 2015); and (vi) having received support for improved seeds is negatively associated with the probability of adoption of MT. Controlling for long-term averages in rainfall shocks that matter for maize (column B), we find that the probability of adoption is significantly lower in places where the variation in rainfall during the first 10 days of maize season is higher. On the other hand, the probability of adoption is significantly higher where the long-term variation in rainfall during the flowering season is higher, indicating that farmersâ incentives to adopt MT are more sensitive to long-term variation in rainfall when excessive rain can damage the crop and could be particularly problematic in high slopes."
151,190,0.985,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"Where, Prob (Simx) is the area weighted probability associated with simulation âxâ, Ax is the area associated with simulation âxâ that resides within the ERF, and âi=1,n (Ai) is the area sum of the areas of all simulations that reside within the ERF. The results of soil and foliar executions of NPAT are shown in Figs. 5 and 6, respectively. These figures present the combined runoff and erosion losses of CPY expressed as the percent of applied active ingredient at the ERF resolution. Similar patterns can be seen in Figs. 5 and 6 with the greatest losses occurring in the high summer rainfall regions on relatively heavy soils in the U.S., such as Mississippi Delta regions of Louisiana, Mississippi, and Arkansas. Relatively large losses are also concentrated in northern Missouri and bordering areas. This latter region of"
256,430,0.985,"Supercomputing Frontiers : 4Th Asian Conference, Scfa 2018, Singapore, March 26-29, 2018, Proceedings","The tunable parameter in our proposed I/O framework that has the greatest impact on performance is the patch size used for restructuring. The patch size affects both the degree of network traffic and the total number of outputted files. To understand the impact of the parameter, we wrote a micro-benchmark"
297,567,0.985,The R Book,"where IQR is the interquartile range and n is the replication per sample. Notches are based on assumptions of asymptotic normality of the median and roughly equal sample sizes for the two medians being compared, and are said to be rather insensitive to the underlying distributions of the samples. The idea is to give roughly a 95% conï¬dence interval for the difference in two medians, but the theory behind this is somewhat vague. Here are the Silwood Weather data (above) with the option notches=TRUE:"
273,350,0.985,Report on Global Environmental Competitiveness (2013),"Table 9.1 lists the evaluation results of the subordinate indicators of EEC and displays the scores and rankings of 3 pillars and 11 individual indicators of EEC in 2012 so as to analyze the influences of the pillars and individual indicators on EEC of the countries. On pillars, ecological safeguard enjoys very high standard deviation, hitting 20.1, indicating that this indicator has a large difference among the countries and is the most primary factor causing EEC differences among the countries. The indicator of air quality also has relatively high standard deviation. The indicator of biodiversity has a low standard deviation, only 4.9, contributing little to EEC differences among the countries. Overall, the countries have large differences on the overall levels of EEC. Such differences are mainly caused by the differences of ecological safeguard and air quality, while biodiversity has very little influence. Hereafter, all"
165,72,0.985,New Methods for Measuring and Analyzing Segregation,"yi is the residential outcome score (y) for the iâth area scored as a function of the pairwise area group proportion y i = f ( p i ) , y1k indicates the residential outcome (y) for the kâth individual in Group 1 (set equal to the residential outcome score for the area in which the individual resides), and y2k indicates the residential outcome (y) for the kâth individual in Group 2 (set equal to the residential outcome score for the area in which the individual resides). I hold that formulating segregation indices in this way is useful for both conceptual and practical reasons. First, it provides a new interpretation for aggregate segregation indices; they now can be understood as registering simple group differences on residential outcomes (y) scored based on area group proportion (p) which has an easy, straightforward interpretation as (pairwise) contact with or exposure to Group 1 (i.e., the reference group) based on co-residence. Simple co-residence, of course, does not necessarily imply harmonious social interaction. But it does indicate common fate regarding many neighborhood outcomes and many shared residential experiences. On this basis, it is a potentially important and meaningful social indicator. Second, this new approach to computing index values places different indices in a uniform, common computing framework that highlights differences between measures on a single, specific point of comparison â the manner in which each index registers neighborhood residential outcomes (y) based on area group proportion (p). Since area group proportion can be understood as contact or exposure based on coresidence with Group 1, all of the indices can be interpreted as group differences in average âscaled contactâ with Group 1. Differences between indices ultimately trace to differences in the specific way that residential outcomes (y) are quantitatively scored based on area group proportion (p). Consequently, differences between indices can be seen as arising solely from differences in the index-specific form of the scaling function y = f ( p ) . This provides a new basis for evaluating segregation indices; they can be compared on the substantive relevance of how each index registers residential outcomes (y) based on contact and exposure with Group 1 as embodied by area group proportion (p). Third, the segregation-relevant residential outcomes (y) used to compute the segregation index score can directly serve as dependent variables in individual-level residential attainment analyses. Thus, in the difference of means formulation, the segregation index score can be equated to the effect of group membership (e.g., coded 0 or 1) in an individual-level residential attainment analysis for the city. This carries minimal practical value for specific task of estimating index scores because the scores can be readily obtained by simpler methods. But it is important because it expands options for understanding and analyzing segregation. It unifies the study of aggregate segregation with the study of residential attainment in a single framework. In doing so it opens the door to a host of new options for segregation analysis including, for example, the ability to easily take account of the role that factors other than group membership (e.g., income) may play in determining segregation and the ability to use multi-level models of residential attainment to study cross-area and cross-time variation in segregation."
311,591,0.985,The Physics of the B Factories,"The parameters a and r play the role of a scattering length and eï¬ective interaction length, respectively, and B (ÏB ) and R (ÏR ) are the magnitudes (phases) for the nonresonant and resonant terms. M and Î (s) are the mass and mass-dependent width, see Eq. (13.2.4), of the K0â (1430) resonance. This parametrization in fact corresponds to a K-matrix approach describing a rapid phase shift coming from the resonant term and a slowly rising phase shift governed by the nonresonant term, with relative strengths R"
71,228,0.985,Advancing Culture of Living with Landslides: Volume 1 ISDR-ICL Sendai Partnerships 2015-2025 (Volume 1.0),"Fragmentation in Rockfalls It is assumed that the detached mass may break up on impact (Cruden and Varnes 1996; Hungr et al. 2014), however little attention has been paid to rockfall fragmentation. Rockfalls may involve a rock mass including discontinuities, which usually disintegrates along the path. The rockfall fragmentation is the process by which the detached mass loses its integrity while falling from a steep slope and breaks up into smaller pieces. Normally, this occurs during the ï¬rst impacts on the ground (Wang and Tonon 2010). The fragmentation (Figs. 9 and 10) may consist of the separation of the rock blocks existing in the detached rock mass bounded by discontinuities (disaggregation), the breakage of the rock blocks during the impacts, or both (Ruiz-Carulla et al. 2015b). Fragmentation invariably leads to a reduction of the particle size. The importance of the rockfall fragmentation in risk analysis has been discussed by Corominas et al. (2012). The deï¬nition of the initial volume of the rockfalls is a basic input parameter for trajectographic analysis. Rock breakage reduce the kinetic energy of the individual particles. Analyses performed with the volume of a non-fragmented rock mass produce results signiï¬cantly different from the obtained if the resultant rock fragments are used instead (Okura et al. 2000; Dorren 2003). Working with the initial volume of a non-fragmented rock mass leads to the overestimation of the kinetic energy and the reach. Large blocks follow straight paths and display farther stopping points than the small ones. These effects may change signiï¬cantly the way rockfalls interact with the terrain and affect the probability of impact on exposed elements, their vulnerability and the design of the protective elements (Volkwein et al. 2011)."
311,723,0.985,The Physics of the B Factories,"eï¬ects. An exception are the most recent determinations of mc and mb from lattice QCD, discussed below, which use a ï¬ne lattice in combination with a highly improved lattice action such that heavy quarks with masses almost up to mb can be treated with a light-quark formalism. A complementary method is based on LCSR which use hadronic dispersion relations to approximate the form factor in terms of quark-current correlators and can be calculated in an operator product expansion (OPE). In inclusive semileptonic decays, one considers the sum over all possible ï¬nal states X that are kinematically allowed. Employing parton-hadron duality one can replace the sum over hadronic ï¬nal states with a sum over partonic ï¬nal states. This eliminates any long-distance sensitivity to the ï¬nal state, while the short-distance QCD corrections, which appear at the typical scale Î¼ â¼ mb of the decay, can be computed in perturbation theory in terms of the strong coupling constant Î±S (mb ) â¼ 0.2. The remaining long-distance corrections related to the initial B meson can be expanded in powers of ÎQCD /mb â¼ 0.1, where ÎQCD is the hadronic scale of order mB â mb â¼ 0.5 GeV. This is called heavy quark expansion (HQE), and it systematically expresses the decay rate in terms of nonperturbative parameters that describe universal properties of the B meson. This is discussed in Sections 17.1.3 and 17.1.5."
32,52,0.985,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","In Fig. 2.4 it is shown that the average TE across all equities is quite stable except for the drop occurring at the time of the change in minimum price increments on the 24th June. A simple shuffling test [14] estimates the TE for unrelated data to be approximately 0.02 nits on average (see the dashed lines, randomly sampled before and after the drop on day 61) but note that numerical estimations of TE are difficult so the TE sometimes drop below zero. This suggests that on average the TE across the DJIA is close to negligible but that some equities clearly have TE values significantly exceeding the 0.02 nits level, as shown by the blue line values. The largest peak in the Maximum TE plot occurs 6 days after the Dow crashes and is from the Disney equity to the McDonalds equity. Finally in Fig. 2.5 is plotted two networks of relationships between the equities based on Pearson correlations and TE. The Pearson correlation network is ordered counterclockwise according to the total link weight of each equity and a link was included if its correlation was greater than 0.4. The TE network is ordered counterclockwise by total link weight, the colour represents the total weight of incoming links and the node size represents the total weight of outgoing links and a link was included if its TE was greater than 0.05 nits. Thresholds were chosen such that 10 % of all links in each network are included. The most notable differences between these networks is the changes in the relative importance of the individual equities. The overall DJIA index (DJI) is significantly correlated with other equities whereas this index is the least significant node in the TE network. Similarly, Walmart (WMT) is very well connected in the TE network but it is the least relevant node in the Pearson correlation network. These are preliminary results using the comparatively small dataset of the 30 equities that make up the DJIA and will need to be confirmed on other indices and other crashes. There is one very significant point that comes out of this study: The driver of correlations between equities in financial markets is not necessarily the changes in the prices of other equities. This is true in the sense that changes in"
175,847,0.985,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","selected, knowing that there is some probability that in reality, or in a simulation model, the actual flow may be less than the selected value q. Hence if the constraint x â¤ q is binding (x = q), the actual allocation may be less than the value of the allocation or diversion variable x produced by the optimization model. If the value of x affects one of the systemâs performance indicators, e.g., the net beneï¬ts, B (x), to the user, a more accurate estimate of the userâs net beneï¬ts will be obtained from considering a range of possible allocations x, depending on the range of possible values of the random flow Q. One way to do this is to divide the known probability distribution of flows q into discrete ranges, i, each range having a known probability PQi. Designate a discrete flow qi for each range. Associated with each known flow qi is an unknown allocation xi. Now the deterministic constraint x â¤ q can be replaced with the set of constraints xi â¤ qi and the term B(x) in the original objective function can be replaced by its expected value, i POi BÃ°xi Ã. Note, when dividing a continuous known probability distribution into discrete ranges, the discrete flows qi selected to represent each range i having a given probability PQi, should be selected so as to maintain at least the mean and variance of that known distribution as deï¬ned by Eqs. 7.5 and 7.6. To illustrate this consider a slightly more involved example involving the allocation of water to consumers upstream and downstream of a reservoir. Both the policies for allocating water to each user and the reservoir release policy are to be determined. This example problem is shown in Fig. 7.10. If the allocation of water to each user is to be based on a common objective, such as the minimization of the total sum, over time, of squared deviations from prespeciï¬ed target allocations, each allocation in each time period will depend in part on the reservoir storage volume. Consider ï¬rst a deterministic model of the above problem, assuming known flows Qt and upstream and downstream allocation targets UTt and DTt in each of T within-year periods t in a"
241,657,0.985,Second Assessment of Climate Change for the Baltic Sea Basin,"et al. 2007). For instance, in winter, model results tend to be too wet in northern Europe, too warm in summer and winter, and too cold in spring and autumn (Jacob et al. 2007). On the one hand, RCMs generally overestimate the number of wet days; this âdrizzle effectâ is partly because RCMs simulate area averages rather than point values. While on the other, RCMs underestimate heavy precipitation events (e.g. Fowler et al. 2007b). Generally, bias is different in different part of the distribution (e.g. Jeong et al. 2011; Fig. 10.6). A major advantage of RCMs is the simulation of spatially coherent ï¬elds. In general, RCMs with a typical resolution of 25 km overestimate the spatial coherence of precipitation events, in particular for convective precipitation. It should be noted that RCMs provide meaningful information only on the scale of a few grid cells (e.g. Fowler and EkstrÃ¶m 2009). In particular, local precipitation is dominated by internal climate variability (Maraun 2012). As RCMs integrate the equations governing the atmospheric circulation, they in principle provide a coherent picture. However, biases in one variable may propagate into strong biases in dependant variables (e.g. Fig. 10.7); for example, Yang et al. (2010) have shown for Sweden that small temperature biases may, via the nonlinear interaction with precipitation around the melting point, lead to large biases in spring river run-off. Inconsistencies arise in particular where parameterisations come into play. For example, Graham et al. (2007b) have shown for the drainage areas"
311,665,0.985,The Physics of the B Factories,"15.2.1 External input In many analyses, the physics observables are extracted from a ï¬t with some of the parameters ï¬xed to values based on external information. Using external information is necessary if for example the statistical power of the selected sample under consideration is not large enough to determine all relevant parameters with suï¬cient accuracy. For instance, in rare B decay searches the peak positions and resolutions of mES and ÎE of signal events are often ï¬xed; in Dalitz plot analyses the masses and natural widths of intermediate resonances are ï¬xed to their PDG values; the mixing parameter Îmd and the B 0 meson lifetime are not allowed to vary in ï¬ts for time-dependent CP asymmetries. The systematic uncertainties that arise from using external input parameters are obtained by checking the deviations in the ï¬tted values after varying the external parameters according to their uncertainties. Unlike the PDG values used as the external parameters, some of the p.d.f. parameters explicitly depend on the detector resolution, and the corresponding uncertainties are determined using data. For instance, the uncertainty of mES is dominated by the beam energy spread and the mES peak position and resolution are determined using high-statistics control samples such as B â D0 Ï and D0 â K + Ï â (Ï 0 ) for decay modes without (with) photons in the ï¬nal state. The corrections between data and simulation and their uncertainties are obtained from these control samples and applied to the decay modes of interest. The same procedure is applied to estimate the correction and uncertainty for the ÎE p.d.f. parameters obtained in simulation. It is preferred to choose a control decay mode with high statistics that has the same numbers of charged and neutral particles in the ï¬nal state as the mode under study. The same consideration can be applied to estimate systematic uncertainties related to ï¬avor tagging, vertexing, mass resolutions and other external parameters. Most analyses also rely on external input to derive the quantity of interest from directly measured quantities. Examples of such external parameters are the integrated luminosity (or, alternatively, the number of BB pairs produced), branching fractions of daughter decays, particle masses and their lifetimes, etc. These quantities and their uncertainties are typically taken from averages calculated by the Particle Data Group, with the exception of the luminosities, which are measured by the B Factories (see Sections 3.2.1 and 3.6.2). At both experiments, the precision of the luminosity measurement is limited by systematic uncertainties, mainly by uncertainties of the Monte Carlo generator(s) used to calculate the cross-sections of the physics processes used to measure luminosity. At Belle, the luminosity is measured using Bhabha scattering to a precision of about 1.4%. BABAR uses both Bhabha scattering and e+ eâ â Î¼+ Î¼â (Lees, 2013i); the systematic uncertainty of the luminosity is about 0.5% for the data collected at the Î¥ (4S). The uncertainties from these external parameters are propagated to the ï¬nal result using either Gaussian error propagation in the simplest cases, or by varying the"
71,58,0.985,Advancing Culture of Living with Landslides: Volume 1 ISDR-ICL Sendai Partnerships 2015-2025 (Volume 1.0),"combination of all acquired scans into a single point cloud. In subsequent processing the individual points are connected to form a triangulated mesh which can be further processed to create a complete, full surface. Finally, photos can be draped over the surface to create a photo-realistic appearance. The interval between points of the point cloud, referred to as resolution, can be chosen by the operator of the laser scanner depending on the required detail and the complexity of the surface. Finally, TLS can also be utilised to identify the different members of a stratigraphic sequence. In fact, the instrument records the RGB values and measures the reflected ray energy, providing the reflectivity index I of the micro-portion surface (Ercoli et al. 2013). Therefore, the reflectivity index associated with each TLS point can provide an indirect estimation of the lithothypes (Fig. 3). Intensity variations as a result of different angles of incident proved insigniï¬cant in this context."
372,778,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"now routinely used, this subsection is included as an introduction to the subject. The quantization characteristic for two-level sampling is shown in Fig. 8.3. The quantizing action senses only the sign of the instantaneous signal voltage. In many samplers, the signal voltage is first amplified and strongly clipped. The zero crossings are more sharply defined in the resulting waveform, and errors that might occur if the sampling time coincides with a sign reversal are thereby minimized. The correlator for two-level signals consists of a multiplying circuit followed by a counter that sums the products of the input samples. The input signals are assigned values of +1 or !1 to indicate positive or negative signal voltages, and the products at the multiplier output thus take values of +1 or !1 for identical or different input values, respectively. We consider sampling both at the Nyquist rate and at multiples of it and represent by N the number of sample pairs fed to the correlator. The twolevel correlation coefficient is %2 D"
224,194,0.985,Ester Boserupâs Legacy on Sustainability : Orientations for Contemporary Research,"Our case study is situated in the Mediterranean mountains of southern Spain, in the province of Granada. The 25.5-km2 study area is part of the Baetic Cordillera chain; the average gradient is 10 %, representing a typical example of mid-height mountain agriculture. The climate in this area is Mediterranean-Continental, with average annual rainfall of 550 mm, an average annual temperature of 15.2â¦ and potential evapotranspiration (PET) of 760 mm. The soil type is primarily Calcic Cambisol according to the FAO classification. MontefrÃ­o, our case study, is a town with a long olive-growing tradition, and its geographical and soil and climatic characteristics are similar to those of the agricultural interior of Andalusia. Until well into the twentieth century, it was fairly isolated because of the characteristics of its relief; hence, its processes of change are an attractive field of study."
307,63,0.985,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"The contraction of a single cardiac cell is initiated by an increase in the transmembrane potential leading to opening of the so-called L-type calcium channels (LCCs). When these channels are open, calcium flows into a rather small space called the dyadic cleft (often simply referred to as the dyad), leading to a locally increased concentration of Ca2C ions. This increased concentration leads to the opening of the ryanodine receptors (RyRs), which control the flow of calcium from the internal stores referred to as the sarcoplasmic reticulum (SR). This process is referred to as the calcium-induced calcium release (CICR) and is of vital importance in the functioning of the heart. A schematic description of the process is given in Fig. 2.1. This CICR process is one of the focal points of interest in these notes. We shall develop a model coupling the effects alluded to in Fig. 2.1. However, in this first chapter we shall simplify the process quite a bit by assuming that we just have three spaces: the SR, the dyad, and the cytosol (see Fig. 2.2). This simplification means that we assume that there is very fast diffusion between the network SR (NSR) domain and the junctional SR (JSR) domain such that the associated concentrations are identical. Furthermore, we ignore the L-type channels and assume that the concentrations in both the SR and the cytosol are constant. This leads to a onedimensional model, in the sense that only the concentration of the dyad changes. The model is useful because it helps illustrate the tools we need in our analysis of the full CICR process and illustrates the properties of optimal drugs that will be more or less inherited in more complex models. Our aim is therefore to understand in some detail what is going on in the process illustrated in Fig. 2.1. However, this figure is in itself a huge simplification of the complex CICR process. The cell consists of 10,000 to 20,000 dyads, each dyad having up to 100 RyRs, and human ventricles consist of billions of cells. Our aim is to focus entirely on a very small but essential element in the CICR mechanism. We model the release of Ca2C ions from the SR to the dyad by formulating a stochastic differential equation governing the concentration of Ca2C ions in the Â© The Author(s) 2016 A. Tveito, G.T. Lines, Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models, Lecture Notes in Computational Science and Engineering 111, DOI 10.1007/978-3-319-30030-6_2"
311,2879,0.985,The Physics of the B Factories,"Belle uses 121.4 fbâ1 of data accumulated at the Î¥ (5S) resonance to search for, and observe, exclusive colorsuppressed Bs0 decays with an underlying b â ccs quark transition. Among these, the pure CP -eigenstate final state decays Bs0 â J/Ïf0 (980) and Bs0 â J/ÏÎ· (â²) are of special interest. These decays, which were not yet observed, could be used in the future to study timedependent CP asymmetries. In addition the ratio of the Bs0 â J/ÏÎ· â² and J/ÏÎ· branching fractions constitutes a test of Î· â Î· â² mixing. The study of the Bs0 â J/Ïf0 (980) decay is described in detail in Li (2011). The reconstruction of the decay mode Bs0 â J/Ïf0 (980) includes f0 (980) decaying into two charged pions. The main backgrounds are from the inclusive decays Bs0 , Bd0 , and B + â J/ÏX, where the contributions from Bs0 â J/ÏÎ· â² and B + â J/Ï(K + , Ï + ) are modeled exclusively and set to their known branching fractions. Since the f0 (980) resonance is wide, â¼ 60 MeV/c2 , we need to describe the MÏÏ spectrum using the FlatteÌ formula with a phase-space factor, and the f0 (1370) lineshape is described using a relativistic Breit-Wigner. Both of these line shapes are described in Section 13.2.1. A twodimensional fit with the variables ÎE and MÏÏ is performed to extract the signal yield, where the mES signal region corresponding to the Bsâ0B â0 s channel was chosen. The data surprisingly also show an enhancement around MÏÏ â¼ 1400 MeV/c2 , where the ÎE distribution is also strongly peaked (see Fig. 23.3.8). Thus Belle includes the contribution from the f0 (1370) resonance coherently with the f0 (980) resonance in the MÏÏ signal fit model. Studies of the decay modes Bs0 â J/ÏÎ· and J/ÏÎ· â² are described in (Li, 2012). Five Î· and Î· â² sub-channels are reconstructed, Î· â Î³Î³, Î· â Ï + Ï â Ï 0 , Î· â² â Î·(Î³Î³)Ï + Ï â , Î· â² â Î·(Ï + Ï â Ï 0 )Ï + Ï â and Î· â² â Ï0 Î³. To use as much information as possible, a simultaneous fit to the twodimensional ÎE - mES distributions of all five sub-channels is performed in order to extract the branching fraction. The signal includes contributions from all three Bs0 pro(â)0 (â)0 duction channels Î¥ (5S) â Bs B s . The fit results are shown in Fig. 23.3.8. Belle observes the three decay modes and measures their branching fractions: B(Bs0 â J/Ïf0 (980); f0 (980) â Ï + Ï â )"
375,69,0.985,Musical Haptics,"smaller than the touched surface. The distribution of receptors is highly related with the geometry of the fingerprint. In particular, the spatial frequency of the Meissner corpuscles is twice that of the ridges. On the other hand, the spatial frequency of the arborescent terminations of the Merkel complexes is the same as that of the ridges. This geometry explains why the density of Meissner corpuscles is roughly five times greater than that of the Merkel complexes [37, 45, 55, 59]. Merkel complexes, however, come in two types. The other type forms long chains that run on the apex of the papillae [60]. The distinctive tree-like structure of this organ terminates precisely at the dermalâepidermal interface. It is useful to perform simple experiments to realise the differences in sensory capabilities between glabrous and hairy skin. It suffices to get hold of rough surfaces, such as a painted wall or even sand paper, and to compare the experience when touching it with the fingertip or with the back of the hand. Try also to get hold of a Braille text and to try to read it with the wrist. The types of receptors seem to be similar in both kinds of skin, but their distribution and the organisation and biomechanical properties of the respective skins vary enormously. One can guess that the receptor densities are greatest in the fingertips. There, we can have an idea of their density when considering that the distance between the ridges of the glabrous skin is 0.3â0.5 mm. The largest receptor is the Pacini corpuscle. It is found in the deeper regions of the subcutaneous tissues (several mm) but also near the skin, and its density is moderate, approximately 300 in the whole hand [11, 71]. It is large enough to be seen with the naked eye, and its distribution seems to be opportunistic and correlated with the presence of main nervous trunks rather than functional skin surfaces [32]. Receptors of this type have been found in a great variety of tissues, including the mesentery, but near the skin they seem to have a very specific role, that of vibration detection. The Pacinian corpuscle allows to introduce a key notion in physiology, that of specificity or âtuningâ. It is a common occurence in all sensory receptors (be it chemoreceptors, photoreceptors cells, thermoreceptors or mechanorectors) that they are tuned to respond to certain classes of stimuli. The Pacinian corpuscle does not escape this rule since it is specific to vibrations, maximising its sensitivity for a stimulation frequency of about 250 Hz but continuing with decreasing sensitivity to 1000 Hz. It is so sensitive that, under passive touch conditions, it can detect vibrations of 0.1 micrometer present at the skin surface [78]. Even higher sensitivity was measured for active touch: results addressing a finger-pressing task are reported in Sect. 4.2. The Meissner corpuscle, being found in great numbers in the glabrous skin, plays a fundamental role in touch. In the glabrous skin, it is tucked inside the âdermal papillaeâ, and thus in the superficial regions of the dermis, but nevertheless mechanically connected to the epidermis via a dense network of connective fibres. Therefore, it is the most intimate witness of the most minute skin deformations [72]. One may have some insight into its size by considering that its âterritoryâ is often bounded by sweat pores [55, 60]. Merkel complexes, in turn, rather than being sensitive axons tightly packed inside a capsule, have tree-like ramifications that terminate near discoidal cell, the so-called Merkel cells. In the hairy skin, these structures are associated with each hair. They"
307,110,0.985,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"Fig. 2.10 Four simulations based on the stochastic scheme (2.7) where the solutions are plotted from 900 to 1;000 ms. The lower curves give the concentrations and we note that the concentrations are quite large but limited above by the upper limit given by cC D 91 M. The upper two lines indicate whether the channel is open (upper) or closed (lower); we see that the channel is open most of the time. These results fit well with the results presented in Fig. 2.9, where the probability density functions are plotted"
142,209,0.985,Subseafloor Biosphere Linked To Hydrothermal Systems : Taiga Concept,"Once an organism settles in a suitable environment of the vent field, the demographic expansion of the fauna begins. The genetic diversity of a population should be correlated positively with its effective population size (number of individuals involved in reproduction), and it is regarded as tracer of past demographic expansion of a population (Rogers and Harpending 1992). Therefore, the genetic diversity of a population can be an estimator of the past population expansion after settlement of vent species, or formation of a vent environment that is suitable for the species. The temporal change of genetic variation can be examined using a molecular clock hypothesis, which has been used to estimate divergence times. The molecular clock hypothesis states that DNA and protein sequences evolve at a mutation rate, which is constant over time and among populations. For example, the partial sequence of mitochondrial cytochrome oxidase c subunit I (COI) gene, which is commonly used as a barcode of species, has been investigated. Reportedly 1.4 % sequence divergence per 106 years occurred on the gene sequence of snapping shrimp (Knowlton and Weigt 1998). However, the rates of accumulation of genetic variation are not the same among"
372,1608,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"Now consider the case of an interferometer operating in the domain of baselines greater than dm , where -+ is a constant equal to -m . This case is most applicable to VLBI arrays or to large connected-element arrays. If the timescale of the fluctuation is short with respect to the measurement time, then, on average, all the visibility measurements are reduced by a constant factor e!-m =2 . Thus, this type of atmospheric fluctuation does not reduce the resolution. However, on average, the measured flux density is reduced from the true value by the factor e!-m =2 . If the timescale of the fluctuations is long with respect to the measurement time, then each visibility measurement suffers a phase error e j+ . Assume that K visibility measurements are made of a point source of flux density S. The image of the source, considering only one dimension for simplicity, is"
241,708,0.985,Second Assessment of Climate Change for the Baltic Sea Basin,"models used here (Meier et al. 2011). The highest percentile summer warming is large in the south-east of the region. This is related to the large-scale pattern of warming in Europe with the strongest summer warming in southern Europe. Also, in the very north-east of the region, there is a large warming, probably connected to ice-albedo feedback. Similar results also exist for other GCM/RCM combinations (Christensen and Christensen 2007; KjellstrÃ¶m et al. 2011a)."
372,1270,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"difference of the channels with high amplitude ripples at the two edges of the passband may be sufficient to separate the ghost into two components, as shown by Bos (1985). This separation will not occur if the .u; v/ values are individually assigned for each spectral channel. Bos (1984) points out that the ghost can be removed, or substantially attenuated, by ""=2 switching of the relative phase between each signal pair before cross correlating, and restoring the phase before transformation of the visibility data to form an image. For the source considered in Eq. (10.94), the introduction of ""=2 into the differential phase for an antenna pair results in the visibility V2 .u/ D je!j2""ul1 D j cos.2""ul1 / C sin.2""ul1 / :"
233,245,0.985,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"The parameter q determines the sensitivity of the measure to the relative frequencies of the species. When q = 0, qH becomes S â 1; When q tends to 1, qH tends to Shannon entropy. When q = 2, qH reduces to the Gini-Simpson index. This family was found many times in different disciplines (Havrdra and Charvat 1967; DarÃ³czy 1970; Patil and Taillie 1979; Tsallis 1988; Keylock 2005). There are many other families of generalized entropies, notably the RÃ©nyi entropies (RÃ©nyi 1961). Although the traditional abundance-sensitive generalized entropies and their special cases have been useful in many disciplines (e.g., see Magurran 2004), they do not behave in the same intuitive linear way as species richness. In ecosystems with high diversity, mass extinctions hardly affect their values (Jost 2010). They also lead to logical contradictions in conservation biology, because they do not measure a conserved quantity (e.g., under a given conservation plan, the proportion of âdiversityâ lost and the proportion preserved can both be 90 % or more); see Jost (2006, 2007) and Jost et al. (2010). Thus, changes in their magnitude cannot be properly compared or interpreted. Also, the main measure of similarity in the additive approach for traditional measures, the within-group or âalphaâ diversity divided by the total or âgammaâ diversity, does not actually quantify the compositional similarity of the assemblages under study. This ratio can be arbitrarily close to unity (supposedly indicating high similarity) even when the assemblages being compared have no species in common. Finally, these measures each use different units (e.g., the Gini-Simpson index is a probability whereas Shannon entropy is in units of information), so they cannot be compared with each other. All these problems are consequences of their failure to satisfy the replication principle. Hill numbers obey the replication principle and resolve all these problems; see section âHill numbers and the replication principleâ."
231,1336,0.985,North Sea Region Climate Change Assessment,"of North American emissions on Europe using a tagging method. Their results are at the lower end of the HTAP estimates. The HTAP and Brandt et al. (2012) calculations are for different meteorological years. Located close to the western continental rim, the intercontinental contribution to the North Sea region is higher than the European average as shown by Jonson et al. (2006). Particulate matter has a short residence time in the atmosphere, and as a result the intercontinental contribution to Europe is in general low. Calculating the ratio of the effect of other (than Europe) source regions to the effect of all source regions (including Europe), indicates that about 5 % of the PM surface concentrations in Europe can be attributed to intercontinental transport (HTAP 2010). However, the"
297,721,0.985,The R Book,"where r is the degrees of freedom in the numerator and s is the degrees of freedom in the denominator. The distribution is named after R.A. Fisher, the father of analysis of variance, and principal developer of quantitative genetics. It is central to hypothesis testing, because of its use in assessing the signiï¬cance of the differences between two variances. The test statistic is calculated by dividing the larger variance by the smaller variance. The two variances are signiï¬cantly different when this ratio is larger than the critical value of Fisherâs F. The degrees of freedom in the numerator and in the denominator allow the calculation of the critical value of the test statistic. When there is a single degree of freedom in the numerator, the distribution is equal to the square of Studentâs t: F = t2 . Thus, while the rule of thumb for the critical value of t is 2, so"
238,417,0.985,Nanoinformatics,"where y is the penetration depth along each GB, t is the exposure time, DL is the lattice diffusion coefï¬cient for oxygen in sapphire, and Cy and Cbg are the respective fractions of 18O at the penetration distance along each GB and the natural abundance (0.00204). DL is also likely to depend on ÂµO in the wafer, similar to the GB diffusion coefï¬cient of Eq. (11.17). However, DL was assumed to be constant at 5 Ã 10â20 m2/s at 1873 K [6] because ÂµO was almost constant in the immediate vicinity of the PO2(hi) surface [21, 22]. The oxygen GB diffusion coefï¬cients were determined from Eq. (11.7) within the range that corresponded to the normalized positions of the wafer, x/L. The Î² values (deï¬ned as Î´(Dgb/Dâ1 L )/2(DLt) ) for the oxygen GB diffusion coefï¬cients must be sufï¬ciently large (Î² > 10) to allow the use of Eq. (11.7). In the present work, all Î² values were larger than 100 and thus met the requirement."
311,280,0.985,The Physics of the B Factories,"6.5.3 Ît resolution function To account for the ï¬nite decay time resolution the p.d.f. describing the physical time evolution in a time-dependent analysis is convolved with a r esolution function which is the response function that describes the distribution of the observed decay time as a function of the true decay time Îttrue . To ï¬rst order the resolution function is a Gaussian function with zero mean and a width corresponding to the average resolution. In practice, the deviations from a Gaussian are important. The parameterization and calibration of the resolution function is described in detail in Section 10.4. Here, we brieï¬y emphasize features of the vertex resolution that impact the Ît resolution in timedependent analyses. The estimated uncertainty in the Btag vertex z position is a function of the number of tracks assigned to the vertex and the direction and momentum of those tracks. It diï¬ers substantially between events, leading to a large variation in the estimated uncertainty on Ît, as shown in"
157,195,0.985,"The Economy as a Complex Spatial System : Macro, Meso and Micro Perspectives","Strong cross-sectional dependence in the errors of a knowledge production function may arise as a result of unobserved common factors, including, for instance, aggregate technological shocks, national policies intended to raise the level of technology or oil price shocks that may influence TFP through their effects on product costs. The heterogeneous effects of these factors may be the result, for instance, of country-specific technological constraints (Ertur and Musolesi 2016). Cross-sectional dependence in the errors of a knowledge production function can also be regarded as a result of spatial effects. Thus, a SAR-CCEP version of the knowledge production function seems to be a natural choice when the panel data is large enough. Some drawbacks of this approach are worth noticing. First, there is a large number of incidental parameters under the joint modeling. Admittedly, this is not a serious problem as long as the model is linear, since inconsistency in the estimation of the incidental parameters is not transmitted to the estimation of the slope parameters of interest (Î² ); but, it may create a problem when nonlinear terms are considered. Second, the ability of the SAR-CCEP method to capture strong cross-sectional dependence and to disentangle spatial spillover effects and common factor effects is crucially affected by the set of covariates included in the model. On the one hand, if the estimated model contains one or only a few regressors, the CEEP estimator may not fully control for cross-sectional correlation (few regressors implies few cross-sectional averages as proxies for unobserved common factors); on the other hand, if the model includes many regressors, the resulting large number of cross-sectional averages hardly leave space for residual spatial spillovers. In Sect. 3.3, we review an alternative semiparametric approach to filter common-factor (or time-related) effects and, thus, to assess the presence of âresidualâ spatial dependence effects which adequately addresses these problems."
311,1299,0.985,The Physics of the B Factories,"The original and first objective for these analyses was to observe RADS = 0. Both BABAR (del Amo Sanchez, 2010m) and Belle (Horii, 2011) have reconstructed the decays B â â DK â and B â â Dâ K â with Dâ â DÏ 0 and Dâ â DÎ³ (del Amo Sanchez, 2010m; Trabelsi, 2013) followed by D â K + Ï â on datasets of 467 and 772 Ã 106 BB pairs respectively. BABAR (Aubert, 2009t) has also selected B â â DK ââ with K ââ â KS0 Ï â using 379 Ã 106 BB pairs. As in the GLW analysis, the B decay final states are fully reconstructed. The selection criteria are usually tighter in order to achieve a higher signal purity, given that the signal rate is typically O(10â2 ) weaker than in the case of D decaying to CP eigenstates. Particular care is taken over the suppression of âpeakingâ backgrounds from misidentified B â â D(â) Ï â or B â â D(â) K â decays. After the selection, the main background is due to qq events. In order to achieve a better continuum background suppression, a non-linear neural network (Section 4.4.4), N N (BABAR) or N B (Belle), of several event-shape quantities (Section 9.3) is used. The final yields are extracted from maximum likelihood fits to mES and N N (BABAR) or ÎE and N B (Belle) for B â â D(â) K â , and to mES (after a tight selection criteria on N N ) for B â â DK ââ (BABAR). These results are summarized in Table 17.8.2. The strongest evidence for suppressed signals have been reported by Belle with a significance (including systematic uncertainties) of 4.1Ï for the B â â DK â mode and of 3.5Ï for the B â â Dâ [DÎ³]K â decay (Fig. 17.8.4)."
393,427,0.985,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 2,"Data clustering is involved in the solution of many important problems. Clustering algorithms yield more clusters (groups) of instances from the data-set where each instance in a cluster is more similar to those inside the cluster than to instances outside the cluster. The considered similarity is depending from the domain. Wide variety of problems ranging from identification of citation duplications, through marketing applications, such as customer profile clustering, seeking of DNA groups, etc. exist. Graph clustering is also exploited by several applications, such as socialnetwork analysis, computer graphics, bioinformatics, and VLSI design. For identification of scientific publication duplications (Aleman-Meza et al., 2006) clustering is also a possible approach (McCallum et al., 2000). The data-sets for clustering can differ in several ways. In the simplest case the instances of the data-set have the same attributes. However, they often differ in the number and type of attributes. Often the instances explicitly refer each other and thus create a network of instances. This network is exploited by graph-based partitioning algorithms. Graphs are appropriate for representation of elements in data-sets. These elements, depending on the domain being examined, might reference each other; the edges may be represented by weighted edges expressing similarity between the two elements. Graph partitioning algorithms can be thus often well applied for arbitrary set of instances, if the similarity between the instances were well estimated. We threat instances often as graph vertices in following text. It should be clear from the context, when we are considered about instanceâs attributes and when about its neighbors in the network of instances. We recall some basic definition from the graph theory used is this chapter. Let G (V , E ) be an undirected graph. Let E ( X ) denote the set of edges existing also in the graph connecting two different vertices in X : E(X ) = {{u, v}â E | u, v â X} ."
285,368,0.985,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","As expected from the strong relationship between sound intensity and loudness, a very similar pattern across auditory regions was found for the relationship between intensity or categorical loudness. Overlaid are the fitted curves as predicted by the fixed effects of the linear ( straight lines) and quadratic ( dotted curves) models with their corresponding RÂ²m statistics. Quadratic fits with loudness are not shown, since only 4 of the corresponding models reached significance across ROIs"
199,61,0.985,Synchronized Factories : Latin America and the Caribbean in the Era of Global Value Chains,"Recapitulating Most of the indicators we used to examine the participation of LAC in global value chains present a similar picture: LACâs participation generally tends to be low relative to other regions. However, there is also significant heterogeneity within the region. For instance, Mexico and countries in Central America are more engaged in production networks, particularly with North America, and tend to participate in the final stages of production networks. For their part, countries in South America typically enter supply chains in the early stages. A set of clear factors explain at least some of these differences. For instance, proximity to the US makes Mexico an ideal recipient of offshoring activities. Likewise, the sheer abundance of natural resources in South America biases countries to participate in more upstream stages of supply chains. Proximity, the endowments of natural resources, and the relative abundance of different classes of labor are obvious drivers behind the levels and types of participation in supply chains. But they are not the only drivers. The next chapter uses a more rigorous analysis to identify a more complete spectrum of factors behind the regionâs relatively subpar participation in international supply chains. Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
372,727,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"It is useful to note that a condition for a pair of square waves of different frequency to be orthogonal, for arbitrary time shifts, is that they do not contain Fourier components of the same frequency. A property of square waves is that all even-numbered Fourier components (i.e., even harmonics of the fundamental frequency) have zero coefficients, but odd-numbered components have nonzero coefficients. Thus, although sinusoids with frequencies proportional to 1, 2, 3; : : : are mutually orthogonal, square waves with such frequencies, in general, are not. For example, square waves of frequencies 1, 2, and 4 have no common Fourier components and are mutually orthogonal, but 1, 3, and 5 have common components and are not mutually orthogonal. DâAddario (2001) shows by generalization of this analysis that the lowest frequency sets of N mutually orthogonal square waves consist of those with frequencies proportional to 2n for n D 0; 1; : : : ; .N "" 1/, that is, the square-wave sets discussed above. Since the different square waves of a set that we are considering contain no common Fourier components, their orthogonality is not affected by relative time shifts. Note, also, that exact orthogonality is not essential for phase switching. Unwanted responses can be reduced by a factor of 104 or more by using square waves with k cycles per averaging period for values of k that are prime numbers greater than 100. For arrays with large numbers of antennas, Walsh functions are generally the preferred waveforms for phase switching. Walsh functions are rectangular waveforms in which transitions between C1 and ""1 occur at intervals that are a varying integral submultiple of a basic time cycle, as in Fig. 7.12. For a description of Walsh functions (Walsh 1923; also Fowle 1904) see, for example, Harmuth (1969, 1972) or Beauchamp (1975). Various systems of designating and ordering Walsh functions are in use. In one system (Harmuth 1972), those with even symmetry are designated as cal(k; t) and those with odd symmetry as sal(k; t). Here, t is time expressed as a fraction of the time base T, which is the interval at which the waveform repeats, and k is the sequency, which is equal to half the number"
246,66,0.985,Rewilding European Landscapes,"Remoteness from national population centres was a measure of the accessibility to the whole British population in addition to the accessibility to the local population in the calculation of wilderness. The authors used multicriteria evaluation (MCE) and explored public perceptions of wilderness through the use of interactive tools by allowing the user to change the weights of the wilderness metrics. As expected, resulting wilderness maps were not radically different, but allowed for insights on what affects the perceptions of wilderness (Carver et al. 2002). This approach was further detailed at the level of the Cairngorms National Park, and the Loch Lomond and The Trossachs National Park in Scotland (Carver et al. 2012) at a resolution of 20 m and later expanded to cover the whole of Scotland in a study by the Scottish Natural Heritage (Scottish Natural Heritage 2012). At lower spatial extents the indicators of wilderness and human footprint remain the same but higher quality data are usually available making the mapping and modelling process more reliable and accurate. For example, Woolmer et al. (2008) rescaled the human footprint methodology of the Sanderson et al (2002) for the area of approximately 300,000 km2 of the Northern Appalachian ecoregion. They used ten datasets compiled from several sources: population density, dwelling density, urban areas, roads, rail, land cover, large dams, watersheds, mine sites, utility corridors for the electrical power infrastructure. The general patterns of human footprint were maintained when comparing the map based on 90 m2 resolution data at ecoregional scale with the map derived from the global analysis of Sanderson et al (2002) conducted with 1 km2 resolution data. However, the Spearman rank correlation coefficients between the two sets of human footprint data steadily decreased with the scale, reaching 0.41 ( p < 0.001) at 0.1 % of the Northern Appalachian ecoregion. The difference in the human footprint scores is that the ecoregion calculation compared with the global calculation leads to a reduction in the area with low levels of human footprint (46 % ecoregion extent vs. 59 % global extent) and an increasing of the area with moderate or high levels of human footprint (34 % ecoregion extent vs. 21 % global extent), evening out more the distribution of human footprint scores. A key finding was also that three parameters models add the most information to the calculation of human footprint while the model incorporating human settlements, roads and land-use was the best approximating model from all combinations of the ten datasets considered. In Europe, an increased wilderness momentum has led to efforts by different actors to protect wilderness and advance a progressive wilderness research agenda (Jones-Walters and ÄiviÄ 2010). A continental level map of wilderness continuum has been produced using population density, road and rail density, linear distance from the nearest road and railway line, naturalness of land cover and terrain ruggedness (Carver 2010). This analysis identified wilderness areas concentrated in the Scandinavian Peninsula and the mountainous regions of Europe, revealing a strong positive altitudinal and latitudinal relationship. The same pattern was maintained even if terrain ruggedness was eliminated from the calculation. Beside the Scandinavian mountains and arctic areas, the Pyrenees, The Eastern Mediterranean islands, the Alps, the British Isles, the south-eastern Europe and the Carpathians also had significant areas of wilderness (Carver 2010) but one has to temper this"
213,341,0.985,Collider Physics Within The Standard Model : a Primer,"with the normalization such that at t D 0,  D 0 D m2H =2v 2 , from the minimum condition in (3.60), and the top Yukawa coupling is given by h0t D mt =v. The initial value of  at the weak scale increases with mH and the derivative is positive at large  because of the positive 2 term (the ' 4 theory is not asymptotically free), which overwhelms the negative top Yukawa term. Thus, if mH is too large, the point where  computed from the perturbative beta function becomes infinite (the Landau pole) occurs at too low an energy. Of course, in the vicinity of the Landau pole the 2-loop evaluation of the beta function is not reliable. Indeed, the limit indicates the frontier of the domain where the theory is well described by the perturbative expansion. Thus the quantitative evaluation of the limit is only indicative, although it has been to some extent supported by simulations of the Higgs sector of the EW theory on the lattice. For the upper limit on mH , one finds [241] mH . 180 GeV for   MGUT âMPlanck ;"
165,522,0.985,New Methods for Measuring and Analyzing Segregation,"Focusing Attention on Individual-Level Residential Outcomes All widely used indices of uneven distribution can be formulated in terms of individual-level residential outcomes (y) that are scored from area group (e.g., racial) proportions (p). This can be done in two distinct ways. One is to formulate index scores as simple overall averages of individual-level residential outcomes (y). The other is to formulate index scores as a difference of group means on individuallevel residential outcomes (y). Both approaches can be used to obtain âcorrectâ index values. But that is a minor benefit as convenient formulas for obtaining correct index values are readily available. The main benefit of these formulations is that they can be used to gain insight into how different indices register and summarize individual residential outcomes. In addition, formulating indices in terms of individual attainments brings certain practical advantages which I note below. Figure B.2 presents computing formulas that highlight how individual-level residential outcomes are registered by six popular measures of uneven distribution â the Gini Index (G), the Delta or Dissimilarity Index (D), the Atkinson Index (A), the Hutchens Square Root Index (R), the Theil Entropy index (H), and the Separation Index (S) (also known as the variance ratio [V], and eta squared [Î·2]). The calculations indicated in these formulas involve first computing area-specific scores (i.e., neighborhoods) based on pairwise group proportions and then averaging these scores over individuals. More specifically, the formulas have the following features: â¢ the core terms in the calculations are scores computed for areas (indexed here by âiâ) based on calculations involving area group proportions; that is involving the values of pi and qi as given in Appendix A, â¢ the area-specific scores are summed over all individuals based on weighting the score for each area by the area-specific combined population count (t) for the two groups in the segregation comparison, â¢ the population-weighted sum of area-specific scores is then divided by the combined population of the two groups for the city (T) to obtain an overall average,"
213,182,0.985,Collider Physics Within The Standard Model : a Primer,"where the F and D couplings are defined in the SU.3/ flavour symmetry limit, and 2 and 3 describe the SU.2/ and SU.3/ breakings, respectively. From the measured first moment of the structure function g1 , one obtains the value of a0 D ÂË: 1 D"
233,464,0.985,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"considered five times as costly to protect, lowering its position in the Zonation ranking. The cost layer can be scaled differently according to how much importance is given to phylogenetic diversity. The equivalent numbers of Raoâs quadratic entropy values went from ca. 1 to 7, and the direct inverse was used in our âmedium weightingâ (that is, cost goes from 0.14 to 1), and this scale was halved (âlow weightsâ, 0.28â1) and doubled (âhigh weightsâ, 0.07â1) to test for sensitivity to this parameter (see Fig. 1 for analysis setups). The latitudinal gradients in species richness and range sizes cause the spatial priorities in analyses at any scale to be concentrated in the more species rich lower latitude areas (Eklund et al. 2011; Moilanen et al. 2013). Even though cost-effective from the perspective of species conservation, focusing conservation efforts into these regions only would be very difficult for many reasons (see section âDiscussion and Conclusionsâ). Therefore we also performed an analysis where countries were considered as independent administrative units, each aiming to conserve the diversity within their borders. This is implemented through the Administrative units analysis in Zonation (Moilanen and Arponen 2011). The analysis would allow for a compromise solution between purely European-scale and purely national-scale analyses, but for our analytical purposes, we chose the extreme cases only. A national-scale prioritization provides an interesting reference for comparison to protected areas. We did this for one tree only. Thus, we ended up with four main Zonation solutions to assess protected area performance regarding the representation of species and phylogenetic diversity at both European and national scales (Fig. 1)."
311,2897,0.985,The Physics of the B Factories,"corresponding coeï¬cient functions are needed, and the gluon FF appears. The NLO results are listed, for example, in Kretzer (2000) and Albino, Kniehl, and Kramer (2005, 2008) for the modified minimal subtraction (MS) scheme. The coeï¬cient functions are now known to the NNLO level for the unpolarized case (Mitov, Moch, and Vogt, 2006). A sum rule exists for the FFs because of energy conservation. As the variable z is the energy fraction for the produced hadron, its sum weighted by the fragmentation functions should be unity:"
339,509,0.985,Etiology and Morphogenesis of Congenital Heart Disease : From Gene Function and Cellular interaction To Morphology,"Understanding the 3D architecture of a genomic locus not only provides insight into the tight relationship between chromatin topology and the complex regulation of developmental gene expression, it could also provide valuable clues in the understanding of the role of functional variation as identified by genome-wide association studies on gene expression. Common genomic variation in the non-coding region upstream of TBX3 and TBX5 was found to influence PR interval and QRS duration in humans [29â31]. The fact that the regulatory domains of TBX3 and TBX5 are strictly separated indicates that the variation found in one of the domains can be exclusively assigned to its respective gene, facilitating our understanding of the functional effect of disease-associated variation. A similar example of how knowledge on the chromatin conformation can increase our understanding of function of common variants is illustrated by studies on the SCN5A/SCN10A locus. SCN5A and SCN10A encode sodium channels important for conduction. GWAS implicated an intronic region in SCN10A as a major risk region for prolonged QRS duration [30]. The role of SCN10A in cardiac conduction however was not previously described, whereas mutations in the adjacent SCN5A are well established to cause several arrhythmogenic disorders, including Brugada and Long QT syndrome [44, 45]. It is therefore possible that the variation identified within the intron of SCN10A impacts the expression of SCN5A, rather than or in addition to that of SCN10A. Indeed, probing the 3D architecture with the promoters of both SCN5A and SCN10A and the site of the variation in the intron as point of view using 4C-seq revealed that this variant region contacts the SCN5A promoter and a strong enhancer downstream of SCN5A, suggesting that it might act as enhancer regulating the expression of SCN5A in the heart. Transgenic reporter assays revealed that indeed this enhancer is essential for cardiac Scn5a expression. In humans, the SNP located within the enhancer that correlates with slowed conduction is associated with lower SCN5A expression"
311,575,0.985,The Physics of the B Factories,"the phase-space term is constant within the kinematically allowed region in the two-dimensional space spanned by Dalitz-plot analysis these variables. In this case the structure of the amplitude becomes apparent. This can be achieved by taking either Editors: the kinetic energies of two of the ï¬nal-state particles, or Thomas Latham (BABAR) the squares of the invariant masses of two pairs of ï¬nalAnton Poluektov (Belle) state particles. The former parameterization is convenient for nonrelativistic decays and was originally proposed by Additional section writers: R. H. Dalitz to study the decay of charged kaons to three Eli Ben-Haim, Mathew Graham, Fernando Martinez-Vidal pions (Dalitz, 1953). The corresponding relativistic formulation was ï¬rst introduced in Fabri (1954). However, the latter approach is generally more suitable for relativistic decays and has an additional advantage that it allows for 13.1 Introduction easy determination of the masses of intermediate states. For a particle of mass M decaying into three particles deDalitz-plot analysis is a powerful technique that involves noted as a, b and c, the diï¬erential decay probability is studying the amplitude for the decay of a parent parti1 cle into a three-body ï¬nal state. Compared to two-body dÎ = |A|2 dm2ab dm2bc , (13.1.1) (2Ï)3 32M 3 decays, the three-body decay possesses intrinsic degrees of freedom that permit the determination of the relative magnitudes and phases of interfering amplitudes. The types where mab and mbc are the invariant masses of the pairs of of measurements that can beneï¬t from using the Dalitz- particles ab and bc, respectively. Thus, any nonuniformity observed in the distribution of the variables m2ab and m2bc plot analysis technique include: is due to the dynamical structure of the decay amplitude â Searches for new states; A. Most of the analyses performed at the B Factories deal â Measurements of properties of resonances â masses, with the Dalitz plot expressed this way; the exception to widths, quantum numbers; this will be considered in Section 13.4.1. â CP violation searches and measurements of the associated parameters; 13.1.2 Boundaries, kinematic constraints â Studies of ï¬avor mixing. This chapter starts with a discussion of the kinematics of three-body decays (Sections 13.1.1 and 13.1.2) before describing the formalisms commonly used to model the three-body decay amplitude (Section 13.2). This is followed by an outline of the experimental eï¬ects that must also be accounted for in order to successfully describe the distribution of the data over the Dalitz plot (Section 13.3). Technical details of the implementation are presented in Section 13.4 before a discussion of the uncertainties arising from the chosen model (Section 13.5). 13.1.1 Three-body decay phase space In the case of a two-body decay, the energies of the ï¬nal state particles in the center-of-mass frame are fully determined by the conservation of energy and momentum, up to an overall rotation. In contrast, the kinematics of three-body decays are not similarly constrained: after requiring energy and momentum conservation in the system of three ï¬nal state particles, there are ï¬ve remaining degrees of freedom. In the case where the initial and ï¬nal state particles all have spin zero, after taking into account arbitrary rotations, two degrees of freedom remain. The amplitude of the decay can thus be represented as a function of two parameters; the scatter plot of this pair of parameters is called the Dalitz plot (Dalitz distribution). There is freedom in the choice of which two parameters one uses to describe the amplitude of a three-body decay. It is often convenient to choose a pair of parameters where"
36,115,0.985,Bats in the Anthropocene : Conservation of Bats in a Changing World,"4.5.1.3 Responses to Landscape Structure Fragmentation studies have increasingly shifted their focus from being largely patch-centered toward taking a broader landscape-scale approach, thus acknowledging the overriding importance of the matrix and the existence of gradients of habitat conditions and quality as crucial determinants of species responses (Kupfer et al. 2006; Driscoll et al. 2013; Cisneros et al. 2015). Such gradients are provided, for example, by mosaics of old-growth forest, successional habitat, and different forms of agriculture. This paradigm shift is to some degree reflected within the more recent bat literature, as a growing number of studies have adopted matrix-inclusive approaches to studying fragmentation, although overall the number of studies is still small. In the broader literature, empirical evidence suggests widespread negative effects of habitat loss on many taxa (i.e., reduced abundance or density), whereas the effects of fragmentation per se are generally much weaker and may vary strongly in magnitude and direction of response (Fahrig 2003). In agreement with this, forest cover is a better predictor of bat assemblage characteristics (species richness or composition) than are measures of landscape configuration in Neotropical landbridge island systems (Meyer and Kalko 2008a; Henry et al. 2010). On the other hand, consistent responses to landscape composition or configuration at the assemblage level were harder to identify in studies conducted in fragmented Neotropical"
365,139,0.985,Climate Smart Agriculture : Building Resilience To Climate Change,"where the BWI is the percentage of the surface that is liquid water (Basist et al. 2001), ÎÎµ, is empirically determined from global SSM/I measurements, Ts is surface temperature from station measurements, Tb is the satellite brightness temperature at a particular frequency (GHz), Ïn (n = 1, 2, 3) is a frequency observed by the SSM/I instrument, Î²0 and Î²1 are estimated coefficients that correlate the relationship of the various channel measurements with observed in situ surface temperature at the time of the satellite overpass. Specifically, as wetness values increase, the differences between the observed surface temperature and the observed channel measurements also increase (Williams et al. 2000). Weekly and monthly average BWI values are very good indicators of the magnitude of water near the surface, which has a relationship to water at greater depths. These observations have proven valuable in agricultural monitoring during the previous 25 years of analytical work. The wetness anomalies have proven valuable in predicting agricultural yields in many areas of the world (Curt Reynold USDA, personal correspondence). Research indicates the wetness product has a gamma distribution, much like precipitation (Gutman 1999); therefore a gamma distribution is used to derive the variation of wetness from the expected value. Since most regions of the world have annual cycles associated with their liquid water near the surface, it is best to calculate anomalies for each pixel, location and time of year. The resolution of the pixel is 33 km by 33 km, and anomalies are calculated on a monthly and weekly basis. A value of 0.01 means that only 1 year in a 100 would realize a value so low (extremely dry) at the location for a particular time of year. Conversely, a value of 0.99 corresponds with an excessively wet event that only occurs one out of a 100 years. In summary, values progressively less than 0.5 indicate increasingly drier conditions and values progressively greater than 0.5 indicate increasingly wetter conditions than the expected value (Fig. 1). The period of record for these wetness and temperature products begins in 1988 and they have been maintained in near real time for decades.1 There is a period of 2 years, 1990 and 1991, when the stability of the microwave satellite instrument was deemed unreliable. Therefore, these 2 years are removed from the analysis. The climatology we use is based on the 23 years of data from 1988 to 2010. A series of operational satellite instruments flown by the United States Meteorological Satellite Service comprise the period of observations. Great effort has been made to seam the observations between the various satellite instruments into one contiguous record. A daily set of observations is composed of 14 orbits across the globe. These observations are sun synchronous over the equator, at an overpass time around 6 a.m. and 6 p.m. every day. The morning and afternoon overpasses are processed independently and then combined together into one set of observations across the globe. Each set of observations is added to this record in near real-time, as both weekly and monthly fields of temperature and wetness values."
8,920,0.985,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","nomenological descriptions based on an observed picture of nature. For example, it is difficult to argue that, were the colour symmetry SU(2) and not SU(3), we would still observe the resonance dominance of hadronic spectra and could therefore use the bootstrap model. All present understanding of phases of hadronic matter is based on approximate models, which requires that Table 32.1 be read from left to right. I believe that the description of hadrons in terms of bound quark states on the one hand, and the statistical bootstrap for hadrons on the other hand, have many common properties and are quite complementary. Both the statistical bootstrap and the bag model of quarks are based on quite equivalent phenomenological observations. While it would be most interesting to derive the phenomenological models quantitatively from the accepted fundamental basisâthe Lagrangian quantum field theory of a non-Abelian SU(3) âglueâ gauge field coupled to coloured quarksâwe will have to content ourselves in this report with a qualitative understanding only. Already this will allow us to study the properties of hadronic matter in both aggregate states: the hadronic gas and the state in which individual hadrons have dissolved into the plasma consisting of quarks and of the gauge field quanta, the gluons. It is interesting to follow the path taken by an isolated quark-gluon plasma fireball in the ; T plane, or equivalently in the ; T plane. Several cases are depicted in Fig. 32.2. In the Big Bang expansion, the cooling shown by the dashed line occurs in a Universe in which most of the energy is in the radiation. Hence, the baryon density  is quite small. In normal stellar collapse leading to cold neutron stars, we follow the dash-dotted line parallel to the  axis. The compression is accompanied by little heating."
372,1257,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"band and subtracting these averages from the corresponding spectral line visibility data. Finally, it is necessary to correct for remaining instrumental phases and for the different atmospheric and ionospheric phase shifts, which may be large for widely separated sites. In imaging strong continuum sources, this can be achieved by using phase closure, as described in Sect. 10.3. A similar approach can be used in imaging a distribution of maser point sources, by selecting a strong spectral component that is seen at all baselines and assuming that it represents a single point source. Then if the phase for this component at one arbitrarily chosen antenna is assumed to be zero, the relative phases for the other antennas can be deduced from the fringe phases. Since these phases are attributed to the atmosphere over each antenna, the correction can be applied to all frequency components within the measured spectrum. This method of using one maser component to provide a phase reference is discussed in more detail in Sect. 12.7, together with fringe frequency mapping, a technique that is useful in determining the positions of major components in a large field of masers."
372,1599,0.985,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"the troposphere that move across an interferometer cause phase fluctuations that degrade the measurements. In the optical region, variations in temperature, rather than in water vapor content, are the principal cause of phase fluctuations. The situation is depicted in Fig. 13.10. A critical dimension is the size of the first Fresnel zone, )h; where h is the distance between the observer and the screen. For ) D 1 cm and h D 1 km, the Fresnel scale is about 3 m. The atmospherically induced phase fluctuations on this scale are very small ($ 1 rad). In this case, the phase fluctuation can cause image distortion but not amplitude fluctuation (i.e., scintillation). This is known as the regime of weak scattering. Plasma scattering in the interstellar medium belongs to the regime of strong scattering, where the phenomena are considerably more complex (see Sect. 14.4). The fluctuations along an initially plane wavefront that has traversed the atmosphere can be characterized by a so-called structure function of the phase. This function is defined as D+ .d/ D hÅË.x/ ! Ë.x ! d/,2 i ;"
311,2172,0.985,The Physics of the B Factories,"Then, Eqs (19.2.30) and (19.2.31) are added together, since in this particular measurement no distinction is made between two possible initial D0 flavors (for measurements of CP violation in such decays, where one tags the flavor of the initial D0 , see Section 19.2.7). We obtain:"
348,116,0.985,Control Theory Tutorial : Basic Concepts Illustrated By Software Examples,"I calculated the values for this controller by using the numerical minimization function in Mathematica to minimize the H2 cost, subject to the constraint that all transfer functions in Eq. 6.1 are stable. See the supplemental Mathematica code. Figure 6.5 compares the optimized system response for the unweighted and weighted cases. Panel a shows the Bode magnitude response of the optimized system for the unweighted case, equivalent to the plot in Fig. 6.3c. Panel b shows the response of the optimized system for the weighted case in this section. The weighted case emphasizes low-frequency load disturbances and highfrequency sensor noise, with low weight on midrange frequencies. Comparing the unweighted case in (a) with the weighted case in (b), we see two key differences. First, the weighted case allows a large rise in magnitudes and associated sensitivity to perturbations for midrange frequencies. That rise occurs because the particular weighting functions in this example discount midrange perturbations. Second, the gold curve shows that the weighted case significantly reduces the low-frequency sensitivity of system outputs, Î·, to load disturbances, d. The gold curve describes the response of the transfer function, G Î·d . Note that, because of the log scaling for magnitude, almost all of the costs arise in the upper part of the plot. The low relative magnitude for the lower part contributes little to the overall cost."
121,529,0.985,Becoming Citizens in a Changing World : IEA International Civic and Citizenship Education Study 2016 International Report,"Ã¼ 6WXGHQWVÃ¨SHUFHSWLRQVRIWKHLPSRUWDQFHRIFRQYHQWLRQDOFLWL]HQVKLS ,57VFDOHQDWLRQDOO\ standardized scores with averages of 0 and standard deviations of 1; see Chapter 5 for details) Ã¼ 6WXGHQWVÃ¨WUXVWLQFLYLFLQVWLWXWLRQV ,57VFDOHQDWLRQDOO\VWDQGDUGL]HGVFRUHVZLWKDYHUDJHV of 0 and standard deviations of 1; see Chapter 5 for details). Across the participating countries, the average percentage of students in the sample with valid GDWDZDV SHUFHQW7KHQDWLRQDODYHUDJHSHUFHQWDJHVUDQJHGIURPSHUFHQWLQWKH'RPLQLFDQ 5HSXEOLF WR  SHUFHQW LQ &KLQHVH 7DLSHL 0LQGIXO RI WKHVH PLVVLQJ YDOXHV ZH FRPSDUHG RXU results with those from models that used an alternative approach to the treatment of missing values, wherein students with missing values on variables received mean scores or median values, DQGPLVVLQJLQGLFDWRUYDULDEOHVZHUHDGGHGIRUHDFKYDULDEOH &RKHQ &RKHQ  %HFDXVH WKHUHJUHVVLRQFRHIÄFLHQWVIURPWKHWZRDSSURDFKHVZHUHDOPRVWLGHQWLFDOZHXVHGWKLVVLPSOHU approach of âlist-wiseâ exclusion of missing values. The results in this section of the chapter from three countriesâHong Kong (SAR), the Republic of Korea, and the Dominican Republicâshould be interpreted with caution: the surveys in Hong Kong (SAR) and the Republic of Korea did not meet the IEA sample participation requirements and are therefore reported in a separate section of the reporting tables; the results from the Dominican Republic are annotated because fewer than 70 percent of participating students had valid data. The multiple regression models were estimated using jackknife repeated replication to obtain correct standard errors (see Schulz, 2011). In a regression model, an estimate of the percentage of explained variance can be obtained by multiplying R2 by 100. Furthermore, in a multiple regression model the variance in the criterion variable can be explained by the combined effect of more than one predictor or block of predictors. By reviewing the contributions of different predictor blocks, we can estimate how much of the explained variance is attributable uniquely to each of the predictors or blocks of predictors, and how much these predictors or blocks of predictors in combination explain this variance. We carried out this estimation by comparing the variance explanation of four additional regression models (each without one of the four blocks of predictors) with the explanatory power of the overall model that included all predictors in combination.5 When interpreting the results from these analyses, readers should keep in mind that the ICCS scale VFRUHVDUHVWDQGDUGL]HGDWWKHQDWLRQDOOHYHO+HQFHUHJUHVVLRQFRHIÄFLHQWVVKRXOGEHLQWHUSUHWHG LQWHUPVRIHIIHFWVL]HZKLFKPHDQVWKDWWKHFRHIÄFLHQWVUHÄHFWFKDQJHVLQWKHVFRUHVIRUWKHWZR dependent variables (studentsâ expected electoral participation and studentsâ expected active political participation), with changes of one standard deviation in each of the participating countries. :KHQUHYLHZLQJWKHVL]HRIWKHUHJUHVVLRQFRHIÄFLHQWVUHDGHUVVKRXOGDOVRNHHSLQPLQGWKDWWKH"
283,188,0.985,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","5.1 Introduction Two of the important performance indicators for a linear code are the minimum Hamming distance and the weight distribution. Efficient algorithms for computing the minimum distance and weight distribution of linear codes are explored below. Using these methods, the minimum distances of all binary cyclic codes of length 129â 189 have been enumerated. The results are presented in Chap. 4. Many improvements to the database of best-known codes are described below. In addition, methods of combining known codes to produce good codes are explored in detail. These methods are applied to cyclic codes, and many new binary codes have been found and are given below. The quest of achieving Shannonâs limit for the AWGN channel has been approached in a number of different ways. Here we consider the problem formulated by Shannon of the construction of good codes which maximise the difference between the error rate performance for uncoded transmission and coded transmission. For uncoded, bipolar transmission with matched filtered reception, it is well known (see for example Proakis [20]) that the bit error rate, pb , is given by   pb = erfc (5.1) Comparing this equation with the equation for the probability of error when using coding, viz. the probability of deciding on one codeword rather than another, Eq. (1.4) given in Chap. 1, it can be seen that the improvement due to coding, the coding gain is indicated by the term dmin . nk , the product of the minimum distance between codewords and the code rate. This is not the end of the story in calculating the overall probability of decoder error because this error probability needs to be multiplied by the number of codewords distance dmin apart. For a linear binary code, the Hamming distance between two codewords is equal to the Hamming weight of the codeword formed by adding the two codewords together. Moreover, as the probability of decoder error at high NEb0 values depends on the minimum Hamming distance between codewords, for a linear binary code, Â© The Author(s) 2017 M. Tomlinson et al., Error-Correction Coding and Decoding, Signals and Communication Technology, DOI 10.1007/978-3-319-51103-0_5"
311,2083,0.985,The Physics of the B Factories,"The approach pioneered by Belle (Widhalm, 2006) for semileptonic decays of D0 mesons is used by the two B-factory experiments to measure absolute leptonic decay branching fractions of Ds mesons. As an example, in the BABAR analysis, the Ds meson production is tagged by considering events from the reaction e+ eâ â cc â Hc KXDsâ Î³; Hc is D0 , D+ , Dâ or Îc exclusively reconstructed, K a K + or KS0 and X a system of at most three pions, including at most one Ï 0 with a total electric charge appropriate to ensure the neutrality of the overall final state. The number of produced Ds mesons is obtained by considering the distribution in the recoil mass Mrecoil (Hc KXÎ³) which has a peak at the Ds mass for signal events. The hadron Hc is reconstructed using 15 modes. In addition to the size of the mass window, several other properties of the Hc candidate are used. The center-of-mass (CM) momentum of the Hc must be at least 2.35 GeV/c in order to remove B meson backgrounds. Particle identification requirements are used on the tracks, a cut is applied on the probability of the Hc vertex fit, and a minimum lab energy of Ï 0 photons is requested. Only Hc KXÎ³ candidates with a total charge, a charm, and a strange quark content consistent with recoiling from a Dsâ , are selected from which the signal yield is extracted. A kinematic fit to each Hc KX candidate is performed and the Hc mass is constrained to its nominal value. The 4-momentum of the signal Dsââ is extracted as the missing 4-momentum in the event. It is required that the Dsââ candidate mass be within 2.5Ï of the signal peak. A similar kinematic fit is performed with the signal Î³ included and with the mass recoiling against the Hc KX constrained to the nominal Dsââ mass in order to determine the Dsâ 4-momentum. It is required that the Dsâ momentum exceeds 3 GeV/c and that its mass be greater than 1.82 GeV/c2 . Having reconstructed the inclusive Dsâ sample one proceeds to the selection of Dsâ â Î¼â Î½ Î¼ events within that sample. The Hc KXÎ³ mass range between 1.934 and 2.012 GeV/c2 is used and it is required that there be exactly one more charged particle in the remainder of the event, and that it be identified as a Î¼â . In addition it is required that the extra neutral energy in the event, Eextra , be less than 1 GeV. Eextra is defined as the total energy of clusters in the electromagnetic calorimeter with individual energy greater than 30 MeV and not overlapping with the Hc KXÎ³ candidate. Since the only missing particle in the event should be the neutrino, the distribution of Eextra is expected to peak at zero for signal events. To extract the signal yield, the distribution of the mass squared of the system recoiling against the Hc KXÎ³Î¼â combination, Mrecoil (Hc KXÎ³Î¼â ) is used. The method is slightly modified in the updated measurement by Belle (Zupanc, 2013b), as described in Section 19.1.2.2. The result is shown in Fig. 19.1.40. To"
231,514,0.985,North Sea Region Climate Change Assessment,"The CMIP5 simulations show a small (about 1Â° for the multi-model ensemble means) poleward shift in the position of the Atlantic jet stream for the RCP8.5 scenario, while its speed remains nearly constant (Barnes and Polvani 2013). The poleward shift in the position of the Atlantic jet steam was found to reduce its north-south wobble as well as to enhance the variability of its speed (i.e. more of a pulsing of the jet stream). Woollings and Blackburn (2012) obtained consistent results based on the CMIP3 simulations, both with regard to a poleward shift in the mean position of the Atlantic jet stream and to considerable variations between individual models, particularly in winter. The poleward shifts were often small compared to the errors in the simulation of the jet stream position. Moreover, Woollings and Blackburn (2012) found that the NAO in combination with the East Atlantic pattern (EA) of the large-scale circulation can describe both the climatological changes and the interannual variations of both the position and strength of the Atlantic jet stream at the tropopause level. It is largely the NAO that describes shifts in the position of the jet, whereas the NAO and EA are both associated with changes in the strength of the jet. The mechanisms underlying a poleward shift in the jet stream are still not fully understood. Changes in the activity of large-scale planetary waves or in the characteristics of the synoptic-scale transient wave activity have been suggested to contribute to the poleward shift (e.g. Collins et al. 2013). Haarsma et al. (2013) found an eastward extension to the zonal winds at 500 hPa over the eastern Atlantic Ocean and western Europe, primarily related to changes in the tropospheric temperature proï¬le. The temperature changes in two regions were found to be important for forcing the changes in mean zonal flow: the relatively strong upper-tropospheric warming in the subtropics and the reduced surface warming in the mid-latitudes. Inter-model differences in the projected changes in mean zonal flow over the eastern Atlantic Ocean and western Europe could be partly attributed to uncertainties in the response of the North Atlantic Ocean to the anthropogenic forcing in both the CMIP3 and CMIP5 models."
36,35,0.985,Bats in the Anthropocene : Conservation of Bats in a Changing World,"effect of high and intermediate urban development, respectively. Random effect models provide an unconditional inference of a larger set of studies from which only a few are included in the meta-analysis and assumed to be a random sample (Viechtbauer 2010). We compared both models based on the reported effect size and assessed the proportion of heterogeneity of bat responses between high and intermediate urban development (Ï2 highly urban- Ï2 small urban/Ï2 highly urban). In a second approach, we pooled data from high and intermediate urbanisation categories to investigate if the potential of bats to adjust to urban environments is determined by phylogeny or rather functional ecology using a mixed model metaanalysis. For this analysis we classified bats according to their taxonomic family and genus, their predominant food item (fruits, nectar and insects), foraging mode (aerial, gleaning) and foraging space (narrow, edge and open, following Schnitzler and Kalko (2001)) and included these classifications as moderators in our mixed model meta-analysis. We further investigated in detail how each of the categorical moderators influences effect size. Further, focusing on aerial insectivores, the majority of study cases in our dataset, we then investigated if moderators influencing the adaptability to urban areas are consistent between North and South America versus Europe, Asia and Australia. P-levels for all models were assessed using a permutation test with 1000 randomizations. In none of our models did the funnel plot technique (Viechtbauer 2013) reveal any significant publication bias or asymmetry in our dataset (function: regtest, package metaphor)."
133,205,0.985,"A Demographic Perspective on Gender, Family and Health in Europe","investigate perceived health: Physical Component SummaryâPCS, Mental Component SummaryâMCS, and poor Self-Rated Healthâpoor SRH. The ï¬rst two measures are quantitative assessments of physical and mental health, positively oriented (the higher the score the better the health) derived from the Short Form-12 Health SurveyâSF-12 included in the Italian survey. SRH is a binary variable derived from the single-item question recommended by the World Health Organization (WHO), which asks: âHow is your health in general?â (De Bruin et al. 1996). Answers are distributed on a ï¬ve point scale: very good, good, fair, bad, very bad. We grouped these ï¬ve categories into two, creating a dummy variable (poor-SRH) of people reporting âpoorâ and âvery poorâ health conditions as opposed to those reporting âfairâ, âgoodâ or âvery goodâ conditions. This aggregation is due to the very unbalanced frequency distribution of respondents in the ï¬ve categories: only a very small proportion of the population were in the two extreme categories (very good/very poor), which would have produced unreliable estimations. The choice of aggregating the category âfairâ together with the positive modalities (âgoodâ and âvery goodâ) is due to its Italian translation, which has a quite positive, rather than a neutral, connotation (Egidi and Spizzichino 2006). We included individual and contextual (household) covariates in the analyses. As individual variables, we consider age, classiï¬ed as <50, 50â64, 65â74, 75+; gender; education (low, i.e. primary education or lower; medium, i.e. lower secondary; and high, upper secondary education or higher); disability (yes/no), according to the highest limitation grade in at least one item of OECD Long-Care Disability Questionnaire; and multichronicity (yes/no), deï¬ned as having three or more chronic illnesses diagnosed by a medical doctor. Finally, we created a new variable from the dataset intended to capture the burden of disability of one household member on the rest of the family. It is labeled cohabitation with disabled and has value one when the individual cohabits with at least one disabled person. Household covariates include: perceived economic status with categories good/ satisfactory versus inadequate; housing conditions, which is based on the presence of very basic housing problems (e.g. absence of heating) and is classiï¬ed as Good (no problems), Fair (1â2 problems), or Bad (more than 2 problems); household size with categories 2â3; 4; 5+ components; household structure with couple-headed families opposed to single-headed families; and city size with two classes based on the threshold of 50,000 inhabitants. We adopted a multilevel approach with which we are able to disentangle the proportion of variability on different levels, i.e. the variability between individuals due to differences between individual themselves or due to household or ALHU communalities. According to the outcome characteristics, we ran linear or logistic multilevel models, with random intercepts at the household and ALHU levels. For each of the three outcomes we ran different multilevel models: â¢ Empty model: the model without any covariates, to evaluate the proportion of variance to be accounted for at each level, by means of the Variance Partition Coefï¬cient (VPC)."
165,587,0.985,New Methods for Measuring and Analyzing Segregation,"Variance Analysis I now consider the relationship S = Î·Â² in more detail. I acknowledge that the expressions and relationships I introduce below are not particularly original. They have been noted elsewhere including, for example, in papers by Becker et al. (1978): 353) and White (1986:207) and also in statistical texts such as Blalock (1979: 81). The contribution of the discussion here is that it collects and calls attention to points not emphasized in most previous discussions. Duncan and Duncan (1955) noted that the separation index (S) (which they termed the variance ratio) is equivalent to the eta squared ( Î·Â² ) statistic from analysis of variance. More specifically, S is equal to Î·Â² for the analysis of how X, an individual-level binomial variable for race (coded 1 for Whites and 0 to Blacks), varies over areas. The value of S thus indicates the proportion of variation in race (X) that is âexplainedâ by area of residence. Under even distribution S will be 0 because the representation of Whites and Blacks in each area will exactly reflect each groupâs representation in the city overall and knowledge of area will not improve the prediction of race above the baseline of assuming the overall city average. Under complete segregation S will be 1 because area of residence will be homogeneous â either all White or all Black â and thus area will perfectly predict race. Intermediate success in prediction is quantified as the ratio BSS/TSS from analysis of variance where BSS is the âbetween group sum of squaresâ for individual deviations from the overall mean and TSS is âtotal sum of squaresâ for individual deviations from the overall mean. The overall mean for X is the proportion White in the city population (P) so TSS = â ( X k â P ) Â² with k used here to index individuals. Predictions for X are based on category means for X which in this case are equal to area proportion White (pi) so BSS = â ( p ik â P ) Â² with i here serving to index areas. Finally, for completeness, inability to explain X is quantified by WSS/TSS where WSS is the âwithin group sum of squaresâ given by WSS = â ( X i â p ik ) Â². It is useful to note at this point that the value of Î·Â² also is equal to the square of the individual-level bivariate correlation of race (X) and area proportion White (pi). Thus, one can interpret S as indicating the degree to which race determines area proportion White (p) for individuals as quantified by rÂ² from the regression of pi on X or of Î·Â² from the analysis of how pi varies by race. Either way, it is clear that the value of S revolves around the impact of race on contact with Whites at the individual level as reflected in the White-Black difference of means in contact with Whites (pi). Under even distribution explanation S will be 0 because all p i = P so the White and Black means for contact with Whites (pi) are the same and knowledge of race will not improve the prediction of contact with Whites (p) above the baseline of assuming the overall city average (P). Under complete segregation S will be 1"
311,1483,0.985,The Physics of the B Factories,"An inclusive measurement of b â sâ+ ââ is expected to have reduced theoretical uncertainties in comparison with the exclusive decays B â K (â) â+ ââ . The situation is similar to b â sÎ³, where understanding of exclusive decays is limited by knowledge of hadronic form factors. In the case of b â sâ+ ââ these reduced theoretical uncertainties apply not only to the inclusive branching fraction, but also to the angular information, e.g. the zero crossing point in the di-lepton forward-backward asymmetry AFB . The angular decomposition of the inclusive decay B â Xs â+ ââ provides three independent observables, HT , HA and HL , from which one can extract the shortdistance electroweak Wilson coeï¬cients that test for new physics (Lee, Ligeti, Stewart, and Tackmann, 2007)."
105,110,0.985,"Computer and information Sciences : 31St international Symposium, Iscis 2016, KrakÃ³w, Poland, October 27â28, 2016, Proceedings (Volume 659.0)",where NC is the number of characters in the result and DVP is the position within the result where the value of the dimension is shown. The Relevance Parameter Weight (RPW) incorporates the userâs perception of relevance by rewarding the ï¬rst attributes of the query Rt(n(t)) as highly desirable and penalising the last ones: RPW Â¼ 1
391,336,0.985,Ocean-Atmosphere Interactions of Gases and Particles,"Despite the success of the high-resolution numerical model in simulating realistic transfer processes of wind-generated waves, DNS of interfacial turbulent boundary-layer flow is still limited to low Reynolds number (Re of the order of 103). The wavelength of the simulated gravity wave is also restricted to centimetres scale, and the wind speeds are limited to low to immediate regime. This is due to the fact that in DNS of the turbulent boundary layer the computation grid must be fine enough to resolve all spatial scales of the velocity, temperature and dissolved gas fields. For isotropic turbulence, the number of spatial modes increases as Reynolds number to the power 3/4, meaning the number of computation grids increases as Re9/4. The minimum spatial scale of dissolved gas field is even smaller than that of velocity fluctuation (e.g., the diffusivities of dissolved CO2 and O2 are"
244,67,0.985,"Advancing Human Assessment : The Methodological, Psychological and Policy Contributions of Ets","where N and Nk denote the sample sizes for the total examinee group and for the subgroup of examinees obtaining total score yk and xi and y are the means of Item i and the test for the total examinee group. As described in Sect. 2.2.1, the point biserial correlation is a useful item discrimination index due to its direct relationship with respect to test score characteristics. In item analysis applications, ETS researcher Swineford (1936) described how the point biserial correlation can be a âconsiderably loweredâ (p. 472) measure of item discrimination when the item has an extremely high or low difficulty value. The biserial correlation (Pearson 1909) addresses the lowered point biserial correlation based on the assumptions that (a) the observed scores of Item i reflect an artificial dichotomization of a continuous and normally distributed trait (z), (b) y is normally distributed, and (c) the regression of y on z is linear. The biserial correla-"
151,113,0.985,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"In (12), C is concentration (g mâ3), Q is emission rate (g hâ1), Ï is the mathematical constant that is the ratio of the circumference of a circle to its diameter, U is wind velocity (m hâ1) and Ïy and Ïz are respectively the horizontal (crosswind) and vertical Pasquill-Gifford dispersion parameters (m) that depend on downwind distance (km) and atmospheric stability class. This equation must be applied with caution because of variation of U as a function of height and topography, but it is used here to suggest the form of an appropriate correlation. Q can be estimated from the total quantity applied and an assumed fraction volatilized during a specified time period. Plots of Ïy and Ïz(m) versus downwind distance x (km) have been given (Turner 1994), and can be expressed as correlations for stability class C (13):"
49,67,0.985,Artificial Intelligence and Cognitive Science IV,"in terms of Euclidean distance. The index of the winning spatial group wt , w â {0, ... , k } is appended to a time ordered list S if wt â  wt â1 . The node ignores repeating states both in training and recognition modes for the reasons explained in Section 3.3. The time ordered list of indices (a sequence of indices) S represents the training data for a Temporal Data Mining algorithm searching for frequent episodes within S. wt represents the state of the receptive field of the node in time t and S represents the recording of the transitions between the states. The Temporal Data Mining algorithm used in this work is described in [7]. It is based on the frequent episode discovery framework [13]. It searches for frequent episodes with variable length. The frequent episodes identified by N are stored in a list E of lists { E0 , ... , E Ne-1 } , where Ne is the number of the identified frequent episodes. The user determines the minimal length of the frequent episode to be stored. It is ensured that the shorter episodes are not contained in the longer episodes because it would create undesired ambiguity. Operation of N in recognition mode is divided into two consecutive stages. First, a novel input RF t is categorized into one of the spatial groups identified in the training process RF t â wt . If wt â  wt â1 , wt is appended to the list BS (the buffer stack) and the oldest item of BS is deleted. Constant length of BS is thus maintained. BS can be seen as a short term memory because it records the recent changes of states of the receptive field of the node. The length of BS is defined by the user. The elements of BS are initialized to -1 at the start of the algorithm. -1 does not appear in the stored frequent episodes therefore BS cannot be found in any of them before it is filled with valid values after start or after reset. Second, in the given time step, the node tries to find which of the frequent episodes stored in E contains BS (in direct and reverse order). The purpose is to recognize whether the sequence of the recent changes in the receptive field has been frequently observed before. The output of N in time t is a binary vector O t . The elements of O t correspond to the stored frequent episodes. If Ei contains BS in the given time step, the i-th element of O t is set to 1 otherwise it is set to 0. If Ei is shorter than BS the corresponding number of older items in BS is ignored and the matching is performed with the shortened buffer stack. There are several conditions modifying the behavior of a node in recognition mode. The node can be active (flag A = 1) or inactive (flag A = 0), with nodes initially starting with A = 1. The conditions are checked in each time step. If wt = wt â1 the counter Tidle is incremented by 1. O t will be equal to O t â1 . If Tidle exceeds a user defined timeout constant Tout, A is set to 0, and the elements of O t are set to 0. The node remains inactive until there is a significant change in its input ( wt â  wt â1 ). If that happens, the node is reset: A is"
297,668,0.985,The R Book,"These functions are useful in a wide range of disciplines. The parameters a and b are easy to estimate from data because the function is linearized by a logâlog transformation, log(y) = log(ax b ) = log(a) + b log(x), so that on logâlog axes the intercept is log(a) and the slope is b. These are often called allometric relationships because when b = 1 the proportion of x that becomes y varies with x. An important empirical relationship from ecological entomology that has applications in a wide range of statistical analysis is known as Taylorâs power law. It has to do with the relationship between the variance and the mean of a sample. In elementary statistical models, the variance is assumed to be constant (i.e. the variance does not depend upon the mean). In ï¬eld data, however, Taylor found that variance increased with the mean according to a power law, such that on logâlog axes the data from most systems fell above a line through the origin with slope = 1 (the pattern shown by data that are Poisson distributed, where the variance is equal to the mean) and below a line through the origin with a slope of 2. Taylorâs power law states that, for a particular system:"
311,842,0.985,The Physics of the B Factories,"In the past few years, the experimental and lattice QCD inputs necessary to calculate |Vtd | and |Vts | to good precision have become available. The B Factories have contributed measurements of Îmd , the mass diï¬erence between the neutral Bd mass eigenstates, and branching"
165,492,0.985,New Methods for Measuring and Analyzing Segregation,"expected since households in the two minority groups are distributed randomly in relation to each other over the entire course of the simulation. The box plots in the bottom left graph depict the distribution of scores for the standard version of the index of dissimilarity (D) for majority-minority segregation. This shows that D is very high at the beginning of the experiment and then falls sharply as households move randomly for ten cycles. But D does not fall to zero due to the intrinsic bias in D. Thus, the final level of D essentially reflects a âbootstrapâ estimate of the expected value of D (E[D]) for majority-minority segregation under random assignment. The box plots in the bottom right graph depict the distributions of scores for D for minority-minority segregation. These reflect only random residential variation over the course of simulation. The surprising finding here is that D"
311,1738,0.985,The Physics of the B Factories,"It is both important and diï¬cult to compare measurements of the e+ eâ â J/Ï X cross section with theory. This is especially true in the case where the recoil system X does not include open or hidden charm (âe+ eâ â J/Ï non-ccâ), as this allows the NRQCD frameworkâ with universal matrix elements describing production in e+ eâ , pp, and other environmentsâto be tested (Section 18.1.4.1). The measurements are described in detail in Section 18.2.4.2; here we treat problems of interpretation. The main pitfall is the selection requiring more than four reconstructed tracks. As described above, B Factory e+ eâ â J/Ï X analyses impose such a requirement to suppress low-multiplicity events of QED origin, which are numerous and poorly understood. While the physics of QED events is straightforward, practical measurement requires control of cases where tracks are missed or misreconstructed, and where beam-background tracks are added to the event, together with photon conversions and bremsstrahlung. The lack of coverage close to the beamlines, and the trigger conditions, are key limitations: see Chapter 2 for the design of the experiments; for the forward-peaked cross-section of QED processes, in a simple case (initial state radiation), see the discussion in Section 21.2.1. The requirement of more than four reconstructed tracks must be taken into account when comparing measurements with theoretical predictions: 1. For double charmonium production, e+ eâ â J/Ï cc, this is straightforward: both collaborations quote results for Ï Ã B>2 (Table 18.2.7), where the factor B>2 describes the fraction of cc decays to final states with more than two charged particles. (J/Ï is reconstructed only in the decay to a lepton pair â+ ââ .) 2. The first B Factory measurements of e+ eâ â J/Ï X quoted the cross section directly, attempting to correct for the eï¬ect of track requirements. The initial BABAR analysis (Aubert, 2001b) required more than four tracks in the J/Ï â e+ eâ case (i.e. not for J/Ï â Î¼+ Î¼â ), with additional selections to suppress e+ eâ â Î³ISR J/Ï and Î³ISR Ï(2S). The initial Belle analysis (Abe, 2002n) required more than four tracks in all events (the same condition used by more recent analyses), with additional selections to suppress e+ eâ â Î³Ï(2S)(â Ï + Ï â J/Ï ). Both experiments incorporated these requirements into their eï¬ciency calculations, making assumptions about the angular distribution and polarization of these events, the fraction of recoil systems X containing charm, and the mix of hadronic final states within the system X in the lightquark case. All of these were poorly known at the time;"
297,783,0.985,The R Book,"It is important to realize that x starts at r and increases from there (obviously, the rth success cannot occur before the rth attempt). The density function dnbinom(x, size, prob) represents the number of failures x (e.g. tails in coin tossing) before size successes (or heads in coin tossing) are achieved, when the probability of a success (a head) is prob. Suppose we are interested in the distribution of waiting times until the ï¬fth success occurs in a negative binomial process with p = 0.1. We start the sequence of x values at 5:"
151,307,0.985,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"categorize risk as follows: (1) If the area under the risk curve was less than the AUC associated with the curve produced by risk products (risk product = exceedence probability Ã magnitude of effect) of 0.25% (e.g., 5% exceedence probability of 5% or greater effect = 0.25%), then the risk was categorized as de minimis. The AUC for risk products of 0.25% is 1.75%; (2) If the AUC was equal to or greater than 1.75%, but less than 9.82% (i.e., the AUC for risk products of 2%), then the risk was categorized as low; (3) If the AUC was equal to or greater than 9.82%, but less than 33% (i.e., the AUC for risk products of 10%), then the risk was categorized as intermediate; and (4) If the AUC was equal to or greater than 33%, then the risk was categorized as high. The risk curves defined by risk products of 0.25, 2 and 10% are shown graphically in Fig. 3. Categories of risk were based on a rationale described previously (Moore et al. 2010a, b) and included several considerations: (1) Losses of small numbers of individuals from a local population should not adversely affect the population (Giddings et al. 2005; Moore 1998). One of the foundations of hierarchy theory (Allen and Starr 1982) is that effects at lower levels of ecological organization (e.g., organism level) are not necessarily transmitted to higher levels of ecological organization (e.g., population level); (2) Although there are exceptions, an adverse effect level of 10% is unlikely to be ecologically significant to a local population. Such an effect generally cannot be reliably confirmed by field studies (Moore 1998; Suter et al. 2000); (3) Based on an analysis of EPA regulatory practice, Suter et al. (2000) concluded that decreases in an ecological assessment endpoint of less than 20% are generally acceptable; (4) The curve corresponding to a risk product of 2% passes through the points corresponding to a very low probability (i.e., 10%) of 20% or greater effect, and a low probability (i.e., 20%) of 10% or greater effect. Thus, based on the considerations described above, if risk products are generally less than the 2% boundary for an exposure scenario, then it can almost certainly be considered a low risk scenario; (5) The curve corresponding to a risk product of 10% passes through the points corresponding to a median probability (i.e., 50%) of 20% or greater effect and a 20% probability of 50% or greater effect. In this assessment, exposure scenarios with risk products generally above the 10% boundary were"
297,804,0.984,The R Book,"We have a square matrix A and two column vectors X and K, where AX = K , and we want to discover the scalar multiplier Î» such that AX = Î»X. This is equivalent to (A â Î»I)X = 0, where I is the unit matrix. This can only have one non-trivial solution when the determinant associated with the coefï¬cient matrix A vanishes, so we must have |A â Î»I | = 0. When expanded, this determinant gives rise to an algebraic equation of degree n in Î» called the characteristic equation. It has n roots Î»1 , Î»2 , . . . , Î»n , each of which is called an eigenvalue. The corresponding solution vector Xi is called an eigenvector of A corresponding to Î»i . Here is an example from population ecology. The matrix A shows the demography of different age classes: the top row shows fecundity (the number of females born per female of each age) and the sub-diagonals show survival rates (the fraction of one age class that survives to the next age class). When these numbers are constants the matrix is known as the Leslie matrix. In the absence of density dependence the constant parameter values in A will lead either to exponential increase in total population size (if Î»1 > 1) or exponential decline (if Î»1 < 1) once the initial transients in age structure have damped away. Once exponential growth has been achieved, then the age structure, as reï¬ected by the proportion of individuals in each age class, will be a constant. This is known as the ï¬rst eigenvector. Consider the Leslie matrix, L, which is to be multiplied by a column matrix of age-structured population sizes, n: L <- c(0,0.7,0,0,6,0,0.5,0,3,0,0,0.3,1,0,0,0) L <- matrix(L,nrow=4) Note that the elements of the matrix are entered in columnwise, not row-wise sequence. We make sure that the Leslie matrix is properly conformed:"
231,777,0.984,North Sea Region Climate Change Assessment,"inflows into the North Sea and a regional ecosystem model to quantify biogeochemical and foodweb variables (Maar et al. 2013). Model results were tested against the long, spatially-resolved time series from the CPR and against detailed seasonal sampling of vertical distribution of life history stages. Increasing temperature is a major factor in observed changes in Calanus phenology, but changes in abundance are also influenced by advection of C. ï¬nmarchicus through the northern boundary of the North Sea, which is to some degree climate-related. The detailed observational time series available for the North Sea allow testing of quite complex process models on scales encompassing regional physical dynamics, water column processes and species life history. Observed changes in distribution and phenology are consistent with global patterns (Poloczanska et al. 2013), but there are important processes occurring at regional and local scales that modify the simple global pattern. It is probably too early to judge whether adaptive responses by marine zooplankton will keep pace with the current rapid changes in climate (Dam 2013). The observational time series for North Sea plankton are longer and have better temporal and spatial coverage and resolution than any other in the world. Since many taxa have plankton life stages these time series can be used to analyse long-term change not only in holoplankton, but also in meroplankton, including benthic species and ï¬sh, and examples are given in later sections."
311,2757,0.984,The Physics of the B Factories,"where Eb and E â² are the energies of the incident and recoiling (tagged) electrons and Î¸ is the scattering angle of the tagged electron. The formation of a resonance in the single-tag process is a fertile field of study at the B Factories, especially in the high-Q2 region where the production cross section is highly suppressed. We impose a kinematic requirement on the single-tag mode that all the final state particles be observed except for the non-tagged incident electron (which can be inferred from a missing-mass constraint). In the B Factory experiments, the pseudoscalar transition form factors have been measured in single-tag modes, and are discussed in section 22.7. It is also possible to produce spin-1 (axial-vector) mesons in the single-tag process, although such measurements have not been reported from the B Factories. 22.1.5 Monte-Carlo Techniques No general-purpose Monte-Carlo (MC) generator for twophoton processes was available during the running of the B Factories. For exclusive two-photon processes, we can explicitly specify the combination of the final-state particles exclusively as well as the W distribution used in the event generation. In addition, angular distributions must be specified in the event generation. For these purposes, several MC generators to simulate a resonance or an exclusive final-state system from two-photon collisions, such as TREPS (Uehara, 1996), GGRESRC (Druzhinin, Kardapoltsev, and Tayursky, 2010), Gamgam (Aubert, 2010g) etc., are prepared and are used in the analysis. In these generators, one can specify functional shapes of distributions for W , Q2 and angles of final-state particles, or they can be generated by built-in default functions. Usually, an equivalent-photon approximation is adopted for zero-tag event generation while a Q2 distribution not based on EPA is sometimes used for the single-tag cases. Even for the non-EPA cases, we can"
281,26,0.984,Stochastics of Environmental and Financial Economics (Volume 138.0),"4.1 The Energy Dissipation In [31] it has been shown that exponentials of trawl processes are able to reproduce the main stylized features of the (surrogate) energy dissipation observed for a wide range of datasets. Those stylized features include the one-dimensional marginal distributions and the scaling and self-scaling of the correlators. The correlator of order ( p, q) is defined by c p,q (s) ="
372,1386,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"iterative #2 minimalization in which # represents the difference between observed visibility and the visibility of a model that is developed. Calculation of gradients of #2 can be used as an aid in the minimalization. Bhatnagar et al. (2013) expand these concepts to cover a wider frequency bandwidth and develop an A-projection algorithm that includes variation of the model parameters as a function of frequency. A wide bandwidth ratio (e.g., 2 W 1) improves the sensitivity of the observations but requires careful consideration since in the outer parts of the beam, the response varies rapidly with frequency. On wideband, wide-field imaging, see also Rau and Cornwell (2011). An extension of the A-projection technique called fast holographic deconvolution, which is particularly useful for very wide field-of-view observations, has been developed by Sullivan et al. (2012)."
311,2958,0.984,The Physics of the B Factories,"Stimulated by the flurry of experimental activity on the pentaquark front, other models of the âquark clusterâ type soon appeared. The first of these, due to Karliner and Lipkin (2003), divided the pentaquark constituents into a di-quark and a tri-quark cluster with the quarks of identical flavor in diï¬erent clusters. Each cluster has isospin zero and is a color non-singlet (separating the pairs of identical flavor); one unit of orbital angular momentum then yields IJ P = 0 12 as expected for the lowest antidecuplet, and the centrifugal barrier keeps the clusters beyond the range of the repulsive color-magnetic force. The individual clusters bind together as a result of colorelectric forces. The model yields a Î(1540)+ mass estimate of â¼ 1.59 GeV/c2 , and an anti-decuplet mass splitting which is only â¼ 50 MeV/c2 . This is a quark-based model which led to a resonant S = +1 baryon state in the vicinity of KN threshold. A second model of this type is due to Jaï¬e and Wilczek (2003). The Î(1540)+ is described in terms of two ud diquarks and a bachelor s quark. The ground state diquarkdiquark-antiquark configuration leads to a degenerate octet and anti-decuplet whose symmetry is broken as a result of the strange quark mass, leading to mixing of the two multiplets. Incorporating the Î(1540)+ as the Y = 2 member of the anti-decuplet leads to a somewhat diï¬erent spectroscopy than that of Diakonov, Petrov, and Polyakov (1997), and in particular yields a J P = 1/2+ nucleon state at a mass lower than the Î(1540)+ which is associated with the broad Roper resonance.182 However, the predicted mass of the Î5 (1860) state is more than 100 MeV below the mass of the state claimed by the NA49 experiment."
30,67,0.984,Determinants of Financial Development,"Note: The models are estimated by OLS. The dependent variable is FD, over 1990â99. The t-values are reported in brackets. Variable descriptions are from Appendix Table A2.1. The standardized coefficients show the change of a standard deviation of FD due to a one standard deviation change in a variable for those other than initial GDP and population, binary variables. â , ââ and âââ significant at 10%, 5% and 1%, respectively."
372,1702,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"(1968) and Hagfors (1976). Beynon (1975) gives interesting historical anecdotes on the early development of ionospheric research. In this section, we treat only those aspects of the ionosphere that have a deleterious effect on interferometric observations. Table 14.1 gives the magnitude of various propagation effects for the daytime and nighttime ionosphere. Most of these effects scale as ! !2 , and they can be minimized by observing at higher frequencies. For small zenith angles, the magnitude of the ionospheric excess path typically equals that of the neutral atmosphere at approximately 2 GHz, but the frequency of this equality can vary from about 1 to 5 GHz. Thus, at 20 GHz and small zenith angles, the ionospheric excess path length is typically only 1% of the tropospheric excess path length. However, at very large zenith angles, i.e., near the horizon, the effects are equal at about 300 MHz."
8,456,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","This low and energy independent multiplicity does not of course concern the final one of pions, etc. In fact, N is the average number of more or less excited particles formed in the first instant (resonances and/or fireballs) which afterwards decay. The actual increase in multiplicities, which is experimentally observed, must be interpreted as an increase in the excitation of the fireballs. The energy independence of N and T? implies immediately [by Eq. (19.47)] that the average kinetic energy stored in the transverse motion is itself independent of the primary energy [although strongly fluctuating from event to event, see Eq. (19.19)]. Since the probability of finding just N particles is W.N/ D P N ;"
372,1556,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"The angle of refraction can also be calculated for more realistic cases. Ignore the curvature of the Earth, and consider the atmosphere to consist of a large number of plane-parallel layers numbered 0 through m, as shown in Fig. 13.4. Let the index of refraction at the surface be n0 , and at the top layer, nm D 1. Applying Snellâs law to"
311,538,0.984,The Physics of the B Factories,"the same helicity. For example, if one of the ï¬nal state particles has spin 0 and hence its helicity is 0, the helicity of the other ï¬nal state particle must also be equal to 0: it is longitudinally polarized. Let us now focus on the case where at least one of the direct decay products of M0 has spin 1 (a vector or axial vector particle) and the other a spin greater or equal to 1. If M1 or M2 is of spin 1, h can take three values: â1, 0, and +1. There is one complex amplitude Ah associated with each case: the longitudinal amplitude A0 and the transverse ones A+1 and Aâ1 . The three amplitudes (A0 , A+1 , Aâ1 ) correspond to helicity eigenstates and deï¬ne the helicity basis. For a CP eigenstate, the longitudinal amplitude is CP even, while the transverse ones are an admixture of CP even and CP -odd components. In the transversity basis (AL , A , Aâ¥ ), the amplitudes correspond to CP eigenstates: CP -even longitudinal CP -even transverse CP -odd transverse"
311,1411,0.984,The Physics of the B Factories,"Table 17.9.3. Measured B â Xs Î³ inclusive branching fractions (in 10â6 ) for several photon energy (EÎ³ ) thresholds, 1.7 GeV and larger. Errors are statistical, systematic and model-dependence (if applicable); if there is no third error, the model dependence is included in the systematic error. The column with EÎ³ > 1.6 GeV contains HFAGâs extrapolations (see text) from the lowest measured threshold (HFAGâs reciprocal factors are shown in the bottom row.) and their computed world average. The measurements are in the order they are described in the text. The CLEO result is taken from HFAG, who corrected CLEOâs published value for the entire spectrum to the value at the listed threshold. The Belle sum-of-exclusive result, which is obtained with EÎ³ > 2.24 GeV and is corrected by HFAG, is listed only for the sake of the HFAG extrapolated value and average. All averages above measured threasholds assume errors are uncorrelated. For the HFAG world average of extrapolated values the ï¬rst error combines statistics and systematics (assumed uncorrelated between experiments), while the second error is from shape function systematics (assumed fully correlated). Measurement Belle un- & lepton tag BABAR lepton tag BABAR reco.-B tag Belle sum-of-excl. BABAR sum-of-excl. Average (Extrapolation factor)"
241,780,0.984,Second Assessment of Climate Change for the Baltic Sea Basin,"The projected reductions in salinity in the Bornholm Basin and Gotland Basin are almost constant with depth and amount to 1.5â2 g kgâ1 in the ensemble mean (Fig. 13.5). However, change in the deep-water is greater than that in the"
32,111,0.984,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","S&P500/DJIA grows up from 1 to 1:2 and then decreases back to 1 (Fig. 5.3b). With the aid of zero-crossing calculations and fitting, the IMF c9 reveals that the variations of the ratios in the scale of 8-year cycle behave as a damped oscillation in the form of exp Å .tn t0 /= Â with damping factors  7183 days (NASDAQ/DJIA) and 138,471 days (S&P500/DJIA) determined from the local minima of IMF c9 . Thus, the combination of c9 and r9 shows the converge of oscillations to values 2 and 1 for NASDAQ/DJIA and S&P500/DJIA, respectively. Meanwhile, the IMF c8 corresponding to (2â4)-year cycle is accompanied with frequency modulation in late of 1990s, implying the trigger of the anomaly in amplitude change and its recovery to regular situation lasts 1:5 oscillatory cycles, about 4â6 years. Since this anomaly does not appear in IMF c9 , it is a local event in time with time scale less than 8-year cycle. Here we should remark that the nature of the EMD method is adaptive. It"
311,275,0.984,The Physics of the B Factories,"criterion is a maximum vertex Ï2 of 20 per degree of freedom (since a track contributes two degrees of freedom, the BABAR criterion is substantially tighter than the Belle criterion). In Belle tracks that have been identiï¬ed as high pT leptons by the ï¬avor tagging algorithm are always kept since those have a large probability to originate from the Btag vertex. If the beamspot is used as a constraint in the Btag vertex reconstruction, even vertices with a single track can be reconstructed. The experiments exploit the beamspot diï¬erently. In Belle the constraint is an ellipsoid in the xy plane, increased in size to account for the Btag transverse motion, as explained in Section 6.4. This use of the beamspot leads to a small bias that is proportional to the Btag decay time and is treated as a systematic uncertainty. In BABAR the Btag direction and origin are reconstructed with a vertex ï¬t using the Brec vertex and momentum and the calibrated beamspot position and Î¥ (4S) momentum. This Btag âpseudo-particleâ is subsequently used as any other track in the Btag vertex reconstruction. The advantage of this approach is that there is no bias due to the beamspot constraint. However, it can only be applied to analyses with a fully reconstructed Brec . Since the Btag vertex has in general fewer tracks than the Brec vertex and may be contaminated by D daughter tracks, the Îz resolution is dominated by the Btag z position resolution. The latter is in the range 100 â 200 Î¼m, which has to be compared to a typical resolution of the Brec vertex of 50 Î¼m. As the total resolution is of the order of the B mixing period, accurate knowledge of the resolution is essential when Ît is used in maximum likelihood ï¬ts to extract the parameters for time-dependent CP violation. The calibration of the so-called resolution function is discussed below. 6.5.2 From vertex positions to Ît To be sensitive to time-dependent CP violating eï¬ects the vertex resolution must be suï¬cient to resolve the oscillations due to B 0B 0 mixing in the decay time distribution. Given a proper decay time t and a momentum vector p, the diï¬erence between the production and decay vertex positions of a B meson is given by xdecay â xprod ="
233,594,0.984,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"inclusion of the countryâs biodiversity. In this study, we use Madagascarâs largest endemic plant family, Sarcolaenaceae, as a model to identify areas with high diversity and to explore the potential conservation importance of these areas. Using phylogenetic information and species distribution data, we employ three metrics to study geographic patterns of diversity: species richness, Phylogenetic Diversity (PD) and Mean Phylogenetic Diversity (MPD). The distributions of species richness and PD show considerable spatial congruence, with the highest values found in a narrow localized region in the central-northern portion of the eastern humid forest. MPD is comparatively uniform spatially, suggesting that the balanced nature of the phylogenetic tree plays a role in the observed congruence between PD and species richness. The current network of PAs includes a large part of the familyâs biodiversity, and three PAs (Ankeniheny Zahamena Forest Corridor, the Bongolava Forest Corridor and the Itremo Massif) together contain almost 85 % of the PD. Our results suggest that PD could be a valuable source of complementary information for determining the contribution of Madagascarâs existing network of PAs toward protecting the countryâs biodiversity and for identifying priority areas for the establishment of new parks and reserves. Keywords Protected areas â¢ Extinction â¢ Endemism â¢ Biodiversity â¢ Species richness"
8,922,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","32.2 Strange Particles in Hot Nuclear Gas My intention in this section is to establish quantitatively the different channels in which the strangeness, however created in nuclear collisions, will be found. In our following analysis (see [6]) a tacit assumption is made that the hadronic gas phase is practically a superposition of an infinity of different hadronic gases, and all information about the interaction is hidden in the mass spectrum .m2 ; b/ which describes the number of hadrons of baryon number b in a mass interval dm2 and volume V  m. When considering strangeness-carrying particles, all we then need to include is the influence of the non-strange hadrons on the baryon chemical potential established by the non-strange particles. The total partition function is approximately multiplicative in these degrees of freedom: ln Z D ln Z non-strange C ln Z strange :"
230,305,0.984,Marine Anthropogenic Litter,"et al. 2004; Fig. 7.1). Subsequent research showed that similar sized particles were present in shallow waters around Singapore (Ng and Obbard 2006). However, while these early reports referred to truly microscopic particles they did not give a specific definition of microplastic. In 2008, the National Oceanographic and Atmospheric Agency (NOAA) of the US hosted the first International Microplastics Workshop in Washington and as part of this meeting formulated a broader working definition to include all particles less than 5 mm in diameter (Arthur et al. 2009). Particles of this size (i.e. <5 mm) have been very widely reported including publications that considerably pre-dated the use of the term âmicroplasticsâ (Carpenter et al. 1972; Colton et al. 1974). There is still some debate over the most appropriate upper size bound to use in a formal definition of microplastics, with perhaps a more intuitive boundary following the SI classification of <1 mm. The European Union have followed the US and adopted a 5-mm upper bound for categorization of microplastics within the Marine Strategy Framework Directive (MSFD, Galgani et al. 2010). There is a similar lack of clarity when considering the lower size bound for a definition of microplastics. Operationally, this, by default, has been assumed to be the mesh size of the particular net or sieve used to separate the microplastic from the bulk medium of sediment or water column (see review by Hidalgo-Ruz et al. 2012). However, as a necessity of construction, collection devices with meshes in the sub-millimetre size range have a high ratio of net/sieve material compared to apertures and as a consequence they will trap particles much smaller than the size of the apertures/mesh size. Hence, it is not sensible to define the minimum size captured on the basis of the mesh used to collect the sample. Within the EU MSFD a pragmatic approach has been taken based on that used by researchers sampling benthic infauna and sediments with sieves (e.g. Wentworth graduated sieves), where the organisms âretainedâ by a particular sieve are reported. In summary, there is no universally agreed definition of microplastic size, but most workers consider microplastic to be particles of plastic <5 mm in size. There is little consensus on the lower size bound. While defining parameters is essential for consistent monitoring, in the wider context of marine debris and concerns about the potential harmful effects of microplastic it may actually be unwise to specify the size definitions precisely at the present time. Differently sized particles are likely to have differing effects. For example, smaller particles could have consequences that are fundamentally different to larger particles, since the particles themselves can accumulate in tissues and/ or may cause disruption of physiological processes (Browne et al. 2008; Wright et al. 2013c). From a monitoring science, rather than a curiosity-driven perspective, a logical rationale for sampling is to consider abundance in relation to any associated impacts. Since our understanding of the potential impacts of microplastics is currently in its infancy it could, for the time being, be unwise to set a formal limit to lower size boundary and, until there is better understanding about which types/sizes of microplastics are of concern a sensible strategy could be to collect from the bulk medium any particles <5 mm and then quantify microplastics according to size categories."
311,3044,0.984,The Physics of the B Factories,allows Vtââ² b Vtâ² s â¼ O(Î») which is a much weaker bound than the constraints obtained from other measurements such as Bs0 â B 0s oscillation. Another example is D0 âD0 mixing where the bâ² quark could contribute in the loop diagram in addition to the SM particles. Saturating the experimental value of xD gives
297,1529,0.984,The R Book,"Model checking involves the use of plot(model2). As you will see, there is no pattern in the residuals against the ï¬tted values, and the normal plot is reasonably linear. Point no. 4 is highly inï¬uential (it has a large Cookâs distance), but the model is still signiï¬cant with this point omitted. We conclude that the proportion of animals that are males increases signiï¬cantly with increasing density, and that the logistic model is linearized by logarithmic transformation of the explanatory variable (population density). We ï¬nish by drawing the ï¬tted line though the scatterplot: windows(7,7) xv <- seq(0,6,0.01) yv <- predict(model2,list(density=exp(xv)),type=""response"")"
241,742,0.984,Second Assessment of Climate Change for the Baltic Sea Basin,"11.10 Conclusion Several numerical climate change simulations have been undertaken since the ï¬rst BACC assessment (BACC Author Team 2008). Models now operate at higher horizontal resolution. Furthermore, the simulations cover a larger degree of the uncertainty range including: a wider range of emission scenarios (sampling the uncertainty in forcing), more climate models (addressing model uncertainty) and ensemble members (addressing natural variability). The picture emerging from these simulations conï¬rms the ï¬ndings of previous studies (e.g. BACC Author Team 2008) in terms of climate change in the Baltic Sea region. Climate model studies suggest that â¢ The future climate will get warmer, especially in winter. Changes increase with time and/or rising emissions of GHGs. There is a large spread between the different models, but they all project warming. â¢ Cold extremes in winter and warm extremes in summer are expected to change more than the average conditions, implying a narrower (broader) temperature distribution in winter (summer). â¢ Future precipitation will be higher than today. The increase is projected to be greatest in winter. In summer, models project an increase in the far north and a decrease in the south. For the transition zone between these two regions, the sign of change is uncertain. â¢ Precipitation extremes are expected to increase although with a higher degree of uncertainty compared to the projected change in temperature extremes. â¢ Future changes in wind speed are highly dependent on changes in the large-scale atmospheric circulation simulated by the GCMs. The results diverge and it is not possible to estimate whether there will be a general increase or decrease in wind speed in the future. A common feature of many model simulations, however, is an increase in wind speed over oceans that are ice-covered in the present climate but not in the future. Future changes in extreme wind speed are uncertain. â¢ The increased number of high-resolution regional model studies driven by many different GCMs has enabled a tighter connection with hydrological models, even though various forms of bias correction are necessary as an interface between the two types of model. Furthermore, the large number of available simulations enables some estimation of uncertainty of impacts. â¢ Statistical downscaling studies using atmospheric predictors have addressed several predictands, with the greatest emphasis given to hydrological variables. The ï¬ndings of detailed studies have been in line with those from studies employing dynamical downscaling."
372,444,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"frequency and obtain Å!e .u C &u/ cos Ä±) "" .!e &u cos Ä±/ D .!e u cos Ä±/. The fringe frequency when tracking is thus the same as for the central points A1 and A2 of the apertures. (This is true for any pair of points; choosing one point at an antenna center in the example above slightly simplifies the discussion.) Thus, if the antennas track, the contributions from all pairs of points within the apertures appear at the same fringe frequency at the correlator output. As a result, such contributions cannot be separated by Fourier analysis of the correlator output waveform, and information on how the visibility varies over the range u "" d% to u C d% is lost. However, if the antenna motion differs from a purely tracking one, the information is, in principle, recoverable. In imaging sources wider than the antenna beams, an additional scanning motion to cover the source is added to the tracking motion. In effect, this scanning allows the visibility to be sampled at intervals in u and v that are fine enough for the extended width of the source. This technique, known as mosaicking, is described in Sect. 11.5. To accommodate the effects that result when the antennas track the source position, the normalized antenna pattern is treated as a modification to the intensity distribution, which then becomes AN .l; m/I.l; m/. The spatial transfer function W.u; v/ for a pair of tracking antennas is represented at any instant by a pair of two-dimensional delta functions 2 Ä±.u; v/ and 2 Ä±.""u; ""v/. For an array of antennas, the resulting spatial transfer function is represented by a series of delta functions weighted in proportion to the magnitude of the instrumental response. As the Earth rotates, these delta functions generate the ensemble of elliptical spacing loci. The loci represent the spatial transfer function of a tracking array. Consider observation of a source I.l; m/, for which the visibility function is V.u; v/, with normalized antenna patterns AN .l; m/. Then if W.u; v/ is the spatial transfer function, the measured visibility is"
280,157,0.984,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"Experiments on the evolution of plasticity in B. anynana have been of two types: microevolutionary population-level studies and macroevolutionary species-wide comparative studies. I will review these two types of experiments in turn. The first type of study focused on testing whether genetic variation controlling the slope of a reaction norm, i.e., the sensitivity of ventral eyespot sizes to rearing temperature, was present in individuals of a single population. The initial rearing of different members of a family (representing similar genotypes) across different temperatures identified significant genetic variation for plasticity in a lab population of B. anynana (Windig 1994). In particular, variation in how each family responded to the same range of environments (temperatures) was captured via the presence of reaction norms with distinct slopes. However, further investigation concluded that this variation translated to minor changes to slopes when artificial selection was directly applied to the slope. These artificial selection experiments were of two types. The first type of experiment selected for steeper slopes by applying truncation selection for large eyespots at high temperature followed by"
311,2931,0.984,The Physics of the B Factories,"Table 24.1.3. The total production cross-sections from Belle (Seuster, 2006) and BABAR (Aubert, 2002d, 2007p) for e+ eâ â Xc Y (or Î+ c X). The uncertainties are statistical, systematic and the uncertainty due to the knowledge of the branching ratios at the time of the measurement. The branching fractions used in Belle and BABAR measurements are listed in second rows of results. Older measurements by CLEO (Artuso et al., 2004; Bortoletto et al., 1988) are also given taking into account the world average of the respective product branching fractions at the time of the Belle publication."
38,429,0.984,The GEO Handbook on Biodiversity Observation Networks,"of predictor variables measured, or estimated, at these same locations, can help to shed light on the relative importance of different drivers in determining spatial patterns in biodiversity, and on the form (shape) of these relationships. The ï¬tting of correlative species distribution models (SDMs) relating observations of presence, presence-absence, or abundance of a given species to multiple environmental variables (e.g., climate, terrain, soil, land-use variables) is probably the best known, and most widely applied, manifestation of such data analysis (Elith and Leathwick 2009). Other examples include statistical analyses of community-level, or ecosystem-level, attributes (e.g., species richness, functional diversity) measured at ï¬eld sites distributed across different classes of land use or management (de Baan et al. 2013; Newbold et al. 2015). Explanatory modelling of drivers affecting the spatial distribution of biodiversity may be all that is required for some applicationsâe.g., to inform development of government policy to reduce the detrimental impact of a particular form of land use or management. However if the environmental variables used in model ï¬tting are also mapped across an entire region of interest (e.g., as grids in a GIS) then a model derived through data analysis can, in turn, provide the foundation for prediction across geographical space (Miller et al. 2004). In the case of an SDM, this involves combining the ï¬tted model with environmental values for each grid-cell in the region to predict occurrence within that cell, thereby producing a complete map of"
311,1342,0.984,The Physics of the B Factories,"Here D represents the Dalitz plot phase space and Di is the bin region over which the integration is performed. The terms si are defined similarly with sine substituted for cosine. Neglecting eï¬ects due to neutral D mixing and CP violation (which are measured or constrained at the 1% level or less; see the text on D-mixing and CP violation in Section 19.2), the strong phase diï¬erence Î´D is antisymmetric (Î´D (m2+ , m2 â) = âÎ´D (mâ , m+ )) and thus the relations ci = câi and si = âsâi hold. The values of the ci 17.8.4.3 Binned model-independent technique and si terms can be measured using quantum correlated pairs of D mesons created at charm-factory experiments In the binned fit approach to Ï3 determination using B Â± â operated at the threshold of DD pair production. The DK Â± , D0 â KS0 Ï + Ï â decays, it is possible to avoid de- wave function of the two mesons is antisymmetric, pendence on a detailed model of the D0 amplitude across (1) (2) (2) (1) the Dalitz plot. Instead, if the plot is divided into bins, Acorr = AD AD â AD AD , (17.8.20) the amplitude in each bin can be described by quantities averaged over that bin. These quantities can be ex- where the indices â(1)â and â(2)â correspond to the two probability dentracted from analyses of charm data, thus allowing for a decaying D mesons. The four-dimensional 0 + â correlated Dalitz plots is completely model-independent measurement of Ï3 . This approach is particularly attractive for precision measure- sensitive to the strong phase diï¬erence. In the case of the ment at a super flavor factory where the model uncertainty binned analysis, the number of events where one D mewould otherwise dominate the precision. The approach son lies in the i-th bin of the Dalitz plot and the other D was first proposed in Giri, Grossman, Soï¬er, and Zupan meson in the j-th bin is"
86,347,0.984,Nuclear Back-End and Transmutation Technology For Waste Disposal : Beyond The Fukushima Accident,"too rapid for this ADS. The operation efficiency, Îµo, is 82.1 % assuming 300 days operation annually. Design and transmutation performance are summarized in Table 19.5. Volume fraction of the inert matrix, ZrN, of the MA-ADS core is 69.8 %, adjusted so that k-effective at the beginning of the cycle (BOC) of the equilibrium core becomes 0.97. The equilibrium core is obtained after calculating ten cycles of burning, cooling, and recycling. Volume fraction of the Pu-ADS is more and that of the Pu+U-ADS is almost the same. The inventory at BOC of the heavy metal in Table 19.5 is proportional to a one-volume fraction of ZrN. An interesting observation is that the amounts of Pu at BOC are equal among three ADSs, which means U and MA contribute very little to the criticality before depletion. However, impacts on the criticality drop after depletion is significant (Fig. 19.3). keff drop of the MA-ADS is as small as 1.5 %dk, although others lose 14 %dk even at the equilibrium cycle around 6,000 days, which means MA is a better fertile than 238U. The Pu-ADS has a steeper decrease than Pu+U-ADS because of the absence of 238U. The huge drop of the Pu- and Pu+U-ADS is not acceptable in the current design of accelerator and target for the MA-ADS; the acceptable drop is about 3 %dk in the MA-ADS. The effective transmutation rate and transmutation half-life are listed at the bottom of Table 19.5. The half-life of the Pu-ADS is shortest because its specific heat is twofold larger than others although its cycle efficiency, Îµc, is much smaller than others."
372,1946,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"study. The positions of the retroreflectors R are continuously adjusted to equalize the lengths of the paths from the source to the combination point B. This delay compensation is usually implemented in evacuated tubes because the geometric delay of the interferometer largely occurs above the atmosphere. If air delay lines were used, a separate mechanism would be needed to compensate for the dispersive component of the delay, which is difficult to implement in wide bandwidth systems [see, e.g., Benson et al. (1997)]. The siderostats are mounted on stable foundations, and the rest of the system is usually mounted on optical benches within a controlled environment. The apertures of the interferometer, determined by the mirrors S, are made no larger than the Fried length df . Thus, the wavefront across the mirror remains essentially plane, and the effect of the irregularities is to produce a variation in the angle of arrival of the wavefront. The variation cannot be tolerated since the angles of the beams at the combination point B must be correct to within 100 . To mitigate this effect, the polarizing beamsplitter cubes P reflect light to quadrant detectors Q, which produce a voltage proportional to any displacement of the angle of the light beam. These voltages are then used to control the tilt angles of the mirrors T, to compensate for the wavefront variation. A servo loop with bandwidth "" 1 kHz is required to follow the fastest atmospheric effects. The filters F define the operating wavelength. The two detectors D1 and D2 respond to points on the fringe pattern spaced by one-quarter of a fringe cycle, and their outputs provide a measure of the instantaneous amplitude and phase of the fringes. This method is described, for example, by Rogstad (1968), who has also pointed out that with a multielement system, the phase information can be utilized by means of closure relationships, as introduced in Sect. 10.3. The system in Fig. 17.8 is shown to illustrate some of the important features used in modern optical interferometers. In practice, the siderostat mirrors may be replaced by large-aperture telescopes, and the paths of the light to the point at which the fringes are formed may be considerably more complicated."
142,210,0.984,Subseafloor Biosphere Linked To Hydrothermal Systems : Taiga Concept,"species (e.g. large animals have lower genetic diversity than small ones; Beebee and Rowe 2008), and the dating of vent activity with genetic diversity can only provide relative ages for many species. The expanded population might be divided into two or more subpopulations either sympatrically or allopatrically. Without gene exchange or connectivity among the subpopulations, genetic differentiation accumulates and eventually causes speciation, which is an irreversible event. With calibration of the molecular clock, speciation ages of some vent animals have been estimated in million year order with geologic events (Shank et al. 1999; Johnson et al. 2006, 2010). Estimation of the speciation date of vent species provides the longest scale. To investigate the relation between biodiversity described above and hydrothermal activity, we examine the dataset of species diversity as well as genetic diversity based on partial COI sequences of vent species in the southern Mariana Trough and the Okinawa Trough. In the Southern Mariana Trough, six active hydrothermal sites have been found in all. Four sites, located linearly across the spreading axis, were specifically examined in this study. In these four sites, the fauna associated with the Archean sites accounted for the largest number of species (Fig. 5.6), which implies that the present vent-related community of the Archean site has a longer history than those of other three sites, even though the number of species was almost half of those in the Central Mariana Trough (see Kojima and Watanabe Chap. 26). In addition, the evenness or relative abundance is important to assess the biodiversity and community structure. Nevertheless, no quantitative analysis has been reported in this area. The Archean site develops at the greatest water depth (2,990 m) among the four Southern Mariana sites between the current spreading axis and an off-axis seamount. The biodiversity of the"
32,123,0.984,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","EMD method further shows that the ratios NASDAQ/DJIA and S&P500/DJIA, normalized to 1971/02/05, approached and then retained 2 and 1, respectively, from 1971 to 2011, through damped oscillatory components in 8-year cycle and damping factors of about 29 years (7183 days for NASDAQ/DJIA) and 554 years (138,471 days for S&P500/DJIA). Note that the damped oscillation of 8-year cycle is not associated with the characteristic time scales in the auto-correlation of the gains and cross-correlation of the returns of the indices. Furthermore, the peak of NASDAQ/DJIA in the period from 1998 to 2002, which is considered as an anomaly in the ratio, is a local event that does not appear in the 8-year cycle. The converge of the damped oscillatory component implies that representative stocks in the pairmarkets become more coherent as time evolves. For the components with cycles less than half-year, behaviors of self-adjustments are observed in the ratios, and there is a relatively active reassessment on the ratio in the time scale of 14-days according to the results of MSE analysis. The behavior of self-adjustment in the ratio for S&P500/DJIA is more significant than in NASDAQ/DJIA. Finally we would like to remark that the damped components found in the study set reasonable bounds to the variations of the indices. It may be informative for risk evaluation of the markets. This requires further investigations. Acknowledgements This work was supported by the Ministry of Science and Technology (Taiwan) under Grant No. MOST 103-2112-M-008-008-MY3. Open Access This book is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
311,2151,0.984,The Physics of the B Factories,"originating in combinatoric background and BB events. BABAR uses a requirement of 2.5 GeV/c for the same purpose. Both experiments reconstruct the Dâ decay chain by performing a vertex fit of the K and Ï candidate tracks, and extrapolating the flight direction of the resulting D0 candidate back to the interaction region; the resulting in2 tersection is taken as the Dâ decay vertex (see Fig. 19.2.3). xâ² + y â² â RD + RD y Ît + (Ît) , (19.2.22) The Ï candidate is constrained to originate from the Dâ vertex. Requirements on the Ï2 of all vertex fits are imKÏ where we used a short hand notation RD = RD . The posed. The resulting decay time t along with its associated parameters xâ² and y â² are the parameters x and y rotated uncertainty Ït is calculated from the fitted vertex posiby a strong phase diï¬erence Î´KÏ between the CF and DCS tions and their uncertainties, and from the reconstructed D0 momentum. A typical value of Ït is (130-160) fs. decays: Some events have multiple Dâ candidates (about xâ² = x cos Î´KÏ + y sin Î´KÏ 5%).127 At Belle, the events in which at least two Dâ cany â² = y cos Î´KÏ â x sin Î´KÏ ; (19.2.23) didates with the opposite charge are found are rejected, which reduces the random Ïs background (see below) by about 30% and the signal by 1%. In the case of same-sign KÏ eâiÎ´KÏ , Î´KÏ is defined through AK â Ï+ /AK â Ï+ = â RD Dâ candidates, the one with the best vertex fit Ï2 was c.f. Eq. (19.2.20). In the following, often the shorter no- retained. At BABAR, if the Dâ candidate shared daughter tation Î´ for Î´KÏ is also used. Equation (19.2.22) reveals an tracks with other Dâ candidates, only the one with the exponential decay modulated by terms linear and quadratic largest P (Ï2 ) was used. in t. The diï¬ering time development of the three terms â There are several background components required in constant, linear, and quadratic â can be used to separate order to correctly describe the two-dimensional RS and the contributions from DCS decays and decays with mix- WS (mKÏ , Îm) or (mKÏ , q) distributions in addition to ing. In this approximation, the time-integrated rate, RWS , the signal components. BABAR defines three background is then types: ârandom Ïs â background, where an unassociated s candidate is paired with a good D candidate; âmis x2 + y 2 RWS = RD + y RD + (19.2.24) reconstructed D â background, where a Ïs candidate is paired with a D0 that was reconstructed incorrectly, either The mixing rate RM is defined as RM = (x + y )/2 = with an incorrect particle hypothesis for one of the daughter tracks, or a multi-body decay reconstructed as a two2 (xâ² + y â² )/2. A mixing-only search (not allowing for CP body decay; and combinatoric background. In RS events, violation) combines D0 and D0 decays together. To in- misreconstructed D0 candidates are primarily from semiclude possible eï¬ects from CP violation, both BABAR and leptonic decays; in WS events, from âswapped D0 â candiBelle apply Eq. (19.2.22) to D0 and D0 decays separately, dates which are RS D0 â KÏ decays with the K and Ï as discussed further in Section 19.2.7. particle identifications interchanged. Belle defines four background types: random Ïs (rnd) above; those with a correct Ïs but with a misrecon19.2.2.2 Measurements of D0 â K + Ï â decays structed D0 decaying to (â¥ 3)-body final states (d3b); charged D+ and Ds+ decays (ds3); and combinatoric backBelle and BABAR used 400 fbâ1 and 384 fbâ1 , respec- ground (cmb). tively, in their 2006 and 2007 studies of KÏ WS mixing Both Belle and BABAR determine the shape of the (Zhang, 2006; Aubert, 2007j). Both experiments used the background p.d.f.s from MC simulation and only their amâ+ + 0 Â± â decay chain D â Ïs D , D â K Ï , using the large plitudes are allowed to vary in the fits. Signal events peak statistics, RS decay D0 â K â Ï + + c.c. to determine in both mKÏ and Îm (or q). Random Ïs events peak in most of the parameters in the p.d.f.s used to describe the mKÏ but not Îm (or q). Misreconstructed D0 decays peak decay structure in four independent variables: the D0 can- in Îm (or q) but not mKÏ . Combinatoric events do not didate mass mKÏ , the mass diï¬erence Îm (or q), the re- peak in either mKÏ or Îm (or q). constructed decay time t, and the event-by-event decay From the fits to the (mKÏ , Îm) or (mKÏ , q) distributime error, Ït . Both, BABAR and Belle perform a âblindâ tions Belle finds 1, 073, 993 Â± 1108 RS signal events and analysis (see Chapter 14), where the analysis procedure is 4024 Â± 88 WS signal events. Fig. 19.2.8 shows mKÏ and q finalized before examining the mixing results. distributions from the Belle analysis for RS and WS data. In both measurements particle identification criteria BABAR fits the RS and WS (mKÏ ,Îm) plane simultaneon charged kaon and pion candidates are imposed, as well ously using shared parameters that describe the signal and as requirements on the quality of selected tracks and/or momentum of slow pions. Belle requires the momentum 127 The largest fraction of multiple Dâ candidates results from of the D0 candidate in the center-of-mass frame to be combinations of a single D0 candidate paired with multiple Ïs > 2.7 GeV/c, which reduces the number of candidates candidates."
372,287,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where C and Ë are real. The spectral components of the function under consideration are appreciable only for small values of j"" ! ""0 j. Thus, C.t/ and Ë.t/ consist of low-frequency components, and the period of the time variation of C and Ë is characteristically the reciprocal of the bandwidth. The real and imaginary parts of the analytic signal can be written as VR .t/ D C.t/ cosÅ2#""0 t ! Ë.t/Â ;"
219,1024,0.984,Economics of Land Degradation and Improvement â A Global Assessment for Sustainable Development,"where, A = Number of SLM technologies adopted; and the vector of explanatory variables xi are similar to those used in Eq. 20.1; (i.e. x1 = a vector of biophysical factors (climate conditions, agro-ecological zones); x2 = a vector of demographic characteristics factors (level of education, age, gender of the household head); x3 = a vector of farm-level variables (access to extension, market access, distance to market, distance to market); x4 = vector of socio-economic and institutional characteristics (access to extension, market access, land tenure, land tenure); zi = vector of country ï¬xed effects; and ei is the error term). Some of the limitations of PRM in empirical work relates to the restrictions imposed by the model on the conditional mean and the variance of the dependent variable. This violation leads to under-dispersion or over-dispersion. Overdispersion refers to excess variation when the systematic structure of the model is correct (Berk 2007). Overdispersion means that to variance of the coefï¬cient estimates are larger than anticipated meanâwhich results in inefï¬cient, potentially biased parameter estimates and spuriously small standard errors (Xiang and Lee 2005). Under dispersion on the other hand refers to a situation in which the variance of the dependent is less than its conditional mean. In presence of under- or over-dispersion, though still consistent, the estimates of the PRM are inefï¬cient and biased and may lead to misleading inference (Famoye et al. 2005; Greene 2012). Our tests showed no evidence of under- or over-dispersion. Moreover, the conditional mean of the distribution of SLM technologies was similar to the conditional variance. Thus PRM was appropriately applied."
394,601,0.984,Biotechnologies for Plant Mutation Breeding : Protocols,20.3.7 Computational Analysis of Sequence Data A bioinformatics pipeline is required to process the large volume of sequence data that is generated and to effectively discriminate between real mutations and background ânoise.â This is complicated by the fact that the intensity of the mutation âsignalâ and the background noise may vary greatly as a result of the sequence coverage and the location of the mutation (see Note 32). The specifics of the bioinformatics analysis are
372,1316,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"parts or the conjugate values (but not both) are counted independently. In images made using the fast Fourier transform (FFT) algorithm, there are equal numbers of grid points in the .u; v/ and .l; m/ planes, but not all .u; v/ grid points contain visibility measurements. To maintain the condition for convergence, it is a common procedure to apply CLEAN only within a limited area, or âwindow,â of the original image. In order to clean an image of a given dimension, it is necessary to have a dirty beam pattern of twice the image dimensions so that a point source can be subtracted from any location in the image. However, it is often convenient for the image and beam to be the same size. In that case, only the central quarter of the image can be properly processed. Thus, it is commonly recommended that the image obtained from the initial Fourier transform should have twice the dimensions required for the final image. As mentioned above, the use of such a window also helps to ensure that the number of components removed does not exceed the number of visibility data and, in the absence of noise, allows the residuals within the window area to approach zero. Several arbitrary choices influence the result of the CLEAN process. These include the parameter ! , the window area, and the criterion for termination. Note that a point-source component in the image can be removed in one step of CLEAN only if it is centered on an image cell. This is an important reason for choosing ! ! 1. A value between 0.1 and 0.5 is usually assigned to ! , and it is a matter of general experience that CLEAN responds better to extended structure if the loop gain is in the lower part of this range. The computation time for CLEAN increases rapidly as ! is decreased, because of the increasing number of subtraction cycles required. If the signal-to-noise ratio is Rsn , then the number of cycles required for one point source is "" log Rsn = log.1 "" ! /. Thus, for example, with Rsn D 100 and ! D 0:2, a point source requires 21 cycles. A well-known problem of CLEAN is the generation of spurious structure in the form of spots or ridges as modulation on broad features. A heuristic explanation of this effect is given by Clark (1982). The algorithm locates the maximum in the broad feature and removes a point-source component, as shown in Fig. 11.2. The negative sidelobes of the beam add new maxima, which are selected in subsequent cycles, and thus, there is a tendency for the component subtraction points to be located at intervals equal to the spacing of the first sidelobe of the synthesized (dirty) beam. The resulting image contains a lumpy artifact introduced by CLEAN, but the image is consistent with the measured visibility data. Cornwell (1983) introduced a modification of the CLEAN algorithm that is intended to reduce this unwanted modulation. The original CLEAN algorithm minimizes"
80,135,0.984,Innovations in Quantitative Risk Management (Volume 99.0),"where PÌ and ÏÌ denote the average option price and the average implied volatility, respectively. Each of these six objective functions can again be subdivided into an unweighted functional for which the weight Ïi = Ï = M1 âi and a weighted functional for which the weight Ïi is proportional to the trading volume of option i. We furthermore consider the possibility of adding an extra penalty term to the objective function in order to force the model prices to lie within their market bid-ask spread. Besides these standard specifications (in terms of the price or the implied volatility of the calibration instruments), we consider the so-called moment matching market implied calibration proposed by Guillaume and Schoutens [12] and which consists in matching the moments of the asset log-return which are inferred from the implied volatility surface. As the VG model is fully characterized by three parameters, we consider three standardized moments, namely the variance, the skewness, and the kurtosis. Since as shown by Guillaume and Schoutens [12], the variance can always be perfectly matched, we either allocate the same weight to the matching of the skewness and the kurtosis or we match uppermost the lower moment, i.e., the skewness. This leads to a total of 26 plausible calibration procedures, each of them leading to a test measure Q â M provided that the proportion of model prices falling within their market bid-ask spread is at least equal to the threshold p."
49,485,0.984,Artificial Intelligence and Cognitive Science IV,"Further, we show an application of a fuzzy controller as a means for processing heuristic rules based on expert experience, which are used for determining some learning parameters in an optimization process. In [17] the use of a fuzzy controller and K-nearest neighbours algorithm for setting-up parameters of the differential evolution (DE) method is proposed. DE [32] represent a relatively new branch in evolutionary computing and has been approved in various applications. Comparing to other GAs and EAs the approach of DE is characterized by several significant improvements like finding global optimum, number of adaptation parameters and convergence speed. However, determining the stopping criterion, i.e. number of iterations, is difficult. There are no known analytical derivations and so users are made to use a trial and error approach. However, an experienced expert knowing the information of the population can estimate the state of the searching process, too. For this reason it is necessary to know distribution of the population in the search space, i.e. a type of clustering is necessary. In this case the K-nearest neighbours method is used. K represents the number of the nearest elements that join on the classification of the investigated element (individual) in the search space into exactly one of disjoint clusters. The value K can be changed. If the mean distance between the investigated element and its K neighbours is less than a given threshold value then these elements share a common cluster else a new cluster will be created and accounted to the total number of clusters, which is the information influencing the number of DE iterations. To determine the number of iterations the so-called Iteration windows method has been designed. After several initial DE iterations these are stopped and information about K and number of clusters is sent to a fuzzy controller, where heuristics and expertâs experience are contented in KB of the controller as fuzzy IFâTHEN"
244,332,0.984,"Advancing Human Assessment : The Methodological, Psychological and Policy Contributions of Ets","Latent regression methods were introduced at ETS by Mislevy (1984) for use in the National Assessment of Educational Progress (NAEP) and are further described in Sheehan and Mislevy (1989), Mislevy (1991), and Mislevy et al. (1992). An overview of more recent developments is given in M. von Davier et al. (2006) and M. von Davier and Sinharay (2013). Mislevyâs key insight was that NAEP was not intended to, and indeed was prohibited from, reporting scores at the individual level. Instead, scores were to be reported at various levels of aggregation, either by political jurisdiction or by subpopulation of students. By virtue of the matrix sampling design of NAEP, the amount of data available for an individual student is relatively sparse. Consequently, the estimation bias in statistics of interest may be considerable, but can be reduced through application of latent regression techniques. With latent regression models, background information on students is combined with their responses to cognitive items to yield unbiased estimates of score distributions at the subpopulation levelâprovided that the characteristics used to define the subpopulations are included in the latent regression model. This topic is also dealt with in the chapter by Beaton and Barone (Chap. 8, this volume), especially in Appendix A; the chapter by Kirsch et al. (Chap. 9, this volume) describes assessments of literacy skills in adult populations that use essentially the same methodologies. In NAEP, the fitting of a latent regression model results in a family of posterior distributions. To generate plausible values, five members of the family are selected at random, and from each a single random draw is made.2 The plausible values are used to produce estimates of the target population parameters and to estimate the measurement error components of the total variance of the estimates. Note that latent regression models are closely related to empirical Bayes models. Latent regression models are very complex and, despite more than 25 years of use, many questions remain. In particular, there are attempts to simplify the"
311,2403,0.984,The Physics of the B Factories,"using the selected decay channels: Dâ0 K + with Dâ0 â D0 Ï 0 , D0 â K â Ï + , and Dâ+ KS0 with Dâ+ â D+ Ï 0 , D+ â K â Ï + Ï + , to reduce systematic uncertainties. The Ds1 (2710)+ can be either a radial excitation or an L = 2 orbital excitation, which are both predicted in this mass region. Observation of the Ds1 (2710)+ â Dâ K decay with rate comparable to that for the DK, suggests that the Ds1 (2710)+ is a radial excitation of the Ds state (Colangelo, De Fazio, Nicotri, and Rizzi, 2008). Interpretations of the DsJ (2860)+ and DsJ (3040)+ are still unknown. 19.3.5 Conclusions Results from the B Factories have given an important input to the experimental status of the charm meson spectroscopy. The observation of the broad states belonging to the L = 1 cuÌ multiplets has validated outstanding predictions of the potential models. Properties of this multiplet need to be refined with higher statistics, while their production has to be studied in various processes. Finding new decay modes, especially radiative ones, could provide further tests of the theory. On the other hand, the csÌ spectroscopy has revealed some surprises. At the moment, it is still not completely clear if the Ds1 (2460)+ and (2317)+ are fully understood in terms of QqÌ mesons, or if we need to include more complex quark configurations to understand their properties. In the simplest scenario, where the Ds1 (2460)+ and Ds0 (2317)+ are the conventional L = 1 csÌ mesons, the potential models may need some serious modifications. The large data samples collected by Belle and BABAR also allowed for precision measurements of already known"
165,260,0.984,New Methods for Measuring and Analyzing Segregation,"reduce values of both D and S dramatically. Fortunately, the practical consequence is usually modest because segregation patterns in US cities are characterized more by large-scale clustering than by small-scale checkering. The second condition is when segregation patterns include homogeneous regions that are smaller than the areal units used in the study design. The practical consequence of this problem is greater when groups are small in size. Even when area polarization is substantial and homogeneous areas for a group are clustered, the value of S cannot reach its maximum value if the overall size of the smaller group does not comfortably exceed the population size of the areal units used to assess segregation. As noted above, the impact will be potentially important for both D and S, but more so for S. As a result, using large spatial units when investigating segregation involving small groups can distort comparisons of D and S making D-S differences appear larger than would be the case if a better research design was used. In light of this, researchers should give the issue careful thought when making decisions about research design. Happily, the problem is easy to understand and, once appreciated, major problems are easy to avoid. The solution is to confirm that the spatial units used to assess segregation have the logical capacity to capture group separation and residential polarization for the groups in the comparison. Brief discussion of a hypothetical example can illustrate the key issues. Assume a hypothetical city with 4 equal size census tracts each containing 4000 people. Also assume that each tract is subdivided into 4 equal size block groups (for a total of 16 block groups) each containing 1000 people. Next assume that the city has two groups, one with 15,000 people and one with 1000 people, and then assume that everyone in the smaller group resides in a single block group. Finally, assume that each block group is divided into 10 equal size blocks each containing 100 people. In this example, S and D will both register perfect segregation ( D = S = 100.0 ) if their values are computed using block data or block group data. However, if they are computed using tract data their values will be 80.0 for D and 20.0 for S. This contrast illustrates two points. The first is that both displacement (D) and separation (S) can be measured without error if the spatial unit used in the research is âright sizedâ as it is in this example when using blocks and block groups. The second point is that when the spatial unit used is too large â meaning specifically that the population of the smaller group is too small to fill multiple areas, as is the case when using tracts in this example â the value of all indices of uneven distribution will be underestimated. Furthermore, while both D and S will be underestimated based on this problem with research design, the impact will tend to be more dramatic for S for reasons give above. This in turn can distort the comparison of D and S. In the worst case scenario, it would produce an incorrect impression that a high D, low S situation of âdispersed displacementâ or âdisplacement without separationâ prevails when a better research design would reveal a high-D, high-S combination indicating a pattern of âprototypical segregationâ. A simple practice can guard against the problem; avoid using spatial units that are too large to reveal group separation and neighborhood polarization involving small groups. A practical rule of thumb is that typical population size for spatial"
233,446,0.984,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"rather than Ws has the same issues with âsumâ versus 1st and 2nd ranks concepts as above. With Wes the Ws values are âdilutedâ by being divided across each site that a species is recorded from and the main beneï¬t is that sites will score more highly in proportion to the uniqueness of their species composition. The resampling methods used here assure that ranking is not driven by a single or very small set of phylogenies, and the resampling with multiple drops indicates the tendency of sites remaining in similar ranking positions with the addition of phylogenies. To the best of our knowledge, this is the ï¬rst time a set of phylogenetic studies are analysed this way (but see the proposition of Miranda-Esquivel, chapter âSupport in Area Prioritization Using Phylogenetic Informationâ), and this seems to be a very promising way of integrating the problems of diversity of sampling effort."
311,2643,0.984,The Physics of the B Factories,"21.3.4.2 Comparison to other determinations Direct comparison with the results from other experiments is complicated by two facts: (i) e+ eâ scan experiments provide cross section measurements at discrete and unequally spaced energy values, while the ISR method provides a continuous spectrum and, (ii) unlike BABAR no other experiment covers the complete mass spectrum from threshold up to energies where the contributions become negligible. Wherever gaps remain, they have been filled by using the weighted-average cross section values from the other experiments. This approach has been followed by Davier, Hoecker, Malaescu, Yuan, and Zhang (2010) from which the relevant integrals are extracted. Correlations between systematic uncertainties have been taken into account, particularly for radiative corrections, when combining the results from all experiments. The combination is performed in small energy bins at the cross section level, taking into account possible disagreements leading to an increased uncertainty of the resulting average. The contribution of the Ï + Ï â channel to aÎ¼ obtained from the combination of all measurements of the e+ eâ â Ï + Ï â cross section is (507.8 Â± 3.2) Ã 10â10 . It is compared in Fig. 21.3.5 with the determinations of aÎ¼ calculated using the data form the individual experiments. All determinations are indeed consistent within the uncertainties, BABAR and CMD-2 (Akhmetshin et al., 2006, 2007; Aulchenko et al., 2005) being almost a factor of two more precise than SND (Achasov et al., 2006) and KLOE (Ambrosino et al., 2009a, 2011). The BABAR result is also consistent with determinations using Ï decay with isospin-breaking corrections from Davier et al. (2010), which are also reported in Fig. 21.3.5. This reduces the previous tension between e+ eâ and Ï values (Davier, Eidelman, Hoecker, and Zhang, 2003a). Looking at the full picture it is important to note that the four inputs (CMD-2/SND, KLOE, BABAR, Ï ) have completely independent systematic uncertainties."
297,1001,0.984,The R Book,"The large negative residuals are all above the line, but the most obvious feature of the plot is the single, very large positive residual (in the top right-hand corner). In general, negative binomial errors will produce a J-shape on the quantileâquantile plot. The biggest positive residuals are much too large to have come from a normal distribution. These values may turn out to be highly inï¬uential (see below). Gamma errors and increasing variance Here the shape parameter is set to 1 and the rate parameter to 1/x, and the variance increases with the square of the mean: eg <- rgamma(31,1,1/x) yg <- 10+x+eg"
244,574,0.984,"Advancing Human Assessment : The Methodological, Psychological and Policy Contributions of Ets","Table 10.2 presents a comparison of the relationships between MLE estimates and two Bayesian estimates with selected outside policy variables. Inspection of Table 10.2 indicates that in the theta metric, the normal prior Bayesian procedure (ST1) shows stronger relationships between gains and coursetaking than do the other two procedures. The differences in favor of ST1 are particularly strong where contrasts are being made between groups quite different in their mathematics preparation, for example, the relationship between being in the academic curriculum or taking math now and total gain. When the correlations are based on the number correct true score metric (NCRT), the ST1 Bayesian approach still does as well or better than the other two approaches. The NCRT score metric is a nonlinear transformation of the theta scores, computed by adding the probabilities of a correct answer for all items in a selected item pool. Unlike the theta metric, the NCRT metric does not stretch out the tails of the score distribution. The stretching out at the tails has little impact on most analyses where group means are used. However, it can distort gain scores for individuals who are in or near the tails of the distribution. Gains in proficiency probabilities at each proficiency level and their respective correlations with selected process variables are shown in Table 10.3. The entries in Table 10.3 demonstrate the importance of relating specific processes with changes taking place at appropriate points along the score distribution."
285,880,0.984,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","Vowel coding studies have historically focussed on representation of spectral peaks, or formants, in AN rates or synchrony to stimulus components near BF. These representations are vulnerable to additive noise and vary with sound level. The sensitivity of midbrain neurons to low-frequency fluctuations inspired a shift of focus to the contrast in pitch-related fluctuations along the BF axis in vowel coding (Carney et al. 2015). In the healthy AN, there is a robust contrast in the profile of the F0synchronized discharge rate across the BF axis (Fig. 2d). Midbrain neuronsâ sensitivities to these F0-synchronized rates transforms the temporally coded AN profile into a discharge rate profile (Fig. 4a). The representation of F0-synchronized rates is affected by several mechanisms that are influenced by SNHL: synchrony capture, threshold (and thus rate saturation), and bandwidth of tuning. Bandwidth, in turn, affects the modulation spectrum of peripheral responses. Wider peripheral bandwidths associated with SNHL result in a wider modulation bandwidth and a reduction of the amplitude of low-frequency fluctuations. These factors have little influence on the mean rates of AN fibres; however, they have a large effect on F0-synchrony in the AN, which in turn affects the responses of IC neurons. These IC model responses have implications for the design of signal-processing strategies for listeners with SNHL. Recreating stimulus spectra in the AN rate profiles will not elicit appropriate responses from central neurons, whereas restitution of the F0-synchronized rate profiles may (Rao and Carney 2014). This result suggests an experimental question: If the profile of peripheral F0-synchrony in response to voiced sounds can be restored to normal for these listeners, can intelligibility of speech, and especially speech in background noise, be improved? The results of such a test would depend on the status of central auditory pathways, which may undergo change following cochlear trauma (Suneja et al. 1998; Salvi et al. 2000). Acknowledgments Supported by NIDCD-010813. Open Access This chapter is distributed under the terms of the Creative Commons AttributionNoncommercial 2.5 License (http://creativecommons.org/licenses/by-nc/2.5/) which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited. The images or other third party material in this chapter are included in the workâs Creative Commons license, unless indicated otherwise in the credit line; if such material is not included in the workâs Creative Commons license and the respective action is not permitted by statutory regulation, users will need to obtain permission from the license holder to duplicate, adapt or reproduce the material."
175,1280,0.984,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","length of stream or river as a function of the magnitude of flow. In watersheds characterized by signiï¬cant elevation changes and consequently varying rainfall and runoff runoff, other methods may be required for estimating average streamflows at ungaged sites. The selection of the most appropriate method to use, as well as the most appropriate gage sites to use for estimating the streamflow, Qst , at a particular site s can be a matter of judgment. The best gage site need not necessarily be the nearest gage to site s, but rather the site most similar with respect to important hydrologic variables. The natural incremental flow between any two sites is simply the difference between their respective natural flows."
295,413,0.984,interface oral Health Science 2016 : innovative Research On BiosisâAbiosis intelligent interface,"Ti-8MnMIM (Fig. 19.11a). The DF image of the Ï spots (Fig. 19.11g) evidences a high-volume fraction of the athermal Ï phase of Ti-8MnMIM. The intensity of the spots attributed to the athermal Ï phase and the apparent volume fraction of the athermal Ï particles gradually decrease with increasing Mn content. It is observed that Ti-18MnLM still shows very diffuse streaks associated to the athermal Ï phase, while Ti-(15â17)MnMIM donât show spots or streaks. This is likely caused by the"
256,516,0.984,"Supercomputing Frontiers : 4Th Asian Conference, Scfa 2018, Singapore, March 26-29, 2018, Proceedings","P-CBCG solver is rather flexible, and the optimum s depends on the following factors. Firstly, the number of All reduce is reduced to 1/s compared to the P-CG method. Here, the communication data size scales as â¼s2 . Secondly, the numbers of floating point operations and memory access per iteration in Matrix kernel scale as f â¼ s and b â¼ const., respectively, and the arithmetic intensity of Matrix scales as f /b â¼ s. Thirdly, cache efficiency of CB is affected by the number of basis vectors. Therefore, the computational performance of each kernel varies depending"
165,498,0.984,New Methods for Measuring and Analyzing Segregation,"distribution of the unbiased indices is affected by the presence of other groups. More specifically, while the mean for unbiased indices is always approximately zero, the standard error of the mean varies inversely with ENS as basic sampling theory would lead one to expect. But this pattern holds for the expected distributions of scores of both standard and unbiased versions of indices of uneven distribution and so does not diminish the advantage of using unbiased versions of indices."
372,1228,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"of these initial parameters is more obvious in the image plane than in the visibility plane, but final analysis is best done in the visibility plane. To fit model parameters, it is necessary to choose a criterion for the goodness of fit. Since the real and imaginary components of the visibility usually have Gaussian noise, the optimum criterion from a maximum likelihood point of view (see Appendix 12.1) is the )2 criterion, which minimizes the weighted meansquared difference between the model and the data set of nd points, i.e.,"
175,1456,0.984,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","The friction factor is dependent on the Reynolds number and the pipe roughness and diameter. Given these equations it is possible to compute the distribution of flows and heads throughout a network of open channels or pressure pipes. The two conditions are the continuity of flows at each node, and the continuity of head losses in loops for each time period t. At each node i:"
228,198,0.984,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"5.5.1 The Aggregationâs Basic Properties Generally, an aggregation is an operation used in those situations when we need to find a single value representing the set of various numbers/data. There can be different application areas specified where an aggregation [2] is needed, for example, making decisions based on multiple criteria, or choosing from a variety of peer evaluations, one of which is treated as the result of them all. One important area of application is also the aggregation of the rule premise in a rule-based fuzzy system, where we have many input variables. The aggregation operation is a function that converts a number of input data into a single value. Transformation depends on the chosen method, but it is expected that in the process of determination of the result all of the input data were considered (in some way). Typically, aggregations where the number of input data is greater than one are used. Moreover, to call a function an aggregation, it should have two elementary properties (see [4]): 1. Boundary conditions. If all input data are minimal (or maximal), the result will also be the minimal (maximal) value. In the case of aggregation A for values from interval [0, 1] (the range of values of a fuzzy set), when all the arguments are equal to 1, the result of aggregation is also equal to 1 and similarly for zeros: A(0, 0, ..., 0) = 0 A(1, 1, ..., 1) = 1"
311,324,0.984,The Physics of the B Factories,"and the sum runs over the B-meson decays to the exclusively reconstructed ï¬nal states f . The Îµf are the corresponding reconstruction eï¬ciencies and the Bf are the branching fractions of the B â f decays. In order to achieve as high eï¬ciency as possible a large number of B-meson decay modes are used for the Btag reconstruction. On the quark level B mesons decay dominantly via b â cW + transitions, where the virtual W materializes either into a pair of leptons âÎ½â (semileptonic decay), or into a pair of quarks, ud or cs, which then hadronize. The most common choice for exclusive Btag reconstruction are therefore semileptonic B â D(â) ââ Î½â decays (semileptonic Btag reconstruction) and hadronic B â D(â) nÏ, D(â) Ds or B â J/ÏKmÏ (hadronic Btag reconstruction), where n and m indicate any number (n, m â¤ 10) of charged or neutral pions and kaons, respectively. The branching fractions of these hadronic decay modes are between 10â3 and up to 10â2 , and the branching fraction for inclusive semileptonic decays33 of a B meson to a D meson plus anything else is around 20%. The two analysis techniques are complimentary and non-overlapping and, as such, can be readily combined to improve the sensitivity of any recoil B analysis. This essentially doubles the size of the available Btag sample. Many decay modes for which the B meson cannot be exclusively reconstructed rely on these methods to make measurements feasible. For the proposed high luminosity asymmetric e+ eâ super ï¬avor factories, measurements of B decays, not related to CP violation or the CKM picture of the Standard Model, will beneï¬t from recoil methods. This corresponds to a wide program of purely leptonic, semileptonic and radiative penguin34 B decays. Furthermore, with a huge dataset the recoil methods will provide a clean âsingle B beamâ which will permit the extraction of hadronic B decay branching fractions using a missing mass technique. In this section the general idea behind the recoil Bmeson reconstruction has been presented. In addition the variables or constraints which can be imposed in studies of B-meson decays involving one or more neutrinos with recoil B-meson technique have been brieï¬y described. The rest of this section is devoted to the description of diï¬erent approaches to Btag reconstruction. More details on analyses of decay modes utilizing the recoil B-meson reconstruction (given in Table 7.4.1) can be found in Sections 17.9, 17.10 and 17.11. 7.4.1 Hadronic tag B reconstruction The full reconstruction of one B meson, decaying hadronically, has been utilized in a multitude of analyses by the B Factories (see Table 7.4.1). The approaches of BABAR and Belle diï¬er somewhat, providing samples which vary"
8,618,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","where  is a new, intensive parameter related to the volume in a similar way as Ë is related to the energy and  to some conserved quantity. The larger , the stronger is the exponential volume suppression in the integral of Eq. (24.2). Thus  is a measure for the pressure and hence the name of this partition function. Rewriting Eq. (24.2), Ë.Ë; ; / D"
285,422,0.984,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","Figure 2 shows responses analysed in the time domain (left panels) and the frequency domain (right) for a single IPM condition (Â± 67.8Â°). Individual (gray lines) and group mean (black) for the slowest IPM rate (3.4 Hz; Fig. 3, top), typically displayed a P1-N1-P2 response to each IPD transition, with component latencies of approximately 50, 100 and 175 ms, respectively. The next two harmonics (6.8 and 10.2 Hz) were also observed clearly in the response to the 3.4-Hz IPM. Evoked responses to the intermediate IPM rate (6.8 Hz; Fig. 2) became steady state at the same rate as the IPM, and so we term this response the IPM-FR. In the time domain (left-middle), the peak amplitude of the IPM-FR is broadly the same as that for the slowest IPM rate (3.2 Hz), but responses were more steady state than those at the low rate, being more consistent in phase and amplitude across epochs (evidenced by a more favourable SNR). This is confirmed by comparing the variability of the individual responses in the time domain as well as the frequency domain (Fig. 2), where the spectral magnitude was almost twice larger than that the largest magnitude obtained at the lowest IPM rate. Moreover, the average spectral magnitude of the next harmonics was almost twice smaller than those obtained at the lowest IPM rate. Finally, the IPM-FR is observed at the most rapid IPM rate (13.7 Hz, bottom panels of Fig. 3). As for the intermediate IPM rate, responses show a steady-state pattern in the time domain, albeit with a reduced amplitude. Frequency domain analysis revealed the following response occurred primarily at the frequency bin corresponding to the IPM rate. Harmonics were no longer observed in the grand averaged data. For analysis of data in the frequency domain, Hotellingâs T2 tests were applied to the frequency bins corresponding to the IPM rate and the next four harmonics. Responses were classified as significant if either the left (Tp9), right (Tp10), or both electrodes observed a significant response for a given frequency bin. The frequency bin corresponding to the IPM rate (second harmonic) elicited the highest number of significant responses. Indeed, responses obtained at IPM rates of 6.8 and 13.7 Hz were observed for all ten subjects for the Â± 45Â° and Â± 135Â° IPD conditions, respectively. This was not the case for the lowest IPM rate where significant responses were observed for only seven subjects. In terms of spectral magnitude, responses obtained at the intermediate IPM rate showed the largest magnitude at the second harmonic, i.e. the harmonic corresponding to the IPM rate, consistent with the hypothesis that a following-response (FR) is evoked by IPM. Finally, we assessed the frequency bin corresponding to the IPM rate. A threeway non-parametric repeated measures ANOVA with factors of electrode position (left or right mastoid), IPM rate, and IPD revealed that factors IPM rate ( p < 0.001), IPD ( p < 0.001) as well as the interaction between IPM rate and IPD ( p = 0.01) were significant. Responses obtained at 6.8 Hz IPM rate were maximal for IPDs spanning Â± 45â Â± 90Â°, whereas responses obtained at 13.4 Hz IPM rate increased monotonically with increasing IPD."
311,1885,0.984,The Physics of the B Factories,"matic fit with a constraint on the Î¥ (1S) mass is then applied on the two lepton candidate momenta, corrected for nearby photons, as described above. The threshold on the confidence level of this fit has been optimized using the Monte Carlo simulation for each specific channel. A requirement on the polar angle of the eâ track with respect to the beam direction, cos(Î¸eââ ) < 0.5, is imposed to further suppress singly or doubly radiative Bhabha events, which represent the dominant QED background. The Bhabha requirement is not included in the Î¥ (2S) â Ï 0 Î¥ (1S) analysis, since the Î¥ (1S) mass constraint provides already a good suppression of the QED processes. The Ï + Ï â candidate for the Î· â Ï + Ï â Ï 0 decay (and also for the Î¥ (2S) â Ï + Ï â Î¥ (1S) transition used by the Belle Collaboration) is selected requiring the two tracks to be oppositely charged, to originate from the primary interaction point, and to have a large opening angle in the e+ eâ CM frame (e.g. cos(Î¸ch + ,châ ) < 0.6 in the Belle Collaboraton result), in order to reject the e+ eâ pairs coming from photon conversions in the inner detector. In the search for the Î¥ (3S) â Î·Î¥ (1S) transition, an additional requirement on ÎMÏÏ = MÏÏll â Mll is imposed, to avoid contamination from the Î¥ (2, 3S) â Ï + Ï â Î¥ (1, 2S) transitions. A second kinematic fit is then performed after the Î· or Ï 0 selection, constraining all final state particles (i.e. the dilepton, the dipion and/or the best photon pair) to have an invariant mass equal to the Î¥ (2, 3S) mass, to generate from the same vertex, and to have a total energy equal to the sum of the beam energies. An unbinned maximum-likelihood fit to the measured distribution of one (Belle, Fig. 18.4.21) or two (BABAR) observables is then performed to extract the signal yields. Each observed distribution is fit to a sum of signal and background components, with functional forms determined from the simulations. The value of the branching fraction for each mode is then extracted. The results of all analyses are summarized in Table 18.4.9. The branching ratio for the Î¥ (3S) â Ï 0 hb (1P ) transition is obtained by combining the product of branching ratio measured by BABAR in the study of the reaction Î¥ (3S) â Ï 0 hb (1P ); hb â Î³Î·b (1S), with B(hb (1P ) â Î·b (1S)), measured by Belle. The evidence of an isospin violating transition at least one order of magnitude larger"
311,939,0.984,The Physics of the B Factories,"mesons, which can be classified into two groups as the 3 P1 nonet (a1 (1260), f1 (1285), f1 (1420), K1A ) and the 1 P1 nonet (b1 , b1 (1170), b1 (1380), K1B ).56 Three-body charmless decays concentrate on final states with Ï or K but can sometimes branch out to include protons and resonances such as K â e.g. B + â ppK + (see Chapter 17.12) and B + â K â0 K â0 K + . The âquasi-two-bodyâ decays are traditionally reconstructed assuming that the resonances decaying to the same final state (such as Ï and f0 (980) decaying to ÏÏ) do not interfere. This has the advantage that branching fractions can be compared to measurements from earlier experiments but the eï¬ect of interference is then considered as a systematic. The main diï¬erences between the ways that decays are analysed are usually dictated by the extent and nature of the background, as the B meson charmless decays have a low signal-to-background ratio. This can be compared to D meson decays which are typically selected with very high purity. Whatever the final state, the candidate selection process follows a broadly similar path (see Chapter 7 for more details on B meson reconstruction). The B meson candidates are reconstructed through their decays. The intermediate resonance will be formed first and then combined with a third particle to form the B meson. The reconstructed mass will usually be required to be less than â¼ 3 times the width from the nominal central value. If the natural widths of the resonances are smaller than the detector resolution, the resonance masses (including Ï 0 âs) are constrained to their nominal PDG values in the fit for the B meson candidate (Beringer et al., 2012); this improves the precision of the parameters obtained in the fit. Quality criteria are applied to the tracks before fitting, such as demanding the tracks are well-measured, have a minimum pT , and originate from close to the beam spot. The momenta of the charged tracks will usually be extracted assuming a particular mass hypothesis determined by the particle type (e.g. pion versus kaon, see Chapter 5). However, in some analyses, such as B 0 â h+ hâ (with h = K, Ï), the B meson will be fitted under one mass hypothesis (usually a pion), and any shift in the value of ÎE is used to diï¬erentiate between decays with one or more kaons. The shift is of the order of â¼ 50 MeV per kaon. The vertexing will apply various constraints to improve the resolution (see Chapter 6) and to take into account the flight distance of long lived particles such as the KS0 meson. These constraints become more important as the number of neutral particles in the decay increases. A further criterion that is sometimes applied is to require that there is at least one additional charged track from the beam spot region; this is a crude indicator that there has been at least one other decay in the event, which is assumed to be the other B meson. Two kinematic variables, mES and ÎE, are used to select the events (see Eqs 7.1.8 and 7.1.5 for definitions). Any linear correlation between these variables can be removed by rotating them in the (mES , ÎE) plane or a two dimensional p.d.f. can be used in the maximum likelihood"
311,1660,0.984,The Physics of the B Factories,"A large set of phenomenological applications of the EFT framework outlined above to quarkonium spectra, decays, and production has been presented elsewhere (Brambilla, Pineda, Soto, and Vairo, 2005; Brambilla et al., 2004, 2011), and discussed in relation to experimental data. Here we briefly recall some selected results. In the regime in which the soft scale mv is perturbative the energy levels of quarkonium have been calculated at order mÎ±S5 (Brambilla, Pineda, Soto, and Vairo, 1999; Kniehl, Penin, Smirnov, and Steinhauser, 2002). Decay amplitudes (Brambilla, Pineda, Soto, and Vairo, 2005; Brambilla et al., 2011; Kiyo, Pineda, and Signer, 2010) and production and annihilation (Beneke, Kiyo, and Penin, 2007) have been calculated in perturbation theory at high order. Since for systems with a small radius the non-perturbative contributions are power suppressed, it is possible to obtain a good determinations of the masses of the lowest quarkonium resonances with purely perturbative calculations in the cases in which the perturbative series is convergent (after the appropriate subtractions of renormalons have been performed) and large logarithms in the scale ratios are resummed. For example, in Brambilla and Vairo (2000) a prediction of the Bc mass98 18.1.5 Lattice calculations has been obtained: (6326 +29 â9 ) MeV/c , to be compared Lattice calculations play a key role for quarkonium physics. to the experimental value of (6277 Â± 6) MeV/c2 (BerinFor an introduction to the lattice treatment of quarkonia ger et al., 2012). An NNLO calculation with finite charm see Brambilla et al. (2004) and for recent results see Bram- mass eï¬ects (Brambilla, Sumino, and Vairo, 2002) prebilla et al. (2011). We already mentioned that the recent dicts a mass that well matches the Fermilab measureprogress in this field relies both on high order perturbative ment (Brambilla et al., 2011) and the lattice determicalculations and on lattice simulations, the results of the nation (Allison et al., 2005). The same procedure has two being often combined inside the EFT framework. been applied at NNLO even for higher states (Brambilla, In fact it is diï¬cult to put a multiscale system on the Sumino, and Vairo, 2002). An NLO calculation reprolattice as the lattice step should be smaller than the small- duces in part the 1P fine splitting (Brambilla and Vairo, est scale (mâ1 ) and the lattice size should be bigger than 2005). Including log resummation at NLL, it is possithe biggest scale of the system Îâ1 ble to obtain a prediction for the Bc hyperfine separaQCD , putting prohibitive requirements on the lattice dimensions. This is true in par- tion Î = 50 Â± 17(th)+15 â12 (Î´Î±S ) MeV/c (Penin, Pineda, ticular for bottomonium, due to its larger mass. In this Smirnov, and Steinhauser, 2004) and for the hyperfine case one could use direct anisotropic lattice simulations separation between the Î¥ (1S) and the Î·b the value of or EFTs. The Lagrangian of NRQCD can be put on the 41Â±11(th)+9 (Î´Î±S ) MeV/c2 (where the second error comes lattice and used to obtain quarkonium energy levels. Re- from the uncertainty in Î±S ; Kniehl, Penin, Pineda, Smirnov, cent results can be found in Daldrop, Davies, and Dowdall (2012), Dowdall et al. (2012), Gregory et al. (2011), and 98 The Bc states constitute a separate class of heavy mesons, Donald et al. (2012). Charmonium spectra may also be distinct both from the quarkonia (e.g. in lacking electromagcalculated on the lattice with relativistic actions. Very re- netic and strong decays) and from the simpler heavy mesons cently new lattice techiques have been introduced that will (in lacking light valence quarks). Because of their masses they eventually allow the excited charmonium spectroscopy to lie outside the scope of research at the B Factories."
311,822,0.984,The Physics of the B Factories,"In this analysis, a potential bias of the ï¬tted yield from the assumed shape of the signal spectrum is reduced by combining the on-resonance data for the interval from 2.1 to 2.8 GeV in a single bin. The lower limit of this bin is chosen so as to retain the sensitivity to the steeply falling BB background distributions, while containing a large fraction of the signal events in a region where the background is low."
217,257,0.984,Finite Difference Computing With Pdes : a Modern Software Approach,"We see that the discrete version of the PDE features only one parameter, C , which is therefore the key parameter, together with Nx , that governs the quality of the numerical solution (see Sect. 2.10 for details). Both the primary physical parameter c and the numerical parameters Âx and Ât are lumped together in C . Note that C is a dimensionless parameter."
372,779,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where N11 is the number of products for which both samples have the value +1, N11N is the number of products in which the x sample has the value +1 and the y sample !1, and so on. The denominator in Eq. (8.20) is equal to the output that would occur if, for each sample pair, the signs of the signals were identical. %2 can be related to the correlation coefficient % of the unquantized signals through the bivariate probability distribution Eq. (8.1), from which P11 D 2$# 2 1 ! %2"
165,92,0.984,New Methods for Measuring and Analyzing Segregation,"I use the term Separation Index (S) to refer to a measure that has been known by many names over the decades. A partial list of past names includes: the correlation ratio and eta squared ( hÂ² ) (Duncan and Duncan 1955; Stearns and Logan 1986; Iceland et al. 2002), r or rij (Coleman et al. 1975, 1982), the variance ratio (V) (James and Taeuber 1985), and segregation index (S) (Coleman et al. 1966; Zoloth 1976).8 I term this measure the separation index because a high value on this index gives a clear and reliable signal that the two groups in the comparison are residentially separated and generally do not reside in the same areas.9 That is, it indicates whether the two groups live apart from each other due to being concentrated in areas that are racially polarized in a pattern of âprototypicalâ segregation wherein, in the example of White-Black segregation, Whites live in predominantly White areas and Blacks live in predominantly Black areas. I clarify the basis for this claim in more detail shortly. For the separation index (S), the relevant function y = f ( p ) for scoring residential outcomes (y) so S can be obtained from ( Y1 â Y2 ) is quite simple; it is the Additionally, S is a special case of Bellâs (1954) revised index of isolation for the situation in which the population has only two groups. As used here, the term separation does not imply that the groups live in areas that are far apart in distance. It implies only that they are residentially separated into distinctly different areas. These can be far apart but they also can be adjoining as standard implementations of all measures of uneven distribution are âaspatialâ in that the arrangements of units in space does not affect index values."
285,865,0.984,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","cal intervals corresponding to integer ratios of 6/5 and 3/2 for the minor third and the perfect fifth, respectively. If this was the only underlying reason, the âbiasâ should have been larger for the minor third than for the perfect fifth, which was not observed. The SDs of the musical interval adjustments were only slightly larger (a factor of 1.1â2.2) for the ZTs than for the PTs, with no increase in listening time. Thus, overall the results suggest that a weak musical pitch can exist in the absence of peripheral phase locking. Burns and Feth (1983) had three musically trained subjects adjust various musical intervals for reference frequencies of 1 kHz and 10 kHz. For the same musical intervals as used in the present study, SDs in their data were a factor of 1.6â11.7 (mean of 4) times larger for the 10-kHz reference than for the 1-kHz reference. Thus, the increase in SDs for the 10-kHz frequency was larger than that observed here for the ZTs. Burns and Feth (1983) interpreted their results in terms of phase locking, and a decrease thereof with increasing frequency. The present results suggest that their relatively large increase in SDs at 10 kHz reflects the combined effect of lack of familiarity with high-frequency pure tones and the transition from a temporal code to a place code. Acknowledgments Supported by intramural funding from the MRC. Thanks to Brian Moore for helpful comments. Open Access This chapter is distributed under the terms of the Creative Commons AttributionNoncommercial 2.5 License (http://creativecommons.org/licenses/by-nc/2.5/) which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited. The images or other third party material in this chapter are included in the workâs Creative Commons license, unless indicated otherwise in the credit line; if such material is not included in the workâs Creative Commons license and the respective action is not permitted by statutory regulation, users will need to obtain permission from the license holder to duplicate, adapt or reproduce the material."
281,51,0.984,Stochastics of Environmental and Financial Economics (Volume 138.0),"Fig. 8 Probability densities on a log-scale of log-returns where the lag for each asset has been chosen such that the variances of the assets in each subplot is the same. The chosen variances are also displayed in Fig. 7 as horizonal lines. For the smallest and largest variances, not all dataset are present since for some datasets those variances are not attained. Top âpreâ, bottom âpostâ"
348,117,0.984,Control Theory Tutorial : Basic Concepts Illustrated By Software Examples,"The previous chapter assumed that the intrinsic process, P, has a given unvarying form. The actual process may differ from the given form or may fluctuate over time. If a system is designed with respect to a particular form of P, then variation in P away from the assumed form may cause the system to become unstable. We can take into account the potential variation in P by altering the optimal design problem. The new design problem includes enhanced stability guarantees against certain kinds of variation in P. Variation in an intrinsic process is an inevitable aspect of design problems. In engineering, the process may differ from the assumed form because of limited information, variability in manufacturing, or fluctuating aspects of the environment. In biology, a particular set of chemical reactions within an individual may vary stochastically over short time periods. That reaction set may also vary between individuals because of genetic and environmental fluctuations. In all cases, actual processes typically follow nonlinear, time-varying dynamics that often differ from the assumed form. We may also have variation in the controller or other system processes. In general, how much variability can be tolerated before a stable system becomes unstable? In other words, how robust is a given systemâs stability to perturbations? We cannot answer those question for all types of systems and all types of perturbations. However, the Hâ norm introduced earlier provides insight for many problems. Recall that the Hâ norm is the peak gain in a Bode plot, which is a transfer functionâs maximum gain over all frequencies of sinusoidal inputs. The small gain theorem provides an example application of the Hâ norm."
45,40,0.984,Measurement and Control of Charged Particle Beams,"where as before the prime indicates a derivative with respect to the longitudinal position s, and we have dropped the subindex âx, yâ. Together Î², Î±, and Î³ are referred to as the Twiss parameters. Henceforth, we will use x instead of u, but, here and in the following, the same equations apply in the horizontal and in the vertical plane. The main difference is that quadrupoles which are focusing in one plane are defocusing in the other. The functions Ïx,y (s) and Î²x,y (s) in (1.12) vary with the azimuthal location s, while the action Ix,y and initial phase Ï0 are constants of motion. The beam is matched to the lattice if the betatron phases are distributed randomly. In this case, the value of Ix,y averaged over all particles of the beam is equal to the rms beam emittance. For example, in the horizontal plane, we then have â¡ Ïx2 /Î²x = Ix,y  (1.14)"
238,296,0.984,Nanoinformatics,"Next, the present method was applied to the fluorite-CeO2 Î£3[110](111) grain boundary. In contrast to the other three grain boundaries, this one possesses a [110] rotation axis. Notably, the Kriging method can determine the most stable structure (which is in agreement with the one reported previously [33]) using only 12 calculations. These results indicate that the Kriging method is applicable to complex oxides and can potentially achieve efï¬ciency improvements by factors of â¼103â104 over the conventional all-candidate calculation method. Finally, the reasons behind the broad applicability of the Kriging method are discussed. As mentioned above, the Kriging method searches for stable structures in a three-dimensional data set, as shown in Fig. 8.5b, with extrapolation of this data"
130,195,0.984,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 1,"parametrized for purpose of particular instantiation, changes of an already created instance are disregarded. In addition do the template, there is also the static form of representation of pattern instances. The challenge of characterization of static properties lies in capturing the variant and invariant part of the pattern. Most of the time we will be using system of design patterns as a reference pattern system. This chapter presents brief survey of advances in describing selected static properties."
311,44,0.984,The Physics of the B Factories,"and low-energy positron beams in order to avoid beaminstability problems due to ion trapping, which are most serious at lower energies. Both facilities had only one interaction region (IR) for the detector in order to optimize the luminosity. The luminosity of an e+ eâ storage ring is given by Nb neâ ne+ f (1.3.1) where the numbers of electrons and positrons in each bunch are given by neâ and ne+ , Nb is the number of bunches, f is the circulation frequency, and Aeï¬ is the eï¬ective cross-sectional overlapping transverse area of the beams at the interaction point (IP). While the ï¬ve parameters are independent at lower beam currents, at high beam currents Aeï¬ becomes strongly beam-current dependent. As the product Nb neâ ne+ is increased, Aeï¬ increases, thereby limiting the luminosity. Particles inside a beam bunch are deï¬ected when they pass through the collective electromagnetic ï¬elds of the oncoming beam bunch at the IP; as a result, the oncoming bunch collectively acts as a focusing lens. However, these beam-beam eï¬ects are highly non-linear and produce spreads in the operating point in the betatronoscillation tune plane, causing considerable complications in the machine operation. These beam-beam interactions, which become larger as the bunch charges are increased, also limit the luminosity by enlarging Aeï¬ . Attempts to raise the luminosity by raising Nb , the number of bunches in each ring, face a diï¬erent prob-"
142,1534,0.984,Subseafloor Biosphere Linked To Hydrothermal Systems : Taiga Concept,"42.3.2.1 Pore Fluid in the Hakurei Field Vertical profiles of pore fluid chemistry collected from the Hakurei field in the Izena Hole are illustrated in Fig. 42.4, and two components diagrams for some major species are illustrated in Fig. 42.5. Due to somehow poor core recovery, profiles from Station BMS-H-1 are rather discontinuous. In spite of the discontinuity, increase in concentration toward deep could be recognized in some major species such as K, Na, Cl and Si. As discussed in the later subsection based on the pore fluid chemistry in the Iheya North Knoll field, this signature could be explained by hydration during hydrothermal alteration in the deeper layer. Observed shift in Î´D"
381,439,0.984,The Dynamics of Opportunity in America : Evidence and Perspectives,"Appendix 7C: Logistic Probability Models Showing Effects of Demographics on Underutilization Rate of Workers We have estimated a set of logistic probability models to illustrate the independent effects of various demographic variables on the underutilization rates of workers in 2013â2014. The dependent variable in this logistic probability model is UNDERUTIL, a dichotomous variable that takes on the value of 1 if the respondent was underutilized at the time of the CPS and the value of zero if he or she was an active member of the labor force but was not underutilized.22 The right-hand side predictor variables include the gender, age, race-ethnic origin, nativity status, disability status, educational"
175,1083,0.984,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","performance indicators. This step gives the modeler an opportunity to calibrate the resulting single system performance indicator. 5. Develop and compare system performance exceedance distributions, or divide the range of performance values into color-coded ranges and display on maps or in scorecards, as illustrated in Fig. 9.21. The area under each exceedance curve is the mean. Different exceedance functions will result from different water management policies, as illustrated in Fig. 9.22. One can establish thresholds to identify discrete zones of performance indicator values and assign a color to each zone. Measures of reliability, resilience and vulnerability can then be calculated and displayed as well. Scorecards can show the mean values of each indicator for any set of sites. The best value for each indicator can be colored green; the worst value for each indicator can be colored red. The water management alternative having the most number of green boxes will stand out and will probably be considered more seriously than the alternative having the most number of red (worst value) boxes. This ï¬ve-step process has been used in a study of improved ways of managing lake levels and flows in Lake Ontario and its discharges into the St. Lawrence River in North America. Performance criteria were deï¬ned for domestic and industrial water supplies, navigation depths, shore bank erosion, recreational boating, hydropower production, flooding, water quality, ecological habitats. The performance measures for each of these interests were identiï¬ed and expressed as functions of one or more hydrologic variable values related to flow and lake level management. Models designed to simulate alternative lake level and flow management policies were used to generate sets of time series for each system performance criterion. These in turn were combined, summarized and compared. The same ï¬ve-step process has been implemented in the Everglades restoration project in southern Florida. The Everglades is a long very"
165,566,0.984,New Methods for Measuring and Analyzing Segregation,"Summary Comments on Formulating G as a Difference of Means (YW â YB) on Relative Rank The relationship in expression (C.1) now can be placed in broader context as follows. The core terms that define G in expression (C.2) map directly and exactly onto the core terms that define YW â YB in expression (C.3). Consequently, G can be described as registering the White-Black difference in average relative rank on area proportion White (p). Examined in the ânaturalâ metric of relative rank scores, the difference of means YW â YB has a logical range of 0.0â0.5 while the logical range of G is 0.0â1.0. Hence, expression (C.1) equates the two measures based on YW â YB = G 2."
8,729,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","The same function Z.T; V/ is expressed in different ways, once by the density of states of the whole system and once by the mass spectrum of its constituents. We must clearly understand the physical meaning of .E; V/ and of .m/ (see Fig. 26.6): â¢ .E; V/dE is the number of states between E and E CdE of an interacting system enclosed in any externally given volume V. â¢ .m/dm is the number of states (i.e., different particles) between m and mCdm of an interacting system confined to its ânatural volumeâ, i.e., to the volume resulting from the forces keeping these masses together as bound states or resonances."
311,583,0.984,The Physics of the B Factories,"Since diï¬erent parameterizations of the resonance lineshapes, especially for broad resonances, often give diï¬erent values, one has to make sure that the values used in the ï¬t were extracted using the same parameterization as in the model. If the resonance is apparent and systematic biases (or external errors) of its parameters are expected to be larger than their statistical errors from the ï¬t, the mass and width can be left unconstrained. The angular dependence Wr is described using either Zemach tensors (Zemach, 1964, 1965), where transversality is enforced, or the helicity formalism (Bonvicini et al., 2008; Jacob and Wick, 1959), which allows for a longitudinal component in the resonance propagator (see Beringer et al. (2012) for a comprehensive summary). The expressions for scalar, vector and tensor states are J = 0 : Wr = 1 ,"
213,176,0.984,Collider Physics Within The Standard Model : a Primer,"A sensible expansion is only obtained by a proper treatment of momentum conservation constraints, also using the underlying symmetry of the BFKL kernel under exchange of the two external gluons, and especially, of the running coupling effects (see the analysis in [49, 141] and references therein). In Fig. 2.21, we present the results for the dominant singlet splitting function xPgg .x; Ës .Q2 // for Ës .Q2 /  0:2. We see that, while the NNLO perturbative splitting function deviates sharply from the NLO approximation at small x, the resummed result only shows a moderate dip with respect to the NLO perturbative splitting function in the region"
311,2647,0.984,The Physics of the B Factories,"Barate et al., 1998), so it became advantageous (Davier and Hoecker, 1998) to use theory above 1.8 GeV. The situation between 1 GeV and 1.8 GeV changed drastically with the advent of ISR BABAR data. In fact an almost complete set of precise measurements is available and a few remaining channels are being analyzed. These measurements benefit from the excellent particle identification, providing access to many previously unmeasured cross sections. They help to discriminate between older, less precise and sometimes contradictory results. Figure 21.3.6 gives a few examples of measured cross sections and demonstrates the impact of the BABAR results. The band shown on all these plots represents the combination by Davier, Hoecker, Malaescu, and Zhang (2011) of all existing data using the HVPTools package (Davier, Hoecker, Malaescu, Yuan, and Zhang, 2010): it is clearly dominated by the BABAR results. The measurement using the full available data set of the production cross section for several final states are in progress at BABAR. Particularly relevant for the calculation of the muon anomaly are Ï + Ï â Ï 0 Ï 0 , K + K â , KS KL , and KS KL Ï + Ï â . Some channels involving Ï 0 multiplicities larger than 2 will probably remain unmeasured. The estimate of the missing channels, obtained using isospin"
169,61,0.984,Riverine Ecosystem Management : Science For Governing Towards a Sustainable Future (Volume 8.0),"According to the River Styles Framework, introduced by Brierley et al. (2002), an organism existing in a local habitat is exposed to controls and biophysical ï¬uxes associated with larger spatial entities. These entities exist as a nested hierarchy that builds up from âhydraulic unitsâ as the smallest up to larger âgeomorphic unitsâ, âriver reachesâ and âlandscape unitsâ and, ï¬nally, up to the catchment and ecoregion as the largest spatial scales. These ï¬uvial features can be seen as physical templates that provide the setting in which ecological processes operate and shape riverine coenoses. Focusing on the ecological functions and the associated biocoenoses of these different spatial entities, aquatic ecologists generally apply the terms micro-, meso-, or macrohabitats. Confusingly, to date, no consistent deï¬nition exists that includes both the geomorphological and the ecological perspectives. A microhabitat, roughly corresponding to âhydraulic units,â refers to a particular site used by an individual for speciï¬c behaviors (e.g., spawning). It can be described by a combination of distinct hydraulic and physical factors such as ï¬ow velocity, depth, substrate type, and vegetation cover. Depending on the species (ï¬sh, invertebrates, macrophytes, algae, etc.) and the life stage, microhabitats may range from near zero to a few meters. Mesohabitats, typically encountered at the scale of âhydraulicâ and âgeomorphic units,â denote discrete patches of a river channel deï¬ned by similar physical characteristics. Such habitats include shallow rifï¬es, deep pools, runs showing high ï¬ow velocities, or sediment bars. Depending on the river type, mesohabitats commonly extend over a few square meters but may also cover some hundreds of square meters. While microhabitats refer to sites of individual organisms, mesohabitats can be seen as the area, where aquatic communities and/or speciï¬c life stages with similar habitat requirements live (spawning sites, juveniles, adults, etc.). Macrohabitats, spatially best associated with âgeomorphic unitsâ or river reaches, typically comprise several mesohabitats shaped by the particular hydromorphological conditions of the respective river reach, branch, or water body (e.g., lotic main channel of an anabranched river, lentic one-side connected backwater, stagnant dead arm). Accordingly, longitudinal continuity and lateral hydrological connectivity and, thus, the distribution and migration possibilities of aquatic organisms are key features for deï¬ning macrohabitats. The different ï¬uvial featuresâor habitats from the ecological point of viewâincluding those in the adjacent ï¬oodplains, undergo permanent hydromorphological and ecological changes owing to inï¬uences and ï¬uxes, such as ï¬ow and sediments, from the reach or catchment scale. Such adaptive processes of riverine features at a certain spatial scale are also pertinent to speciï¬c time scales. The evolution of a new river terrace, for example, usually encompasses longer time spans than the formation of a gravel bar. In many cases, the consequences of physical modiï¬cations on the ï¬uvial system are not immediately apparent. Rather, they depend on system-inherent thresholds of response and manifold legacy effects. Understanding the complex spatiotemporal nature of river landscapes is an essential prerequisite for sustainable and integrative river restoration. However, under daily pressure to balance short-term demands with scarce ï¬nancial means, the consideration of such complex process-response systems is a challenging task for planners and authorities as well (see Chaps. 15 and 16)."
169,278,0.984,Riverine Ecosystem Management : Science For Governing Towards a Sustainable Future (Volume 8.0),"3. Fish usually migrate within the main current or, in the case of too high ï¬ow velocities, parallel to it. Furthermore, the migration corridor (i.e. surface vs. bottom-oriented and shoreline vs. open water) depends on species-speciï¬c preferences (Seifert 2012). Bypasses have to be directly connected to migration corridors of all relevant species, and attraction ï¬ows should enhance their traceability. Facilities for Upstream Migration Several guidance documents on planning, construction and operation of ï¬sh passes were already developed or are currently under development (Dumont et al. 2005; BMLFUW 2012; Seifert 2012; Schmutz and Mielach 2013, 2015; DWA 2014). As upstream migrations mostly serve reproduction, facilities have to support at least sexually mature age classes. Three main aspects have to be considered: (1) the perceptibility of the entry, (2) the passability of the ï¬sh pass and (3) post-passage effects. Perceptibility depends to a high degree on the position and attraction ï¬ow of the ï¬sh pass entry. In general, it should directly link the ï¬sh pass to the natural migration corridor of ï¬sh and therefore be located close to the barrier, the main current (for hydropower plants, this means close to the turbines) and the shoreline. For oblique weirs, the pointed angle of the weir proved to be advantageous. For bottom-dwelling ï¬sh, a continuous connection to the river bottom is required (e.g. by a ramp with rough substrate and a slope <1:2). Success may depend on a combination of several factors: multiple entries or collection galleries to cover wide barriers (>100 m), varying water levels and several species with different migration corridors and/or swimming capabilities. The attraction ï¬ow has to provide a continuous connection between the migration corridors up- and downstream of the barrier. It should be as parallel as possible to the main current (e.g. <30 ), cause no turbulences and provide a high impulse of ï¬ow (deï¬ned as the product of volume and ï¬ow velocity; Larinier 2002; Seifert 2012). While the ï¬ow velocity is limited by the speciesâ swimming capabilities, the volume can be further increased. At least 1â5% of the turbined ï¬ow are required as attraction ï¬ow (Larinier 2002; Dumont et al. 2005). In many cases, the operational discharge, which only serves the passability of the ï¬sh pass, is too low and has to be enhanced by additional ï¬ow introduced into the lowest part of the ï¬sh pass. In this case, the installation of attraction ï¬ow turbines can reduce energetic losses (Hassinger 2009a; Seifert 2012). Passability of a ï¬sh pass is ensured, if it provides a suitable migration corridor for all relevant species. This is the case when (a) hydraulic conditions do not exceed swimming capabilities, (b) the minimum rheoactive ï¬ow velocity is provided, (c) the spatial dimensions and geometry (depth, width and length) allow adult ï¬sh of the size-decisive species (i.e. species with highest spatial demands) to pass the entire ï¬sh pass and (d) continuous rough substrate supports bottom-dwelling and weaker species by ensuring moderate ï¬ow velocities towards the bottom. With regard to post-passage effects, ï¬sh should be able to continue their migration (without the risk of downstream drift) and ï¬nd suitable habitats. As"
133,431,0.984,"A Demographic Perspective on Gender, Family and Health in Europe","Modelling Strategy and Statistical Analysis We performed two types of GEE-regressions (Ziegler 2011) with binary outcome variables and logistic link functions. (1) In the âLevel Modelsâ we predicted slow walking speed in the follow-up by the characteristics of the previous wave. This implies that for individuals below age 78, all of whom had one follow-up after six years, only one outcome measurement was included in the model and that the characteristics from the BL were used as predictors of the walking speed in the 6YFU. People aged 78 and above in principle had two follow-ups, and the characteristics from the BL were used as predictors for walking speed in the 3YFU, and the characteristics from the 3YFU were used as predictors for walking speed in the 6YFU. Thus, each individual contributed a maximum of two outcome measurements and one outcome measurement in the case of death, attrition, or missing value. (2) In the âChange Modelsâ we explored the decline in walking speed between two waves using the characteristics from the ï¬rst of the two waves as predictors. Similar to above, characteristics from the BL were used to predict the change in walking speed for those below age 78 until the 6YFU, and they can only be included once in the model. For those aged 78 and above the characteristics of the BL predicted the change in walking speed by the 3YFU, and the characteristics of the 3YFU predicted the change by the 6YFU, and they can be included in the model a maximum of two times. In both model types we used an indicator variable to account for the different length of the follow-up periods. The within-person residual covariance matrix was evaluated with the unstructured correlation structure. To establish the gross-effect of the (change in) family position, we ran sex-speciï¬c models controlled for age, the respective design variables, and the walking speed at the previous wave. We refer to these models as Model 1. We then explored the interaction effect between the partnership and the child variables using the category childless, no partner as the reference category. We depict the gradient of the relationships in two ï¬gures. To explore possible interdependencies of the current family situation with other health characteristics, we introduced additional variables (Model 2: type of residence & SES; Model 3: Model 2 + BMI, alcohol consumption and physical activity; Model 4: Model 3 + number of chronic morbidities, depression, Model 5: Model 4 + alcohol consumption and physical activity + number of chronic morbidities, depression). All models were estimated separately for the two sexes. All calculations were performed in Stata 12.1, (StataCorp, TX, USA)."
365,875,0.984,Climate Smart Agriculture : Building Resilience To Climate Change,"Estimation by full information maximum likelihood at the plot-level. Sample size: 2801 plots. Robust standard errors clustered at the woreda level in parentheses. The dependent variable âskewnessâ refers to the third central moment f3(x, Î³3) (i.e., downside risk exposure) of production function (2), and it has been rescaled by 10 milliards; Ïidenotes the square-root of the variance of the error terms Îµji in the outcome Eq. (4a and 4b), respectively; Ïj denotes the correlation coefficient between the error term Î·i of the selection Eq. (1) and the error term Îµji of the outcome Eq. (4a and 4b), respectively. *Significant at the 10% level; **Significant at the 5% level; ***Significant at the 1% level."
372,1733,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"14.2.1 Gaussian Screen Model We begin the discussion by considering a simple model that serves to illustrate many features of the problem. This model was first developed by Booker et al. (1950)) to explain ionospheric scintillation and was refined by Ratcliffe (1956). Scheuer (1968) applied it to pulsar observations. The model assumes that the irregular medium is confined to a thin screen and that the irregularities (blobs) have one characteristic scale size a: Diffraction effects are neglected within the irregular medium; only the phase change imposed by the medium is considered. Diffraction is taken into account in the free-space region between the irregular medium and the receivers. The geometric situation is shown in Fig. 14.5. The thin-screen assumption is not particularly restrictive. However, the assumption that the screen is filled with plasma blobs having one characteristic size is restrictive and distinguishes this model from the power-law model, in which a range of scale sizes is present. From Eqs. (14.5)"
175,1528,0.984,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","12:4 There exists a modest-sized urban subdivision of 100 ha containing 2000 people. Land uses are 60% single-family residential, 10% commercial, and 30% undeveloped. An evaluation of the effects of street-cleaning practices on nutrient losses in runoff is required for this catchment. This evaluation is to be based on the 7-month precipitation record given below. Present the results of the simulations as 7-month PO4 and N losses as functions of street-cleaning interval and efï¬ciency (i.e., show these losses for ranges of intervals and efï¬ciencies). Assume a runoff threshold for washoff of Qo = 0.5 cm. Precipitation (cm)"
311,2382,0.984,The Physics of the B Factories,"does not really work. One possibility is that these states are not canonical csÌ. However, the agreement with theory would be improved for the Ds0 (2317)+ , if other prominent decay modes existed in addition to the Ds+ Ï 0 . 19.3.4.6 Precision measurements of Ds1 (2536)+ properties BABAR measured the mass of the Ds1 (2536)+ with a significant improvement compared to the world average, and, for the first time, measured directly its decay width, instead of reporting an upper limit only (Lees, 2011d)."
175,651,0.984,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","Many important random variables in water resources are functions whose values change with time. Historical records of rainfall or streamflow at a particular site are a sequence of observations called a time series. In a time series, the observations are ordered by time, and it is generally the case that the observed value of the random variable at one time influences the distribution of the random variable at later times."
213,247,0.984,Collider Physics Within The Standard Model : a Primer,"current vertex, the triple weak gauge vertices VW W C V appear in the â and Z exchange diagrams. The Higgs exchange is negligible because the electron mass is very small. The analytic cross-section formula in the Born approximation can be found, for example, in [307] (in the section entitled Cross-section formulae for specific processes). The experimental data are compared with the SM prediction in Fig. 3.5. Within the present accuracy, the agreement is good. Note that the sum of all three exchange amplitudes has a better high energy behaviour than its individual components. This is due to cancellations among the amplitudes implied by gauge invariance, connected to the fact that the theory is renormalizable (the cross-section can be seen as a contribution to the imaginary part of the eC e ! eC e amplitude). The quartic gauge coupling is proportional to g2 ABC W B W C ADE W D W E . Thus in the term with A D 3, we have four charged W particles. For A D 1 or 2, we have two charged W particles and two W 3 particles, each W3 being a combination of â and Z according to (3.10). With a little algebra the quartic vertex can be cast in the g g ;"
273,174,0.984,Report on Global Environmental Competitiveness (2013),"In which, Xi represents the obtained non-dimensional value of Indicator i, Nondimensional Indicator i for short; xi is the original value of the indicator, xmax and xmin represent the maximum and minimum original values of similar indicators under comparison respectively. After dimensionless treatment, the value of each indicator will be within the range of 1â100, with consistent polarity."
175,1283,0.984,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","Equation 11.2 applies for all time periods t and for all reach segments s in a particular reach. Different reaches will likely have different values of the parameters n, Î±, and Î². The calibrated values of Î± and Î² are nonnegative and no greater than 1. Again a mass balance equation updates each segmentâs initial storage volume in the following time period. The outflow from each reach segment is the inflow into the succeeding reach segment. The four-parameter approach assumes that the outflow, Ost, is a nonlinear function of the initial storage volume and inflow Ost Â¼ Ã°a Sst Ã¾ bIst Ãc"
118,173,0.984,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"above illustrates, annual doses exceed the 20 mSv/year level. This fact indicates that efforts to reduce the surface concentration of cesium should be focused in these regions to achieve the first guideline. To achieve the second guideline requires decontamination of a much broader area. With the proportionality between the surface concentration and the annual dose, the target area of decontamination would be all places with a surface contamination greater than 50 kBq/ m2, in other words the areas corresponding to the first through the seventh bars in the legend for Fig. 4.3."
372,1288,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where &0 and ,0 are the rest wavelength and frequency as measured in the reference frame of the source, the corresponding unsubscripted variables are the wavelength and frequency in the observerâs frame, v is the magnitude of the relative velocity between the source and the observer, and + is the angle between the velocity vector and the line-of-sight direction between source and observer in the observerâs frame (+ < 90Ä± for a receding source). The numerator in Eq. (A10.6) is the classical Doppler shift caused by the change in distance between the source and the observer. The denominator is the relativistic time dilation factor, which takes account of the difference between the period of the radiated wave as measured in the rest frame of the source and the rest frame of the observer. Because of the time dilation effect, there will be a second-order Doppler shift even if the motion is transverse to the line of sight. For the rest of this discussion, we consider only radial velocities; that is, + D 0 or 180Ä± . In this case, the Doppler shift equation is"
238,104,0.984,Nanoinformatics,"obtain the mean RMSE and its standard deviation. The results of d-band center prediction evaluations are shown in Figs. 3.5 and 3.6 for the surface impurity and surface overlayer trials, respectively. Among the various methods examined, the GBR approach exhibited the best prediction performance. This was not unexpected, since GBR [29] is widely used and has performed well in top-level data prediction contests such as the Kaggle competition in recent years [30, 31]. Technically, it is an ensemble model composed of boosted regression trees, which often give accurate and stable predictions. Figure 3.7 illustrates the predictive performances of four typical ML methods (OLS, PLS, GPR, and GBR) in a single-shot cross-validation with a 75% training set (â) and a 25% test set (â). These trials used all 18 descriptors: nine for the host and nine for the guest metal. The x-axis in these plots represents the DFT-calculated d-band center values (the ground truth values), while the y-axis gives the predictions from the ML methods. Deviations from the x = y line indicate prediction errors. Clearly, the predictions by linear models (OLS and PLS) exhibit larger"
256,550,0.984,"Supercomputing Frontiers : 4Th Asian Conference, Scfa 2018, Singapore, March 26-29, 2018, Proceedings","mance than the OMP kernel of the BDW. Similar to the SKX, the KNL has much higher peak performance than the BDW; thus the performance improvement of the KNL is insufficient relative to the performance gap between the BDW and KNL. Figure 15 shows the entire execution time of the BiCGSTAB method in all target environments. Here, although the iteration count was not exactly the same, only the total computation times are compared. Nearly all vector and matrix calculations of the BiCGSTAB method were executed on the GPU with the A1 kernel. Similarly, nearly all vector and matrix calculations of the"
256,357,0.984,"Supercomputing Frontiers : 4Th Asian Conference, Scfa 2018, Singapore, March 26-29, 2018, Proceedings","(in terms of overall precision, recall, and F1-score) in the entire dataset. As we estimated, the precision, recall, and F1-score of the majority class are very high on both algorithms (0.98, and as high as 0.99). In contrast, all metrics on the minority class are lower than those on the majority class (e.g. F1-score: 0.74 vs 0.99). However, the overall average of precision, recall, and F1-score achieved very high scores on both algorithms (all around 0.97), due to combining absolute quantity and relative quantity subsets into an entire imbalanced dataset. There is a slight difference in precision and recall between the two algorithms; XGBoost outperforms the RF in precision by 0.02, while decreases the RFâs recall by 0.01. Thus, the precision, recall, and F1-score on the minority class are fairer metrics than those of the majority class when evaluating model performance."
238,287,0.984,Nanoinformatics,"[001]/(410), Î£5[001]/(310), Î£5[001]/(210), and Î£13[001]/(230) respectively. The predicted grain boundary energies of all grain boundaries obtained using the predictor are also plotted in the same ï¬gure. Although the absolute value is not identical to the previous studies due to the differences in empirical potential, the overall proï¬le of the grain boundary energy is in good agreement with previous reports. Notably, small cusps at 16.26Â° and 67.38Â° are also reproduced by the prediction model (other cusps at 28.07Â°, 36.87Â°, and 53.13Â° were used for training). In addition to the GB energy, it was also conï¬rmed that those predicted models ï¬t well to the other calculation and TEM observations. The above results clearly demonstrate that the presented virtual screening method based on machine learning is sufï¬ciently robust and powerful for predicting stable interface structures and energies from initial atomic conï¬gurations. The success of this method implies that the initial atomic conï¬guration is correlated to the grain boundary energy, and its correlation is studied by machine learning."
30,47,0.984,Determinants of Financial Development,"Typically, the number of possible models, 2p given p candidate variables, is large. Most applications of BMA to larger datasets do not average over all possible models, but use a search algorithm to identify the subset of models with greatest relevance. The Occamâs Window and Markov Chain Monte Carlo techniques can be adopted for this purpose.14 The approach developed by Hoeting et al. (1996) has the advantage of selecting variables and identifying outliers simultaneously, but requires a larger sample size relative to the regressor set, and so this method will be applied only in Table 2.1 below. The simpler version of BMA used elsewhere in this study follows Raftery et al. (1997) which focuses only on the subset defined by the Occamâs Window technique and treats all the worstfitting models outside the subset as having zero posterior probability. Embodying the principle of parsimony,15 the use of the Occamâs Window technique considerably reduces the number of possible models, and in the meantime encompasses the inherent model uncertainty present. Once the Occamâs Window technique excludes the relatively unlikely models, the posterior model probabilities for the well-fitting models are then calculated. Once we have posterior model probabilities, we are ready to implement a systematic form of inference for different quantities of interest. For example, when the interest is one of the regression parameters being present, whether positive or negative, what we need to do is to sum up the posterior model probabilities for all models in which the parameter is non-zero, be it positive or negative. In Sections 2.4 and 2.5 below, on the empirical results, the output of the BMA analysis includes the posterior inclusion probabilities for variables and a sign certainty index. The posterior inclusion probability (PIP) for any particular variable is the sum of the posterior model probabilities for all of the models including that variable. The higher the posterior probability for a particular variable, the more robust that determinant for financial development appears to be. For PIPs greater than 0.20, a sign certainty index rather than sign certainty probability is presented, indicating whether the relationship appear to be either positive or negative.16 2.3.2"
256,517,0.984,"Supercomputing Frontiers : 4Th Asian Conference, Scfa 2018, Singapore, March 26-29, 2018, Proceedings","on s. Finally, the communication performance is also affected when the data size is changed from a latency bound regime to a bandwidth bound regime. Although a simple performance model was presented in Refs. [13,17], we need more detailed performance models to predict the above complex behaviors. In this work, we chose s = 12 from s-scan numerical experiments. The strong scaling of the P-CG, P-CACG, and P-CBCG solvers are summarized in Fig. 3. In the strong scaling test, we use 500, 1,000, and 2,000 processors on ICEX and KNL, respectively. On ICEX, all Krylov solvers show good strong scaling, because the computation part is dominant in all cases and the communication part is suppressed below â¼10 s. Therefore, the P-CACG and P-CBCG solvers are slower than the P-CG solver, because of additional computation in CA Krylov methods. On the other hand, on KNL, the computation part is significantly accelerated (3.5 Ã â¼5.1Ã) and the communication part is comparable or slower (0.3Ãâ¼1.1Ã) compared to ICEX. Here, the cause of slower communication performance on KNL is still under investigation. As a result, the remaining communication part, in particular, All reduce becomes a severe bottleneck. On KNL, the cost of All reduce in the P-CG solver increases with the number of processors. This tendency is observed even in the P-CACG solver. However, in the P-CBCG solver, the cost increase of All reduce is suppressed, and at 2,000 processors, it is reduced to â¼1/3 and â¼1/2 compared to the P-CG and P-CACG solvers, respectively. Because of this CA feature, the best performance on KNL is obtained by the P-CBCG solver, and the P-CBCG solver is 1.38Ã and 1.17Ã faster than the P-CG and P-CACG solvers at 2,000 processors, respectively. It is noted that in Ref. [3], the P-CACG solver on the K-computer showed ideal cost reduction of All reduce by 1/s. However, in the present numerical experiment, the cost reduction of All reduce from the P-CG solver is limited to â¼2/3 and â¼1/3 in the P-CACG and P-CBCG solvers, respectively. These performance ratios are far above the ideal one 1/s. In order to understand this issue, more detailed performance analysis for All reduce is needed. Another issue is that the cost of halo data communications increases in the P-CBCG solver, while the number of SpMVs is almost the same as the other solvers. It is confirmed that this cost becomes comparable to that in the P-CACG solver, when the number of CA-steps is reduced to s = 3. Therefore, the performance degradation of halo data communications seems to depend on the memory usage, which increases with s. These issues will be addressed in the future work."
164,156,0.984,"Marginality : Addressing the Nexus of Poverty, Exclusion and Ecology","Marginality is a complex issue that often lies at the root of poverty. To geographically reflect the different dimensions that influence marginality, we undertook this mapping exercise in a first attempt to take into account the different âspheres of life,â instead of focusing on subsets of social, economic, or environmental aspects. The most important limitation of this approach were the variable scales of the different data sets. In the absence of comprehensive sub-national data, we used GNI and political stability data at national-level scales. In contrast data on stunting were provided on sub-national scales, and soil constraints on a pixel-level scale of 5-arc-minute grid cells. These different scales made comparisons difficult. Therefore further work is needed to find more representative data sets at finer levels. An additional advantage would be to refine the pixel resolution, as our mapping approach used a pixel size of 8 Ã 8 km, which is a small scale for the purpose of global mapping, however, 64 km2 is a relatively large area in terms of the number of people potentially living there and therefore interpretations needed to be made carefully. We also acknowledge that the determination of the cut-off points in each dimension were arbitrary and therefore debatable. Further research is needed to assess how different cut-off points influence marginality hotspot mapping. One example was our definition of âdistantâ using the accessibility dataset of Nelson (2009) based on an infrastructure and cost-distance model. Most of the extremely poor live in rural areas, particularly in remote areas (Sachs 2005). Even where available public transport might be unaffordable to those in extreme poverty, the distances in the map were based on the minimum time needed to get to the nearest large settlement using vehicular transportation. The time expenditure is much higher for people without access to vehicular transportation. Further research could apply"
311,2296,0.984,The Physics of the B Factories,"CP violation is parameterized by q/p = |q/p|eiÏ with |q/p| and Ï as free parameters of the fit. The CP violation in decay would be manifest as a diï¬erence between the magnitudes or phases of the individual intermediate states contributing. This type of the CP violation is searched for first by allowing these amplitudes to diï¬er for D0 and D0 decays, while not (yet) introducing additional parameters describing the other two types of the CP violation. Such an approach is used due to possible complications in the fitting procedure.144 In this measurement no significant deviations between the amplitudes of intermediate-state contributions to D0 and D0 decays were found, and hence one can conclude that no sign of CP violation in decay was observed. Following this, the individual amplitudes are fixed to be the same for D0 and D0 and the parameters |q/p| and Ï are introduced. The results are listed in Table 19.2.8. BABAR (del Amo Sanchez, 2010f) uses the same approach for the K + Ï â Ï 0 final state, fitting separately D0 and D0 tagged samples with duplicated mixing parameters denoted as x+ , y + and xâ , y â , respectively. The results are given in Table 19.2.8."
311,800,0.984,The Physics of the B Factories,"17.1.4.2 Measurements of Branching Fractions and q 2 Distributions The semileptonic decay B â ÏâÎ½ has been studied with diï¬erent experimental approaches at the B Factories. The goal is a precise measurement of the branching fraction and the spectrum of the squared momentum transfer, q 2 , to allow for a determination of the q 2 dependence of the B â Ï form factor. The main experimental challenge is the reduction of the much more abundant background from B â Xc âÎ½ decays, where Xc is any hadronic ï¬nal state with a charm quark. It is also diï¬cult to separate B â ÏâÎ½ decays from the other B â Xu âÎ½ decays, where Xu is a charmless hadronic ï¬nal state, due to very similar decay kinematics. The B â ÏâÎ½ analyses are based on event samples with a tagged B meson or on untagged event samples. In the tagged analyses, one of the two B mesons in the BB event is either fully reconstructed in a hadronic decay mode or partially reconstructed in a semileptonic decay mode. While the tagged analyses provide a very clean environment, they are statistically limited for the B Factory data samples. At present, untagged analyses, which were ï¬rst performed by the CLEO collaboration (Athar et al., 2003), still provide the most precise results for B â ÏâÎ½. In untagged analyses, the four-momentum of the undetected neutrino is inferred from the missing energy and momentum in the whole event. The reconstructed neutrino is combined with a charged lepton (â = e, Î¼) and a pion to form a B â ÏâÎ½ candidate. The dominant background at low q 2 is due to e+ eâ â qq (q = u, d, s, c) continuum events, where the charged lepton originates from a semileptonic decay of a produced hadron (mostly from e+ eâ â cc events) or the misidentiï¬cation of a"
105,162,0.984,"Computer and information Sciences : 31St international Symposium, Iscis 2016, KrakÃ³w, Poland, October 27â28, 2016, Proceedings (Volume 659.0)","At each iteration, the solution having the best value of the Fitness function and the position of the food source found by bees are saved. All these steps are repeated for a predefined number of iterations or until a stopping criterion is satisfied."
3,205,0.984,Instructional Scaffolding in STEM Education : Strategies and Efficacy Evidence,"Abstract In this chapter, I conclude this book on computer-based scaffolding in science, technology, engineering, and mathematics (STEM) education. I note the overall effect size point estimate for scaffoldingâg = 0.46âand compare that to other effect size estimates in the literature. I summarize the wide variation in contexts in which and learner populations among which scaffolding is used, as well as note the characteristics along which the magnitude of scaffoldingâs impact does not varyâcontingency, generic versus context specific, and intended learning outcomeâas well as characteristics along which it doesâproblem-centered model with which scaffolding is used, and grade level and learner characteristics. I also note areas in which more research is neededâmotivation scaffolding, scaffolding for students with learning disabilities, and scaffolding in the context of projectbased and design-based learning. Keywords Content learning Â· Context specificity Â· Problem-centered instruction Â· Scaffolding customization Â· Scaffolding strategy Â· STEM disciplines"
311,1920,0.984,The Physics of the B Factories,"The BABAR Collaboration performed a search for lepton flavor violation using the decays Î¥ (nS) â (eÂ± /Î¼Â± )Ï â , where n = 2, 3. The searches were performed using (98.6Â± 0.9) Ã 106 Î¥ (2S) decays and (116.7 Â± 1.2) Ã 106 Î¥ (3S) decays (Lees, 2010c). In addition, data from other resonances and from runs taken away from Upsilon resonances were used to characterize and study the backgrounds. The search uses the fact that the collider is producing the parent Upsilon resonance at rest in the CM frame; the electron or muon that results directly from the Upsilon decay (the âprimary leptonâ) will have energy very close â the single-beam energy in the Upsilon rest frame, EB = s/2. The primary tau lepton will decay, and a second lepton, or a charged pion, consistent with tau decay is then searched for. If a second lepton is found, it is required to have a diï¬erent flavor from the primary lepton. If a pion is found, one or two additional neutral pions must also be reconstructed in the same event. These measures are required in order to suppress Bhabha events or Î¼-pair backgrounds. The main source of background in this search is from e+ eâ â Ï + Ï â events. The four individual search channels each have other sources of background, arising primarily from lepton and hadron misidentification. Bhabha events are further suppressed by requiring that the visible mass in each candidate event has less than 95% of the collider CM energy (indicating the presence of neutrinos, which are not present in Bhabha events). In addition, the missing momentum in each event must not point close to the beamline. Higher-order QED backgrounds, as from twophoton fusion, are suppressed by requiring that the transverse momentum component of the chargeâparticlesâ vector sum is more than 20% of the quantity s â |p1 | â |p2 |, where pi is the three-momentum of charged particle i. The primary lepton momentum is defined by the requirement that x â¡ |p1 |/EB > 0.75. For the hadronic tau decays, the momentum of the tau daughter charged particle is required to satisfy |p2 |/EB < 0.8, and the invariant masses of the track and neutral pion(s) system must be consistent with the mass of either the ÏÂ± or the aÂ± 1 . The Î¼-pair background in the Î¼Ï channel is suppressed by requiring that the opening angle between the charged tracks in the plane transverse to the beams is less than 172â¦ . After all selection criteria are applied, the selection eï¬ciency determined from a signal MC simulation range"
213,200,0.984,Collider Physics Within The Standard Model : a Primer,"and mt measured separately (the top mass is measured from the invariant mass of the decay products), as can be seen from Fig. 2.28 [150]. The mass of the top (and the value of Ës ) can be determined from the crosssection, assuming that QCD is correct, and compared with the more precise value from the final decay state. The value of the top pole mass derived in [27] from the cross-section data, using the best available parton densities with the correlated value of Ës , is mt D 173:3Ë2:8 GeV. This is to be compared with the value measured at the Tevatron by the CDF and D0 collaborations, viz., mt D 173:2 Ë 0:9 GeV. This quoted error is clearly too optimistic, especially if one identifies this value with the pole mass which it resembles. This error is only adequate within the specific procedure used by the experimental collaborations to define their mass (including Montecarlo, with assumptions about higher order terms, non-perturbative effects, etc.). The problem is how to export this value to other processes. Leaving aside the thorny issue of the precise relation between mt with mt , it is clear that there is good overall consistency."
372,1212,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"The antenna gain terms, gm and so on, contain the effects of the atmospheric paths to the antennas as well as instrumental effects, and since these terms do not appear in Eq. (10.34), it is evident that the combination of the three correlator output phases constitutes an observable quantity that depends only on the phase of the visibility. This property of the phase closure relationships was first recognized and used by Jennison (1958). If a point source is observed, then the visibility phases are all zero, and, in the absence of receiver noise, the closure phase is also zero. Note that p if the rms phase noise on each baseline is $, the rms noise in the closure phase is 3$. To help visualize the phase closure concept, consider three stations of an array observing a point source, as shown in Fig. 10.7. We depict the origin of the instrumental phase terms associated with each station as being caused by atmospheric delay along each line of sight. The total visibility phase on each"
372,599,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"The second term on the right side represents the end effects and is approximately zero if there are an integral number of half-cycles of the fringe frequency within the period &a . It also becomes relatively small as !f &a increases, and we assume here that there are enough fringe cycles (say, ten or more) within time &a that end effects can be neglected. To determine the effect of fringe fitting on the noise, we represent the sampled noise by n.i&s /, multiply by the cosine function, and determine the variance (mean squared value). Averaged over time &a , the result is "" N 1 X n.i&s / cos.2""i!f &s / N iD1"
213,300,0.984,Collider Physics Within The Standard Model : a Primer,"representation of SO.10/ which is anomaly free. So GUTs can naturally explain the cancellation of the chiral anomaly. An important implication of chiral anomalies together with the topological properties of the vacuum in non-Abelian gauge theories is that the conservation of the charges associated with baryon (B) and lepton (L) numbers is broken by the anomaly [336], so that B and L conservation are actually violated in the standard electroweak theory (but B L remains conserved). B and L are conserved to all orders in the perturbative expansion, but the violation occurs via nonperturbative instanton effects [87] [The amplitude is proportional to the typical non-perturbative factor exp. c=g2 /, with c a constant and g the SU.2/ gauge coupling.] The corresponding effect is totally negligible at zero temperature T, but becomes relevant at temperatures close to the electroweak symmetry breaking scale, precisely at T  O.TeV/. The non-conservation of B C L and the conservation of B L near the weak scale plays a role in the theory of baryogenesis that aims quantitatively at explaining the observed matterâantimatter asymmetry in the Universe (for reviews and references, see, for example, [115])."
213,205,0.984,Collider Physics Within The Standard Model : a Primer,"2.10 Measurements of Ës Very precise and reliable measurements of Ës .mZ / are obtained from eC e colliders (in particular LEP), from deep inelastic scattering, and from the hadron colliders (Tevatron and LHC). The âofficialâ compilation due to Bethke [99, 311], included in the 2012 edition of the PDG [307], is reproduced here in Fig. 2.32. The agreement among so many different ways of measuring Ës is a strong quantitative test of QCD. However, for some entries the stated error is taken directly from the original works and is not transparent enough when viewed from the outside (e.g., the lattice determination). In my opinion one should select a few of the theoretically cleanest processes for measuring Ës and consider all other ways as tests of the theory. Note that, in QED, Ë is measured from a single very precise and theoretically clean observable (one possible calibration process is at present the electron g 2 [242]). The cleanest processes for measuring Ës are the totally inclusive ones (no hadronic corrections) with light cone dominance, like Z decay, scaling violations in DIS, and perhaps Â£ decay (but for Â£ the energy scale is dangerously low). We will review these cleanest methods for measuring Ës in the following."
372,1357,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where the subscripts k and p indicate the kth visibility value at the pth pointing position, and $kp is the variance of the visibility. An initial model is required, and the procedure follows a series of steps described by Cornwell (1988): 1. For the first pointing center, multiply the current trial model with the antenna beam as pointed during the observation, and take the Fourier transform with respect to .l; m/ to obtain the predicted visibility values. 2. Subtract the measured visibilities from the model visibilities to obtain a set of residual visibilities. Insert the residual visibilities into the accumulating #2 function of Eq. (11.17). 3. By Fourier transformation, convert the residual visibilities, weighted inversely as their variances, into an intensity distribution. Taper this distribution by multiplying it by the antenna beam pattern, and store in a data array of dimensions equal to the full MEM model."
359,27,0.984,"Micro-, Meso- and Macro-Dynamics of the Brain","also large differences in efferent and afferent connections as well as sharp genetic variations that delineate regions across the longitudinal axis (reviewed in Strange et al. 2014). In further support of anatomical segregation is the finding that place cells in the septal versus the temporal hippocampus have been shown to remap at different rates (Komorowski et al. 2013) and to possess different place field properties on the radial arm maze, linear track, and zig-zag maze (Royer et al. 2010). The anatomy and physiology of CA1 projections to the subiculum strongly suggest that single subicular cells have access to a large range of the longitudinal axis of CA1. Cell reconstruction studies have shown that CA1 cells project to âslabsâ of the subiculum that span a narrow range of the transverse axis but up to 2 mm along the longitudinal axis (Tamamaki and Nojyo 1990, 1991). Those subicular cells would integrate across a broad range of hippocampal theta phases (~60 ). In vitro comparisons of physiology in hippocampal slices versus that in an intact preparation showed large differences in the theta phase offsets between CA3 and the subiculum and in the theta frequency, suggesting that the slice preparation severed processes necessary for communication across lamellae (Jackson et al. 2014). Physiological studies, like those done between CA3 and CA1 (Andersen et al. 2000), are needed to determine the strength of these cross-laminar projections. If cross-laminar communication is substantial, the compression that had been hypothesized to occur over time may occur instead over co-active neurons firing at different local phases (Lubenov and Siapas 2009). In this scheme, information is communicated by which neurons are co-active and not by their inter-spike intervals (Harris 2005). Segmentation of the environment would still be evidenced by which regions of space were represented by the ensemble at each phase, though these segments may not change within a theta period (for a different perspective see Shankar and Howard 2015)."
179,1124,0.984,"Habitats and Biota of the Gulf of Mexico: Before the Deepwater Horizon Oil Spill: Volume 1: Water Quality, Sediments, Sediment Contaminants, Oil and Gas Seeps, Coastal Habitats, Offshore Plankton and Benthos, and Shellfish","7.3 HABITAT DEFINITIONS This section is a broad summary of the different physical habitats within the entire offshore ecosystem of the Gulf of Mexico. This classification is based for the most part on water depth, but also on other physical characteristics that are or can be important in determining what types of organisms live in that habitat. These categories are important because the abundance and diversity can vary widely between habitats, depending on the physical (chemical and geological) conditions. Each habitat and its biota will thus provide different ecosystem services."
238,358,0.984,Nanoinformatics,"ï¬lm growth of inorganic solids, especially oxides. It has several advantages compared to the other deposition techniques, such as sputtering. In the case of PLD, the chemical composition of the resultant ï¬lm is almost same as that of the target material, although generally it differs from the target because the chemical species show different sputtering yields in the case of sputtering. Moreover, the atmosphere in the PLD chamber can be widely controlled from an ultrahigh vacuum to â¼102 Pa, allowing a thermodynamically nonequilibrium crystalline phase of a material to be fabricated. As an example, PLD growth and characterization of the SrTiO3-SrNbO3 solid solution system are explained. SrTiO3 has attracted increasing attention as the next generation of oxide electronics [2]. Doping with the appropriate substituent, such as Nb5+ (Ti4+ site) or La3+ (Sr2+ site), easily varies the charge carrier concentration of SrTiO3 from insulating to metallic (n3D â¼ 1021 cmâ3). Electron-doped SrTiO3 is one of the most extensively studied materials for thermoelectric applications [3, 4]. In 2001, Okuda et al. [5] synthesized Sr1âxLaxTiO3 (0 â¤ x â¤ 0.1) single crystals by the floating-zone method. They reported that the crystals exhibit a large power factor (S2 â Ï) of 2.8â3.6 mW mâ1 Kâ2 at room temperature. Later, Ohta et al. reported the carrier transport properties of Nb- and La-doped SrTiO3 single crystals (carrier concentration, n â¼ 1020 cmâ3) at high temperatures (â¼1000 K) to clarify the intrinsic thermoelectric properties of these materials [6]. The experimental discovery of unusually large thermopower outputs from superlattices and two-dimensional electron gases in SrTiO3 [7, 8] spurred substantial research efforts into SrTiO3 superlattices [9, 10] and heterostructures [11â13] for thermoelectric applications. For example, a superlattice composed of one unit cell (uc) of SrTi0.8Nb0.2O3 and 10 uc of SrTiO3 exhibits a giant thermopower, most likely due to an electron conï¬nement effect. Although electron conï¬nement is strongly correlated with the electronic structure [14, 15], a full understanding of the fundamental electronic phase behavior of the SrTi1âxNbxO3 solid solution system has yet to be developed. Although high-quality single crystals of SrTi1âxNbxO3 species with x > 0.1 are not available due to the low solubility limit of Nb in the lattice [16], epitaxial ï¬lms with these material compositions can be fabricated by PLD [17]. As summarized in Fig. 10.2, pure SrTiO3 (space group Pm3Ìm, cubic perovskite structure, a = 3.905 Ã) is an insulator with a bandgap of 3.2 eV. The bottom of the conduction band is composed of triply degenerate, empty Ti 3dât2g orbitals, while the top of the valence band is composed of fully occupied O 2p orbitals [18]. The valence state of Ti ions in crystalline SrTiO3 is 4 + (Ti 3d0). On the other hand, pure SrNbO3 (space group Pm3Ìm, cubic perovskite structure, a = 4.023 Ã) is a metallic conductor [19â21]. The valence state of the Nb ion is 4 + (Nb 4d1). In between SrTiO3 and SrNbO3 in the SrTi1âxNbxO3 ss, there are two possible types of valence state changes in the Ti and Nb ions, as shown in Fig. 10.2b and c. In the case of isovalent substitution (Fig. 10.2b), the mole fraction of Ti4+ proportionally decreases with increasing Nb4+ (x). On the other hand, heterovalent substitution, in which two Ti4+ or Nb4+ ions are substituted by adjacent (Ti3+/Nb5+) ions, can occur, as shown in"
256,479,0.984,"Supercomputing Frontiers : 4Th Asian Conference, Scfa 2018, Singapore, March 26-29, 2018, Proceedings","We examined the computational performance and convergence properties of the LOBPCG method. We solved the 2-D 4 Ã 5-site Hubbard model with 5 up-spin electrons and 5 down-spin ones. The dimension of the Hamiltonian derived from the model is about 240 million. The number of non-zero off-diagonal elements is about 1.6 billion. We solved for one, five and 10 eigenvalues (and corresponding eigenvectors) of the Hamiltonian on 768 cores (64 MPI processes Ã 12 OpenMP threads) of the SGI ICE X supercomputer (see Table 1) in Japan Atomic Energy Agency (JAEA). Table 2 shows the results for a weak interaction case (U/t = 1) and a strong one (U/t = 10). Table 3 shows the elapsed times of some representative operations. The results for U/t = 1 indicate that point Jacobi (PJ) and zero-shift point Jacobi (ZSPJ) preconditioners hardly improve the convergence compared to without using a preconditioner at all. When we solve for many eigenvalues, the PJ and ZSPJ preconditioners have little effect on the speed of the calculation. On the other hand, the Neumann expansion preconditioner can decrease the number of iterations required for convergence. Moreover, the larger the Neumann expansion series s, the fewer iterations required. When we solve for only Table 1. Details of SGI ICE X Processor"
311,3023,0.984,The Physics of the B Factories,"25.2 Benchmark new physics models Editors: Emi Kou, Jure Zupan (theory) In this section we review the impact of the B Factories on our understanding of the flavor structure in the Standard Model (SM) and on constraining new physics (NP) models. The most important overall result of the B Factories physics program is the fact that the CP violation observed in flavor changing processes with quarks is due to the Kobayashi-Maskawa (KM) mechanism (Kobayashi and Maskawa, 1973). For instance, prior to the B Factories, the kaon sector was the only system where CP violation was observed (see Section 16.1). The observed strength of the CP violation in mixing, Ç«K â 2.3 Ã 10â3 , was consistent with the KM mechanism with an O(1) CP phase in the CKM matrix. While encouraging, this by no means constituted a proof that the KM mechanism was really the origin of the observed CP violation. The first test of the KM mechanism was then done by the measurement of sin 2Ï1 by the B Factories. By now the KM mechanism has been tested at the level of â¼ O(10%), while deviations from its predictions at levels smaller than this are still allowed. In the KM mechanism there is only one weak phase, providing a single source of CP violation. The consistency of Ç«K with the observed CP violation in Bd0 âB 0d mixing and the measurements of the sides and angles of the standard CKM Unitarity Triangle all point to this common origin of CP violation. In addition, the size of Îms and the recent LHCb bound on the size of the weak phase in Bs0 â B 0s mixing both agree with the KM mechanism within errors. This agreement between b â d, b â s and s â d transitions is nicely summarized in the CKM fit plot (see Fig. 25.1.1). Another important indicator that the CP violation we are observing in flavor changing processes of quarks is due to the KM mechanism, comes from a comparison of a fit with only CP conserving observables with a fit with only CP violating observables. Both fits point to the same region in the ÏÌ and Î·Ì plane, which is a strong test of KM nature of CP violation (see Fig. 25.1.2). The measurements at the B Factories also had a direct impact on new physics models. For instance, a measurement that sin 2Ï1 is O(1) immediately excluded approximate CP models. In these models all the couplings which govern the low energy phenomena are real or almost real, with imaginary components always much smaller than the real ones. In the SM, the observed Ç«K and Ç«â² , representing the CP violation in the kaon sector, are small, which can be explained by the smallness of the CKM matrix elements entering into the description of these observables. In the approximate CP models it would be small because CP symmetry is only slightly broken and thus all CP violating phases are small. A set of well motivated realizations in the SUSY framework was put forward (Abel and Frere, 1997; Babu and Barr, 1994; Babu, Dutta, and Mohapatra, 2000; Eyal, Masiero, Nir, and Silvestrini, 1999; Eyal and Nir, 1998). For instance approximate CP conservation could naturally solve the âSUSY CP problemâ explaining"
372,830,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"Equations (8.79) and (8.83) can easily be evaluated numerically and provide values of quantization efficiency for any number of equally spaced levels. Since no significant approximations were made, the same method can be used for cases in which the number of quantization levels is small and consequently the quantization noise is relatively large. Values of (Q for two, three, and four levels can be obtained by considering the effect of the quantization noise at a correlator input, following the method used above. In cases such as that in Appendix 8.3, for which the assigned values for the levels are chosen to optimize (Q , or for which the spacing between the level thresholds is not uniform, the formulas derived here cannot be applied directly. However, the same general approach of considering the spacings between levels can be used. For three-level quantization, the levels for maximum quantization efficiency are Ë0:612# .* D 1:224/. Then we have hxOxi D"
372,865,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"in the correlator outputs, which can be largely eliminated by phase switching, as described in Sect. 7.5. Alternately, the offsets in the samplers can be measured by incorporating counters to compare the numbers of positive and negative samples produced. Correction for the offsets can then be applied to the correlator output data [see, e.g., Davis (1974)]. In samplers with three or more quantization levels, the performance depends on the specification of the levels with respect to the rms signal level, #. An automatic level control (ALC) circuit is therefore sometimes used at the sampler input. Errors resulting from incorrect signal amplitude become less important as the number of quantization levels is increased; with many levels, the signal amplitude becomes simply a linear factor in the correlator output. In systems using complex correlators, two samplers are usually required for each signal, one at each output of a quadrature network. The accuracy of the quadrature network and the relative timing of the two sample pulses are also important considerations."
311,904,0.984,The Physics of the B Factories,"There are several factors that must be taken into account when reconstructing B decays to charmonium where the charmonium are reconstructed via dileptons. The first is that the invariant mass of the two leptons is often significantly below the nominal J/Ï or Ï(2S) mass. This is the result of both final state radiation and energy loss in the detector via bremsstrahlung. This is particularly true for the dielectron mode. Analyses often correct for this energy loss by adding in the energy of photon showers that are within a small angle (typically 50 mrad) of the initial electron direction (e.g. Aubert, 2009m; Guler, 2011) to the invariant mass calculation. The second is that for fully reconstructed B mesons, it is important to perform a mass-constrained fit of the J/Ï or Ï(2S). This improves the energy resolution of the reconstructed B significantly as most of the energy of a charmonium meson coming from a B decay is in its mass.54 This fit or a global fit for the B meson usually includes the well-measured dilepton vertex. For charmonium states with radiative decay to J/Ï or Ï(2S), radiative Î³ candidates must pass a minimum energy cut, typically 30 MeV. A common additional requirement is that the Î³ candidate not be a part of a Ï 0 â Î³Î³ candidate."
228,56,0.984,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"Another way of ranking fuzzy numbers is to use their Î±-cuts [23]. The extension of the concept of fuzzy numbers are Ordered Fuzzy Numbers (OFNs) proposed in [14, 15]. The OFNs are ordered pairs of continuous real functions defined on the interval [0, 1] and their applications are the subject of research [4, 12,"
166,363,0.984,Surveying Human Vulnerabilities across the Life Course (Volume 3.0),"We investigate the problem of attrition using the Swiss Household Panel (SHP). The SHP is a longitudinal survey with annual repetition and its main objective is the analysis of socio-economic change within households, in particular the dynamics of living conditions of the population in Switzerland. In this contribution, we present two kinds of attrition analysis: first we show in Sect. 4 how variables can be affected by attrition by comparing means and frequencies of the answers given in the first wave by all respondents (all longitudinal sample members) with the longitudinal sample members still participating at a later wave. We also assess to what extent the use of weights correct for any bias. Second, we show which sociodemographic characteristics are most related to attrition patterns. This second part focuses on the relation between indicators of latent vulnerability and panel attrition and is addressed in Sect. 5. Using a discretetime competing risk model, we analyse the impact of these variables on the dropout probability. Before turning to the analytical part, we describe in Sect. 2 the relation between being at risk of vulnerability and several demographic characteristics by using different theoretical approaches and point out how in turn these characteristics are related to attrition by presenting results from other studies. In Sect. 3 we give a short overview of the data used for our analyses. The last part of this contribution, Sect. 6 concludes the chapter by discussing the potential and limits of both analysing and countering selective attrition."
372,1885,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"in the eastâwest direction; for an example, see Thompson (1982). The clustering also suggests the possibility of reducing the interference response by deleting any questionable visibility data near the v axis. The resulting degradation of the .u; v/ coverage would increase the sidelobes of the synthesized beam. The discussion above applies to cases in which the observation is of sufficiently long duration that the .u; v/ plane is well sampled, and in which the strength of the interfering signal remains approximately constant during this time. If only a fraction Ë of the .u; v/ loci cross the v axis, then a factor of Ë should be introduced into the denominators of Eqs. (16.14) and (16.16). Strong, sporadic interference can produce different responses from that considered above."
142,499,0.984,Subseafloor Biosphere Linked To Hydrothermal Systems : Taiga Concept,"sampled an individual flow or related flows. The only exceptions are samples from site CIR-S1 and the Knorr Seamount, where dredging appears to have sampled several flows that are exposed at these locations. Some basalt from the Knorr seamount exhibits significant K2O enrichment. Because they are enriched only in K in terms of trace element chemistry (see below), the enrichment might result from alteration. Na8 values, calculated as the Na2O content at an MgO value of 8.0 wt%, are indicative of the degree of partial melting (Klein and Langmuir 1987), where higher Na8 values are indicative of lower average degrees of partial melting, and vice versa. The Na8 values of the majority of basalts from the southern CIR range from 2.5 to 3.0, barring samples from the central CIR-S2 segment and the northern parts of CIR-S3 and CIR-S4, all of which are associated with basalts with Na8 values of <2.5 (Fig. 13.3)."
372,577,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"that the only inputs to the correlators in Fig. 6.3 are the noise waveforms nm , nn , and nH m , where the last is the Hilbert transform of nm produced by the quadrature phase shift. The expectation of the product of the real and imaginary outputs is hnm nn nH m nn i, which can be shown to be zero by using Eq. (6.36) and noting that the expectations hnm nn i, hnm nH m i, and hnm nn i must all be zero. Thus, the noise from the real and imaginary outputs is uncorrelated.5 The signal and noise components in the measurement of the complex visibility are shown in Fig. 6.8 as vectors in the complex plane. Here V represents the visibility as it would be measured in the absence of noise, which is assumed to be along the x, or real, axis; and Z represents the sum of the visibility and noise, V C "". We consider Z and "" to be vectors whose components correspond to the real and imaginary parts of the corresponding quantities. The components of "" are independent Gaussian random variables with zero mean and variance , 2 . Hence, the noise in both components of Z has an rms amplitude ,, and hjZj2 i D jVj2 C 2, 2 :"
311,1046,0.984,The Physics of the B Factories,"B 0 and B Â± decays, and combinatorial background from continuum. Peaking background is dominated by the following sources: B 0 â Dâ+ Ï â and B 0 â Dâ+ Ïâ with secondary-lepton or fake lepton tags; B 0 â Dâââ Ï + , B + â Dââ0 Ï + , and B 0 â Dââ Ï + Ï 0 decays with primary-lepton, secondary-lepton, or fake lepton tags. Peaking and non-peaking background p.d.f.s are convolved with their own resolution functions. From the fit Belle obtains Îmd = (0.509 Â± 0.017 Â± 0.020) psâ1 , where the first error is statistical and the second is systematic. The systematic error in Îmd is dominated by uncertainties in the background fractions (0.014 psâ1 ) and the signal Ît resolution function (0.012 psâ1 ). 17.5.2.4 Fully-reconstructed ï¬nal states Hadronic decay modes BABAR reconstructs neutral B mesons in the decay modes B 0 â D(â)â Ï + , D(â)â Ï+ , D(â)â a+ using a data 1 , J/ÏK sample of 29.7 fb (Aubert, 2002a,b). Belle uses the B decays to the hadronic final states Dâ Ï + , Dââ Ï + , and Dââ Ï+ in a data sample of 29.1 fbâ1 (Tomura, 2002b). The B 0B 0 mixing analyses with fully-reconstructed final states reconstruct the same decay modes of the B 0 daughters as in the B 0 lifetime measurements described in Section 17.5.1.3 (Aubert, 2001c; Abe, 2002m). Both experiments reduce background from continuum events by applying requirements on the normalized second FoxWolfram moment R2 and the angle between the thrust axis of the particles that form the reconstructed B candidate and the thrust axis of the remaining tracks and unmatched calorimeter clusters in the event, computed in the Î¥ (4S) frame. Neutral B candidates are identified by their ÎE and mES values. BABAR selects events with mES > 5.2 GeV/c2 and |ÎE| within Â±2.5Ï of zero. They use the events in the background-dominated region mES < 5.27 GeV/c2 to determine the parameters of the background Ît distributions. Belle requires mES and ÎE to be within Â±3Ï around their expected means. They use candidates from a sideband region in the mES â ÎE plane to determine the background parameters. Events with a reconstructed B 0 are then analyzed to determine the flavor of the other B using the B flavor tagging algorithms described in detail in Chapter 8. Belle assigns 99.5% of the events to a flavor tag category, while BABAR rejects the 30% of events with marginal flavor discrimination. The decay time diï¬erence Ît between B decays is determined from the measured separation Îz = zrec â ztag along the z axis between the vertices of the reconstructed Brec and the flavor-tagging Btag according to Eq. (17.5.13). BABAR applies an event-by-event correction for the directions of the B meson momenta with respect to the z direction in the Î¥ (4S) frame. A description of this correction and details of the calculation of zrec and ztag and their respective resolutions for fully-reconstructed B decays are given in Chapter 6. In its paper, BABAR notes"
311,1403,0.984,The Physics of the B Factories,"(3.66 Â± 0.85 Â± 0.60) Ã 10â4 (BABAR, EÎ³ > 1.9 GeV), (17.9.25) where the errors are statistical and systematic (including some small model-dependence). Although this analysis is currently statistically limited, it is a promising method for the future, i.e., at a high-luminosity B Factory. The dominant systematic uncertainties (e.g., from BB backgrounds) may also be reduced significantly with a larger data sample. (This is equally true for the untagged and lepton-tag methods, despite the use in those cases of oï¬resonance data.) An improvement might also be possible by including semileptonic tags. 17.9.2.4 Sum of exclusive modes"
213,189,0.984,Collider Physics Within The Standard Model : a Primer,"s and D Q2 =s. The value of the LO cross-section is inversely proportional to the number of colours NC , because a quark of given colour can only annihilate with an antiquark of the same colour to produce a colourless lepton pair. The order Ës .Q2 / NLO corrections to the total rate were computed long ago [42, 273] and found to be particularly large, when the quark densities are defined from the structure function F2 measured in DIS at q2 D Q2 . The ratio corr =LO of the corrected and the Born cross-sections was called the K-factor [28], because it is almost a constant in rapidity. More recently, the NNLO full calculation of the K-factor was completed in a truly remarkable calculation [240]. Over the years the QCD predictions for W and Z production, a better testing ground than the older fixed-target DrellâYan experiments, have been compared with experiments at CERN SpNpS and Tevatron energies and now at the LHC. Q  mW;Z is large enough p to makepthe prediction reliable (with a not too large K-factor) one has and the ratio D Q= s is not too small. Recall that, in lowest order, p x1 x2 s pD Q2 , so that the parton densities are probed at x values p around . We D 0:13â0.15 (forpW and Z production, respectively) at s D 630 GeV (CERN SpNpS collider) p D 0:04â0.05 at the Tevatron. At the LHC at 8 TeV or at 14 TeV, one has  10 2 or  6  10 3 , respectively (for both W and Z production). A comparison of the experimental total rates for W and Z with the QCD predictions at hadron colliders [327] is shown in Fig. 2.23. It is also important to mention that the cross-sections for di-boson production (i.e., WW, WZ, ZZ, W , Z ) have been measured at the Tevatron and the LHC and are in fair agreement with the SM prediction (see, for example, the summary in [285] and references therein). The typical precision is comparable to or better than the size of NLO corrections. The calculation of the W=Z pT distribution is a classic challenge in QCD. For large pT , for example pT  O.mW /, the pT distribution can be reliably computed in perturbation theory, and this was done up to NLO in the late 1970s and early 1980s [183]. A problem arises in the intermediate range QCD mW , where the bulk of the data is concentrated, because terms of order Ës . p2T / log m2W =p2T become"
372,1438,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"To overcome the limitations of fringe-frequency analysis, techniques for the precise measurement of the relative group delay of the signals at the antennas were developed. The use of bandwidth synthesis to improve the accuracy of delay measurements has been discussed in Sect. 9.8. The group delay is equal to the geometric delay *g except that, as measured, it also includes unwanted components resulting from clock offsets at the antennas and atmospheric differences in the signal paths. The fringe phase measured with a connected-element interferometer observing at frequency ) is 2"")*g , modulo 2"". Except for the dispersive ionosphere, the group delay therefore contains the same type of information as the fringe phase, without the ambiguity resulting from the modulo 2"" restriction. Thus, group delay measurements permit a solution for baselines and source positions similar to that discussed above for connected-element systems, except that clock offset terms also must be included. It is interesting to compare the relative accuracies of group delay and the fringe frequency (or, equivalently, the rate of change of phase delay) measurements. The intrinsic precision with which each of these quantities can be measured is derived in Appendix 12.1 [Eqs. (A12.27) and (A12.34)] and can be written &f D"
165,396,0.984,New Methods for Measuring and Analyzing Segregation,"individuals and households cannot be assigned in fractional parts (1965: 231â235).1 Later methodological studies characterized Dâs positive expected value under random assignment (i.e., E [ D ] > 0) as âbiasâ and raised awareness that bias in D varies in complex ways that can make scores for D problematic in many situations (e.g., Cortese et al. 1976; Winship 1977). The issue has now received regular attention for four decades and a large literature has grown with contributions from many methodological studies that have considered the nature of index bias, its practical consequences, and possible approaches for diagnosing and dealing with it (e.g., Taeuber and Taeuber 1976; Cortese et al. 1976, 1978; Blau 1977; Winship 1977, 1978; Massey 1978; Falk et al. 1978; Farley and Johnson 1985; Boisso et al. 1994; Carrington and Troske 1997; Ransom 2000; Allen et al. 2009; Mazza and Punzo 2015). Consensus exists on many important points relating to certain technical aspects of index bias. Several key understandings trace to Winshipâs (1977) influential early analysis of the bias behavior of D and S. Of particular note, Winship introduced two analytic formulas for calculating the expected value of D (denoted by E[D]) under random distribution. Both formulas are based on a formal model of random distribution of households from two groups over areas of constant population size (ti). He termed one formula âexactâ because it implements detailed calculations based on the binomial probability distribution and can be applied at both small and large values of area population size. He termed the other formula an âapproximationâ because it draws on simpler calculations that yield satisfactory results when area population size is not small (i.e., when t i Â³ 25). Examining the approximation formula, E [ D ] = 1 / 2p t i PQ , clarifies how E[D] varies over study design and demographic conditions. Specifically, it reveals that two terms âarea population size (ti) and the relative size of the reference group (P) â determine how the value of E[D] varies with city racial composition and with study design (i.e., the size of spatial units used in assessing segregation). The first term, the area pairwise population count (ti), has an inverse relationship with E[D]; all else equal, E[D] declines as ti increases. This relationship can provide a rationale for why research moved from once commonly assessing segregation using small areas such as blocks to more often using larger areas such as census tracts. It also provides a rationale for avoiding group comparisons which involve small combined populations. The practice provides a measure of protection against index bias, but this comes with substantial costs. It eliminates the option of investigating segregation in smaller cities and communities where tracts are too big to capture segregation patterns. It also eliminates the option of studying segregation involving small groups and subpopulations."
311,861,0.984,The Physics of the B Factories,"We begin our discussion with b â cuÌd transitions. The case of b â cuÌs is completely analogous. Examples for these transitions are shown in Fig. 17.3.1. A popular and useful approach to calculate decay rates (especially for two-body B decays) is the factorization ansatz. To understand this technique, consider the decays that are shown in Fig. 17.3.2. In this ï¬gure only the electroweak contributions to the decay amplitudes are shown. A naÄ±Ìve attempt to calculate the decay rate would write the matrix element in terms of the usual currents, e.g., cÎ³u (1 â Î³5 )b. However, this is clearly a drastic approximation as it neglects the all important role of gluons in the production of the ï¬nal state hadrons. Nevertheless, at this early stage of calculation an important distinction becomes apparent. The decay B + â D0 Ï + can proceed through two amplitudes as shown in Figs 17.3.2a) and b). Since all ï¬nal state particles must be color singlets, diagram b) will be suppressed due to color matching relative to a) by 1/Nc , with Nc the number of colors. Amplitudes such as Fig. 17.3.2 a) are known as âcolor-allowedâ while an amplitude such as Fig. 17.3.2 b) is often called âcolor-suppressedâ. The decay"
165,341,0.984,New Methods for Measuring and Analyzing Segregation,"A series of recently completed studies by Amber Fox Crowell provides insight into what the future of research on residential segregation is going to look like.17 The primary focus of her research is on the factors determining White-Latino segregation. Her dissertation research (Fox 2014) presents detailed analyses investigating White-Latino in six major metropolitan areas. The analyses draw on restricted micro-data files of the 2000 decennial census and the restricted micro-data files of the 2008â2012 American Community Survey. Crowell applies the methods discussed in this work to the full potential that can be achieved with extant data. She measures residential outcomes at the level of census blocks and performs sophisticated quantitative analyses using the method of fractional regression to assess the impact of social and economic characteristics on White and Latino residential attainments. She then performs standardization and components analysis to assess the role of group differences in social and economic characteristics in explaining White-Latino residential segregation. Her studies present detailed results for analyses pertaining to segregation measured both using the separation index (S) and the dissimilarity index (D). I limit the presentation here to selected results from her analyses focusing on group separation (S) but note that the results for the dissimilarity index are similar in overall pattern. The most striking contribution of her research is her ability to investigate how a comprehensive set of social and economic characteristics shape residential outcomes for Latino households. The list of micro-level predictors and the estimated coefficients indicating their impact on the residential attainments of Whites and Latinos in Houston, Texas in 2000 and in 2010 is presented in Table 9.9. Results for other cities are not presented to conserve space, but the results for Houston give the full flavor of the analyses Crowell is able to conduct. Her attainment equations include a wide range of relevant predictors including age, level of education, household income, military service, nativity and citizenship, year of immigration, English ability, marital/family status, and recent immigration experiences. No previous study has ever been able to take all of these factors into account simultaneously to quantitatively assess their impact on overall (city-level) residential segregation. The results reported in Table 9.9 show that all of the micro-level variables have statistically significant effects in both the equation for Whites and the equation for Latinos. The âcenteredâ constant reported in the table is the expected value of contact with Whites when independent variables are set at reference categories (for categorical variables) or values (for interval variables). The coefficients reported are fractional effects. These are additive effects on the logit value of the mean for contact with Whites. Positive effects are seen for education, income, and English language ability, produce greater average contact with Whites. Negative effects are"
77,297,0.984,A Life Course Perspective on Health Trajectories and Transitions (Volume 4.0),"over time. The regression coefficient for time for the linear model (model 1) is 0.240, so for each year, the BMI increases (linearly) with 0.240 kg/m2 . The corresponding standard error (0.003; indicative of the preciseness in the estimation of the regression coefficient â the smaller the more precise) can be used to assess the statistical significance of the regression coefficient by calculating the zstatistic (Altman 1991; Twisk 2013). The z-statistic is calculated by dividing the regression coefficient with the corresponding standard error. Values above 1.96 indicate statistical significance. The next step is to extend the model with a random slope for time, indicating that the development of BMI over time is different for the subjects in the sample (model 2). These two models with a random intercept and a random intercept plus random slope can be compared by the likelihood ratio test to assess whether or not the inclusion of a random slope is necessary and improves the model fit. For each model, the -2 log likelihood is provided and although the value itself has no interpretation, the difference between two -2 log likelihoods can be used to compare neighbouring models. In our case, the distribution of the difference in the two -2 log likelihoods follows a Chi-square distribution with two degrees of freedom and this is highly statistically significant (i.e. 10,469 9,425 D 1,044; this difference is much larger than the critical value). The result of the likelihood ratio test thus indicates that a model with both a random intercept and a random slope is significantly better than a model with a random intercept only. These two relatively straightforward models assume that the development of body mass index is linear. It could also be possible that the development over time is better represented for example by a second-, third- polynomial function. Table 9.2 additionally shows the results of a model assuming a quadratic development (time as well as time * time) of body mass index over time. Whether or not this quadratic term is needed, can again be evaluated by the likelihood ratio test, or by evaluating the P-value of the regression coefficient of the quadratic term. The likelihood ratio test indicates a model assuming a quadratic development has better fit compared to a model assuming linear development over time and this finding is confirmed by a significant quadratic term ( 0.007/0.0004 D 17.5, which corresponds with a P < 0.05). The interpretation of the regression coefficients is similar to those in a linear model, except that there are two coefficients to interpret; one for the linear slope and one for the quadratic slope."
297,1068,0.984,The R Book,"where the ci are the contrast coefï¬cients (above), n i are the sample sizes within each factor level and Ti are the totals of the y values within each factor level (often called the treatment totals). The signiï¬cance of a contrast is judged by an F test, dividing the contrast sum of squares by the error variance. The F test has 1 degree of freedom in the numerator (because a contrast is a comparison of two means, and 2 â 1 = 1) and k(n â 1) degrees of freedom in the denominator (the error variance degrees of freedom). 9.23.2"
391,348,0.984,Ocean-Atmosphere Interactions of Gases and Particles,"number exponent n(s) transitions from n(s) Â¼ 2/3 for a smooth interface to n(s) Â¼ 1/2 for wavy conditions. The exact functional form and details of the transition are still subject to research. Equation 2.25 allows the scaling between different gas species. Since the surface conditions will be the same for the two species, the quotient between the transfer velocities k1 and k2 of the species results in"
187,60,0.984,Managing The Complexity of Critical infrastructures : a Modelling and Simulation Approach (Volume 90.0),"where x and c are the vectors composed, respectively, by the level of inoperability and by the external failure and A is the influence matrix, i.e. the matrix elements aij of such matrix represent the fraction of inoperability transmitted by j-th infrastructure to i-th one or, in other terms, how much the inoperability of j-th infrastructure influences i-th infrastructure. The overall inoperability corresponding to a perturbation c is given by: x Â¼ Ã°I"
311,2529,0.984,The Physics of the B Factories,"to a higher precision than is currently obtained with tau decays. The determination of this ratio by tau decays is presently limited by the measurement of Î(Ï â â Î½Ï K â ), but BABAR has significantly improved the precision of this mode, as discussed in Section 20.8. For the two-pion final state, the hadronic matrix element is parameterized in terms of the so-called pion form factor FÏ (s), defined through [s â¡ (pÏâ + pÏ0 )2 ] Ï â Ï 0 |dÎ³ Î¼ u|0 â¡ 2 FÏ (s) (pÏâ â pÏ0 ) . (20.6.5) Isospin symmetry relates this quantity to the analogous form factor measured in e+ eâ â Ï + Ï â . Accurate measurements of FÏ (s) are a critical ingredient of the Standard Model prediction for the anomalous magnetic moment of the muon. Owing to the diï¬erent quarks involved, two form factors are needed to characterize the decays Ï â Î½Ï KÏ,"
311,1781,0.984,The Physics of the B Factories,"this would imply an n = 3 hyperfine splitting of (98 Â± 8) MeV/c2 , almost twice as large as the n = 2 splitting of (47 Â± 1) MeV/c2 . The BABAR study of the Y (3915) state favors a 0++ quantum number assignment, for which the closest charmonium level is the 2 3P0 state, the so-called Ïâ²c0 . In this case, its 2 3P2 multiplet partner is likely the Z(3930) with a measured mass of (3927Â±3) MeV/c2 , implying an anomalously small 3P2 -3P0 fine splitting of only â 10 MeV/c2 for the n = 2 triplet P -wave multiplet (an order of magnitude smaller than the corresponding n = 1 splitting). Moreover, the Ïâ²c0 is expected to have a partial decay width to DD of order 30 MeV/c2 (Barnes et al., 2005), which, by itself, is substantially wider than the measured total width of the Y (3915). Even though no experimental limits on B(Y (3915) â DD) have been reported to date, no signs of a signal for Y (3915) â DD are evident in the measured DD invariant mass distributions for B â DDK decays published by BABAR (Aubert, 2008bd) or Belle (Brodzicka, 2008), even though both studies see prominent signals for B â Ï(3770)K, Ï(3770) â DD. The Z(3930) has measured properties that match well to the expectations for the 2 3P2 charmonium state and has no need for an exotic interpretation."
175,493,0.984,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","are used as they are here, mutation processes have to be deï¬ned. Any reasonable mutation scheme can be deï¬ned. For example, suppose the mutation of a base 10 number reduces it by 1, unless the resulting number is infeasible. Hence in this example, a mutation could be deï¬ned such that if the current value of the gene being mutated (reduced) is 0, then the new number is 5. Suppose the middle digit 1 of the second new individual, 112, is randomly selected for mutation. Thus, its value changes from 1 to 0. The new string is 102. Mutation could just as well increase any number by 1 or by any other integer value. The probability of a mutation is usually much smaller than that of a crossover. Suppose these paring, crossover, and mutation operations have been carried out on numerous parent strings representing possible feasible solutions. The result is a new population of individuals (children). Each childâs ï¬tness, or objective value, can be determined. Assuming the objective function (or ï¬tness function) is to be maximized, the higher the value the better. Adding up all the objective values associated with each child in the population, and then dividing each childâs objective value by this total sum yields a fraction for each child. That fraction is the probability of that child being selected for the new population of possible solutions. The higher the objective value of a child, the higher the probability of its being selected to be a parent in a new population. In this example, the objective is to maximize the total beneï¬t derived from the allocation of water, Eq. 5.9. Referring to Eq. 5.9, the string 301 has a total beneï¬t of 16.5. The string 102 has a total beneï¬t of 19.0. Considering just these two children, the sum of these two individual beneï¬ts is 35.5. Thus the child (string) 301 has a probability of 16.5/35.5 = 0.47 of being selected for the new population, and the other child (string 102) has a probability of 19/35.5 = 0.53 of being selected. Drawing from a uniform distribution of numbers ranging from 0 to 1, if a random number is in the range 0â0.47, then the string 301 would be selected. If the random number exceeds 0.47, then the string 102 would be selected. Clearly in"
372,842,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"For more than two quantization levels, the relationship is more complicated, and although the nonlinearity of the quantized correlation becomes less serious with an increasing number of levels, correction may still be necessary. As very large instruments come into operation, it becomes increasingly important to remove the responses to strong radio sources in order to study the fainter emission from the most distant regions of the Universe. This requires very accurate calibration of the received signal strengths."
175,1361,0.984,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","derived from each of the other three functions, as indicated by the arrows, in the same manner as described in Fig. 11.12. In mathematical terms, the annual expected probability of levee failure, E[PLF], found in the lower right quadrant of Fig. 11.19, equals"
80,364,0.984,Innovations in Quantitative Risk Management (Volume 99.0),"6.1 Tail Dependence Results Below, we will examine the time-varying parameters of the maximum likelihood fits of this mixture CFG copula model. Here, we shall focus on the strength of dependence present in the currency baskets, given the particular copula structures in the mixture, which is considered as tail upside/downside exposure of a carry trade over time. Figure 3 shows the time-varying upper and lower tail dependence, i.e. the extreme upside and downside risk exposures for the carry trade basket, present in the high interest rate basket under the CFG copula fit and the OpC copula fit. Similarly, Fig. 4 shows this for the low interest rate basket. Remark 2 (Model Risk and its Influence on Upside and Downside Risk Exposure) In fitting the OpC model, we note that independent of the strength of true tail dependence"
383,193,0.984,Elements of Risk Analysis with Applications in R,"where are now four parameters to optimise: Î±, Î², Î³ and y0 . The parameters Î± and Î² play the same role as in Holtâs exponential smoothing. The parameter Î³ controls how much we use the seasonal component in our next prediction (0 â¤ Î³ â¤ 1 â Î±). The seasonal equation shows a weighted average between the current seasonal index, (yt â âtâ1 â btâ1 ) and the seasonal index of the same season last year (hence m time periods ago). Finding the optimal values of parameters for Holt-Winterâs seasonal exponential smoothing model. To find the optimal values of the parameters, we need to search over combinations of Î±, Î², Î³ and y0 to see which gives the smallest SSE value. This optimisation is already implemented in function ses R (see in Section R Lab). Forecasting using Holt-Winterâs model. The forecast function is not flat, it has a trend and seasonality. The hâstep ahead forecast is calculated via the forecast Equation 3.40 and is equal to the last estimated level plus h times the last estimated trend value, plus the estimated seasonality. Example. Overseas visits. (continues) We next fit two HW models to Overseas data: one where the seasonality is additive (i.e. added to the trend and noise), and one where the seasonality is multiplicative (i.e. multiplied with the trend and noise), see Figure 3.19. Visually both models appear to do well, although the additive model seems to not be able to fit well the peak and valley in the second year. All the model estimates are: Î± = 10â4 , Î² = 10â4 , Î³ = 10â4 (see the R Output at the end of this chapter, in the Section R Lab 1)."
280,29,0.984,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"Some of the simplest and most widespread patterns are stripes that run along the midline of a compartment and patterns that run parallel to the wing veins. Figure 1.6 illustrates several examples. The patterns show that the veins do not induce pattern along their entire length. In Fig. 1.6a the pattern is only induced in the mid-region of the vein but not near the proximal and distal ends. There is often a proximodistally graded width of the venous bands suggesting (e.g., Fig. 1.6dâe) that the strength of induction, or the propagation rate of the inductive signal, is graded. These patterns are readily produced by the grass-fire model, as illustrated in Fig. 1.7. A proximodistal gradient of reaction rate constants produces venous bands that taper along the length of the vein (Fig. 1.7c). Intervenous stripes (Fig. 1.7a) can be made if the entire wing vein induces the pattern and both the fuel and reaction rates are homogeneously distributed. Reed and Serfas (2004) have shown that in butterflies without eyespots, but with intervenous stripes, there is a long central midline stripe of notch and Distal-less expression. Notch and Distal-less also specify the position of eyespot foci (see below), thus the patterns of P1 and P2 may simulate the expression of these two peptides."
307,443,0.984,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"14.4.2 The Open State Blocker Repairs the Effect of the Mutation In Fig. 14.3, we show the late current for the wild type, the mutant  D 20, and the drug using the optimal open state blocker defined by (14.14) and (14.17). We observe that the late current induced by the mutation is repaired by the open state blocker. The statistics of the open probability density function (for the wild type, the mutant ( D 20), and the mutant where the drug has been applied) are given in Table 14.2 and the corresponding probability density functions are shown in Fig. 14.4. Again we note that the open blocker repairs the main features of the solution."
6,49,0.984,Teaching Tolerance in a Globalized World,"The analytical strategy consisted of three steps. The ï¬rst step involved the evaluation of the measurement adequacy of the scales using CFA. Secondly, we evaluated the measurement invariance of the proposed scales using MGCFA. Finally, we present descriptive statistics of the newly created scale and a cross-country comparison of the country averages. In order to evaluate the goodness of ï¬t for each country model using CFA, we implemented a chi-square test as an initial procedure. This index is used to test the reasonability of the measurement hypothesis âin terms of how well the solution reproduces the observed variances and covariances among the input indicatorsâ (Brown 2006, p. 41), although we note that this index has been criticized as less sensible for large samples (Brown 2006; Rutkowski and Svetina 2014). In order to circumvent this weakness, we also used three other indicators: the comparative ï¬t index (CFI), Tucker-Lewis index (TLI), and root mean square error of approximation (RMSEA). Brown (2006) proposed a set of cut-off point criteria for evaluating a good modelâs ï¬t:  0.06 in the case of RMSEA and closer to 0.95 or greater for CFI and TLI. As an alternative, Brown (2006) proposed that CFI and TLI values in the range 0.90â0.95 could be considered acceptable. In the case of MGCFA, the evaluation of the model and the invariance testing were evaluated sequentially. The conï¬gural model (baseline), estimates the same conï¬guration of items for each group. The metric model, estimates the model constraining the factor loadings to the same value for each group. Finally, the scalar model constrains the factor loadings and intercepts to be the same for each group. In each of the three cases, we evaluated the model ï¬t using the criteria proposed by Brown (2006). We used the change in the ï¬t indexes between the higher to lower levels of invariance to test the invariance. The main index used to account for the invariance was based on a chi-square test, where the relative change is evaluated. For instance, when comparing the baseline model (conï¬gural) with a more constrained model (metric), an increase in chi-square indicating a degradation of the model can be expected. If the degradation of the constrained model is statistically signiï¬cant, then the proposed model is non-invariant. Nevertheless, this index has the same weakness as noted for CFA, namely it is less sensible for large sample sizes (Brown 2006). Rutkowski and Svetina (2014) developed guidelines more"
30,195,0.984,Determinants of Financial Development,"a downwards bias in standard errors, and therefore higher significance levels attached to the coefficients. In examining the origins of financial openness, Quinn and InclÃ¡n (1997) argue that it is critical to consider a common trend, such as changes in consumer tastes and technology, that may exert substantial effects on government liberalization policies as âfundamental but unobservable forcesâ. The particular nature of the dependent variable and the possibility of error dependence suggest that another estimation approach would be worthwhile. The wide range of scores on the original financial liberalization index from 1 to 18 and the policy change, FLi,t , from -1 to 1 (after transformation) makes a simpler linear regression method a possible choice for this context. This chapterâs approach centres on the Pesaran (2006) common correlated effect pooled (CCEP) estimator, a generalization of the fixed effects estimator which allows for the possibility of cross-section correlation.92 To adjust for serial correlation in individual errors,93 the panel-robust standard errors from Arellano (1987) are computed for the CCEP estimates, allowing the errors not only to be serially correlated for a given country, but also to have variances and covariances that vary across countries. Pesaran (2006) proposes two common correlated effect (CCE) approaches for large heterogeneous panels whose error contains unobserved common factors. More specifically, this approach augments the one-way fixed effects model with the (weighted) cross-sectional means of the dependent variable and the individual specific regressors, analogous to a two-way fixed effects model. Including the (weighted) cross-sectional averages of the dependent variable and individual specific regressors is suggested by Pesaran (2006, 2007) as an effective way to filter out the impacts of common factors, which could be common technological or macroeconomic shocks, causing between group error dependence. The Pesaran (2006, 2007) approach exhibits considerable advantages. It allows unobserved common factors to be possibly correlated with exogenous regressors and exert differential impacts on individual units. It permits unit root processes amongst the observed and unobserved common effects. The proposed estimator is still consistent, although it is no longer efficient, when the idiosyncratic components are not serially uncorrelated. In this context, the cross sectional means of FLit , FLi,tâ1 , GDPi,tâ1 and OPENit are considered since these variables may be especially likely to reflect common effects. To allow the effects to be heterogeneous across regions, the models are augmented with the interactions between regional dummies and cross sectional means of the above variables, and"
84,572,0.984,Eye Tracking Methodology,"the two idealized models. That is, when a cumulative distribution of human visual search performance is plotted, it should appear somewhere between the two curves, as indicated by the dashed curve in Fig. 22.4. Using this methodology, it is then possible to gauge the efficiency of an individual inspector: the closer the inspectorâs curve is to the idealized systematic model representation, the better the inspectorâs performance. This information can then be used to either rate the inspector, or perhaps to train the inspector to improve her or his performance. Tracking eye movements during visual inspection may lead to similar predictive analyses, if certain recurring patterns or statistics can be found in collected scanpaths. For example, an expert inspectorâs eye movements may clearly exhibit a systematic pattern. If so, then this pattern may be one that can be used to train novice inspectors. In the study of visual inspection of integrated circuit chips, Schoonard et al. (1973) found that good inspectors are characterized by relatively high accuracy and relatively high speed, and make many brief eye fixations (as opposed to fewer longer ones) during the time they have to inspect. In a survey of eye movements in industrial inspection, Megaw and Richardson (1979), identify the following relevant eye movement parameters. â¢ Fixation times. The authors refer to the importance of recording mean fixation times (perhaps fixation durations would be more appropriate). Megaw and Richardson state that for most inspection tasks, the expected average fixation duration is about 300 ms. Longer fixation times are associated with the confirmation of the presence of a potential target and with tasks where the search times are short. â¢ Number of fixations. The number of fixations is a much more critical parameter in determining search times than fixation times and is sensitive to both task and individual variables. â¢ Spatial distribution of fixations. The coverage given to the stimulus material can be found by measuring the frequency of fixations falling in the elements of a grid superimposed over the display or by finding the frequency of fixations falling on specific features of the display. In both cases these frequencies correlate with the informativeness of the respective parts of the display as revealed by subjective estimates made by the searchers. â¢ Interfixation distances. With static displays this measure is equivalent to the amplitude of the saccadic movements. It is possible that when a comparatively systematic strategy is being employed, interfixation distances may reflect the size of the useful field of view (visual lobe). â¢ Direction of eye movements. Horizontal saccades may occur more frequently than vertical ones, which may reflect the elliptical shape of the effective useful field of view (visual lobe). â¢ Sequential indices. The most popular of these is the scanpath. In their survey, the authors review previous inspection studies where eye movements were recorded. These include inspection of sheet metal, empty bottles, integrated circuits, and tapered roller bearings. In an inspection study of tin-plated cans, the authors report that experienced inspectors exhibited smaller numbers of fixations and that each inspector used the same basic scanpath from one trial to the next although"
231,619,0.984,North Sea Region Climate Change Assessment,much more uncertain. For the western parts of the North Sea a decrease is suggested in more than a half of the projections (Grabemann et al. 2015). These ï¬ndings are in agreement with the results of other studies (e.g. Debernard and RÃ¸ed 2008). The changes described are also consistent with a projected increase in the frequency of stronger winds from westerly directions.
359,71,0.984,"Micro-, Meso- and Macro-Dynamics of the Brain","in higher mammals has been shaped, to a large degree, with a hypothesis-driven conception of sensory architecture in mind. This bias is best illustrated by the 50-year dominance of the hierarchical schema of genesis of RF specificity (Hubel and Wiesel 1962, 1968, 2005), where visual attribute extraction in the early visual system results from a serial processing starting from non-oriented retinal and thalamic RFs to a cascade of oriented edge detectors of increasing complexity across multiple successive relays (thalamus, primary and secondary cortical areas). This feedforward view of visual processing is based on the repetition, at each stage of integration, of canonical, but highly specific, rules of anatomical convergence from which the function derives. Consequently, our current understanding of RF genesis is largely bottom-up driven and contradicts Aristotleâs principle that âthe whole is greater than the sum of its parts.â Although this simplifying view of sensory processing has led to major advances [review in Alonso (2002); see also the remarkable obituary tribute to David Hubel by Kevan Martin (2014)], it fails to account for the functional complexity expected from the recurrent structural connectivity of cortical subcircuits (Douglas and Martin 2004) on the one hand and the non-linear nature of the dynamic interactions between excitation and inhibition during sensory processing (Borg-Graham et al. 1998; Monier et al. 2008). Furthermore, as emphasized by Bruno Olshausen (2014), RF classification has been established using highly standardized and parametrized sensory contexts (spots, bars and gratings), which have little to do with the rich spatio-temporal statistics experienced during the natural scene-viewing conditions of our everyday life. The principle of maximization of a single neuronâs firing rate, which initially guided the âneuronal doctrineâ (Barlow 1972), does not hold any more when RFs are engaged by non-optimal stimuli most of the time, leading, for natural scene processing, to a sparser and unexpectedly temporally precise spiking regime in the primary visual cortical area (Vinje and Gallant 2000; Baudot et al. 2013). A third conceptual limit to our present knowledge of early visual processing is that most modeling efforts have been targeted at explaining sensory discharges only at the spike level in a purely phenomenological perspective (see Carandini et al. 2005 for a review) rather than aiming at elucidating causal, conductance-based mechanisms regulating the temporal selectivity of the spiking opportunity window (Haider et al. 2010; Baudot et al. 2013). Although in vivo intracellular recordings are difficult to practice in higher mammals, and thus often limited to the anesthetized and paralyzed preparation (but see Tan et al. 2014 for a âtour de forceâ in the behaving non human primate), they still appear to be the technique of choice to address quantitatively the synaptic nature of the RF (Fig. 1a, b). In contrast to most imaging methods (voltage-sensitive dye, two-photon) or correlation studies based on multiple recordings, intracellular recordings allow us to differentiate the local microscopic integration process achieved by the single neuron from the more mesoscopic contribution of the network of unseen units that influence at each point in time the activity of the recorded cell (Fig. 1c). We review here recent intracellular electrophysiological studies from the FreÌgnac lab (CNRS-UNIC), done in the visual cortex of the anesthetized"
307,244,0.984,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"In Fig. 6.4, we show the cost function defined by the norm (see (2.40) on page 46) of the difference between the open probability density function of the wild type (solution of (6.1) and (6.2) with  D 1/ and the open probability density function of the solution of the system (6.8)â(6.10) with  D 3: By minimizing the cost function, using Matlabâs Fminsearch with default parameters and kob D kbo D 1 as an initial guess, we find that an optimal open state blocker is given by"
151,93,0.984,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"minimum, maximum, and mean or median concentrations. Such results are reported as three points. Given these limitations, only an approximate distribution of observed values can be obtained. Concentrations in air that exceed 20 ng CYP mâ3 were generally near sources (areas of application), while those in the range 0.01â10 ng CYP mâ3 were regarded as âregionalâ, corresponding to distances of up to 100 km from sources. Concentrations less than 0.01 ng CYP mâ3 were considered to be âremoteâ. There is a possibility that lesser concentrations could have been measured close to sources if the prevailing wind direction is not from the source region. Approximately 70% of the data for concentrations in air were in the range of 0.01â1.0 ng CPY mâ3. For rain, the greatest frequency (40%) was in the range 1â10 ng CPY Lâ1. The distribution of concentrations of CPY in snow exhibited similar patterns, but with more concentrations in the range 0.01â0.1 ng CPY Lâ1. Physical-chemical properties of chlorpyrifos and chlorpyrifos-oxon. The model developed here was designed to describe transport and fate of CPY from source to remote destinations and thus obtain a semi-quantitative assessment of its LRT characteristics and provide estimates of exposure concentrations at remote locations. Estimates can then be compared with measured concentrations from monitoring programs. The sensitivity of the results to uncertainty in the various input parameters can also be determined. Fundamental to assessing and predicting LRT of CPY and CPYO are reliable values for physical-chemical properties and rates of reaction by different processes that determine partitioning and persistence in the environment. Data from the literature were compiled and critically assessed to obtain consistent values of these physical-chemical properties (Tables 5, 6, and 7)."
30,71,0.984,Determinants of Financial Development,"Note: The dependent variable FDBANK is the index of financial interdediary development over the period, 1990â99. Variable description is in Appendix Table A2.1. There are 91 observations in the whole sample, 70 observations in the developing country sample and 40 observations in the La Porta sample. BMA analysis yields the posterior probabilities of inclusion (PIPs) and the sign certainty index of a relationship (Sign). No sign given means the sign of estimated relationship being uncertain. The Gets analysis yields coefficients and t -values for the variables in the final model. See text for the description of PcGets output."
216,145,0.984,Advances in Production Technology,"temperatures higher than 1,000 Â°C. The austenite grain size is decisive for the cyclic properties of the ï¬nal component; therefore inhomogeneous grain growth has to be avoided. Consequently, the precipitation behaviour has to be controlled along the entire process chain from the steel shop via casting, forming, heat treatments to the manufacturing of the gear component. For this example, thermodynamic modelling provides the key for the design process of material and process chain (Fig. 7.7). Here, the program MatCalc is utilized allowing to follow the precipitation evolution along the production chain continuous casting, rolling, forging, annealing, and ï¬nal carburizing and thus to control the grain size evolution by grain boundary pinning (Kozeschnik et al. 2007). Figure 7.7 shows the principal design concept; that is to identify a process window for the high-temperature carburization utilizing different simulation programs within a multiscale approach. In this example regions of different grain size stability are calculated as a function of Zener pinning pressure and initial austenite grain size for a thermal treatment of 1 h carburization at 1,050 Â°C. In this calculation the chemical composition and the precipitation state determines the Zener pinning pressure, which in turn is determined in a thermodynamic calculation (Prahl et al. 2008)."
283,667,0.984,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","An example of the parity-check matrix of a Gallager (Î», Ï) LDPC code is shown in Fig. 12.1a. It is a [16, 4, 4] code with a Î» of 3 and a Ï of 4. The parity-check matrix of the (Î», Ï) Gallager codes always have a fixed number of non-zeros per column and per row, and because of this property, this class of LDPC codes is termed regular LDPC codes. The performance of the Gallager LDPC codes in the waterfall region is not as satisfactory as that of turbo codes for the same block length and code rate. Many efforts have been devoted to improve the performance of the LDPC codes and one example that provides significant improvement is the introduction of the irregular"
151,104,0.984,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"Estimation of mass loss by transformation and deposition. As a parcel of air containing 100 ng CPY mâ3 is conveyed downwind, the total mass and concentrations of CPY decrease. The mass decreases as a result of transformation processes, primarily reaction with â¢OH radicals and net deposition. Oxidation primarily results in the formation of CPYO. The rate of the overall process can be represented (2) as follows: V Ã C Ã kR or 0.693 Ã V Ã C t"
253,518,0.984,"Autonomous Driving : Technical, Legal and Social Aspects","16.2.2 Traffic Flow Theory Neither the macroscopic parameters v, q, and k, nor the corresponding microscopic values, deï¬ne in themselves a trafï¬c state. In order to deï¬ne a trafï¬c state, knowledge of their interdependencies is a prerequisite. The three macroscopic quantities, trafï¬c volume, trafï¬c density and momentary speed, are dependent on one another according to the equation q Â¼ k  v Ã°k Ã Measurements of trafï¬c volume and mean speed resulted in a detectable decrease in speed when trafï¬c volume increases, i.e., with increasing mutual influence of vehicles. One of the ï¬rst models to describe trafï¬c flow on an open stretch road came from observations made by Greenshields [1], who researched the relationship between the speed v and the trafï¬c density k. With the help of regression analysis, he established a linear relationship for v Â¼ vÃ°kÃ"
231,567,0.984,North Sea Region Climate Change Assessment,"A study by RÃ¤isÃ¤nen et al. (2003) permits a closer look at the North Sea region, as it is based on a set of regional climate simulations for Europe with the RCAO RCM, with lateral boundary conditions originating from two different GCMs for both the SRES A2 and B2 scenarios. By the end of the 21st century they found an increase in annual mean cloud cover of up to 8 % in the northern part of the North Sea region and a decrease of up to 8 % in the southern part. The projected changes in cloud cover are particularly strong during summer, with a typical reduction of 12â20 % in the southern part of the North Sea region, depending on the driving GCM and the scenario used. In the northern part, on the other hand, cloud cover typically increases by 4â 12 %. In this, the projected future changes during summer are considerably stronger than during winter. Furthermore, the general structure of the patterns of projected change varies little between the different simulations in summer, emphasising the robustness of these projections. In winter, on the other hand, the patterns of simulated changes in cloud cover are strongly affected by the choice of driving GCM. While the simulations driven by HadAM2H project an increase in cloud cover over all land areas with the exception of the British Isles, the simulations driven by ECHAM4/OPYC project a slight decrease in most of this area. The only exception is the respective simulation for the SRES B2 scenario with enhanced cloud cover over western Europe. According to these results, the projected changes in cloud cover during winter are not as robust as those during summer, presumably owing to the greater uncertainty in the projected changes in the large-scale circulation over Europe due to natural climate variability."
311,1709,0.984,The Physics of the B Factories,"In a study of Î·c production in Î³Î³ fusion BABAR estimates the uncertainty on the Î·c mass and width due to interference eï¬ects (Lees, 2010b). In the baseline fit to the measured KS0 KÏ mass spectra for the selected Î³Î³ events the interference term is ignored. To estimate the possible mass shift a fit assuming the maximum (full) interference with the continuum Î³Î³ â KS0 KÏ background is performed. The Î·c mass value changes by 1.5 MeV/c2 , which is the dominant contribution to the systematic uncertainty. Belle observes a clear Î·c signal in the mass spectrum of Î· â² Ï + Ï â combinations produced in two-photon collisions (Zhang, 2012), and measures its mass and width. As in the BABAR analysis above, the eï¬ect of interference can only be estimated: the diï¬erences in the Î·c parameters with and without interference, ÎM = 0.3 MeV/c2 and ÎÎ = 1.4 MeV, are taken as model-dependent uncertainties of the measurement. In the Belle study of B â KÎ·c followed by Î·c â KS0 KÏ (Vinokurova, 2011) an angular analysis is used to distinguish the contributions from the coherent and noncoherent KS0 KÏ continuum amplitudes from B â K(KS0 KÏ) decays, mediated by the penguin diagram. This analysis takes interference into account with no assumptions on its phase or absolute value. If interference is turned oï¬, the fitted mass and width do not vary significantly. Finally, a recent BES paper (Ablikim et al., 2012a) has presented a high statistics measurement of the interference: considering such results in future measurements would help to reduce systematic uncertainties."
372,718,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"7.3.8 Multichannel (Spectral Line) Correlator Systems In multichannel correlators, the input band is divided into many channels, and the signals for corresponding channels are cross-correlated. The number of channels is usually an integral power of two and commonly 1024 or more. Within any channel, the relative variation of frequency is very small. Thus, at any instant, the effect of a delay error ""% is to introduce a phase error 2&""%!c , where !c is the center frequency of the channel. Since the frequency variation is small across a single channel, the loss in signal amplitude that occurs in a wide (continuum) band, resulting from the frequency variation of the phase error, is avoided. The time variation of the delay error results in a varying phase error that can be corrected by inserting a phase correction for each channel at the correlator. Thus, with a multichannel correlator, it is possible to avoid the need for delay increments finer than %s , so long as the extra processing steps to correct the phase can be incorporated. For an individual antenna, the maximum delay error is %s =2 D 1=.2""!/, and for the highest channel, centered very close to frequency ""!, the maximum phase error is &. The time for the delay error variation to complete one cycle is d% dt =%s D dt ""!=2. (Note that the rate of change of delay, d% , is different for each antenna.) This is greater than the time for one fringe cycle by a factor of 2 % (signal frequency at the antenna)/(signal frequency in the baseband 0 "" ""!). At any instant, the phase error for the signal from an antenna is equal to 2& % (delay error) % (channel frequency in the baseband 0 "" ""!). If the correction is applied at the correlator output, the corrections for both antennas of each pair must be included. Carlson and Dewdney (2000) describe a multichannel correlator designed to handle wide bandwidth signals (the WIDAR system). The signals from the antennas are Nyquist sampled, and then the band is divided into a number of channels, Nc . The Nyquist sample rate appropriate for each channel is equal to the original sample rate divided by Nc , and the sample rates are adjusted to this value at the filter outputs. The outputs of the filters then go to separate cross-correlators. In this way, the total bandwidth that can be processed is not limited by the capacity of a single correlator. A value of Nc D 32 would be sufficient to reduce the loss in sensitivity resulting from the delay errors to an acceptably small value. Adjusting the phases of the signals at the correlator inputs removes the phase errors resulting from delay errors and also provides fringe stopping. Phase adjustment at this point is possible because the samples are in complex form, having been through the filtering process. Since multichannel correlators give a means of removing channels that are contaminated by interference, they are widely used for continuum as well as spectral line observations."
353,225,0.984,"Disability, Health and Human Development","where: â The symbol  refers to the difference of a given variable between wave t+1 and t; â yi denotes changes in work status for individual i: work exit and return to work transitions are considered in turn as dependent variable. â Fi is the change in functional difficulties: for work exit, the sample includes persons with no difficulty and working in wave 1 and a value of â1â refers to a new difficulty in wave 2, a value of"
311,1621,0.984,The Physics of the B Factories,"The measurement of angular correlations of the outgoing meson in the dibaryon rest frame can provide further insight into the underlying mechanism for three-body decays. From the theoretical point of view, within pQCD it is expected that the meson in B â B1 B2 M decays has a stronger correlation with the antibaryon than with the baryon in the dibaryon rest frame. Hence, the opposite correlation eï¬ect seen in B â â ppK â and ÎpÏ â is astonishing and entirely unexpected. Experimental results"
165,75,0.984,New Methods for Measuring and Analyzing Segregation,"reversed, the contact interpretation will be reversed. But all substantive implications of the patterns of group differences in contact will remain intact and unchanged. The second technical point I mention is that p is computed using only counts for the two groups in the segregation comparison. Thus, p is not Group 1âs proportion among the total population of the area; it is Group 1âs proportion among the combined count of the two groups in the segregation analysis. To emphasize this point, I sometimes term p as a âpairwiseâ group proportion. However, as this is the primary way I use p in this monograph, I often drop the âpairwiseâ modifier in the interest of economy of expression. Note that this âpairwiseâ construction is not at all controversial in segregation measurement; relevant terms in all of the standard formulas for measures of uneven distribution reviewed earlier are based on pairwise implementations of group proportions for areas (i.e., p and q) and for the city as a whole (i.e., P and Q). The general outline of the approach is now set. The next task is to review how the difference of means framework can be implemented with the most popular and widely used segregation indices."
275,363,0.984,Foundations of Trusted Autonomy,"13.2 Foundations A derivation of maximum likelihood classifiers to match a target error profile follows. Noting, in this preliminary study, there is not yet any established theory as to how the overlaps in classification can be traded off, so the examination will necessarily be empirical. In supervised learning, the Error Matrix, or Confusion Matrix indicates the correct and error classifications across categories from either training or test data. Each row represents the number of instances in an actual (true) class and each column represents the number of instances in an estimated class. The term confusion is used, as this representation quickly shows how often one class is confused with another. Type I errors refer to a true class X being incorrectly classified as a different class Y and is indicated in the non-diagonal values in the rows of the confusion matrix. Type II errors refer to a classified class being X when the true class was Y, and is indicated in the non-diagonal column values of the confusion matrix. Logistic Regression for the binomial case (two class) and multinomial [3] have been well studied in the literature. However, in order for the reader to comprehend the gradient function for the pairing of the multi-class logistic function also termed âsoftmaxâ [14] function with alternate objective functions, it is necessary to derive these from first principles. Multinomial logistic regression is also extended to the Gaussian case to gain insight and contrast the form of the gradient function with our proposed objective. The following forms a foundation for the proposed technique for multinomial softmax regression on the confusion matrix to follow in the next section."
84,112,0.984,Eye Tracking Methodology,"3.2.1 Perception of Motion in the Visual Periphery In the context of visual attention and foveoâperipheral vision, the temporal response of the HVS is not homogeneous across the visual field. In terms of motion responsiveness, Koenderink et al. (1985) provide support that the foveal region is more receptive to slower motion than the periphery, although motion is perceived uniformly across the visual field. Sensitivity to target motion decreases monotonically with retinal eccentricity for slow and very slow motion [cycle/deg; Boff and Lincoln (1988)]. That is, the velocity of a moving target appears slower in the periphery than in the fovea. Conversely, a higher rate of motion (e.g., frequency of rotation of grated disk) is needed in the periphery to match the apparent stimulus velocity in the fovea. At higher velocities, the effect is reversed. Despite the decreased sensitivity in the periphery, movement is more salient there than in the central field of view (fovea). That is, the periphery is more sensitive to moving targets than to stationary ones. It is easier to peripherally detect a moving target than it is a stationary one. In essence, motion detection is the peripheryâs major task; it is a kind of early warning system for moving targets entering the visual field."
311,409,0.984,The Physics of the B Factories,"Figure 9.4.3. The contribution to the BABAR and CLEO Fisher discriminants, for a single 1 GeV particle, as a function of the angle of its momentum and the thrust axis of the B candidate. The nine-step line indicates the values of the nine cone coeï¬cients in 10â¦ bins for the CLEO Fisher, while the continuous blue line is the resulting function for the F used by BABAR. The dash-dotted line corresponds to a three-variable Fisher (shown for illustration only, not used in actual BABAR analyses). The coeï¬cients for these Fisher discriminants were optimized using samples of charmless two-body B decays for signal, and data events from mES sidebands for background. The ï¬gure is adapted from a BABAR Thesis (Pivk, 2003). The vertical scale is in arbitrary units (a.u.)."
32,318,0.984,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","of Japanâs population in 2010. The distributions of the regions with high-density populations such as Kanto and Kansai are plotted on the right side compared to other regions. The distributions of the regions with low-density populations such as Hokkaido are plotted on the left side compared to other regions. These properties denote the distribution locality. The population distributions vary by region. To find the distribution quantities that do not depend on the region, we focused on the population distributionâs shape. For a small scale (BS D 0:5 [km]), the right tail of the distributions rapidly falls. As BS becomes larger, the right tail of the distributions becomes gentler. The slopes of the right tail seem close to each other for a small BS. The value of the logarithmic differences between populations whose values are close to each other seems to share similar quantities of population distribution slopes.1"
372,215,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"The function in the brackets is a sinc function (see Fig. A2.1) centered at x0 D x, having a width between first nulls of 2=s0 and an integral, which happens to equal the area of the triangle formed by the peak and the first nulls, of unity. The limit of this function can be used as a definition of the Dirac delta function (often called the impulse function in much of engineering literature),"
142,1146,0.984,Subseafloor Biosphere Linked To Hydrothermal Systems : Taiga Concept,"uptake into the solid phase during subseafloor fluid circulation (Fig. 30.1). High K/Cl ratios are typical characteristics of hydrothermal fields in ABA systems, including the OT fields (>0.09) (Fig. 30.1). The K enrichment in the ABA fluids is attributed to probable K enrichment in surrounding rocks, with which fluid interacts during fluid circulation (Sakai et al. 1990a). The magnitudes of Na-loss and K-gain in each OT field seem consistent between each other (Fig. 30.1), implying quantitative exchange of Na for K during fluid-mineral interaction. In addition, a geographical trend in the K/Cl variation within the OT is also found: higher K/Cl ratios in southern OT fields (Daiyon-Yonaguni and Hatoma) than in northern OT fields (particularly Minami-Ensei). This trend may reflect a difference in the K content of the basement rocks, although there is little data on the host rock chemistry of the OT hydrothermal fields (Shinjo and Kato 2000). The Ca/Cl ratios are lower in the OT fields (<0.05) than in the others, including sedimentassociated MOR fields (Fig. 30.1). The Li/Cl ratios are substantially higher in all of the OT fields (>0.002) than in the other fields. Because of the almost quantitative partitioning of alkali elements into the fluid phase during high-temperature fluid-rock (-sediment) interactions, the high Li/Cl ratios in the OT fluids suggest Li enrichment in the OT basement rocks and/or the OT-filling sediment. Low Fe/Cl ratios (<0.001) are typical in the sediment-associated fields while the low Fe/Cl ratios of the OT fluids may be associated with low Tmax. The Mn/Cl ratios are similar regardless of tectonic/geological background, excluding several ABA fields where the Mn/Cl ratios are extremely high. The Sr/Cl ratios so far observed in hydrothermal fluids vary among the fields regardless of tectonic/geological backgrounds. The hydrothermal fluid Sr/Cl range, higher or lower than the seawater Sr/Cl ratio in each field, indicates that both net gain and loss of Sr from the starting seawater during the hydrothermal fluid circulation is possible. Strontium isotope ratios (87/86Sr) are high (0.709) in the OT Minami-Ensei and Jade fields (Noguchi et al. 2011) and a sediment-covered Escanaba field but low (approximately 0.704) in sediment-starved MOR and ABA fields and sediment-covered Guaymas and Middle Valley fields. The high 87/86Sr ratios of the OT fluids are attributed to those in the OT-filling sedimentary component (Noguchi et al. 2011)."
391,114,0.984,Ocean-Atmosphere Interactions of Gases and Particles,"1.2.1.2 DMS Yield Given the many conversion pathways that originate from DMSP, it is difficult to assess what fraction of DMSP ultimately ends up as atmospheric DMS. As a general rule, the fraction of DMSP produced that is emitted to the atmosphere as DMS is thought to be between 1 % and 10 % (Bates et al. 1994). This far larger DMSP pool therefore leaves room for a several-fold change in the DMS flux, which could arise when shifts in any of the conversion pathways occur. Unfortunately, there is no database of DMSP concentrations comparable to the one for DMS, which makes it difficult to verify underlying ecosystem pathways (see Chap. 5 for discussion of such databases). The potential for shifts in the DMS yield from DMSP can be illustrated with four scenarios (Fig. 1.1). The terms âLowâ and âHigh DMSPâ refer to variation in the species composition of a system, where one system is comprised of species that do not produce large amounts of DMSP â such as diatoms, prochlorophytes and cyanophytes â and the other of species that do produce DMSP â such as haptophytes, chrysophytes and dinoflagellates (Stefels et al. 2007). The term âHigh stressâ refers to conditions that adversely affect algal or bacterial physiology, such as nutrient limitation and light inhibition."
233,382,0.984,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"but rather by sample completeness (Alroy 2010; Jost 2010; Chao and Jost 2012). Completeness, when measured by a statistic known as coverage (Good 1953), is the proportion of individuals in a community that are represented by species in a sample from that community (Chao and Jost 2012). When samples differ in their coverage, they should be standardised to equal coverage before a âfairâ comparison can be made. Much like expected species richness, the coverage of a sample can be estimated from the sample size and the distribution of individuals among the species in the sample (Chao and Jost 2012). Given that standardisation by sample completeness has been shown to yield a less biased comparison of species richness between communities (Chao and Jost 2012), it would be desirable to have a similar method of standardisation for PD. Since rarefaction of coverage is mathematically related to rarefaction of sample size, the recent work on estimating PD from sample size will no doubt form the basis from which estimated PD for sample coverage will be developed. Finally, a general issue when considering any PD measure is uncertainty regarding the length of branches and the topology (branching pattern) of the tree. All PD measures (including those presented here) assume that the branch lengths and their arrangement in the tree are perfectly known. This is obviously an abstraction, although PD can be surprisingly robust to this source of variation (Swenson 2009). One solution to this dilemma is to calculate PD, including rarefied PD, for a large number of possible trees and report the mean and confidence limits. The output from a Bayesian phylogenetic analysis is a large number of trees, each with their own topology and corresponding branch lengths (see for example Jetz et al. 2012) and so lends itself well to this approach. However, when the possible trees number in the thousands and tens of thousands, this is obviously computationally intensive. An analytical solution, directly incorporating uncertainty into the calculation, would therefore be desirable. This is not an easy extension of the PD rarefaction solution because both variation in branch length and topology (affecting the probability of encountering internal branches) would need to be taken into account. It is worth remembering that phylogenetic relationships are not the only source of uncertainty when investigating real ecological communities â neither the abundance, nor even the presence (occupancy), of species are necessarily known with precision."
103,265,0.984,Solar Particle Radiation Storms Forecasting and Analysis : The Hesperia Horizon 2020 Project and Beyond (Volume 444.0),"populations (Trottet et al. 2015) and the variation of their contribution with particle energy (Dierckxsens et al. 2015). If the SEPs above 10 MeV were predominantly accelerated by CME shocks, and those above 100 MeV by flares or similar processes lower in the corona, no direct correlation would be expected. The other reason is that the microwave flux density spectrum depends strongly on the magnetic field strength and orientation in the radio source, which is also not expected to have an effect on the SEPs. Finally there is an interesting hint that radio bursts with relatively high flux density at 15.4 GHz (and flat SEP spectra) were lacking in solar cycle 24. Forecasting schemes can of course use empirical correlations independently of our understanding of the physical relationships. But this does not seem convincing in the present case."
372,969,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"Fig. 9.1 An example of the multiple field center imaging technique with data from the EVN at 1.6 GHz. âP-Centreâ is the pointing center of the individual antennas, and the circle shows the primary beam size (FWHM) of a 32-m-diameter antenna. The phase calibrator is J2229+0114. Fifteen other sources were detected in the field, and the images of three of them p are shown in the inset panels. The contour levels start at the 3) level and increase by factors of 2. From H.-M. Cao et al. (2014), reproduced with permission. Â© ESO."
311,859,0.984,The Physics of the B Factories,"17.3.1 Introduction B meson decays into all hadronic ï¬nal states containing open charm or charmonium account for almost three quarters of all B decays. Despite constituting the majority of ï¬nal states, these decays pose a challenge to both experiment and theory. The large available phase space in a B meson decay means that there are hundreds of possible ï¬nal states all with rather small branching fractions, typically a few tenths of a percent. Therefore to study in detail any particular ï¬nal state a very large sample of B mesons is necessary as well as a detector capable of measuring the energy, momentum, and identity of the ï¬nal state particles to high precision. Since these are all hadronic ï¬nal states, decay rate calculations must be done using non-perturbative QCD. For the majority of ï¬nal states, a quantitative prediction with controlled theoretical uncertainties remains out of reach. Only the decay rates of the simplest hadronic decays to charm, such as B 0 â D+ Ï â , can be calculated from ï¬rst principles using QCD. In spite of the above drawbacks, hadronic B decays to charm play an important role in the more glamorous aspects of B physics, i.e. the determination of the CKM parameters, measurements of CP violation, and search for physics beyond the Standard Model. If for no other reason these decay modes must be measured in order to understand the possible backgrounds involved in a measurement of a CKM parameter. Although the branching fractions here are small, it is still possible to collect very clean samples of B events using modes such as B â DÏ, B â Dâ Ï, etc. Two-body decays such as D0 Ï + and Dâ Ï + provide important detector calibration tools for determining momentum resolution (Ï Â± , K Â± ; see Sections 2.2.2, 6.2), electromagnetic energy resolution (Ï 0 , Î·; see Section 2.2.4), mass resolution (D, B; see Chapter 7), secondary vertex location (D, Ks0 ; see Chapter 6), and particle identiï¬cation eï¬ciency and rejection (Ï/K; see Chapter 5). Finally, precision measurements of modes such as B â D(â) Ï, D(â) ÏÏ may serve as standard candles for QCD calculations. In this section we are mainly concerned with decay rates and not the speciï¬cs of how the ï¬nal states are reconstructed and the techniques involved. These techniques are described in detail in Chapters 7 (B reconstruction), 12 (angular analysis), and 13 (Dalitz analysis)."
105,459,0.984,"Computer and information Sciences : 31St international Symposium, Iscis 2016, KrakÃ³w, Poland, October 27â28, 2016, Proceedings (Volume 659.0)","The number of hops is an important parameters that inï¬uences of the delays in communications, and especially in ad-hoc or grid network on the energy consumption (the longer the path are, the more resources are used by the intermediate node to deliver message. The results are presented on Fig. 7."
311,1330,0.984,The Physics of the B Factories,"mismatch between the experimental results zÂ± and the set of physical observables (Ï3 , rB , Î´B ) due to statistical fluctuations (Yabsley, 2006). Both collaborations use a frequentist approach to obtain Ï3 , although the details of the treatment diï¬er. In both cases the method requires knowledge of the probability density function (p.d.f.) p(z|Î¼) of the vector z of measured parameters zÂ± , zÂ± , and zÂ± a function of the true parameters zÌ, which can easily be expressed in terms of the vector Î¼ = (Ï3 , rB , Î´B , rB , Î´B ÎºrB , Î´B ). To obtain this p.d.f., Belle uses a simplified Monte Carlo (MC) simulation of the experiment which incorporates the same eï¬ciencies, resolution, and backgrounds as used in the fit to the experimental data. Belle constructs three-dimensional regions in the Î¼ space, using the unified approach of Feldman and Cousins"
311,230,0.984,The Physics of the B Factories,"5.2 PID algorithms and multivariate methods In the most simple method, PID selectors are based on cuts applied to the most relevant variables for every particle type (e.g. E/p for electrons, the distance traveled in the return yoke for muons, the Cherenkov angle for K/Ï separation, ...). Better performance is obtained with the use of likelihood based selectors, in which the information from the various subdetectors is used to compute a set of likelihoods Lk that the measured properties of the charged track in question would be produced by a true k-particle. For an example of implementation of a selector based on likelihood ratios, see Eq. (5.2.1). Belle has always used selectors based on likelihood ratios throughout the whole life of the experiment. Cut and likelihood based selectors are very stable over the data-taking periods and do not need re-tuning to compensate for the aging of the detectors and the changes introduced by the reprocessing of the data. However, signiï¬cant improvements can be achieved by considering a larger set of variables, even some with very mild discrimination power, in the implementation of PID selectors. BABAR uses more sophisticated statistical tools such as Neural Networks (NN), Bagged Decision Trees (BDT), and Error Correcting Output Code (ECOC) algorithms, to accommodate a large number of input variables (up to 36) and the signiï¬cant correlations among them. Due to their higher sensitivity to variations in the performance of the detector, the selectors based on multivariate methods need to be re-trained on data control samples (see Section 5.3) after every major change in the reconstruction algorithms. Particularly important for BABAR, which was aï¬ected by large variations in the performance of the IFR, is the inclusion of the data taking period as"
45,155,0.984,Measurement and Control of Charged Particle Beams,"Large numbers of orbits and BPM data for excitations of different correctors can be fitted to determine the skew quadrupole component of each magnet in the beam line. 2.8.3 Beam Response after Kick The driving term |Îºâ | may be measured by first kicking the beam, and then observing its response in the plane of the kick over many turns. In the vicinity of the difference resonance, the envelopes of the oscillations in the horizontal and vertical plane exhibit a beating (energy exchange between the two planes) with a characteristic total modulation amplitude of [67, 70] S = 2min . (2.85) xÌmax Here xÌ denotes the envelope of the betatron oscillation in the plane in which the kick was applied; xÌmin is its minimum value, and xÌmax its maximum value; these two extreme values are assumed alternately, with a modulation (or beating) period T . The driving term for the difference resonance, |Îºâ | of (2.81), is given by [70] |Îºâ | = (2.86) frev T Thus measuring the modulation period T and the squared envelope ratio S after a kick is sufficient to infer |Îºâ |. An example from the ATF Damping Ring is shown in Fig. 2.29. The frequency spectrum from a horizontal BPM signal is viewed over a wide frequency range on a spectrum analyzer (top figure), and the frequency of the betatron signal is identified as the peak of the spectrum. The span of the spectrum analyzer is then set to zero, and its center set to the betatron frequency. This produces a signal proportional to the square of the betatronoscillation amplitude. The output signal of the spectrum analyzer can be viewed on an oscilloscope, with results as displayed in Fig. 2.29 (bottom). The slow oscillation in this picture corresponds to synchrotron motion (the BPM is at a location with nonzero mismatched dispersion), while the fast beating reflects the transverse coupling. The picture was taken for a tune separation of |Qx â Qy + qâ | â 0.02. If the two tunes are separated further, the modulation period increases and the modulation amplitude decreases. Using (2.86) with T â 17.6 Âµs and S â 0.3â0.7, we infer a coupling term of |Îºâ | â 0.02, consistent with other measurements [71]. It is of course possible to perform a much more detailed analysis of multiturn BPM data. For example, one can determine the evolution of the coupled optical functions (e.g., the tilt angle of the two transverse eigenplanes) around the ring. An example may be found in [72]."
280,176,0.984,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"context, this would be insufficient to explain butterfly wing patterning in which the distribution of eyespots occurs with differing frequency in different parts of the wing. For this reason, we allow the reaction rate to be a function of space, which appears to provide sufficient freedom to generate the necessary source profiles from this one-dimensional model that produces any arbitrary eyespot configuration observed on butterfly wings. The resulting model is a two-stage model for focus point formation in which the first stage corresponds to solving the Schnakenberg surface reaction-diffusion system Eq. (6.2) to steady state and in the second stage the solution u2 to this model is used to determine the proximal boundary profiles for a1 in the eyespot reaction-diffusion system model Eq. (6.1) within each of the wing cells."
73,330,0.984,Balanced Urban Development : Options and Strategies For Liveable Cities,"To determine the shape complexity of the urban wetland Fractal Dimension Index (FRAC) was calculated using Fragstat software (Table 13.1). From the FRAC value it is evident that Dhanmondi Lake has the most complex shape and Alubdi Lake has a comparatively simpler shape, as it is changing from its natural shape due to the earth filling. The test results of the water quality of the selected urban wetlands are given on Table 13.2. From the water quality report it is evident that the quality of water at Dhanmondi Lake is best and Hatirjheel Lake is the worst. The quality of water at Alubdi Lake located at urban fringe are much closer to Dhanmondi Lake specially in terms of faecal coliform count although this water body is not protected like the other three. One of the reasons could be the presence of large amount of reed plants at the edge of this water body as shown in the Fig. 13.1. Reed plants are also present Table 13.1 Fractal dimension index of the lake shape"
173,74,0.984,"intelligent Human Computer interaction : 9Th international Conference, Ihci 2017, Evry, France, December 11-13, 2017, Proceedings","One classification step of our model takes about [1.6eâ05, 3.8eâ05] of computation time (in s). As Fig. 4 indicates, the time required to crop the cloud to its relevant parts is linearly dependent on the number of points within the cloud. This is the main bottleneck of our approach as all other steps within the pipeline are either constant factors or negligible w.r.t. computation time required. During real-time tests our systems achieved frame rates of up to 40 fps."
283,974,0.984,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","The coefficients ki will usually be small in order to produce near zero correlation of the codewords smax,i with w(t) except for the coefficient kj in order to produce a strong correlation with the codeword smax,j . The choice of the multiplicative constants, k0 and k1 or the multiplicative constants ki for the general case (these adjust the energy of the components of the information signal), depends upon the expected levels of additional noise or interference and acceptable levels of decoder error probability. If the marked signal to noise ratio is represented as SNRz , the marked signal energy as Ez , and the difference in highest correlation to next highest correlation of the codewords is âc , then the probability of decoder error p(e) is lower bounded by: p(e) â©½"
151,98,0.984,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"partition coefficient (KOA) can be used to determine partitioning from air to aerosol particles. Its value is not used directly, but is estimated from the ratio KOW/KAW; however, its relatively low value proves to be less important because monitoring data confirm that CPY does not partition appreciably to aerosol particles in the environment (Yao et al. 2008) or indoors (Weschler and Nazaroff 2010). From the perspective of LRT, the single most important parameter determining concentrations at any given location and the distance that a chemical can be transported, is transformation half-life in the atmosphere. Results of a study of the atmospheric chemistry of CPY and CPYO at the EUPHORE experimental facility in Spain have been reported showing that the principal process that transforms CPY in the atmosphere is reaction with â¢OH radicals, although there are also contributions from direct photolysis and reactions with ozone and nitrate radicals (MuÃ±oz et al. 2012). In that study, a second-order rate constant for transformation of 9.1 Ã 10â11 cm3 moleculesâ1 sâ1 was determined. Combining that second order rate constant with a concentration of 1.5 Ã 106 â¢OH molecules cmâ3 gives a first order rate constant of 13.6 Ã 10â5 sâ1 which corresponds to a half-life of 1.4 h. Halflives of CPY, thus depend directly on the assumed concentration of â¢OH. For CPYO, the corresponding rate constant is less certain (0.8â2.4 Ã 10â11 cm3 moleculesâ1 sâ1) and was estimated to be a factor of approximately 5.5 slower. Experimental results indicated a 10â30% yield of CPYO from transformation of CPY, which is judged to be relatively small, given the absence of significant yields of other transformation products. In their assessment of LRT, Muir et al. (2004) used the AOPWIN, structure activity (SAR) program to predict a second-order rate constant for CPY of 9.17 Ã 10â11 cm3 moleculesâ1 sâ1, a value almost identical to that estimated by MuÃ±oz et al. (2012). Muir et al. used a more conservative concentration of â¢OH that is tenfold less, which yielded an estimated half-life of 14 h (Muir et al. 2004). The lesser concentration of â¢OH was selected to account for concentrations of â¢OH likely to occur in more remote regions and at higher latitudes, for example in Canada. Global concentrations of â¢OH have been compiled and a concentration of 0.9 Ã 106 â¢OH molecules cmâ3 was reported for April in the Central Valley of California and increasing to 1.46 Ã 106 in July and decreasing to 0.63 Ã 106 in October (Spivakovsky et al. 2000). At the latitude of Iowa, USA, concentrations of â¢OH in summer were approximately 80â85% of the concentrations observed in California. In the assessment of LRT reported here, atmospheric half-lives of 3 and 12 h were selected as being reasonable and conservative daily averages for CPY and CPYO, respectively. The actual half-lives of CPY could be a factor of two shorter, especially during midsummer daylight hours and polluted conditions when concentrations of â¢OH are greater. Monitoring data suggest that CPYO might have a shorter half-life. Half-lives, based on experimental data for CPY-methyl (CPY-methyl), have been reported to be in the range of 3.5 h for reactions between CPY-methyl and â¢OH, 15 h for direct photolysis, >8 d for reactions with ozone (O3) and a half-life of 20 d for transformation of CPY-methyl through reactions with nitrate radicals (Munoz et al. 2011). Given the structural similarity between CPY and CPY-methyl, it is likely that similar proportions apply to both substances for reactions in the atmosphere, but not necessarily in other media such as rainwater and surface water where rates are pH-dependent."
311,2816,0.984,The Physics of the B Factories,"form factor. Therefore, the Q2 distribution for selected Î·c events is divided by the number of no-tag two-photon Î·c â KS K â Ï + events, and the normalized form factor F (Q2 )/F (0) is measured. The number of single-tag events containing Î·c meson is determined from the fit to the KS K â Ï + invariant mass distributions with a sum of an Î·c resolution function, a J/Ï resolution function, and a quadratic background distribution. The J/Ïâs are produced in the process e+ eâ â e+ eâ J/Ï. The fitted number of Î·c events in the Q2 region from 2 to 50 GeV2 is about 500. To obtain the Q2 distribution this region is divided into 11 intervals. The process e+ eâ â e+ eâ J/Ï with J/Ï decaying to Î·c Î³ is the dominant source of background peaking at Î·c mass. The background is estimated from the Q2 distribution for e+ eâ â e+ eâ J/Ï events with J/Ï â KS K â Ï + . Its fraction changes from about 1.0% for Q2 < 10 GeV2 to about 5% at Q2 â¼ 30 GeV2 ."
38,440,0.984,The GEO Handbook on Biodiversity Observation Networks,"10.3.2 Community-Level Approaches In the species-level approaches discussed above, modelling is used to predictively map changes in the distribution of individual species. We now turn our attention to so-called âcommunity-level approachesâ to modelling, and thereby mapping, changes in the distribution and retention of biodiversity within whole communities, without providing explicit information on the individual (named) species comprising this diversity. These approaches have particular utility in situations where the number of species in a biological group of interest is so high, and/or the average amount of information available for each of these species is so low, that species-level"
10,400,0.984,Governance For Drought Resilience : Land and Water Drought Management in Europe,"The outcomes of the analysis of the drought-related water governance issues in the NW European regions involved in the DROP project can be summarized by the three following main points: â¢ A low level of awareness exists, as regards the drought issue, creating a poor context for responsibilities and resources, and leading to a very low level of intensity of drought-related actions â¢ However, an effective water governance, particularly for networks of actors, and their involvement at different levels and scales exists in all regions â¢ Although variable according to the region, there is a low level of flexibility in the governance context"
372,1575,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"In effect, the atmospheric loss is modeled by an equivalent attenuator at the receiver input. Suppose that TR D 30 K, Tat D 290 K, and &# D 0:2; then the effective system temperature is 100 K. In such a situation, the atmosphere would degrade the system sensitivity by a factor of more than three. Note that the loss in sensitivity results primarily from the increase in system temperature rather than from the attenuation of the signal, which is only 20%. The emission from the atmosphere induces signals in spaced antennas that are uncorrelated and thus contributes only to the noise in the output of an interferometer. The absorption can be estimated directly from measurements made with a radio telescope. In one technique introduced by Dicke et al. (1946), called the tipping-scan method, the opacity is determined from the atmospheric emission. If the antenna is scanned from the zenith to the horizon, the observed brightness temperature, in the absence of background sources, will depend on the zenith angle, since the opacity is proportional to the path length through the atmosphere, which varies approximately"
169,568,0.984,Riverine Ecosystem Management : Science For Governing Towards a Sustainable Future (Volume 8.0),"â Distribution of core metrics within the metric types and exclusion of redundant metrics â Deï¬nition of upper and lower anchors and scaling â¢ Generation of a multi-metric index â Development of a general multi-metric index â Development of a stressor-speciï¬c multi-metric index â¢ Setting class boundaries â¢ Interpretation of results Depending on purpose, ecosystem type, organism group, and available data multi-metric indices may be designed differently. In many cases, a general assessment reliably reï¬ecting the integrity of an ecosystem is sufï¬cient. In other cases, more speciï¬c data on the causes of deterioration is required. Thus, we distinguish two main forms of multi-metric index: (1) the general approach and (2) the stressorspeciï¬c approach (e.g., OfenbÃ¶ck et al. 2004). Stressor-speciï¬c multi-metric indices"
165,528,0.984,New Methods for Measuring and Analyzing Segregation,"Summary of Difference of Means Formulations I now review a second way in which indices of uneven distribution can be formulated in terms of individual-level residential outcomes. This is to cast each index as a difference of group means on individual-level residential outcomes. Groups are designated as groups 1 and 2 with group 1 being taken as the reference group.2 Each segregation index value (S) is then given as the difference of group means ( Y1 â Y2 ) on individual residential outcomes (y) that are scored as a function of the pairwise proportion for group 1 in the area in which the individual resides (i.e., y = f ( p ) ). Figure B.4 gives formulas for calculating values of popular segregation indices in this manner. My intent here is only to introduce formulas that place popular indices of uneven distribution in the general âdifference of group meansâ framework. Appendices C-F provide detailed discussions of the mathematical basis for the formulas given here. The body of the monograph provides a more general discussion of this new measurement approach and the benefits associated with adopting it."
302,228,0.984,Freshwater Microplastics : Emerging Environmental Contaminants?,"Simple model which is easily applicable and adjustable. It includes soil, water, and atmosphere compartments. Drawback is the large uncertainty in estimates and the lack of spatial or temporal resolution"
393,352,0.984,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 2,"Distance between logical processes The simple synchronous protocol described above does not exploit model topology. It determines the time window width as global minimum of timestamps of all future messages. Consider a model where one of the LPs has significantly smaller lookahead than the other LPs. Then the window will be much shorter than necessary for most LPs. This results in many âemptyâ iterations, i.e. iterations where the LPs will have no safe events to process. In this case the synchronization method does not exploit the available lookahead which leads to performance degradation. In general, this happens in models where the lookahead of individual LPs differs significantly. The method based on distance between LPs addresses this issue (Ayani & Rajaei, 1992). The distance Dij between LPi and LPj is the lower bound on the amount of simulation time which elapses until a state change in LPi propagates and affects the state of LPj. It is determined as the shortest path from LPi to LPj in the model graph where the vertices represent logical processes and edges represent event message flow and are weighted by the corresponding minimum scheduling delays. The distances can be organized in a square matrix called the distance matrix where the element in row i and column j represents the distance Dij. The matrix is used to determine the lower bound on the timestamp (LBTS) of any future message a logical process can receive. Let Ti be the timestamp of the earliest pending event in LPi, or â if there are no events in LPi. Then LBTSi=minâj{Tj+Dji}"
311,2473,0.984,The Physics of the B Factories,"where f (x) is given by Eq. (20.2.2), assuming that the neutrino masses are negligible (Tsai, 1971). Also, in this equation, small corrections of the order m2e,Î¼ /m2W and the diï¬erence between Î±(me ) and Î±(mÎ¼ ) are ignored, see Eq. (20.2.2). The relation between the weak coupling constant gl and the Fermi coupling constant Gl , for the lepton l, is given by Gl = â l 2 . (20.3.2) 4 2MW The HFAG group has performed a constrained fit (Amhis et al., 2012) using 157 branching fraction measurements and 47 constraint equations that fit 86 quantities. For example, there are measurements of the total branching fraction of all decays to three charged pions or kaons plus any number of neutrals. In addition, there are separate measurements of exclusive branching fractions to specific final states that have three identified charged mesons. One constraint is that the sum of exclusive 3-prong decays, the decays involving three charged particles in their final statess, must equal the inclusive 3-prong measurement. The fit is statistically consistent with the constraint that the sum of all base modes is equal to one, referred to as the âunitarity constraintâ, but the unitarity constraint is not explicitly applied. From that fit, which uses all available data including the recent BABARâs results (Aubert, 2010f), we obtain B(Ï â â Î¼â Î½ Î¼ Î½Ï )/B(Ï â â eâ Î½ e Î½Ï ) = 0.9761 Â± 0.0028, which includes a correlation coeï¬cient of the branching fractions. This yields a value"
231,1596,0.984,North Sea Region Climate Change Assessment,Fig. A4.6 Comparison of temperature projections for SRES scenarios and RCPs. a Time-evolving temperature distributions (66 % range) for the four concentration-driven RCPs computed with a representative equilibrium climate sensitivity (ECS) distribution and a model set-up representing closely the climate system uncertainty estimates of IPCC AR4 (grey areas). Median paths are shown in yellow. Red shaded areas indicate time periods referred to in âbâ. b Ranges of estimated average
311,1238,0.984,The Physics of the B Factories,"where the first error is statistical and the second is systematic. The systematic uncertainties account for a variety of systematic eï¬ects. These include biases in Ît due to detector misalignment and beam profile; uncertainties on parameters that are fixed in the fit; uncertainties on the parameterization of the detector Ît resolution function (main one for S), particle-identification performance, and flavor tagging performance and CP violation in the Btag"
297,1675,0.984,The R Book,"You can see very substantial variations in the value of the intercept from 118.52 on farm 2 to 52.32 on farm 17. Slopes are also dramatically different, from negative â0.555 on farm 2 to steep and positive 1.7555 on farm 17. This is a classic problem in regression analysis when (as here) the intercept is a long way from the average value of x (see p. 460); large values of the intercept are almost bound to be correlated with low values of the slope. Here are the slopes and intercepts from the model speciï¬ed entirely in terms of random effects: a population of regression slopes predicted within each farm with nitrogen as the continuous explanatory variable, and a population of intercepts for each farm: random.model <- lme(size~1,random=~N|farm) coef(random.model)"
87,146,0.984,"Bioeconomy : Shaping The Transition To a Sustainable, Biobased Economy","equal all reverse reactions, so that the state of a system remains stable. However, such a state may only be achieved in closed systems. A more moderate concept, stability, thus has been applied to highlight the absence of excessive fluctuations of outcomes. In this sense, outcomes of systems remain in a defined range of parameters. However, these concepts are more important for engineering and the physical world. Ecosystem resource has shown that outcomes may vary considerably, and, if they vary, radical shifts may occur not only due to external shocks but as a normal condition (consider summer and winter aspects of ecosystems in the North or the dry season/rainy seasons in the South). For the analysis of such systems, the concept of resilience has been widely adopted. It is defined as the capacity of an (eco)system to respond to a perturbation or disturbance by resisting damage and recovering quickly (Schiere et al. 2004). Table 4.4 presents selected opposing characteristics in a simplified way. To make this distinction operational, qualities such as âsmallâ or âlargeâ number or âfewâ or âmanyâ"
45,119,0.984,Measurement and Control of Charged Particle Beams,"It is possible to considerably extend this simple closed-orbit distortion scheme. For example, the response of all BPMs to every single steering corrector may be combined into a big matrix, which can be used as an input to a sophisticated statistical fitting program, such as LOCO [36, 37]. LOCO then varies the individual gradients of the quadrupoles in a computer model, e.g., using MAD [32], to find the modified quadrupole gradients that best reproduce the measured orbit response data. The advantage of using multiple data sets and multiple fits lies therein that the numerical solutions are overconstrained (which reduces the influence of systematic errors) and that multiple error sources, if present, can be more accurately ascertained. 2.4.3 Phase Advance Instead of fitting trajectories, one can also use (2.32) to compute the beta functions from the measured phase advance around the ring. Then either the quadrupoles fields may be adjusted in the model or the actual magnet settings of the accelerator may be changed to improve the agreement between the measured and predicted phase advance and so identify the source of the discrepancy. An example from PEP-II has been presented in Fig. 2.13 [38]. From top to bottom the improved agreement of model and measurement is clear as the strength of a quadrupole pair in the interaction region was changed by a total of 0.15%. For each quadrupole value, the left column shows the entire ring while the right column shown an expanded view of a particular section. As can be seen, the final quadrupole strength (bottom) yields a satisfactory agreement with the model."
241,154,0.984,Second Assessment of Climate Change for the Baltic Sea Basin,"northern Europe and established a relationship between their occurrence and mean summer and winter temperatures. This approach is still applied to quantitative palaeoclimatic reconstructions (e.g. Zagwijn 1994). The second method, developed by V.P. Grichuk from Iversenâs method, is known as the mutual climatic range method (Grichuk et al. 1984). This consists of determining the modern ranges of climatic factors that permit the existence of all the species of plants identiï¬ed in the composition of a given fossil flora, either from pollen or plant macrofossil data. For each plant species, the modern climate ranges are determined and the intersection of all climate ranges of all species present in a given fossil flora determines the climatic conditions that allowed the existence of all species identiï¬ed in a given sample. The third method was also developed by V.P. Grichuk. It consists of reconstructing the main climatic indices from fossil plant data using a concept developed by Szafer (1946). This author proposed to locate a modern analogue of a palaeoflora by comparing present-day ranges of plants. This method has been widely used to reconstruct the Lateglacial and Holocene climate in the Russian territory (e.g. Borisova 1990, 1997). The accuracy of this method for determining mean summer and winter temperatures has been estimated at about Â±1 Â°C. For annual precipitation, the estimated accuracy is about Â±50 mm (Grichuk et al. 1984), assuming no change in plant physiology. Other methods for climate reconstructions using pollen data have recently been developed. These are based on multi-variate statistical techniques. A number of different mathematical techniques, ranging from multiple regressions to correlation analysis, can be used to design transfer functions. The calibration of a statistical transfer function for pollen data requires a collection of modern surface pollen samples that are mathematically correlated to modern climate parameters. Transfer functions have been developed for different parts of the Baltic Sea basin and can be applied to climate reconstructions in the LateglacialâHolocene period (SeppÃ¤ 1996; SeppÃ¤ and Birks 2002; SeppÃ¤ et al. 2002a, b, 2004a, b, 2005, 2008; Birks 2003; HeikkilÃ¤ and SeppÃ¤ 2003, 2010; Antonsson 2006; Antonsson et al. 2006, 2008; Antonsson and SeppÃ¤ 2007;"
354,705,0.984,Diseases of The Abdomen and Pelvis 2018-2021 : Diagnostic Imaging - Idkd Book,"dimeglumine) can be administered to provide arterial, portal venous, and equilibrium-phase imaging but has the added advantage of revealing additional characteristics at the delayed hepatobiliary phase of contrast enhancement. HCC typically do not show contrast retention of liver-specific contrast medium in the hepatobiliary phase, which can add confidence toward the detection and characterization of HCC (Fig. 17.15) [57]. It has been shown that using gadoxetic acid-enhanced MRI can improve the detection of small or early HCCs, as it is superior for detecting HCC measuring <1â2 cm in size compared with CT [58]. In addition, subcentimeter lesions detected by gadoxetic acid-enhanced MRI are likely to be or can transform to become HCC within a short interval [59]. Hence, several evolving guidelines for the imaging evaluation of HCC are incorporating the role of liver-specific contrast media for the diagnosis of subcentimeter HCC. Subcentimeter HCC may be treated by locoregional therapy, thus avoiding the morbidity and mortality associated with radical surgery. However, it is important to note some potential pitfalls of using liver-specific contrast media for HCC evaluation. Some benign regenerating nodules may appear hypointense at the hepatobiliary phase of contrast enhancement, although the majority appears isointense of the liver [60]. In addition, some well-differentiated or moderately differentiated HCC may appear isointense or hyperintense on delayed images due to higher levels of OATP1B3 and MRP3 receptor expression. For this reason, the use of ancillary imaging features at MRI can improve the confidence of HCC diagnosis. These include mild to high T2 signal intensity and impeded diffusion on high b-value DWI. The use of liver-specific contrast agents may also help toward the identification of isoenhancing or hypoenhancing HCC that do not show typical hypervascularity in the arterial phase of contrast enhancement. With regard to the use of diffusionweighted MRI for HCC evaluation, higher b-value (e.g., 800 s/mm2) DWI may help in the identification of disease, particularly if the suspected nodule also demonstrates typical vascularity pattern at contrast-enhanced MRI. Because of background liver cirrhosis, higher-grade/poorly differentiated HCC are more likely to show impeded diffusion and lower ADC values compared with low-grade/well-differentiated HCC. To summarize, many MR characteristics are often associated with HCC (arterial-phase hyperintensity, T2 hyperintensity, venous- or equilibrium-phase washout, lack of hepatobiliary MR contrast agent uptake on hepatobiliary phase images, and restricted diffusion on high-b-value DWI). However, for each of these findings, there is only ~60â80% sensitivity, and benign lesions show these findings in 16â65% of cases, depending on finding, contrast agent used, and series reported [60, 61]. Based on data from numerous studies, the American Association for the Study of Liver Disease (AASLD) and the European Association for the Study of the Liver (EASL) formed recommendations for the noninvasive"
157,376,0.984,"The Economy as a Complex Spatial System : Macro, Meso and Micro Perspectives",The off-shoring of the entire manufacturing activity to country 2 represents an equilibrium of the model (2) as well as concentrating manufacturing in country 1. These are only two of several possible long-run location patterns and the
359,223,0.984,"Micro-, Meso- and Macro-Dynamics of the Brain","over an area of 10 cm2 or more. Under most conditions, it has little discernible relationship with the firing patterns of the contributing individual neurons, largely due to the distorting and attenuating effects of the soft and hard tissues between the current source and the recording electrode. The recently introduced âhigh-densityâ EEG recordings, in combination with source modelling that can account for the gyri and sulci (as inferred from structural MRI imaging) of the subject, have substantially improved the spatial resolution of EEG (Nunez and Srinivasan 2006; Ebersole and Ebersole 2010). Magnetoencephalography (MEG) uses superconducting quantum interference devices (SQUIDs) to measure tiny magnetic fields outside the skull (typically in the 10â1000 fT range) from currents generated by the neurons (HaÌmaÌlaÌinen et al. 1993). Because MEG is non-invasive and has a relatively high spatiotemporal resolution (~1 ms, and 2â3 mm in principle), it has become a popular method for monitoring neuronal activity in the human brain. An advantage of MEG is that magnetic signals are much less dependent on the conductivity of the extracellular space than EEG. The scaling properties (that is, the frequency versus power relationship) of EEG and MEG often show differences, typically in the higherfrequency bands, that have been attributed to capacitive properties of the extracellular medium (such as skin and scalp muscles) that distort the EEG signal but not the MEG signal (Dehghani et al. 2010). Functional magnetic resonance imaging (fMRI) is an imaging technique that monitors oxygenation levels of blood flow in the brains of animals and humans. Specifically, the BOLD contrast has been used as a proxy for neural activity, though the exact relationship between neural processing and the output signal is a complicated one (Logothetis and Wandell 2004). A number of pivotal studies have appeared over the years relating the BOLD signal with depth LFP recordings rather than spiking (Logothetis et al. 2001; Logothetis and Wandell 2004; Nir et al. 2007; Schâ¬olvinck et al. 2010). The main advantage of fMRI is that it can be applied in a brain-wide fashion, allowing for whole-brain associations, and it is non-invasive. At the same time, the temporal sampling rate is fairly slow (typically fractions or a few Hz) and the voxel size of the signal acquisition is considerable (e.g., from fractions to a few mm). Linking spatially distributed measurements with the biophysics and workings of networks and circuits all the way to single-cell and synaptic contributions typically measured via local measurements has remained a challenge, mainly due to the multiple spatiotemporal scales involved requiring simultaneous monitoring at all levels. While such monitoring is difficult to pursue in humans, recent advances in sensing technology have allowed performing it in other animals, particularly rodents. For example, as mentioned earlier, recent advances in material and technology have allowed simultaneous measurement of spiking, LFPs and ECoG in rodents (but also humans), offering the possibility to link between micro-, mesoand macroscopic electric signals (Khodagholy et al. 2015). In similar fashion, the relationship between the BOLD fMRI signal has been studied in conjunction with spiking and LFP measurements (e.g., Logothetis et al. 2001; Nir et al. 2007; Whittingstall and Logothetis 2009) and, recently, by engaging specific neural population via optical perturbation (Lee et al. 2010)."
30,224,0.984,Determinants of Financial Development,"Notes: 55 countries, 1973â97. Dependent variable is FLi,t . Using normal one-way within groups estimator (WG) and Pesaran (2006)âs CCEP estimator, this table, based on a larger dataset associated with the Chinn-Ito measure (2006), presents new results corresponding to Table 5.1B. The within groups R-squared is reported. Variable descriptions are presented in the Appendix Table A5.1. Countries included are listed in the Appendix Table A5.2. Non-robust standard errors are reported for WG estimates, while panelrobust standard errors are reported for CCEP estimates. â significant at 10%; ââ significant at 5%; âââ significant at 1%."
71,97,0.984,Advancing Culture of Living with Landslides: Volume 1 ISDR-ICL Sendai Partnerships 2015-2025 (Volume 1.0),"monitoring. Results of the analysis show that the area of Petra Archaeological Park is relatively safe from medium-large size rock fall (Fig. 19). A possible seasonal effect on rock slope and monuments is evident, without a major permanent ground deformation, potentially leading to incipient collapse.. Minor rock falls (<50m3) are clearly not detectable with medium-resolution satellite imagery, such as the one used in this study. In any case, further analyses may consider the use of the new satellite radar sensors (e.g. COSMO-SkyMed), characterized by a signiï¬cant increase in both, spatial resolution of the data, and temporal sampling of the acquisitions, compared to those available until 2007."
175,773,0.984,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","(a) Compute the probabilities of recreation levels RB1, RB2, RB3, and of dry and wet weather. (b) Compute the conditional probabilities P (wetâ£RB1), P(RB3â£dry), and P(RB2â£wet). 6:3 In flood protection planning, the 100-year flood, which is an estimate of the quantile x0.99, is often used as the design flow. Assuming that the floods in different years are independently distributed"
307,412,0.984,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"where, as usual, o ; c ; and b denote the probability density functions of the open (O), closed (C), and blocked (B) states, respectively, and where the fluxes are defined by (13.18). In Fig. 13.7, we compare the open probability density computed by solving the system (13.21) with the open probability density of the wild type. The"
45,248,0.984,Measurement and Control of Charged Particle Beams,"where the first moments have been subtracted, and the average (. . .) is taken over the distribution function of the beam; recall also (1.27â1.29). An analoguous expression holds for the vertical plane. For a coupled system, the general form of (4.1) must be taken. Control of the beam emittance and prevention of emittance dilutions are mandatory for achieving high brilliance in light sources and high luminosity in colliding beam accelerators. Some sources of emittance dilution, e.g., beam-gas scattering, are inevitable and can be reduced only via hardware improvements. In the same vane, processes involving space-charge dilutions, which constitute a predominant limitation for low-energy ion or proton beams, have after-theobservation been treated by fundamental changes in the acceleration optics, as illustrated, e.g., by the FNAL linac upgrade, and a new booster ring at BNL. Another class of dilutions involving man-made sources, e.g., component vibration, ground motion, power supply regulation, etc., may be curable using sophisticated measurement devices and feedback or feedforward schemes. In this case, the crucial ingredient leading to improved performance is the detection of the offending presence, e.g., using model-independent analysis, as discussed in Chap. 2, or more commonly analyses in the frequency domain. An interesting example of the latter case is shown in Fig. 4.1. Plotted is the Fourier transform of pulse-by-pulse BPM measurements from the SLC linac. The data evidence a strong component at about 1 Hz which seemed This chapter has been made Open Access under a CC BY 4.0 license. For details on rights and licenses please read the Correction https://doi.org/10.1007/978-3-662-08581-3_13"
209,146,0.984,Deliberative Public Engagement With Science : An Empirical investigation,"Higher values for the âRisks-Benefitsâ column indicate greater valuation of the benefits over the risks of nanotechnological development. Higher values for the âRegulation Sliderâ column indicate preferences for less regulation and more development of nanotechnology; âMean Absolute Changeâ indicates the absolute value of attitude change from A2-Pre to that assignment. âMean Extremitizationâ indicates change in the direction of oneâs A2-Pre attitude score, with positive values indicating movement to a more extreme position and negative values indicating movement in the opposite direction (a score of 4 out of 7 indicates the midpoint for the Risks-Benefits item, and a score of 50 indicates the midpoint for the Regulation Slider item); in cases where oneâs A2-Pre attitudes were at the exact midpoint of the scale, movement in either direction is considered extremitization and thus receives positive values; mean differences were calculated using between-groups F-tests and pairwise comparisons with the Tukey HSD standard for statistical significance when there were more than two groups; * next to values indicates differences that are significant at the p < 0.05 level; ^ indicates differences that are significant at the p < 0.10 level; when there are more than two conditions, superscripts are used such that conditions with the same letter are significantly different from one another"
372,1265,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"The response to an extended source is the convolution of the sky intensity I.l; m/ with the synthesized beam b0 .l; m/. Note that since there is often no measured visibility value at the .u; v/ origin, the integral of b0 .l; m/ over all angles is zero; that is to say, there is no response to a uniform level of intensity. At any point on the extended source where the intensity varies slowly compared with the width of the synthesized beam, the convolution with b0 .l; m/ results in a flux density that is approximately IË0 . Thus, the scale of the image can also be interpreted as intensity measured in units of flux density per beam area Ë0 . For a discussion of imaging wide sources and measuring the intensity of extended components of low spatial frequency, see Sects. 11.5 and 10.4."
297,730,0.984,The R Book,"The gamma distribution is useful for describing a wide range of processes where the data are positively skew (i.e. non-normal, with a long tail on the right). It is a two-parameter distribution, where the parameters are traditionally known as shape and rate. Its density function is: f (x) ="
372,1921,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"Note that frequency (or wavelength) does not enter directly into Eq. (17.18), but the values of several parameters, for example, S, &"", and TS , depend upon the observing frequency. As an example, consider an observation at a frequency in the 100â300MHz range for which we use A D 2000 m2 , TS D 200 K, and &"" D 2 MHz. For an example of a radio source, we take S D 10!26 W m!2 Hz!1 (1 Jy) and !s D 500 . vm is typically 0.300 s!1 . With these values, Eq. (17.18) gives &! D 0:700 . Although Eq. (17.18) is derived using a geometrical optics approach, this does not limit its applicability. For an observed occultation curve, the equivalent curve for geometrical optics can be obtained by adjustment of the phases of the Fourier components. The bandwidth of the receiving system has the effect of smearing out angular detail in an occultation observation. Thus, since the SNR increases with bandwidth, for any observation there exists a bandwidth with which the sensitivity to fine angular structure is maximized. This bandwidth is approximately "" 2 &! 2 Rm =c, which can be derived from the requirement that the phase term in Eq. (17.13) not change significantly over the bandwidth. This result can be compared to the bandwidth limitation for an array [given by Eq. (6.70)] by noting that a measurement by lunar occultation with resolution &! involves examination of the wavefront, at the distance of the Moon, on a linear scale of '=&!. Such an interval subtends an angle '=&! Rm at the Earth. Further discussion of such details, and of the practical implementation of Scheuerâs restoration technique, is given by von Hoerner (1964), Cohen (1969), and Hazard (1976). Note that a source may undergo a number of occultations within a period of a few months, with the Moonâs limb traversing the source at different position angles. If a sufficient range of position angles is observed, the one-dimensional intensity distributions can be combined to obtain a two-dimensional image of the source [see, e.g., Taylor and De Jong (1968)]. In radio astronomy, the use of lunar occultations has become less important since the development of very-long-baseline interferometry. The method of lunar occultation has been widely used in optical and infrared astronomy to measure the size and the limb darkening of stars, and the separation of close binary stars. Consistency of the results with those of optical interferometry proves that the lunar occultation method is not corrupted by variations in the lunar topography, which can be expected to become important when the size of the variations is comparable to the Fresnel scale. Angular sizes have been routinely measured down to about 1 mas. The analysis of stellar occultation curves is usually done by fitting parameterized models, rather than the reconstruction methods used in radio observations described above. A review of special considerations for lunar occultation observations at optical and infrared wavelengths can be found in Richichi (1994). Extensive measurements of stellar diameters [see, e.g., White and"
241,469,0.984,Second Assessment of Climate Change for the Baltic Sea Basin,"Elken et al. (2003, 2006) investigated the large halocline variation and related mesoscale and basin-scale processes in the northern Gotland BasinâGulf of Finland system. The authors suggested that long-lasting pulses of south-westerly winds cause an increase in the water volume of the Gulf of Finland. The resulting increase in hydrostatic pressure in the gulf leads to an outflow of deep water. Such counter-estuarine transport weakens the stratiï¬cation of water masses at the entrance of the Gulf of Finland. As a consequence, the same energy input leads to an intensiï¬ed diapycnal mixing as compared to the classical situation at the entrance (strong upward vertical advection). Owing to the variable topography both in the northern Gotland Basin and in the Gulf of Finland, the basin-scale barotropic flows are converted into baroclinic mesoscale motions with a large isopycnal displacement (more than 20 m within a distance of 10â20 km), which causes intra-halocline current speeds of more than 20 cm sâ1. So, Elken et al. (2006) concluded that the nearbottom layers of the Gulf of Finland actively react to the wind forcing, a reasoning which considerably modiï¬es the traditional concept of the partially decoupled lower layer dynamics of the Baltic Sea. The multitude of processes at the entrance of the Gulf of Finland makes modelling the deepwater inflow extremely difï¬cult. The internal wave activity is high, the production of strong eddies and topographically controlled local currents are frequent, and thus the diapycnal mixing is intense."
241,882,0.984,Second Assessment of Climate Change for the Baltic Sea Basin,"northern Italy are reflected in the measurements. Unfortunately, other regions with high estimated wet deposition have only a limited number of measurement sites (e.g. the Netherlands, Belgium), and so it is difï¬cult to evaluate model performance here. The EMEP model has a tendency to underestimate wet deposition in Nordic sites. For oxidised nitrogen (Fig. 15.16b), ï¬ve sites stand out as having much higher measured wet deposition than modelled. The reason for this seems to be that the observed precipitation at the sites far exceeds the modelled precipitation (by a factor of two for the Norwegian site). However, there is good agreement between model results and measurements at almost all other sites, which provides conï¬dence that the modelled wet deposition budget is within the uncertainty of the measured value."
376,878,0.984,"Rangeland Systems : Processes, Management and Challenges","ing that key areas can be selected to be truly representative of the conditions in an area, a set of key areas will underestimate the variance of any indicator because they are a sample of only the average conditions in the area. As a result, confidence intervals will underestimate the uncertainty in the indicator estimates, and statistical tests will tend to show significant differences more than is warranted (i.e., inflated Type I errors). Finally, key areas are often representative of a single land use. Because the spatial distribution of different land uses and resource concerns vary across a landscape, a set of key areas selected for monitoring livestock impacts may not be representative for other objectives. Additionally, the ability of a key area to represent the land condition also may change over time as conditions within a landscape change (e.g., new roads are created or range improvements installed). A statistically valid sample of a resource has several properties (Thompson 2002; Lohr 2009). First, the estimates of an indicator for that resource are unbiased. Unbiased in this context means that there is nothing inherent in the sampling design approach that would result in systematically over- or underestimating the indicator values. This is accomplished through explicit and careful definition of the study area (i.e., population) and sampling units (Table 16.3). The use of randomization techniques in selecting sampling units for monitoring allows the uncertainty about indicator estimates to be characterized and confidence intervals to be constructed around monitoring results. Uncertainty estimates (often expressed as a standard error or confidence interval, or used as part of a statistical test) are not direct measures of the variability of an indicator, but a reflection of how close an indicator estimate is to the actual indicator value in the study area."
285,882,0.984,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","Abstract The present study investigated the influence of various sources of response variability in consonant perception. A distinction was made between source-induced variability and receiver-related variability. The former refers to perceptual differences induced by differences in the speech tokens and/or the masking noise tokens; the latter describes perceptual differences caused by within- and across-listener uncertainty. Consonant-vowel combinations (CVs) were presented to normal-hearing listeners in white noise at six different signal-to-noise ratios. The obtained responses were analyzed with respect to the considered sources of variability using a measure of the perceptual distance between responses. The largest effect was found across different CVs. For stimuli of the same phonetic identity, the speech-induced variability across and within talkers and the across-listener variability were substantial and of similar magnitude. Even time-shifts in the waveforms of white masking noise produced a significant effect, which was well above the within-listener variability (the smallest effect). Two auditory-inspired models in combination with a template-matching back end were considered to predict the perceptual data. In particular, an energy-based and a modulation-based approach were compared. The suitability of the two models was evaluated with respect to the source-induced perceptual distance and in terms of consonant recognition rates and consonant confusions. Both models captured the source-induced perceptual distance remarkably well. However, the modulation-based approach showed a better agreement with the data in terms of consonant recognition and confusions. The results indicate that low-frequency modulations up to 16 Hz play a crucial role in consonant perception."
241,1147,0.984,Second Assessment of Climate Change for the Baltic Sea Basin,"Some of the most profound effects of the projected salinity decline involve losses in functional diversity that would accompany the loss of marine elements in the fauna. Also, the potential increase in primary production and sedimentation of organic matter in the northern Baltic Proper, as well the climate-driven decrease in trophic efï¬ciency, as suggested for the Gulf of Bothnia, are potentially important factors for benthic communities. Human-induced pressures, such as overï¬shing and eutrophication, may erode the resilience of the Baltic Sea ecosystem, thereby making it more vulnerable to climatic variations. The Baltic Sea communities, that are poor in both species and genetic diversity, may therefore be particularly vulnerable to external forcing factors caused by the climate change. Open Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
256,232,0.984,"Supercomputing Frontiers : 4Th Asian Conference, Scfa 2018, Singapore, March 26-29, 2018, Proceedings","Here, Dt is the time interval, ci is the lattice vectors of pseudo particles, and Xi is the collision operator. It is important to choose a proper lattice velocity (vector) model by taking account of the tradeoff between efï¬ciency and accuracy. Since their low computational cost and high efï¬ciency, the D3Q15 and D3Q19 models are popular. Recently, it was pointed out that these velocity models do not have enough accuracy at high Reynolds number with complex geometries [17]. On the other hand, the D3Q27 model is suitable model for a weakly compressible flow at high Reynolds number. Figure 1 shows schematic ï¬gures of the above velocity vector models. Since airflows in urban cities are turbulent with high Reynolds number, we adapt the D3Q27 model. The components of the velocity vector are deï¬ned as"
238,461,0.984,Nanoinformatics,"sequence from 9R to 3C (space group R3Ìm ) cubic perovskites. In this hexagonal sequence, pressure increases the frequency of corner-sharing octahedra. This relation can be extended to cubic perovskite (3C), which only consist of corner-sharing octahedral, as shown in Fig. 12.8. For BaSiO3, the density increases for the transitions from 9R to 6H and 6H to 3C are 3.5% and 1.4%, respectively."
311,585,0.984,The Physics of the B Factories,"3 (p Â· q) â (|p| |q|) , J = 2 : Wr = where q and p are the momenta of one of the resonance daughters and the bachelor particle, respectively, evaluated in the rest frame of the resonance. The decision as to which daughter to choose is a matter of convention and it is very important that this choice be documented since it aï¬ects the interpretation of the relative phases. The angle between q and p is known as the helicity angle (see also Section 12.1) and p Â· q is proportional to the cosine of the helicity angle cos Î¸H . The Zemach expressions are essentially Legendre polynomials of cos Î¸H multiplied by coeï¬cients that contain the momenta of the daughter and bachelor particles raised to the power J. The form factors FP and Fr usually use the BlattWeisskopf parameterization for the decay vertex (Blatt and Weisskopf, 1952). The expressions for the BlattWeisskopf penetration factors depend on the spin J of the intermediate resonance J =0: F ="
359,102,0.984,"Micro-, Meso- and Macro-Dynamics of the Brain","a continuum. All cells within a module shared the same grid spacing, and modules of increasing scale became more abundant as the tetrodes were turned to more ventral MEC locations. Cells that shared the same grid spacing within animals also had a common grid orientation, defined as the orientation of the grid axes relative to the local boundaries of the environment. Most grid cells also demonstrated small but consistent deviations from perfect hexagonal symmetry, expressed by the fact that the inner ring of fields in the grid pattern formed an ellipse rather than a circle. These deformations were consistent across cells in the same grid module (Stensola et al. 2012). No modular organization was apparent within the population of head direction cells in the MEC (Giocomo et al. 2014). Modular organization was also expressed in the temporal modulation of spike activity. Grid cells are tuned to the ongoing population activity, manifested as oscillations in the local field potential (Hafting et al. 2008; Jeewajee et al. 2008). Several models implicate theta rhythms in the generation of the grid pattern (Burgess et al. 2007; for review, see Moser et al. 2014). Previous work had shown that cells at ventral locations of the dorsoventral MEC axis oscillated with a slower beat frequency than dorsal cells, and it was suggested that this gradient arose from gradients in the expression of specific ion channels (Giocomo et al. 2007; Giocomo and Hasselmo 2008; Garden et al. 2008; Jeewajee et al. 2008). We found that grid cells in geometrically defined modules were modulated by the same theta frequency. On average, modules with greater grid spacing had lower theta frequencies, but within animals, modules were not strictly confined to this trend. The consistency of geometric features within but not across modules made it possible to define module membership for all cells with an automated multidimensional clustering approach (K-means clustering). After defining the modules, we could turn to the question of how modules were distributed in the MEC tissue. Several signs of anatomical clustering existed within the entorhinal system (Ikeda et al. 1989; Solodkin and Van Hoesen 1996; Burgalossi et al. 2011), pointing to possible anatomical substrates for the functional clustering. Individual modules occupied extensive portions of MEC. We found that, on average, a module spanned >1 mm of the dorsoventral MEC axis. There was extensive module overlap in the intermediate-to-ventral parts of MEC such that, at any MEC location, cells from several modules could be present. Grid modules were found to cut across cell layers; cells that were part of one module were found in several layers. In contrast to the organization along the dorsoventral axis, there was no discernable topography along the mediolateral axis. Instead, modules extended across large mediolateral distances (~1 mm, which was the limit of our recording arrays), suggesting modules distributed as mediolateral bands along the dorsoventral axis. Based on this knowledge, combined with the distribution of modules along the dorsoventral axis, we could estimate the number of distinct modules within animals to be in the upper single-digit range. This anatomical distribution of modules does not match any known anatomical clustering in the entorhinal cytoarchitecture. With previous reports having suggested a set relationship between scale steps (Barry et al. 2007), we next quantified the relationship between module scales"
391,866,0.984,Ocean-Atmosphere Interactions of Gases and Particles,"MERSEA ODYSSEA SST analysis http://www.mersea. eu.org/Satellite/sst_validation.html, the NOAA AVHRR OI (Reynolds) http://www.ncdc.noaa.gov/oa/climate/ research/sst/oi-daily.php, the Meteorological Service of Canada (CMC) 1/3 SST analysis http://www.mscsmc.ec.gc.ca/contents_e.html, and the BMRC GAMSSA SST analysis http://podaac.jpl.nasa.gov/dataset/ABOML4LRfnd-GLOB-GAMSSA_28km. As an example central to oceanâatmosphere interactions at the heart of SOLAS science, Fig. 5.5 presents the GMPE ensemble SST anomaly map for January 18, 2012 showing clearly that La NinÌa conditions are present across the Equatorial Pacific. SSTs are at least 0.5  C below average across much of the central and eastern equatorial Pacific ocean. A horseshoe pattern of above-average SSTs extends from the Maritime Continent into the mid-latitudes of the Pacific Ocean. The sea surface height anomalies from Jason-2 for January 20, 2012 confirm that La NinÌa is peaking in intensity in the equatorial Pacific (Fig. 5.6). This image is based on the average of 10 days of data centered on January 20, 2012. It depicts places where the Pacific sea surface height is higher than normal (due to warm water), and where the sea surface is lower than normal (due to cool water). Green colour indicates near-normal conditions. The La NinÌa episode changes global weather patterns and is associated with less moisture in the air over cooler ocean waters. This results in less rain along the coasts of North and South America and along the equator, and more rain in the far Western Pacific. Ocean surface winds are needed to estimate momentum transfer (surface stress) and gas transfer velocity between the atmosphere and the ocean, and are instrumental for determining large-scale ocean circulation and transport. Accurate wind speeds are essential for ensuring reliable computations of air-sea heat and mass fluxes making surface winds critically important for budgeting energy, moisture, gases and particles (Fairall et al. 2010). Several reviews of space-based wind measurements and applications have been published (i.e. Liu 2002; Liu et al. 2008; Bourassa et al. 2010). The challenge is to continuously improve the present ocean wind system by means of better bias removal and calibration for low and very high wind speeds, increased temporal sampling using a constellation of instruments, finer spatial resolution and improved methods of fusing observations from multiple platforms."
311,3036,0.984,The Physics of the B Factories,"nonMFV-SUSY While flavor blind SUSY breaking is well motivated, it is important to keep track of other possibilities â with more general flavor breaking patterns. A very useful approach is the mass insertion approximation (MIA). The approximation is easiest to explain in the basis, where the SM fermions have diagonal masses, while the sfermions have undergone exactly the same flavor transformations as their SM fermion partners (this is the so-called superCKM basis). All neutral gauge interactions are still flavor diagonal, charged currents are proportional to the CKM elements, while the sfermion mass matrices have also oï¬-diagonal flavor violating entries. Phenomenologically, the oï¬-diagonal entries need to be small. They can thus be treated as perturbations, compared to the diagonal ones. The size of the resulting flavor violation is usually parametrized by dimensionless ratios of oï¬-diagonal and (the average of) diagonal entries in mass matrices, (Î´AB )ij , where A, B are the chiralities (L, R) and q indicates the (u, d) type. In principle (Î´AB )ij are only bounded by the experiments, but are otherwise completely general."
175,569,0.984,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","Streamflows and other natural phenomena that are necessarily nonnegative often have distributions with positive skew coefï¬cients, reflecting the asymmetric shape of their distributions. When the distribution of a random variable is not known, but a set of observations {x1, â¦, xn} is available, the moments of the unknown distribution of X can be estimated based on the sample values using the following equations. The sample estimate of the mean"
175,997,0.984,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","â¢ Estimating the mean and standard deviation of the outputs. â¢ Estimating the probability the performance measure will exceed a speciï¬c threshold. â¢ Assigning a reliability level on a function of the outputs, e.g., the range of function values that is likely to occur with some probability. â¢ Describing the likelihood of different potential outputs of the system. Show the application of Monte Carlo sampling and analysis, Latin hypercube sampling, generalized likelihood uncertainty estimation and factorial design methods."
244,424,0.984,"Advancing Human Assessment : The Methodological, Psychological and Policy Contributions of Ets","estimates, and so sampling errors were not published.8 John Tukey, a professor at Princeton University, was a consultant on this project. He discussed with Coleman and Beaton the possibility of using the jackknife method of error estimation. The jackknife method requires several passes over slightly modified data sets, which was impossible within the time and resource constraints. It was decided to produce self-weighting samples of 1,000 for each racial/ethnic grouping at each grade. Linear regression was used in further analyses. After the EOS report was published, George Mayeske of the U.S. Office of Planning, Budgeting, and Evaluation organized further research into the equality of educational opportunity. Alexander Mood, then Assistant Commissioner of NCES, suggested using commonality analysis. Commonality analysis was first suggested in papers by Newton and Spurell (1967a, b). Beaton (1973a) generalized the algorithm and detailed its advantages and limitations. John Barone analyzed the EOS data using the commonality technique. This resulted in books by Mayeske et al. (1972, 1973a, b), and Mayeske and Beaton (1975). The Mayeske analyses separated the total variance of student performance into âwithin-schoolâ and âamong-schoolâ components. Regressions were run separately for within- and among-school components. This approach was a precursor to hierarchical linear modeling, which came later (Bryk and Raudenbush 1992). Criterion scaling was also an innovation that resulted from experiences with the EOS. Large-scale analysis of variance becomes tedious when the number of levels or categories is large and the numbers of observations in the cells are irregular. Coding category membership by indicator or dummy variables may become impractically large. For example, coding all of the categorical variables for the ninth-grade students used in the Coleman report would entail 600 indicator variables; including all possible interactions would involve around 1075 such variables, a number larger than the number of grains of sand in the Sahara Desert. To address this problem, Beaton (1969) developed criterion scaling. Let us say that there is a criterion or dependent variable that is measured on a large number of students who are grouped into a number of categories. We wish to test the hypothesis that the expected value of a criterion variable is the same for all categories. For example, let us say we have mathematics scores for students in a large number of schools and we wish to test the hypothesis that the school means are equal. We can create a criterion variable by giving each student in a school the average score of all students in that school. The regression of the individual mathematics scores on the criterion variable produced the results of a simple analysis of variance. The criterion variable can be used for many other purposes. This method and its advantages and limitations were described by Pedhazur (1997), who also included a numerical example."
275,209,0.984,Foundations of Trusted Autonomy,"as â... the arising of novel and coherent structures, patterns and properties during the process of self-organization in complex systemsâ [3]. Complexity has been defined by Kennedy et al. as: âThe interaction of many parts of a system, giving rise to behaviours and/or properties that are not found in the individual elements of the systemâ [4]. Or as Wolfram put it: âIt is possible to make things of great complexity out of things that are very simple. There is no conservation of simplicityâ [5]. Self organisation is defined by Camazine et al. as â... a process in which pattern at the global level of a system emerges solely from numerous interactions among the lower-level components of the systemâ [6]. Features of self organising systems that are essential to emergent behaviour are the existence of: positive feedbackâthat leads to amplification of fluctuations; negative feedbackâto counterbalance amplification and provide stabilisation; multi stabilityâthe coexistence of many stable states; and the existence of state transitionsâleading to dramatic change of the system behaviour, i.e. âbifurcationsâ in behaviour occur when some parameter/s are varied. Goldstein [3] identifies five essential features of emergence: â¢ Radical noveltyânovel behaviour occurs that cannot be predicted. â¢ Coherence or correlationâthe novel behaviour has some level of coherence over time. â¢ Global or macro-level behaviourâcoherence occurs at the macro level. â¢ Dynamicalâthe macro-level, while having some coherence in time, also evolves over time. â¢ Ostensiveâemergent behaviours are recognised ostensively, i.e. by showing themselves. While Goldstein identifies that emergence is inherently unpredictable, Fromm [7] proposes that there are four types of emergence, only two of which are unpredictable. The four types of emergence proposed by Fromm are shown in Table 6.1. Using Frommâs classification scheme, there is a clear gradation in the complexity of systems that display emergent behaviour, from the least complex in Type I, to the most complex in Type IV. The following Sections will examine the implications of emergent behaviour in swarm intelligence systems, specifically in relation to their potential use in autonomous systems. As identified in Table 6.1, based on Frommâs classification scheme, swarm intelligence systems fall into Type II âWeak and predictableâ emergence. Table 6.1 Frommâs classification of types of emergence Predictability"
311,348,0.984,The Physics of the B Factories,"8.1 Introduction The goal of B-ï¬avor tagging is to determine the ï¬avor of a B meson (i.e. whether it contains a b or a b quark) at the time of its decay. At the B Factories, ï¬avor tagging is needed for most measurements of time-dependent CP asymmetries and B meson mixing. As will be discussed in Chapter 10, these measurements usually require full reconstruction of the decay of one of the B mesons (referred to as Brec or âsignalâ B), measurement of the decay time diï¬erence Ît between the two B meson decays, and ï¬avor tagging of the other B meson (referred to as Btag in the following). At the B Factories, in contrast to hadron colliders, B meson pairs are produced in isolation (apart from any initial-state radiation), since there is no âunderlying eventâ and the fraction of events with multiple e+ eâ interactions (âpile-upâ) is negligible. Therefore, if a Brec decay is fully reconstructed, the remaining tracks in the event can be assumed to come from the Btag decay. In this case ï¬avor tagging is to a good approximation independent of the speciï¬c Brec decay mode reconstructed (but of course still depends on whether decays of B 0 /B 0 , B + /B â or, when running at the Î¥ (5S), Bs0 /B 0s are tagged), and the ï¬avor tagging performance can be measured using fully reconstructed ï¬avor-speciï¬c Brec decays. For inclusive reconstruction of the signal B, ï¬avor tagging in general depends on the speciï¬c Brec reconstruction since the remaining tracks in the event cannot be unambiguously assigned to either the Brec or Btag meson. The tagging of neutral B 0 /B 0 mesons from Î¥ (4S) decays assuming a fully reconstructed Brec decay is the primary use case for ï¬avor tagging at the B Factories. This is the situation considered in the following. Flavor tagging relies on the fact that a large fraction of B mesons decay to a ï¬nal state that is ï¬avor speciï¬c, i.e. to good approximation, can only be reached either through the decay of a b quark, or through the decay of a b quark. Because of the large number of decay channels, full reconstruction of a suï¬ciently large number of ï¬avor-speciï¬c Btag decays is not feasible. Instead inclusive techniques are employed that make use of diï¬erent ï¬avor-speciï¬c signatures of B decays. For example, in semileptonic decays B 0 â Dââ â+ Î½â the charge of the lepton unambiguously identiï¬es the ï¬avor of the decaying B meson as long as the lepton can be clearly associated with the"
241,549,0.984,Second Assessment of Climate Change for the Baltic Sea Basin,"9.3.2.2 Changes in Seasonal Variability The long-term changes in Baltic Sea sea level have been also found to depend on the season. Sea level displays an annual cycle and is generally higher during winter months and lower during spring (as demonstrated by HÃ¼nicke and Zorita 2008, see Fig. 9.10). Changes in the annual sea level cycle were studied by Ekman and Stigebrandt (1990), Ekman (1999) for the Stockholm sea level record (covering the ninetieth and twentieth centuries). The authors associated the increase in the amplitude of the dominant annual component to changes in winter wind condition. HÃ¼nicke and Zorita (2008) statistically analysed 30 sea level records from PSMSL (covering the twentieth century) and four long historic sea level records (covering the ninetieth and twentieth centuries, including Stockholm), together with climatic datasets, to investigate centennial trends in the amplitude of the annual cycle. They detected a statistically signiï¬cant increase in amplitude (winter-spring sea level) in the annual cycle in almost all sea level records investigated, but of a weaker sign than the decadal variations in the annual cycle. As the magnitude of the trends appeared almost uniform, with the exception of the Skagerrak area, they concluded that the driving mechanism for the trends in the annual cycle is very likely to be of non-regional origin. Testing several hypothesised factors to explain these centennial trends (wind through the SLP ï¬eld, the barometric effect, temperature and precipitation), only precipitation remained a plausible candidate in their analysis. The physical mechanisms responsible for the long-term trend due to changes in precipitation could be related to salinity changes and their effect on changes on water density. This remains to be resolved. Plag and Tsimplis (1999) investigated the spatial and temporal variability in the seasonal cycle of sea level for PSMSL tide gauge records in the North Sea and Baltic Sea."
151,45,0.984,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"kinetics (de Vette and Schoonmade 2001). Nonetheless, some of the half-lives reported in SI Table A-2 that have been derived from 1st-order degradation kinetics might overestimate the persistence of CPY in the environment. There have been several approaches to calculate rate constants of degradation for this biphasic degradation of CPY. The DT50 values reported by Bidlack were calculated using the Hamaker two-compartment kinetic model (Nash 1988), but details of the goodness of fit were not provided and the DT50 values do not correspond to degradation rate constants (Bidlack 1979). Also, bi-phasic degradation, described by use of the double first-order parallel (DFOP) model, best characterized the data from three dissipation studies performed in terrestrial environments (Yon 2011). To obtain the biphasic rate constants for the available aerobic soil degradation results, a dissipation model was structured with two compartments for the parent compound; one adsorbed in such a manner that was not available for biological degradation or abiotic hydrolysis, and the other in which these processes can occur (Fig. 2). The initial thought was to consider these as adsorbed and dissolved compartments, respectively. However, it is known that partitioning of CPY between soil and soil pore water reaches equilibrium within hours (Racke 1993), whereas the biphasic degradation process observed for CPY occurs over a period of several days. The two compartments were identified as Labile CPY and Adsorbed CPY. Reversible movement of parent CPY between these compartments was represented as two simple first-order processes shown by arrows F1 and F2 in Fig. 2, with rate constants kads and kdes. This model has advantages over older two-compartment models in that simple first-order equations are used and the rate constants are not concentration-dependent as they are in the Hamaker kinetic equations (Nash 1988). Since the reported concentrations of CPY include both compartments, the model was configured so that measured values are entered as the sum of the amounts in these two compartments at each time point (Fig. 2). The sum of processes that degrade CPY was also described as a first-order kinetic process F3, but was nonreversible. The rate constant for this process was designated km. The resulting set of three first-order equations was integrated numerically using Model-Maker Version 4.0 software from Cherwell Scientific Software Ltd. UK. Metabolism data from 11 soils reported in two studies (Bidlack 1979; de Vette and Schoonmade 2001) were fit to this model. It was assumed that the CPY was entirely in the labile compartment at time-zero, and the rate of degradation was determined by km and the concentration"
311,901,0.984,The Physics of the B Factories,"Due to the presence of cross-feed events, the fit for the branching fraction for any one channel uses as inputs the branching fractions from the other channels. Since these branching fractions are not a priori known, BABAR employs an iterative procedure to obtain the 22 branching fractions. It has been shown that D(â) D(â) K events contain resonant contributions (Aubert, 2008bd). In order to measure the branching fractions inclusively without any assumptions on the resonance structure of the signal, BABAR estimates the eï¬ciency as a function of location in the Dalitz plane of the data. BABAR uses this eï¬ciency at the event position in the Dalitz plane to reweight the signal contribution. To isolate the signal contribution eventper-event, BABAR uses the s Plots technique (Pivk and Le Diberder, 2005) (see Chapter 11). The s Plots technique exploits the result of the mES fit (yield and covariance matrix) and the p.d.f.s of this fit to compute an eventper-event weight for the signal category and background category."
49,159,0.984,Artificial Intelligence and Cognitive Science IV,"4.4 The effect of the chemical signal diffusion coefficient For the purpose of generality, a relatively wide value range of the chemical signal diffusion coefficient DS is considered in the following text. Figure 5a shows that the target localization time tL can be significantly decreased when the signal diffusion coefficient DS decreases. This decreasing effect is the most significant in the case of weak diffusiophoretic mobility, Î± = 1 Âµm2 s-1, and fast chemical signal release rate, Î² = 1 s-1. Medium and high values of the diffusiophoretic mobility (Î± = 10, 100 Âµm2 s-1) together with medium and low chemical signal rate constants (Î² = 0.01, 0.1 s-1) affect the target localization time tL only weakly. An analogous situation can be observed in the case of the target localization success rate uâ: the same values of the parameters that decrease the target localization time tL increase the target localization success rate uâ (cf. Figure 5b). For medium and high values of the diffusiophoretic mobility Î± and high values of the chemical signal release rate Î², the influence of the diffusion coefficient of chemical signals DS is negligible for both, the target localization time tL as well as the target localization success rate uâ. On the other hand, the decrease of value of the chemical signal diffusion coefficient DS tends to increase the target residence time tR in almost all observed cases (cf. Figure 5c). However, the readers should be aware of the fact that only a very narrow range of the chemical signal diffusion coefficient DS is valid for the diffusiophoresis effect with respect to electrolytes to be the driving method of the robots self-propulsion. This fact has two important consequences: first, the realistic values of the chemical signal diffusion coefficient DS are around 1000 Âµm2 s-1 (relatively narrow interval) and second, the diffusiophoretic mobility Î± measured for colloids with the hydrodynamic radius of 100 nm is in the range of several hundred (using the same units) which corresponds to a relatively strong self-propulsion [29, 30, 40]. Taking this into account, the overall effect of the chemical signal diffusion coefficient DS could be considered as weak at low values of the diffusiophoretic mobility Î± and high values of the chemical signal release rate Î², no matter what is its actual value. In fact, the presence of the signal itself is beneficial enough when compared to target discovery realized purely by chance by means of the random Brownian motion of the robots."
285,686,0.984,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","In this study we compare the resting state connectivity of a group of normal hearing listeners compared to a group of experienced adult CI users. Brain function in the resting state, or its default-mode activity (Raichle and Snyder 2007) is thought to reflect the ability of the brain to predict changes to the environment and track any deviation from the predicted. Resting state connectivity (as measured by the correlation of resting state activity between different cortical regions) is influenced by functional organisation of the brain, and hence is expected to reflect plastic changes such as those due to deafness. In this study, we hypothesised that CI users would exhibit resting-state connectivity that was different from that of normally-hearing listeners. We used a 4 Ã 4 montage of 8 sources and detectors (24 channels) in each hemisphere (Fig. 1d), which covered the auditory and somatosensory regions of the brain. Sources and detectors were separated by 3 cm. Data in each channel were pre-processed to remove âglitchesâ due to head movements and âgoodâ channels were identified by a significant cross-correlation ( r > 0.75) between the data for the two wavelengths. Any âpoorâ channels were discarded. The data were then converted to HbO and HbR based on a modified Beer-Lambardt Law (Cope et al. 1988). Since resting state connectivity is based upon correlations between activity in relevant channels, it is important to carefully remove any aspects of the data (such as fluctuations from heartbeat, breathing, or from movement artifacts that are present in all channels) that would produce a correlation but is not related to neural activity. These processing steps to remove unwanted signals from the data included: minimizing regional drift using a discrete cosine transform; removing global drift using PCA âdenoisingâ techniques, low-pass filtering (0.08 Hz) to remove heart and breathing fluctuations. Finally the connectivity between channels was calculated using Pearsonâs correlation, r. To account for missing channels, the data were reduced to 9 regions of interest in each hemisphere by averaging the data from groups of 4 channels."
359,21,0.984,"Micro-, Meso- and Macro-Dynamics of the Brain","showed that hippocampal activity during theta could reflect more than a representation of current state and may reflect a vicarious trial-and-error important for planning (Schmidt et al. 2013). A similar observation has been made by decoding position using CA3 firing rates at the choice points. This analysis reveals transient moments in which CA3 represented positions ahead of the rat, sweeping down the potential paths before the rat made its decision (Johnson and Redish 2007). These findings are closely related to the fact that the phase of spiking contains information about heading direction in two-dimensional environments (Huxter et al. 2008), as would be expected if theta sequences code for upcoming positions. Overall, studies to date have demonstrated that theta sequences always begin with place representations behind the subject and end with representations of the future. However, the exact span coded by theta sequences has not been addressed carefully. If the cells that are active at the trough of the CA1 theta cycle code for the current position in the context of past and future locations, how is the span of the past and future determined at the physiological level? One possibility is that theta sequences code for a fixed amount of time or distance around the current location. Alternatively, each geometric segment (e.g., individual corridors) and event along the journey could be represented separately as a âneural wordâ and such words would be concatenated, perhaps via sharp wave ripples (Foster and Wilson 2006; Davidson et al. 2009; Wu and Foster 2014), to represent the entire journey from the beginning to the end. Yet another possibility is that the start and end (reward) locations of a complex trajectory through a maze are coded in a given cycle. This final option raises the question of just how much space could be segmented within a theta cycle. Data collected in our lab demonstrate that theta periods segment the environment either according to goals or to environmental geometry. As a rat ran down the track, the probability that it occupied any given position given the observed CA1 spiking pattern was computed by comparing the instantaneous rates to a template of the session averages, the cellsâ place fields. When these posterior probability distributions were calculated at every theta phase (Zhang et al. 1998), we observed theta sequences that started at one end of the track and finished at the other (Fig. 2). Thus, in addition to the goal being represented at late theta phases (Wikenheiser and Redish 2015), our findings show that the start location is represented at early phases. Combining these observations, the phase code is defined by the current location in the context of a past bounded by a journeyâs beginning and a future bounded by the journeyâs end. Separation of the future and past boundaries is assured by the strongest inhibition at the peak of the theta cycle (BuzsaÌki 2002). Recall the studies in which place fields expanded when familiar environments were stretched. How do place fields expand with the environment? An answer begins to emerge when one considers that the theta sequences are anchored to the boundaries. The amount of space represented within the sequence, the compression, dictates the resolution of the spatial code. When boundaries are moved apart, either in the stretched environments or for journeys of different lengths, theta sequences that are bookended by those boundaries necessarily represent more space which, in"
228,407,0.984,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"13.5 Experimental Results This section includes results of our own tests solved using the OFNBee algorithm. Ten calculations were performed for each of the three optimization problems described below. Average values were calculated from the obtained results and then compared with results available in the literature for the classic ABC algorithm. Program settings for individual problems are presented in Table 13.1. where individual abbreviations mean, respectively: â¢ Number of populations: NS â¢ Number of iterations: MC â¢ Change rate: MR â¢ Scout generation time: SPP For the first tested problem, the welded beam, the best results for program settings were obtained by the classic ABC algorithm and are shown in Table 13.2. The fillet weld width is defined by the variable x1, and x2 is the parameter defining the weld length, x3, the beam height, and x4, the beam width. After listing and comparing the results obtained using OFNBee and the classic ABC algorithm, one can notice that the solutions are similar. The result of the experiment for the objective function differs from the second decimal place which confirms repeatability of good results (Fig. 13.7). Another problem tested in the experiment is the pressure vessel problem. The wanted variables are shown in Fig. 13.8. The aim of this experiment is to minimize the total material costs of the cylinder and its welding. Solutions found by the ABC algorithm for this problem for given program settings are shown in Table 13.3. Comparing the experimental outcomes with the results available in the literature one"
283,669,0.984,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","has 50, 26, 17 and 7% of the columns with 2, 3, 5 and 10 ones per column, respectively, and 80 and 20% of the rows with 14 and 15 ones per row, respectively. Various techniques have been proposed to design good degree distributions. Richardson et al. [27] used density evolution to determine the convergence threshold and to optimise the degree distributions. Chung et al. [4] simplified the density evolution approach using Gaussian approximation. With the optimised degree distributions, Chung et al. [3] showed that the bit error rate performance of a long block length (n = 107 ) irregular LDPC code was within 0.04 dB away from the capacity limit for binary transmission over the AWGN channel, discussed in Chap. 1. This is within 0.18 dB of Shannonâs limit [30]. The density evolution and Gaussian approximation methods, which make use of the concentration theorem [28], can only be used to design the degree distributions for infinitely long LDPC codes. The concentration theorem states that the performance of cycle-free LDPC codes can be characterised by the average performance of the ensemble. The cycle-free assumption is only valid for infinitely long LDPC codes and cycles are inevitable for finite block-length LDPC codes. As may be expected, the performance of finite block-length LDPC codes with degree distributions derived based on the concentration theorem differs considerably from the ensemble performance. There are various techniques to design good finite"
307,381,0.984,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"12.4.2 Numerical Experiments Comparing the Properties of the Wild Type and the Mutant Sodium Channel In Fig. 12.3, we show the probability density functions of the open state, the inactivated state, and the sum of the closed states for the wild type case . D 1/ and two mutations . D 10 and  D 30/: The properties of the solutions are summarized in Table 12.3, which presents the expected values of the open state, the inactivated state, and the sum of the closed states."
372,1789,0.984,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"is a quasar along the line of sight to a galactic HII region known as NGC6334B, which has an angular size of 300 at 1.5 GHz (Trotter et al. 1998). The apparent sizes of interstellar masers, which are mostly found in the Galaxy at low galactic latitudes, are sometimes set by interstellar scattering (Gwinn et al. 1988). An example of a compact radio source that suffers a high degree of interstellar scattering is Sagittarius A* at the dynamical center of our Galaxy. This source has an angular size of about 1:000 at a wavelength of 30 cm (1.5 GHz) [compared with 0:500 predicted by Eq. (14.81)]. The angular size varies approximately as the wavelength squared over the entire measuring range $ 0:3â30 cm, as shown in Fig. 14.10. The measurements by Doeleman et al. (2008) show that the intrinsic source size exceeds the scattering size at 1.3 mm. If the scattering can be modeled accurately, as in the case of Sgr A*, then its effects on the image can in principle be removed. The observed visibility Vm is the true visibility times Vs D e!D) =2 [see Eq. (14.48)]. If, for example, D) D a(2 d2 , appropriate if the baseline is less than the inner scale of turbulence, then Vs is a simple Gaussian function, and the true visibility can be"
238,252,0.984,Nanoinformatics,"(iv) calculate the difference between the estimated density and the central Gaussian which approximates the true density; and (v) deï¬ne the optimal voxel size as that which has the minimum difference. The speciï¬cs of each step are expanded below. Kernel density estimation (KDE) methods can capture the contribution of each atom toward the voxel and to obtain a smooth overall density function representing the voxel. Each atom is represented by a kernel, which is a symmetric function which integrates to one and contributes to a value at the center of the voxel. The center is considered representative of the region encompassed by the voxel. The contribution of the atoms within the voxel to the voxel center is a function of the atomâs location and is determined by a sampling formula [57]. In the simplest case, the contribution of each atom, located at position âxâ, toward a voxel of unit length centered around the origin can be represented by a Parzen window [58] given by:"
80,640,0.984,Innovations in Quantitative Risk Management (Volume 99.0),"Clearly, there are three ODE equilibria, namely two stable nodes at x1 = x2 = s Â± 1 and a saddle at x1 = x2 = s. For graphical illustrations of the response, see Figs. 3 and 4. Figure 3 depicts the quick attraction of the trajectories (starting at 0.1) toward the smaller node at s â 1 = 1. This dynamical response is shown again in Fig. 4 in the x1 , x2 -phase portrait. As a background, this figure shows 11 trajectories of the deterministic kernel, where the random perturbation is switched off. Starting from 11 initial points in the plane, the trajectories approach one of the two stable nodes. This part of the plane consists of two basins of attraction, separated by a separatrix that includes the saddle. The phase portrait of the deterministic kernel serves as skeleton of the dynamics possible for the randomly perturbed system. Now imagine to increase the strength of the random force (enlarge Î³ ). For sufficiently large Î³ , the trajectories may jump across the wall of the separatrix. Then the dynamics is attracted by the other node. Obviously, these transitions between the two regimes may happen repeatedly. In this way, one of the stylized facts can be modeled, namely the volatility clustering [2]5 . This experiment underlines the modeling power of such nonlinear systems."
84,420,0.984,Eye Tracking Methodology,"Given such a sequence of look points {C1 , C2 , . . . , Cn }, several important requirements arise, namely: 1. a model of saccadic velocity (i.e., position and duration); 2. a model of the spatio-temporal fixation perturbation; and 3. control of the simulation timestep and sampling rates. Setting time step h to an arbitrarily small value allows dissociation of the simulation clock from the sampling rate being modeled. The synthetic look point can then be sampled at arbitrary rates, 30, 60, 1000 Hz, etc., producing inter-sample periods of 33, 16, 1 etc. milliseconds, respectively. In the simulation, t denotes the duration of a saccade, which is dependent on the distance between successive fixation points, known as the saccade amplitude. Saccades are ballistic and stereotyped, with the relation between amplitude (Î¸ ) and duration (t) expressed by the linear equation t = 2.2Î¸ + 21 (ms)"
241,553,0.984,Second Assessment of Climate Change for the Baltic Sea Basin,"Sea with its complex coastline and the many different processes modulating its response to global warming. Nevertheless, the question as to whether the rate of sea level rise in the Baltic Sea is accelerating is relevant for adaptation to climate change and other planning purposes over the coming decades. A preliminary indication of acceleration may be conveyed by a change in the long-term linear trends in two different periods of the recent past. Ekman (2009) suggested that the rise identiï¬ed in the Stockholm sea level record is accelerating because the trends before and after 1950 look different (Fig. 9.7, upper right panel). However, this simplistic view should be complemented, if possible, by studies that analyse jointly the observed evolution of sea level and the theoretical evolution that sea level should display as a response to anthropogenic climate change. A change in the rate of sea level rise may not necessarily be projected by numerical climate models. Most studies implicitly deï¬ne the concept of âaccelerationâ of sea level rise, possibly influenced by anthropogenic climate change, in one of two ways. The ï¬rst approach is to look for a long-term increase in the rate of sea level rise, while the other compares the recent rate of sea level rise against rates observed in the past. The difference between both approaches can be illustrated by the following example. Given a sea level record, the linear trend of sea level change in sliding windows of ï¬xed length (such as 30 or 50 years) can be calculated (Richter et al. 2011). If the linear rate in the most recent window is the highest, it could be claimed, following the second approach, that the present rate of increase is unprecedented, and thus possibly related to anthropogenic climate forcing. However, due to decadal variability in the rate of change, the linear rate in the most recent window may not be the highest over the whole record length. However, if the linear trends over the sliding windows itself display a linear trend, it could be claimed, following the ï¬rst approach, that sea level rise is accelerating in that particular record. In a variant of the ï¬rst approach, the"
107,249,0.984,"Symbiotic interaction : 5Th international Workshop, Symbiotic 2016, Padua, Italy, September 29â30, 2016, Revised Selected Papers","The goal of this study was to predict the level of difficulty in a Tetris video game from power modulations in different EEG frequency bands associated with a change in the participants workload. We therefore employed the state-of-the-art spatial filtering method SPoC [9] in order to extract relevant features from the EEG. Whereas established methods like CSP are limited to settings with two classes, SPoC can use any continuous measure or signal as a target variable. We found that our approach predicted difficulty levels with high accuracy, yielding mean correlations of 0.85 and a mean prediction error of less than one level. An inspection of both the optimal frequency band and the patterns corresponding to the spatial filters found by SPoC allowed us to study which signals contributed to the prediction. Regarding the theta band, we find that the characteristic mid-frontal component is consistently represented in all participants. This is in line with findings which show that this topology is associated both with changes in operator workload [10] and reflects time pressure effects on visuomotor tasks [11]. The results from the frequency sweep analysis further suggest that for participants 1 and 6 alpha power modulations had higher predictive value than in the theta band. Interestingly, for those two participants SPoC found alpha band components that where lateralized over motor regions. Thus, for participants 1 and 2 the contribution to the prediction from the alpha band presumably reflected modulations of the sensorimotor rhythm (SMR) caused by increasing motor demands during high difficulty levels and were thus not directly related to cognitive workload. We therefore repeated our analysis twice, one time using only theta band components and one time using only alpha band components. We found that while in the theta-only analysis a mean correlation of 0.79 Â± 0.04 was achieved, which was lower but still comparable to the dual-band case, in the alpha-only analysis the mean correlation dropped considerably to 0.63 Â± 0.10 and was not significant for all participants as revealed by a permutation analysis. A conspicuous observation in the predicted levels in Fig. 5 is that low difficulty levels tend to be overestimated, while high level tend to be underestimated. Thus, the ability of our model to generalize seems to depend on where the levels lie within the continuum of difficulty. Furthermore, it is worth noting that the participants did not practice the Tetris game before the experiment and there was no individual adaptation of the minimal and maximal difficulty level. Such a calibration could ensure a more constant contrast of gaming experience across participants and thus allow for a better comparability. Our approach allows for a continuous measurement from ongoing EEG activity without intervening with the participantsâ engagement with the video game. This is in contrast to alternative approaches that rely on the evocation of eventrelated potentials by means of secondary stimuli [12]. Our approach can furthermore be extended to online application e.g. for adaptive learning systems to adjust progress and presentation of material. With this study we demonstrated the use of novel machine learning techniques for an EEG-based assessment of mental states, thus advancing the endeavor to enhance the symbiotic interaction between human operators and machines."
103,379,0.984,Solar Particle Radiation Storms Forecasting and Analysis : The Hesperia Horizon 2020 Project and Beyond (Volume 444.0),"10.2 Space and Ground Based Measurements of GLEs SEP events, where protons are accelerated to energies above 500 MeV, occur a few times per solar cycle. Protons with such energies penetrate the Earthâs atmosphere and produce secondary particle showers which can increase the intensities recorded by NMs on the ground. Such intense SEP events are also known as Ground Level Events or GLEs. Initially designed by Simpson (1948), NMs are used for precise monitoring of spectral and directional variations in the cosmic-ray flux. The detection of a GLE event by an NM on average occurs a few times per solar cycle. The first GLE was observed on February 28, 1942 and the first GLE observed by NMs was the one on February 23, 1956 (see gle.oulu.fi). Since 1942 until the end of 2015 a total of about 70 GLEs have been observed, i.e. one GLE per year. Each GLE has its typical characteristics (amplitude, spectrum, duration, spatial distribution of flux, etc.). During a GLE the measurements of the ground-based NM network show an increase in the count rate within typically a few minutes and decreasing intensities to background levels within hours. In some"
8,754,0.984,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","that is, on the critical curve, the whole system assumes the density 4B of its constituents [remember Eq. (26.37)] and therefore has become just one giant âparticleâ. Closer inspection [35] yields for the pressure and the baryon number density lim Preal .T; / D 0 ;"
241,92,0.984,Second Assessment of Climate Change for the Baltic Sea Basin,"1.2.3.1 Models and Methodology Projections of future climate change make use of general circulation models (GCMs) that describe climate based on a set of grid points regularly distributed in space and time. The grid scale (i.e. the difference between two neighbouring points) of present-day GCMs is in the range 100â300 km. However, many important processes, such as cloud formation, convection, and precipitation, occur on much smaller"
175,1161,0.984,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","a more critical low-flow condition for judging whether or not ambient water quality standards are being met. This can also be seen from Eqs. 10.12, 10.14, 10.15, and 10.16. This often is the basis for the assumption that the smaller (or more critical) the design flow, the more likely it is that the stream quality standards will be met in practice. This is not always the case, however. Different regions of the world use different design low-flow conditions. One example of such a design flow, that is used in parts of North America, is the minimum 7-day average flow expected to be lower only once in 10 years on average. Each year the lowest 7-day average flow is determined, as shown in Fig. 10.6. The sum of each of the 365 sequences of seven average daily"
256,508,0.984,"Supercomputing Frontiers : 4Th Asian Conference, Scfa 2018, Singapore, March 26-29, 2018, Proceedings","Computational kernels of the P-CG method consist mainly of SpMV (lines 3, 4), BJ (lines 7, 8), and Vector (AXPYs, lines 5, 6, 9). Here, SpMV and BJ involve the following inner product operations in the same loop. The numbers of floating point operations f and memory access b and the resulting arithmetic intensity f /b of each kernel are summarized in Table 2. Since the pressure Poisson equation is solved using the second order accurate centered finite difference scheme, SpMV and BJ have relatively low arithmetic intensity f /b < 0.2. In addition, the remaining AXPYs are memory-intensive kernels with f /b = 0.1. Therefore, the high memory bandwidth on KNL has a great impact on the acceleration of the P-CG solver, and the performance ratio between ICEX and KNL exceeds RICEX > 6. Although AXPYs in Vector achieve ideal sustained performances with the performance ratio against the roofline model RRL â¼ 0.9 both on ICEX and KNL, stencil computations in SpMV and BJ show performance degradation from RRL â¼ 0.8 on ICEX to RRL â¼ 0.6 on KNL."
2,299,0.984,Promoting Active Citizenship : Markets and Choice in Scandinavian Welfare,"made at the institutions have real influence on policymaking. In practical terms, this is an issue of the advocacy role of institutions and individual users vis-Ã -vis the municipal political and administrative level. User surveys are not part of the advocacy efforts of users, but represent the only tool for the municipalities to directly collect the views of many users at the same time and are thus a relevant aspect to consider. The deï¬nition of active citizenship covers what we may refer to as the basic level or background concept (Goertz 2006; Adcock and Collier 2001). The second level consists of the three dimensions. These dimensions are ontological in the sense that they constitute the background concept. Based on these dimensions, I developed indicators than can be evaluated. These indicators are not necessarily internally correlated, but can be functionally equivalent, which means that the strong occurrence of one indicator can substitute for the lack of occurrence of another indicator of the same dimension (Goertz 2006, 15). The qualitative data gathering process was designed to capture variations in the dimensions of active citizenship and to be able to conduct qualitative comparisons between institutions. The scores on the indicators are therefore qualitative. What is interesting is the value on one providerâs indicator as compared to the other providers. Table 6.1 illustrates the relationship between different levels of the concept. Table 6.1 Active citizenship, dimensions, and empirical indicators Background concept Active citizenship"
16,277,0.983,"Autonomous Control For a Reliable internet of Services : Methods, Models, Approaches, Techniques, Algorithms, and Tools","For the purposes of evaluation we use Matlab simulation. The clientâs buffer is simulated as a queuing model, where the âDOWNLOADEDâ segments are arrivals and the âPLAYEDâ segments are departures. To simulate the network traffic, we use real traces recorded from a network [23]. Moreover, to simulate congestion we use the parameter âbandwidth factor3 â, which is a metric of the network congestion/traffic and takes values between 0 and 1 (the higher this factor the lower the congestion)."
8,726,0.983,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","where .m/dm is the number of different sorts of particles in fm; dmg, while Zm .T; V; : : :/ is the ideal gas partition function for an unrestricted number of particles of mass m, and dots indicate further variables (chemical potentials). The number of particles has to be unrestricted because: â¢ their total number is unrestricted due to particle creation and annihilation, â¢ the number of each sort changes via âchemicalâ reactions, e.g., N C N ! N C N C 2 : For the partition function of an ideal gas of an arbitrary number of particles, we find in any textbook mT 3=2 (26.25) ln Zm .T; V/ D Vf .m; T/e m=T ; f .m; T/ ! Note that the factor e m=T is missing in most textbooks, since in non-relativistic statistical mechanics it is an irrelevant normalizing factor. In the relativistic situation it is the important part as it governs the equilibrium between particle creation and annihilation. Hence, ln Z.T; V/ D V"
200,163,0.983,"Earthquakes, Tsunamis and Nuclear Risks","The snow hazard indexes are the annual maximum snow depth and the annual maximum daily snowfall depth. Snow hazard curves for the two indexes were developed using 50-year historical weather records at the prototype SFR site which is located in Japan Sea side central area [4]. In this study, a snow hazard evaluation methodology was developed according to the following procedure. At first, the annual maximum data of the snow depth and daily snowfall depth were collected. Using these data, the annual excess probability was evaluated by plotting position formula Weibull, Hazen, and Cunnane for general use. Of the three formulas, it is said that the Cunnane is the best suitable and applicable to all probability distributions. Next, the parameters of Gumbel or Weibull cumulative probability distributions were determined by a least square method. Using the annual excess probability, the snow hazard curves were successfully obtained after checking the conformance and stability evaluations in terms of the annual maximum snow depth and the annual maximum daily snowfall depth. Figure 9.2 shows the snow hazard curves using the Gumbel and Weibull distributions. It should be noted that the difference between the two distributions becomes large in a low-frequency range exceeding the measured data (~10 2/year). This may be caused by epistemic uncertainty (i.e., lack of knowledge). Considering this uncertainty, conservative evaluations or sensitivity analysis is useful and recommended in the risk assessment. Fig. 9.2 Snow hazard curve"
311,1404,0.983,The Physics of the B Factories,"An alternative technique to measure the inclusive branching fraction is to reconstruct the B â Xs Î³ decay chain with the Xs final state as the sum of as many excluAs an alternative to the lepton-tag approach, BABAR (Au- sive modes as possible. This method is a development bert, 2008q) has also used the recoil-B technique (Sec- of the âpseudoreconstructionâ method used by CLEO for tion 7.4), in which the signal (ârecoilâ) B meson is tagged the continuum background suppression, a Ï2 technique in by fully reconstructing the non-signal B meson in a hadronic which an Xs candidate is required to have mES and ÎE decay mode. This technique (along with reconstruction in within broad ranges around expected values. semileptonic decay modes) has been widely used at the An early analysis by Belle (Abe, 2001a) has used 6 fbâ1 , B Factories to study rare decays with multiple neutrinos, and explicitly reconstructed a set of 16 final states with e.g., B â Ï Î½ and B â KÎ½Î½. one kaon and 1 to 4 pions of which only one is allowed to be In BABARâs reconstructed-B-tag analysis, based on a Ï 0 . The Xs mass is restricted to be below 2.05 GeV/c2 , 210 fbâ1 of Î¥ (4S) data, more than 1000 diï¬erent hadronic which corresponds to an EÎ³ threshold of 2.24 GeV. The final states are reconstructed, representing 5% of the de- quoted branching fraction: cay width of the B meson. This large number of final +0.50 states is essential in order to reach a maximal signal ef- (3.36 Â± 0.53 Â± 0.42 â0.54 ) Ã 10 (Belle, EÎ³ > 2.24 GeV) ficiency for B â Xs Î³, although it is still only 0.3%. A (17.9.26) high-energy photon is required among the remaining particles in the event. After applying a Ï 0 veto, suppressing is for the full energy range, where the errors are statisticontinuum background using event-topology criteria, and cal, systematic and model-dependence, respectively. This selecting events with ÎE of the hadronic B candidates in method has not been used by Belle with a larger dataset 17.9.2.3 BABAR fully inclusive with reconstructed-B tagging"
36,193,0.983,Bats in the Anthropocene : Conservation of Bats in a Changing World,"of bats in the short-term. Use of gaps by forest bats following a decade or more of successional change is likely to be different, however, with overall declines in activity plausible as open/edge-space species disappear or decline in abundance with increasing gap clutter. Such temporal changes need to be identified along with the optimal gap size(s) and the density of gaps required by different species of bats to permit commercially viable, sustained yield harvests while fostering high levels of bat activity and provision of roosting habitat in managed forests. In contrast to many North American studies that have been undertaken in gaps soon after harvesting, in Australia, most bat research has focused on the use of older regrowth regenerating from group selection harvest, particularly characterising bat species by their traits in relation to the use of these dense stands. There is a general pattern of forest clutter increasing over time after group selection harvest so that old regrowth (>30 years) has significant higher clutter levels than young or older forest, which constrains use by bats to closed-space species with a low wing aspect ratio (Law and Chidel 2002; Webala et al. 2011). Less manoeuvrable edge-space species with a high wing aspect ratio tend to be scarce in regrowth forest (except on flyways provided by tracks and creeks), although their activity is greater in the subcanopy and canopy than understorey (Adams et al. 2009). Vegetation is more cluttered in regrowth at these upper heights (closer stems and less vertical space in the subcanopy), and this leads to less bat activity in such situations (Adams et al. 2009). It is not known whether open-space and low-frequency edge-space species are active above the canopy of these young forests, although this was confirmed by MÃ¼ller et al. (2013) for mature forests in Europe."
372,624,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where b0 is the synthesized beam. The fractional decrease in the peak response to the point source is most easily considered in the .l0 ; m0 / plane. With an eastâwest baseline, the contours of the synthesized beam are approximately circular in the .l0 ; m0 / plane, as long as the observing time is approximately 12 h, which results in spacing loci in the form of complete circles in the .u0 ; v 0 / plane. If we assume that the synthesized beam can be represented by a Gaussian function, as in the calculations for the bandwidth effect, the curve for the rectangular bandwidth in Fig. 6.12 can also be used for the averaging effect. In one case, the spreading function is radial and of width r1 (!=!0 , and in the other, it is circumferential and of width r10 !e &a . Thus, for the averaging effect, we can replaceqr1 (!=#b !0 in Eq. (6.75) and Fig. 6.12 (solid curve)"
311,818,0.983,The Physics of the B Factories,"Electron Momentum (GeV/c) Figure 17.1.17. BABAR analysis of the electron momentum spectra in the Î¥ (4S) rest frame (Aubert, 2006x): (a) onresonance data (open circles - blue), scaled oï¬-resonance data (solid circles - green); the solid line shows the result of the ï¬t to the non-BB events using both on- and oï¬-resonance data; (b) on-resonance data after subtraction of the ï¬tted non-BB background (triangles - blue) compared to simulated BB background (histogram) that is adjusted by a combined ï¬t to the on- and oï¬-resonance data; (c) on-resonance data after subtraction of all backgrounds (data point - red), compared to the simulated B â Xu eÎ½ signal spectrum (histogram); the error bars indicate errors from the ï¬t, which include the uncertainties in the ï¬tted yields for continuum and Xc eÎ½ backgrounds. The shaded area indicates the momentum interval for which the on-resonance data are combined into a single bin for the purpose of reducing the sensitivity of the ï¬t to the simulated shape of the signal spectrum in this region."
295,22,0.983,interface oral Health Science 2016 : innovative Research On BiosisâAbiosis intelligent interface,"Figure 1.7 [29, 30] shows the Youngâs moduli of the Tiâ(13â18)Mo alloys subjected to solution treatment and CR. Except Tiâ13Mo alloy, all the alloys subjected to ST exhibit low Youngâs moduli of <80 GPa, which is much lower than those of SUS 316 L stainless steel (SUS 316 L), commercially pure titanium (CP Ti), and Tiâ6Alâ4 V extra low interstitial (ELI) alloy (Ti64 ELI). For the Tiâ(15â18)Mo alloys, the Youngâs moduli of the alloys subjected to CR are higher than those subjected to ST. For the ST specimens, with increasing Mo content, the Youngâs modulus first decreases slightly from 79 GPa for Tiâ15MoâST to 73 GPa for Tiâ17MoâST, after which the value slightly increases to 75 GPa for Tiâ18MoâST. According to the TEM observation and XRD analysis results, the amount of athermal Ï-phase in the alloys decreases with increasing Mo content. There is no athermal Ï-phase in Tiâ17MoâST or Tiâ18MoâST. The Ï-phase is known to have a significant effect on the mechanical properties of Ti alloys and is likely to increase the Youngâs modulus. In this case, the athermal Ï-phase is the main factor contributing to the change in the"
65,109,0.983,Handbook of Ocean Wave Energy,"which can have a signiï¬cant impact on the power generation of a WEC as discussed above. This second issue is sometimes reduced by producing multiple scatter diagrams for a particular site, which are used to separate the wave climate by peak wave direction or season, but there is clearly a practical limit to the number and range of scatter diagrams that can be produced and used effectively. Another representation of the wave climate that is often used is the wave rose as shown in Fig. 3.10. A wave rose is a graphical representation of the average wave power or Signiï¬cant Wave Height from different directional sectors. Similar to a set of wave roses may be produced based on season in order to provide additional information that may be useful in understanding the wave climate, especially where different meteorological conditions are responsible for different wave conditions at different times of the year."
165,296,0.983,New Methods for Measuring and Analyzing Segregation,"In sum, the patterns documented in Tables 9.1 and 9.2 lend plausibility to the hypothesis that group differences in social characteristics might play a non-trivial role independent of race in contributing to overall segregation. Without going into the same level of detail, I note that similar conclusions can be drawn based on reviewing the data on residential outcomes that determine the value of the dissimilarity index (D) presented in Table 9.3. The key finding is that the subgroup means that determine D vary across poverty and family type within race. This raises the possibility that group differences in distribution across these social categories may be a factor contributing to segregation as measured by D."
307,264,0.983,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"We started in Chap. 2 by assuming that the concentrations of the junctional sarcoplasmic reticulum (JSR) and the network sarcoplasmic reticulum (NSR) are identical and that the L-type current can be ignored and thus we studied a onedimensional problem where the calcium concentration of the dyad was the only variable of interest. The model is illustrated in Figs. 2.1 and 2.2. Then, in Chap. 5, we extended the model to account for the varying concentrations in the dyad and the JSR, but we still ignored the effect of the voltage-gated L-type channels and kept the concentration of the cytosol and the NSR constant. The two-dimensional model is illustrated in Figs. 5.1 and 5.2. Our aim is now to include the effect of L-type channels. The L-type channels open and close depending on the transmembrane potential V, so the model will therefore be parameterized by V. The model is illustrated in Figs. 8.1 and 8.2. It should be noted that we are still interested in the dynamics related to the dyad and not to the whole cell. We therefore keep the concentration of the cytosol and NSR constant and assume that the concentration of the extracellular space (ce ) only affects the concentration of the dyad through the voltage-gated L-type calcium channels (LCCs). In a whole-cell model, this would be different in many ways, but we shall not consider that topic here. The state of a voltage-gated channel is governed by a Markov model where the transitions depend on the transmembrane potential (or voltage for short). If the electrical potential in the dyad is given by Vi (intracellular potential) and the extracellular potential is given by Ve , we define the transmembrane potential to be V D Vi"
372,845,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"To calculate hOxyO i for each combination of two quantization intervals, the joint probability of the required unquantized variables falling within these intervals is multiplied by the product of the corresponding values assigned in the quantization process. These results are then summed for all the pairs of intervals. Since the probability distributions of xO and yO are both symmetrical about zero, first consider the case in which both of these variables are positive and run from zero to N. As noted above, we take the step size to be unity. Let L.i/ be the series of N C 1 values that define the positive quantization steps, i.e., 0; 1; 2; : : : ; .N ! 1/; : : : ; 1. Thus, for i D 1 to N, L.i/ D i!1, and L.N C1/ D 1. For y, there is an identical series of levels represented as L. j/. Then the component of hOxyO i that results from the positive ranges of x and y is"
175,805,0.983,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","fact that observations are paired (i.e., simulation j for 11 â¤ j â¤ 20 in both tables were obtained with the same streamflow sequence) to perform the analysis with the sign test. (b) Use the sign test to demonstrate that the average deï¬cit with Policy 1 (Table 6.14) is stochastically smaller than with Policy 2 (Table 6.15); use all simulations. 6:32 The accompanying table provides an example of the use of non-parametric statistics for examining the adequacy of synthetic streamflow generators. Here the maximum yield that can be supplied with a given size reservoir is considered. The following table gives the rank of the maximum yield obtainable with the historic flows among the set consisting of the"
393,334,0.983,Å tÃºdie vybranÃ½ch tÃ©m programovÃ½ch a informaÄnÃ½ch 2,"Consider a space-parallel simulation model consisting of submodels, running on multiple processors. It is hardly feasible to assume anything about the relative speeds of the processors, and about the load of physical processors. The load of a processor depends on the number of submodels mapped to that processor, the âdensity of eventsâ on the time axis in different submodels which may vary during the simulation, the computational requirements of individual event handlers, etc. For a while, assume that the submodels are simulated independently, without coordination of the advancement of their local simulation times. Then it can easily happen that a submodel receives a message with timestamp less than its local simulation time. This situation is undesirable, because an event with smaller timestamp has the potential to change the state and thus affect events that occur later. Such violation of causality is called causality error. In sequential simulation, the single event list is a means to establish a total order of events and thereby avoid such errors. In the parallel case, the sufficient condition to avoid causality errors is that events within each submodel are processed in correct i.e. non-decreasing timestamp order. This condition is known in PDES literature as the"
12,208,0.983,Self-Assembled Molecules â New Kind of Protein Ligands : Supramolecular Ligands,"and supramolecular ribbon-like structure of CR is lost. The geometry of the wide nanotube increases the contact surface between CR molecule and the nanotube which allows to ascribe the dominant role in the creation of the systemâs equilibrium structure to the pi-pi stacking. Nanotubes of smaller diameter have a smaller side wall area that is characterized by a greater curvature. For this reason, partial preservation of the supramolecular ribbon structure of CR is preferable. The molecular modelling results show a tendency for CR to group into ribbons, while simultaneously leaving a portion of the nanotube surface exposed, which was also observed in microscopic images. These results were also confirmed by analysis of the radial distribution functions (rdf) between CR and SWNT [30]."
372,559,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"will be averaged to a small residual if .2Ä±!/""1 is small compared with the integration period at the correlator output, or if an integral number of fringe cycles fall within such an integration period. If the frequency of the second LO is increased by Ä±! instead of decreased, the lower sideband will be stopped and the upper one averaged out. To apply this scheme to an array of na antennas, the offset must be different for each antenna, and this can be achieved by using an offset nÄ±! for antenna n, where n runs from 0 to na $ 1. An advantage of this sideband-separating scheme is that it can be implemented using the variable LOs required for fringe stopping, and no other special hardware is needed. Unlike the ""=2 phase-switching scheme, one sideband is lost in this method. However, as mentioned above, sideband separation schemes of this type separate only the correlated component of the signal and not the noise. To separate the noise, the SIS mixers at the receiver inputs can be mounted in a sideband-separating circuit of the type described in Appendix 7.1. In such cases, the isolation of the sidebands achieved in the mixer circuit may be only ! 15 dB, which is sufficient to remove most of the noise contributed by an unwanted sideband, but not sufficient to remove strong spectral lines. The Clark technique described above is nicely suited to increasing the suppression of an unwanted sideband that has already suffered limited rejection at the mixer. Fringe-frequency effects can also be used for sideband separation in VLBI observations. In VLBI systems, the fringe rotation is usually applied during playback. Fringe rotation then has the effect of reducing the fringe frequency for one sideband and increasing it for the other. If the fringe rotation is set to stop the fringes in one sideband, then since the baselines are so long, fringes resulting from the other sideband will often have a sufficiently high frequency that they will be reduced to a negligible level by the time averaging at the correlator output. The data are played back to the correlator twice, once for each sideband, with appropriate fringe rotation."
311,2820,0.983,The Physics of the B Factories,"Figure 23.1.1. Measured ratio of hadronic to e+ eâ â Î¼+ Î¼â cross-sections, Rb , as a function of the CM energy, from an energy scan performed by the BABAR collaboration (Aubert, 2009x). The result of a ï¬t to a function including background and Î¥ (5S) and Î¥ (6S) resonances is shown by the curve. The error bars represent the statistical and the uncorrelated systematic uncertainties added in quadrature."
311,215,0.983,The Physics of the B Factories,"for example, in the âlikelihood methodâ for particle identiï¬cation (Chapter 5). In this approach, detector measurements such as dE/dx, time-of-ï¬ight, calorimeter response, and muon detector response are combined by multiplying their likelihoods for a given particle type interpretation. Then rectangular cuts are applied to ratios of these likelihoods for diï¬erent particle hypotheses. This approach to combining the available information has the merits of ease of application and interpretation. It also has some motivation from the fact that the likelihood ratio provides a uniformly most powerful test in the case of simple hypotheses. Table 5.2.1 shows a comparison of âcut-basedâ (that is, making rectangular cuts on the basic detector quantities) and âlikelihood basedâ muon selection: for an eï¬ciency loss of less than 10%, the likelihood method decreases the pion contamination by approximately 30%. The likelihood function is constructed from the sampling p.d.f., so the form of the distribution must be known including any correlations among variables. This can be a diï¬culty with this approach if this information is not readily available. The âsupervised learningâ methods (neural networks and decision trees) described below have an advantage in this respect, because subtle features, including correlations, are usually included automatically in the training samples. Maximum likelihood ï¬ts have been used widely at the B Factories and are discussed in Chapter 11."
241,478,0.983,Second Assessment of Climate Change for the Baltic Sea Basin,"seasonal structural thermal fronts after winters of different severity. Structural fronts are related to the temperature of maximum water density. In spring, the front advances northwards at a rate of about 11â16 km dâ1 traversing the breadth of the Baltic Sea within 8â10 weeks. After severe winters, the horizontal temperature gradient is more pronounced and the rate at which the Baltic Sea is traversed decreases. For a full overview of the potential impacts of these processes on biogeochemistry and ecosystems, see Chaps. 18 and 19, respectively."
280,231,0.983,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"At the cellular level, one cell builds one scale (Nijhout 1991), which may be dubbed the one-cell one-scale rule. Therefore, any morphological features of scales directly indicate the developmental status of the scale-building cells (or simply scale cells). Scale size distribution is graded from the basal to peripheral areas of a wing in butterflies and moths (Kristensen and Simonsen 2003; Simonsen and"
32,247,0.983,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","Abstract This paper describes the application of percolation theory to a Japanese business relation network composed of approximately 3,000,000 links. In this network, we examined the process in which links are randomly removed. At the percolation transition point, we calculate the survival rate for each node as an indicator of its global network connectivity. The basic properties of each node are determined in connection with the values characterising these complex networks, such as the link number and job category. We confirm that this index has strong correlation with degree and shell number, also has significant correlation with sales and number of employee. Finally, we define the network robustness for each prefecture in Japan by using this new indicator."
45,408,0.983,Measurement and Control of Charged Particle Beams,"7.4.7 Application: Measuring the Central Frequency Measuring the chromaticity for different sextupole strengths determines the âcentral frequencyâ. This is the rf frequency for which the orbit on average passes through the center of all sextupoles [27, 28]. An example of such a measurement is shown in Fig. 7.17. Usually adjacent sextupoles and quadrupoles are well aligned with respect to each other, so that one can ex-"
365,293,0.983,Climate Smart Agriculture : Building Resilience To Climate Change,"The main result of the model is the nonlinear effect of temperature on maize yields which is illustrated in Fig. 3. The effects of precipitation are not presented here because the scenarios do not alter the level of precipitation. Exposure to temperatures above 30 Â°C appear detrimental to maize yields. The response function reflects the fact that years with higher exposure to high temperature tend to be associated with lower than average maize yields in the study region. This is in line with previous findings in the literature. The lower part of the Fig. 3 represents the baseline temperature distribution across temperature bins. This is somewhat similar to the distribution within bins illustrated in Fig. 2. Again, for the baseline climate, exposure beyond 30 Â°C is not very common. However, a uniform warming scenario shifts the temperature distribution to the right, which increases the frequency of high temperatures. The anticipated consequence is that maize yields would decrease as exposure to detrimental temperature levels rises."
105,53,0.983,"Computer and information Sciences : 31St international Symposium, Iscis 2016, KrakÃ³w, Poland, October 27â28, 2016, Proceedings (Volume 659.0)","highest percentage contribution to the scores. P-values lower than 0.05 means that the parameter is found to contribute significantly to the performance with a confidence level of 95 %. Table 2 shows the p-values of the parameters at each time. The contribution of the PopSize parameter is found to be significant in 6 out of 10 time periods, whereas the intensity of mutation parameter contributes significantly in only 2 out of 10 time periods and the contribution of the other parameters was not found to be significant. In order to investigate the effect of Depth of Search (DoS) further, we increased the number of domains considered to 9 (and thus used 18 training instances). The main effects of the parameter values are shown in Fig. 2 and Tables 3 and 4 show the percentage contributions and p-values for each parameter. It can be observed from Fig. 2 that the best parameter value does not change over time for the PopSize, TourSize and IoM parameters. The best parameter setting could be predicted for these three parameters after only 1 nominal minute of run time. However, for the depth of search parameter, the best setting indicated in [7] is found only when the entire run time has been used. The best"
45,454,0.983,Measurement and Control of Charged Particle Beams,"The tradeoff between bunch length and energy spread in a linear accelerator is illustrated in Fig. 8.19, which shows the effect of the combined voltages from the power source ERF and the longitudinal wake field W . On the left is depicted the longitudinal phase space of a long bunch while the projection onto the energy axis is given on the right. In the limit of long bunches one can see that a âdouble-hornedâ distribution produces the minimum rms energy spread. Measured energy spread profiles taken at the end of the SLAC linac are shown in Fig. 8.20. A wire scanner located in a dispersive region of a downstream transport line was used to measure the profile Ïw . The energy spread ÏÎ´ was inferred by subtracting out, in quadrature, the contribution from the betatron beam size ÏÎ² = Ç«x Î²x : ÏÎ´ = Ïw 2 â ÏÎ² 2 . (8.28) The angle Ï denoted in the figure shows the BNS phase angle at the time of the measurement. These data show clearly the effects of not only misphasing the linac, but the additive contributions of the short-range longitudinal wake field and have been used together with simulation [44, 45] to determine the longitudinal bunch distribution at the SLC."
175,632,0.983,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications",Here the record length of 40 years is too short to reliably deï¬ne the shape parameter Îº so that result of using the prior is to increase Îº slightly toward the mean of the prior. The other two parameters adjust accordingly.
36,40,0.983,Bats in the Anthropocene : Conservation of Bats in a Changing World,"Effect size Fig. 2.2 Effect of urbanisation (log odds ratio and the estimated standard error) on relative intensity of habitat use in relation to the predominant food item (a), foraging space (b), and foraging mode (c). Solid symbols indicate the mean effect size (log odds ratio) and whiskers indicate the estimated standard error. Values of the estimated effect size, including the 95 % confidence intervals are listed on the right side of the figure"
165,400,0.983,New Methods for Measuring and Analyzing Segregation,"â¢ Other things equal, E[D] grows larger as city racial composition becomes more imbalanced. More exactly, E[D] is lowest when P = Q = 0.5 and increases at an increasing rate as P departs further from 0.5. â¢ The joint impact of area population size (ti) and city racial composition (P) on the magnitude of bias is complex. Specifically, the effects of each factor are nonadditive and nonlinear such that a bias-promoting change in one factor amplifies the other factorâs impact on bias. The most important conclusion to be drawn from these studies is more general and deserves to be separated from the others. Bias can be non-trivial in magnitude in many cases and it can vary greatly in magnitude from case to case including different cities, different group comparisons, or a given city-group comparison tracked over time. Consequently, bias can complicate measurement and potentially lead researchers to draw incorrect conclusions about the levels and patterns of variation in uneven distribution across group comparison, across cities, and over time. Significantly, all of the points just listed apply to all popular indices of uneven distribution except one. More specifically, the points listed above apply to the gini index (G), the Atkinson index (A), the Hutchens square root index (R), and the Theil entropy index (H). One popular measure â the separation index (S) â is an exception; index bias is less of a problem for this index than for any other widely used index of uneven distribution. Bias for S is smaller in magnitude than for any other popular index. In addition, variation in bias for S across cases is less complicated than for any other popular index. The major reason for this is that bias for S is determined by just one factor â area population size (ti) â with E[S] being given by the simple calculation E [S] = 1 / t i (Winship 1977). Thus, in contrast to other indices, bias for S does not vary with city racial composition (P). Accordingly, analyses reported in Chap. 16 show that the separation index (S) exhibits a lower level of bias than other indices under all conditions and especially when city racial composition is imbalanced. Indeed, the levels of bias for the separation index (S) are so much lower and so much less complicated than for other indices, this alone could be a compelling reason to always consider using S in empirical analyses. That said, E[S] is never zero and bias can render scores for S problematic in some extreme circumstances. Consequently, while using S to measure uneven distribution can go a long way to protecting against the potential distorting impact of index bias, using S cannot in itself guarantee that bias does not adversely affect index scores."
311,250,0.983,The Physics of the B Factories,"Figure 6.2.2. Measurements of the diï¬erences between the ï¬tted track parameters of the top and bottom stubs of cosmic ray muons with a momentum above 2 GeV/c in BABAR. The data are shown as points, Monte Carlo simulation as histograms. The (blue) smooth curves are the results of a Gaussian ï¬t to the data. From (Brown, Gritsan, Guo, and Roberts, 2009)."
391,474,0.983,Ocean-Atmosphere Interactions of Gases and Particles,"By convention, positive flux values indicate emission from the ocean and negative flux values indicate uptake by the ocean. For CO2 these simplified equations neglect its possible chemical enhancement, although this effect is thought to be small (Wanninkhof 1992; Matthews 1999). They further assume that away from the air-sea interface both the lower troposphere and upper ocean are well mixed, so that bulk measurements within them can be used to define a gas concentration gradient at the interface. A final assumption is that the water temperature at the interface (the skin temperature) is the same as that of the well-mixed upper ocean (Robertson and Watson 1992; Van Scoy et al. 1995). Atmospheric CO2 needs to equilibrate with the large pool of dissolved inorganic carbon in seawater, resulting in an equilibration time-scale of the surface ocean for gas exchange of nearly 1 year (Broecker and Peng 1982), i.e. much longer than the time-scale typically associated with upper ocean perturbations (such as by the seasonal cycle). The equilibration time-scale for CH4 and N2O is about 10 times shorter than for CO2 so that the deviations of these gases from equilibrium are generally smaller unless strong sources are present. The gas transfer velocity (k) is a function of turbulence at the sea surface and is often parameterised as a function of wind speed, as discussed in detail in Chap. 2. Several parameterisations of k as a function of wind speed have been proposed (e.g. Liss and Merlivat 1986; Wanninkhof 1992; Wanninkhof and McGillis 1999; Nightingale et al. 2000; Ho et al. 2006; Sweeney et al. 2007; Prytherch et al. 2010). The uncertainty in k, which has been estimated at 30 % (Sweeney et al. 2007), adds further uncertainty to estimates of net gas uptake and/or emission determined from surface water measurements (Sect. 3.6)."
311,1546,0.983,The Physics of the B Factories,"the expected B 0 â â+ ââ Î³ branching fractions are about 10â10 (Aliev, Ozpineci, and Savci, 1997; Eilam, Halperin, and Mendel, 1995). Observation of such signals with current sensitivities of BABAR and Belle would provide clear evidence for new physics. B 0 â â+ ââ Î³ is discussed in Section 17.11.1.3. 17.11.1.1 B 0 â âÂ± ââ²â (â, ââ² = e, Î¼) Searches for B 0 decays to pairs of light leptons, â = e, Î¼, have been performed at both B Factories and at hadron colliders; the latter are able to probe not only B 0 but also Bs0 decays. Searches for B 0 â e+ eâ and B 0 â Î¼+ Î¼â as well as the LFV mode B 0 â eÂ± Î¼â by BABAR and Belle provided the most stringent limits on new physics in these modes until around 2008, when they were superseded by results from Run-II at the Tevatron. LHC experiments have now pushed the experimental results considerably beyond the current B Factory sensitivities. The BABAR and Belle analyses are described in the following. BABAR has searched for these decays in a data sample of 384 Ã 106 BB pairs (Aubert, 2008as). The signal candidates are reconstructed by pairing oppositely charged leptons. Leptons are identified with stringent requirements which retain â¼ 93% (â¼ 73%) of eÂ± (Î¼Â± ), while less than â¼ 0.1% (â¼ 3%) of pions are misidentified as electrons (muons). The signal candidates are required to satisfy mES > 5.2 GeV/c2 and |ÎE| < 0.15 GeV. To partially recover the energy lost by electrons due to final-state radiation or bremsstrahlung, photons consistent with originating from the e+ or eâ track have their 4-momentum added to the track. Using MC simulations, peaking background contributions from B 0 â h+ hâ²â (h, hâ² = Ï or K) decays are estimated to be of the order of 10â4 or less. After applying the lepton ID requirements, other BB background is found to be negligible. The backgrounds from non-BB events, such as q qÌ (q = u, d, s, c) continuum and Ï + Ï â production, are reduced by using event shape variables which are combined into a single Fisher discriminant F. The signal yields for e+ eâ , Î¼+ Î¼â and eÂ± Î¼â (see Section 17.11.4 for further discussion of LFV modes) are independently obtained by maximum likelihood (ML) fits to mES , ÎE and F, where the p.d.f.s used in the likelihood function is composed of an uncorrelated product of the p.d.f.s of the individual discriminating variables. As an example the distribution of ÎE in the search for B 0 â Î¼+ Î¼â is shown in Fig. 17.11.2. No significant excesses of signal were seen in any modes, and the 90% C.L. upper limits on the corresponding branching fractions are"
8,727,0.983,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","is the partition function for the strongly interacting âN gas, i.e., for the simplest strongly interacting hadron gas. What now is .m/? We have the , N, all nuclei with their excited states, resonances, A resonances, N states and their resonances, etc. Only a finite number of them is known, but there are many more still unknown. The finite number of known states is, in general, sufficient to calculate some interesting quantities. This has been done for a long timeârecently and in the context of nucleosynthesis in the early universe as well as for the relativistic heavy ion collision in some pioneering papers [31, 32]. In particular, the two papers by A.Z. Mekjan [32] are an excellent introduction to many fundamental concepts and open questionsâmost recommended reading! What a finite number of states, included in the integral of Eq. (26.26) for ln Z, cannot do, is to generate a singularity of the partition function, in other words, generate a phase transition. As one sees from Eq. (26.26), ln Z.T; V/ is analytic in the entire right half of the complex T plane if OÅ.m/Â D mË , Ë < 1. If .m/ grows exponentially, .m/  CmË exp.m=T0 /, then the integral of Eq. (26.26) does not exist for Re.T/ > T0 and ln Z.T; V/ has a singularity at T0 , as first observed by Yu.B. Rumer [33], years before the SBM was proposed."
84,381,0.983,Eye Tracking Methodology,"where ri is the distance from the ith (fixation) point to its nearest neighbor, and Ï is the density of the observed distribution, i.e., Ï = n/A where A is the observation area (e.g., width Ã height in pixels, or perhaps in degrees visual angle, so long as the units match those used in the distance computation). The NNI is fairly straightforward to compute, as the kd-tree spatial data structure can be used for fast nearest-neighbor queries."
173,175,0.983,"intelligent Human Computer interaction : 9Th international Conference, Ihci 2017, Evry, France, December 11-13, 2017, Proceedings","To compute the spatial weight for each channel, the quantitative vector, L = Si âS Î±i Si [17] was used where Si is the channel sets and Î±i are their weights. The spectral weights were computed as given in [12] and then projected onto the frequency bands. In addition, the temporal information were also obtained and visualized. The training dataset are preprocessed under the spatial-spectral pre-condition Ïm â Ï, which results in a new dataset on which spatial ï¬ltering is done using CSP to obtain the spatial patterns. Then the ï¬rst two components obtained by CSP are projected onto the space yielding the CSP ï¬ltered signal Em. The peak amplitude PmCi for Em and each channel Ci â C. Then the PmCi is averaged over all set of preconditions Ïm â Ï, computed as 1 â PCi = ( ) Ïm âÏ Î±m PmCi where Î±m is the corresponding weight for the mth condition, which is then visualized using a 2-D topoplot map. From the topoplot, it can be observed that the left hand and right hand movement resulted in activation over the right and left hemisphere of the brain, the foot movement activated the central cortical area and tongue showed activation in the motor cortex region. The classiï¬cation results of the test dataset for the proposed method and the other competing method i.e., Regularized CSP (RCSP) is detailed as follows. In all the subjects the maximum number of iterations, M of the boosting algorithm was set to 180, which"
105,158,0.983,"Computer and information Sciences : 31St international Symposium, Iscis 2016, KrakÃ³w, Poland, October 27â28, 2016, Proceedings (Volume 659.0)","Where xmin and xmax represent the bounds of the search space and rand[0, 1] generates a random number between 0 and 1. Each employed bee evaluate the nectar amount of a food source corresponding to the quality (Fitness) of the associated solution, by: F itness(i) ="
233,518,0.983,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"a speciesâ evolutionary distinctiveness and the probability it will go extinct (see Isaac et al. 2007). We also use the conservation priority method HEDGE (Heightened EDGE), which like EDGE measure evolutionary distinctiveness by IUCN levels of extinction but additionally considers how future extinction of species will affect the evolutionary distinctiveness of remaining species. In sum, HEDGE estimates the expected terminal branch length of the focal species in light of future extinction risk (Steel et al. 2007). Both conservation methods generate ï¬xed probabilities of extinction as described in Table 1. For more information on how IUCN levels of imperilments are transformed into probabilities of extinction see Moores et al. (2008). Because there are not estimated probabilities of extinction available for data deï¬cient species we arbitrarily assigned an extinction risk score in between the two lowest IUCN extinction categories: least concern and near threatened (Table 1). All four metrics were calculated using both ârawâ branch lengths (estimated by MrBayes) as they contain information on the unique evolutionary information of terminal taxa and ultrametrized trees. Furthermore, with the purpose of comparing this approach to identify conservation priority areas with other commonly used conservation prioritization criteria we calculated evolutionary distinctiveness (ED) (Isaac et al. 2007) also implemented in the TUATARA module and gathered information on species richness. To identify conservation priority areas (CPAs) we used distribution range maps from the IUCN spatial database (2013) as a baseline to produce species richness, ED, EDGE and HEDGE maps, under both IUCN extinction probabilities transformation methods, pessimistic and IUCN50. The IUCN spatial database depict speciesâ range distribution as polygons based on the extent of occurrence, which is deï¬ned as the area contained within a minimum convex hull around speciesâ observations or records. This convex hull or polygon is further improved by including areas known to be suitable or by removing unsuitable or unoccupied areas based on expert knowledge. For each species the distribution range was converted to a grid system with cells of 5â² Ã 5â² (approximately 10 Ã 10 km at the Equator line). This spatial resolution was selected for its practical compromise between intensive computing and a reasonable representation of geographic patterns. Traditionally, a one-degree cell (100 Ã 100 km) has been used in macroecological analyses, but there is no ecological reason behind"
311,2606,0.983,The Physics of the B Factories,"where the factor 1 + Î´(s) summarizes the QED radiative corrections, which can be as large as 10% for slowly varying cross sections. On the other hand, the emission of initial state radiation allows the study of e+ eâ annihilation for a continuous spectrum of energies below the nominal beam energy, without changing the operating conditions of the collider, as outlined long ago (Baier and Khoze, 1965; Bonneau and Martin, 1971). This becomes clear when we write the diï¬erential form of the cross section dÏ(s, x) = W (s, x) Ï0 (s(1 â x)), (21.2.4) and note that the reduced CM energy after photon emission is  just the invariant mass of the hadronic system: m = s(1 â x). In terms of m, the diï¬erential cross section becomes dÏ(s, m) W (s, m) Ï0 (m). (21.2.5) It should also be noted that the dominant contribution from ISR processes comes from the diagram shown in Fig. 21.2.2, with a single photon emitted. From the experimental point of view, the Born diï¬erential cross section for the process e+ eâ â f as a function of m is obtained from the measurement of the cross section for e+ eâ â Î³ISR +f ."
32,305,0.983,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","Figure 13.3 shows the final size distribution of the modified SIIR model. The simulation is performed in the same conditions as Fig. 13.2. The profile of the SIIR curve in Fig. 13.2 looks the same as the curve in Fig. 13.3. Therefore, the cause of the discontinuous final size distribution is the division of the infection period into two stages, and the mobility of I has no effect on the discontinuity."
70,259,0.983,Optics in Our Time,"Imagine an electron beam of an inï¬nitesimally small diameter is incident on a solid surface, as depicted in . Fig. 6.3 below. As these electrons penetrate the solid, they will interact with the sampleâs constituent atoms. This interaction is referred to as âelectron scatteringâ and is further divided into two categories: (a) an inelastic scattering, where the incident electron gives up part of its energy as a result of its collision with the solidâs atoms; and (b) an elastic scattering, which causes the electron to change its direction of travel with almost no energy loss. It is the ï¬rst scattering type, however, that gives rise to the signals enumerated in . Fig. 6.3, whilst the second type is normally what determines the shape of the interaction volume of the incident electrons within the solid under study, as depicted in . Fig. 6.4 . The shape and size of the interaction volume depends on the incident electron energy, its angle of incidence with respect to the surface and on the average atomic number of the sample under study. If one concentrates on the emitted electron signal alone and plots their number against their energy, one would in principle collect a distribution similar to that shown in . Fig. 6.5 . The collection of one of the signals resulting from the interaction depicted in . Fig. 6.3 has over the years resulted in a speciï¬c class of instruments reï¬ecting the type of information gathered from such interaction. For example, if one collects the resulting X-ray photons of a given element making up the solid, the collected image would be a map of the distribution of such an element in the solid, normally"
45,177,0.983,Measurement and Control of Charged Particle Beams,"In practice, there are many uncertainties whose presence must be appreciated when correcting the beam orbit in both linear and circular accelerators. Such uncertainties include the variations in the electronic and/or mechanical centers of the beam position monitors (BPMs), in the magnetic center of the quadrupoles (inside which the position monitors are often mounted), or in the electromagnetic center of accelerating structures. Consider the case illustrated in Fig. 3.1. In this case, the absolute beam position, with respect to a reference axis, is given by x = xd + xb + xm ,"
311,1246,0.983,The Physics of the B Factories,"Figure 17.7.6. The Ît distributions of events enriched in signal for (a) B 0 and (b) B 0 tagged B â Ï+ Ïâ events. The solid lines are the sum of signal and backgrounds and the dashed lines are the sum of backgrounds. The time-dependent CP asymmetry is presented in (c). (As there is no entry in the second bin from the left in (b), the corresponding point is at 1 in (c).) The curve corresponds to the measured asymmetry (from Aubert (2007b))."
372,1247,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where - is the gamma function. S0 is 1.3 times the flux density of a uniformly bright source of radius a. This source has the interesting characteristic that its angular size varies as , 0:7 (because a scales as , !2:1 ), and the flux density varies as , 0:6 (see the example of MWC349A in Fig. 1.1). However, the second and higher moments of the intensity distribution are infinite. Nonetheless, the visibility function can be calculated from Eq. (10.78). It is shown in Fig. 10.13. It has the interesting characteristic that it decreases linearly (rather than quadratically) with q, that is, V.q/ ' S0 .1 ! bq/ ;"
65,322,0.983,Handbook of Ocean Wave Energy,"1. By considering the incident and resultant wave ï¬elds. The difference in wave energy transport by the two tells how much power has been absorbed. 2. By calculating the product of machinery force and velocity. This requires a quite elaborate modelling of the wave-absorbing system or extensive experimental campaigns. Simpliï¬ed and rough estimates of the average absorbed power may be found: â¢ From experience data based on similar systems. A commonly used measure is the relative absorption width or length, also referred to as the capture width ratio, where experience indicates that values between 0.2 and 0.5 are typical across the wide range of converter designs proposed [14]. It is expected that somewhat higher numbers can be reached with improved conversion systems combined with efï¬cient control algorithms as these are in general both still at a low level of maturity. Average capture width ratios higher than 1.0 have been shown to be realistic for operation of point absorbers in irregular waves [15]. â¢ From theoretical upper bounds on the power that can be absorbed by oscillating bodies. These will be treated in the following. Firstly, the power is limited by the radiation pattern for waves generated by the oscillating system. It is useful to look at this limit for two idealised cases: (i) An axisymmetric body, which is symmetric about the vertical axis, and (ii) a large body of width comparable to or larger than the wavelength, often referred to as a terminator device. The ï¬rst type will radiate circular waves when oscillating in heave, and dipole-pattern waves when oscillating in surge or pitch. The second type will radiate plane waves over a limited width, see Fig. 6.10 for illustrations. These properties result in the following limits to the power that can be absorbed [5]:"
98,114,0.983,"The Third Sector As a Renewable Resource For Europe : Concepts, Impacts, Challenges and Opportunities","Other dimensions of the European third sectorâthe scope of activity by field and the revenue structureâalso vary considerably by region. Due to data availability limitations, however, we can only examine these variations on a much smaller set of European countries and on a smaller set of institutionsâthat is, only for the NPI components of the TSE sector and only for the 20 countries covered by the Johns Hopkins Comparative Nonprofit Sector Project. As Fig. 3.9 shows, the distribution of service and expressive activities of NPIs is very different in the Scandinavian countries than it is in Northern and Southern Europe. Thus, in the Scandinavian region, 57 percent of nonprofit FTE employment is"
167,45,0.983,The Interconnected Arctic â UArctic Congress 2016,"The detailed description of the standard long-term monitoring design and method of GLORIA, is given in the field manual available online (Pauli et al. 2015), where the authors of this paper contributed as well. According to this Multi-Summit Approach a target region in the Polar Urals was established in 2001. It comprises a suite of four summits (see Table 2.1) distributed in equal elevation intervals and representing an elevation gradient of vegetation patterns, characteristic for this mountain region (Gorchakovky 1975). The summits are situated in Yamalo-Nenets region of Russia in the valley of Sobâ river ca. 45 km NNW Salekhard (66Â°54â² â 67Â°00â² N, 65Â°35â² â 65Â°46â² E) along the Sobâ-Elets passageway and quite easily accessible by the railway from Labytnangi to Ust-Vorkuta. The sampling area covered each summit from its top (highest summit point) down to the 10 m contour line and was divided into eight summit area sections (SAS). The standard records for each SAS include a complete species list with the estimation of the abundance of each species and percentage top cover of surface types. The 3mÃ3m quadrat clusters are placed at the 5-m level in all four main compass directions (Fig. 2.1). Each quadrat cluster includes four 1-m2 quadrats, where the top cover of surface types and cover of each vascular plant species are recorded (Pauli et al. 2015). Four T-loggers (GeoPrecision Mlog-5W) positioned on each summit has been measuring the soil temperature 10 cm below the surface at hourly intervals since 2001 until 2015. Polar Urals is one of the few GLORIA target regions, where invertebrate monitoring as an extra approach is used according to research protocol by one of the authors (Mikhailov 2015). The field teams included leading experts of regional biodiversity, able to identify all vascular plant taxa at the vegetative stage on site. For arthropods the exact determination of collected specimens followed later on by comparing with reference"
372,1782,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"The denominator in Eq. (14.75) is the total intensity. Ë.y/ is the Faraday depth, which increases monotonically into the source as long as the sign of the longitudinal magnetic field direction does not change. In any case, we can superpose all the radiation from the same Faraday depth and write the integrals in Eq. (14.75) as a function of Ë instead of y, yielding"
365,871,0.983,Climate Smart Agriculture : Building Resilience To Climate Change,"may be some underlying differences in their risk functions, for instance. To control for this possible source of heterogeneity, we included a set of dummy variables to capture the specificity of the different crops.20 The question now is whether farm households that implemented climate change adaptation strategies experienced a reduction in downside risk exposure (e.g., a decrease in the probability of crop failure). As described in the previous section, we assess the probability distribution of the stochastic production function by applying a moment-based approach. A simple approach to answer the aforementioned question consists in estimating an OLS model of downside risk exposure that includes a dummy variable equal to 1 if the farm household adapted, 0 otherwise (Table 2, column (1)). An increase in skewness implies a reduction in downside risk exposure. This approach would lead us to conclude that the adaption significantly reduces farm householdsâ downside risk exposure (the coefficient of the dummy variable adaptation is positive), although the effect is weak (significant at the 10 percent statistical level). This approach, however, assumes that adaptation to climate change is exogenously determined, while, in fact, it is a potentially endogenous variable. As such, the estimation via OLS would yield biased and inconsistent estimates. In addition, OLS estimates do not explicitly account for potential structural differences between the skewness functions of the adapters and non-adapters. The estimates presented in the last two columns of Table 2 account for the endogenous switching in the skewness function. Both the estimated coefficients of the correlation terms Ïj are not significantly different from zero (Table 2, bottom row). This implies that the hypothesis of absence of sample selectivity bias may not be rejected. However, the differences in the coefficients of the skewness functions between the farm households that adapted and those that did not adapt illustrate the presence of heterogeneity in the sample (Table 2, columns (3) and (4)). The skewness function of the adapters is significantly different from the skewness function of the nonadapters (Chow test p-value = 0.000). Among farm households that in the past adapted to climate change, assets such as animals are significantly associated with an increase in the skewness, and so in a decrease in downside risk exposure. Inputs such as seeds display an inverted Uâshape relationship. The total marginal impact (estimated at the sample mean) is positive. This implies that seeds have a positive effect in reducing downside risk exposure for the group of the adapters. While it is difficult to understand the reasons behind such results, one may speculate that the adapters may have better access to markets for inputs and this allows them to better manage risk of crop failure. Infertile soils are instead associated with an increase in downside risk exposure. However, these factors do not significantly affect the downside risk exposure of farm households that did not adapt.21 We find instead that climatic factors play a very important role in explaining risk exposure of the group of non-adapters. These non-adapters are, indeed, significantly affected by the rainfall"
151,242,0.983,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"would be affected by environmentally-relevant concentrations of CPY and that indirect effects would occur higher in the food web. Thus, protection of these other components of the food web would also be protective of plants in general and phytoplankton in particular. Crustacea. Data for toxicity of CPY from 23 species of crustaceans met the criteria for inclusion in the analysis (Table 2). The range of LC50s was from 0.035 to 457 Î¼g CPY Lâ1. The model for the cumulative frequency distribution (CFD) with the best fit was the Gumbel (SI Table 4, SI Fig. 1) and the SSD is shown in Fig. 3. The HC5 (95%CI) was 0.034 (0.022â0.051) Î¼g CPY Lâ1 (SI Table 4). Insects. Toxicity data for CPY from 17 species of aquatic insects met the criteria for inclusion in the analysis (Table 2). The range of LC50s was from 0.05 to >300 Î¼g CPY Lâ1. Two values were reported as âgreater thanâ and were included in the calculations of the ranks for constructing the SSD (Fig. 4). The Extreme Value model gave the best fit for the CFD (SI Table 4), but visual inspection of the plots of the various models showed that the fit in the lower tail was better for the Gumbel model (SI Fig. 1). The lower tail is where exceedences are more likely and, for this reason and for consistency with the other taxa, this model was used. The HC5 (95%CI) was 0.087 (0.057â0.133) Î¼g CPY Lâ1 (SI Table 4). Fish. Data for toxicity of CPY from 25 species of fish met the criteria for inclusion in the analysis (Table 2). One value reported as âgreater thanâ was included in the calculations of the ranks for constructing the SSD (Fig. 5). The model that exhibited the best CFD fit was Gumbel (SI Table 4 and SI Fig. 1). The range of LC50s was"
311,1915,0.983,The Physics of the B Factories,"and lie well within the central part of the electromagnetic calorimeter. Additional photons can be present as long as their individual energies are less than that of the signal photon and their total energy in the laboratory frame does not exceed 0.14 GeV. A multilayer perceptron neural network is then used to combine kinematic variables from the dipion system into a single discriminant that can reject background. The neural network is trained on data taken at a collider CM energy below the Î¥ (2S) resonance, and on signal MC simulation. In the low-mass region, this approach retains 87% of simulated signal events while rejecting 96% of events from non-Î¥ (2S) (continuum) events. In the high-mass region, this approach retains 73% of signal while rejecting 98% of continuum background. In addition to backgrounds from sources other than the Î¥ (2S), there could be backgrounds from real Î¥ (1S) radiative decays where the final-state products are diï¬cult to detect reliably. For instance, Î¥ (2S) â Ï + Ï â Î¥ (1S), where the Î¥ (1S) then decays to either Î¥ (1S) â Î³nnÌ or Î¥ (1S) â Î³KL0 KL0 , are allowed decays where the final-state hadrons are not eï¬ciently reconstructed in the BABAR detector. To reject these backgrounds, events are rejected where there is activity in the BABAR instrumented flux return within a 20â¦ window opposite the reconstructed signal photon. This requirement is only applied in the low-mass region for mA0 < 4 GeV/c2 . In the high-mass region there is a potential contamination from the process e+ eâ â e+ eâ Î³ â Î³ â where Î³ â Î³ â â Î· â² and Î· â² â Î³Ï + Ï â while the electron and positron escape detection at lowangles to the beams. This is largely rejected by requiring the opening angle between the photon and the dipion system be no more than 160â¦ . The signal is extracted from a maximum likelihood fit to two variables: the dipion recoil mass Mrecoil (Ï + Ï â ) and the âmissing massâ, i.e. the mass of the system recoiling against the reconstructed dipion and photon, Mrecoil (Ï + Ï â Î³) = (Pe+ eâ â PÏ+ Ïâ â PÎ³ )2 ."
289,1141,0.983,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","The top-level theorem proved of our high-level functional specification of MW is: Theorem perstep weights noregret : (expCostsR â OPTR)/T â¤ Î· + ln size A / (Î· âT). The expression expCostsR is the cumulative expected cost of MW on a sequence of cost vectors, or the sum, for each time t, of the expected cost of the MW algorithm at time t. OPTR is the cumulative cost over T rounds of the best fixed action. The number Î· (a dyadic rational required to lie in range (0, 1/2]) is the learning parameter provided to MW and ln size A is the natural log of the size of the action space A. T is the number of time steps. In contrast to the interpreter and semantics of Sect. 5.3 (where we do exact arithmetic on dyadics), for reasoning and specification at the level of the proof we use Coqâs real number library and real-valued functions  such as square root and log. By choosing Î· to equal ln size A / T , Corollary 1 showed that itâs possible to restate the right-hand side of the inequality in perstep weights noregret to"
311,2286,0.983,The Physics of the B Factories,"D , AM or Ï, must be diï¬erent from zero. The sensitivity of AÎ to the CP violating parameters is limited by the small magnitude of the mixing parameters x and y. Measurements of this asymmetry were typically performed together with the measurements of yCP . The average of measurements (Staric, 2012a; Lees, 2013d) using the K + K â and Ï + Ï â final states is found to be AÎ = (0.02 Â± 0.17)% ,"
274,141,0.983,Firm innovation and Productivity in Latin America and The Caribbean : The Engine of Economic Development,"shows that non-innovative firms are smaller, are less likely to obtain public financial support for innovation, are less likely to have patents, are less likely to export, are less likely to cooperate with other firms or institutions in terms of innovative activity, and are less likely to be foreign-owned. thus, conclusions about the relationship between productivity and innovation spending, based on non-parametric testing, may be, at least in part, driven by differences in other firm characteristics. to account for differences in characteristics when comparing distributions, DiNardo et al. (1996) developed an approach that allows for graphical assessment of the difference in distributions of an outcome variable of interest between two groups by disentangling what is due to differences in characteristics and what remains unexplained. In essence, their approach is a semi-parametric method based on the construction of counterfactual densities obtained by reweighting observations according to differences in the underlying characteristics. In our context, this means calculating the distribution of productivity of non-innovative firms if they had the characteristics of innovative firms. More specifically, each individual observation may be considered a vector (PROD, Z, INNOV ), where Z is the vector of firm attributes other than innovation that are correlated with productivity. the joint distribution of productivity and characteristics conditional on innovation status may be defined as F (PROD, Z|INNOV = 0,1). the density of productivity for innovative firms, fINNOV=0,1(PROD), may then be expressed as the integral of the density of productivity, conditional on some firm characteristics and on innovative activity, f(PROD|Z, INNOV = 0), over the distribution of firm characteristics F (Z|INNOV = 1): f ( PROD ; INNOV = 0, 1) = â«dF ( PROD , Z | INNOV = 0, 1)"
311,353,0.983,The Physics of the B Factories,"Thus the resulting Q is always larger or equal to the one obtained when all events are treated as a single category. One gains most from dividing events into categories when the diï¬erences in dilution (or mistag fraction) between categories can be made large. However, the characteristics and any systematic eï¬ects, such as correlations with the tag vertex resolution, tag-side interference (see Section 15.3.6), or background levels, are expected to be determined by the diï¬erent ï¬avor-speciï¬c signatures. For this reason one would prefer a grouping of events according to diï¬erent signatures over a category deï¬nition based on w. The mistag probability w that can be achieved for a given set of Btag decay modes is determined by the ï¬avorspeciï¬c signatures present in these decays. Fortunately, the mistag probabilities of diï¬erent ï¬avor-speciï¬c signatures tend to be diï¬erent. For example, in semileptonic decays the charge of a reconstructed high-momentum electron or muon gives a much better indication of the correct tag than the charge of a low momentum pion (âslow pionâ) from a secondary Dâ decay. Therefore a grouping of events into tagging categories according to the mistag probability naturally provides a grouping according to the diï¬erent signatures of the corresponding Btag decays. Conversely, a grouping according to diï¬erent signatures leads to an approximate grouping according to mistag probabilities. As a result it is possible to deï¬ne tagging categories that both optimize the tagging performance and group events according to diï¬erent signatures."
241,1075,0.983,Second Assessment of Climate Change for the Baltic Sea Basin,"difference between the surface water pCO2 and the relatively stable atmospheric pCO2. It is not known whether the Baltic Sea is a net sink or a net source of atmospheric CO2. Whereas Thomas and Schneider (1999) reported a CO2 uptake of 0.9 mol mâ2 yearâ1 by the central Baltic Sea, Wesslander et al. (2010) concluded that the Gotland Sea and Bornholm Sea are sources of atmospheric CO2 and release 1.6 and 2.4 mol mâ2 yearâ1 CO2, respectively, to the atmosphere. These discrepancies are due to the different data sources, all showing insufï¬cient temporal and spatial coverage in view of the huge daily, seasonal, interannual and regional variability in surface water pCO2. The deep water of the major basins is subject to continual mineralisation of organic matter that leads to the accumulation of total CO2 during periods of stagnation. To estimate the effect of increasing CT on pH, concurrent alkalinity changes must be taken into account (Edman and Omstedt 2013). During oxic conditions, nitriï¬cation of ammonia"
365,923,0.983,Climate Smart Agriculture : Building Resilience To Climate Change,"The control variables in both countries are carefully constructed to control for potential endogeneity issues as much as possible in cross-sectional studies. Institutional variables are taken from the district/enumeration area level dataset rather than from householdâs self-reported values and wealth indices are constructed using the ownership of pre-determined durables. Given the crosssectional nature of the analyses, this is the best that can be done to control potential endogeneity."
167,137,0.983,The Interconnected Arctic â UArctic Congress 2016,"in the ecosystem components for both of the studied ecosystem. Related exponential equations are presented for both ecosystem components. The magnification for DDT and PCB in the biota and environment is presented in Fig. 7.6. The average pollutant concentration in the sea water, sea-bed sediment and biota were used for the diagrams. The analysis of the diagram indicated that a considerable increase in the POP accumulation upwards in the hierarchical level of investigated ecosystems took place."
214,139,0.983,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"columns over major land masses, to represent temperature and precipitation, that then drive economic models in different countries or regions. The oceans might be represented by one column for each ocean. Intermediate complexity models capture some of the basic energy budget components and limited circulation. These models are used when the needed climate variables are simple. Simple outputs might be the average surface temperature or precipitation in a region or over an entire country. These outputs can then be used to efï¬ciently drive a country-level economic model. Finally, we come to models that have ï¬nite elements covering the earth. These can be regional (a regional climate model3) or global. Regional climate models or limited-area models have boundaries: They do not represent the entire surface of the earth (see Fig. 5.5e). The limited area enables them to be run with ï¬ner resolution than a global model. Finer horizontal resolution is good for representing the effects of surface features like mountains (topography). If you want to understand the climate of a region near or within mountain ranges, it is critical to represent the effects of the mountains correctly. This may be easy to understand in terms of a small region like a mountain valley, or the region on the dry side of a mountain range away from the coast, such as eastern Oregon in the United States, or the high deserts on the Andes mountain range in Peru and Chile. Limited-area models may also include more processes because they represent smaller regions with fewer boxes, a limited domain. The difï¬culty is that they have âedgesâ or boundaries: Air must blow into and out of them, along with the energy associated with the air, and water in vapor or in clouds (the âblowingâ around is also called transport, or advection). Limited-area model boundaries (the region just outside the domain of the model) have to be deï¬ned from somewhere to determine what values are given to the model at the boundary. For examining present-day climate, the conditions at the boundary can be taken from observations, often using data collected for large-scale weather prediction. Indeed, many ï¬ne scale models of weather have limited domains. For climate prediction, limited-area models are difï¬cult because the boundaries mean energy and mass can leave the model (nonconservation). It is a challenge, however, to use these models for the future, since they need to have speciï¬ed future boundary conditions. The results are often or usually strongly dependent on the boundary conditions speciï¬ed. But these models, with their small grid spacing and detailed representation of processes, are good for representing details of local climate variation through weather events: extremes of precipitation and temperature, for example. Models with a global grid are good for representing the overall patterns of motion of the atmosphere (see Fig. 5.5f). They have no horizontal boundaries. These global grids are known as General Circulation Models (GCMs). They need only a top (space) and a bottom (the surface of the earth) boundary condition. The"
200,211,0.983,"Earthquakes, Tsunamis and Nuclear Risks","Current method was proposed to evaluate component failure probabilities by Kennedy et al. in 1980. The characteristics are as follows: â¢ Uncertainty of seismic hazard is expressed by the fractile curves that are composed of multiple curves corresponding to the percentage of the confidence level or aggregate curves corresponding to each set of alternative models and assumptions in calculating the hazard curve. â¢ Main causes of variability of model and data expressing response and capacity are categorized to âaleatory uncertaintyâ (or âuncertainty due to randomnessâ) and âepistemic uncertaintyâ (or âuncertainty due to lack of knowledgeâ). The first one canât be reduced by the insights of experiments or theoretical studies because this type of variability is caused by inherent randomness of natural phenomena. The second one can be reduced by the insights of expansion of experimental data and enhancement of analysis models because this variability comes from lack of knowledge or simplification of analysis model. â¢ Usually, uncertainties in hazard analysis, fragility analysis, and in parameters of accident sequence models are propagated to the uncertainty in core damage frequency, while uncertainty of event tree and fault trees used in accident sequence analysis are considered by sensitivity studies."
26,165,0.983,Cognitive Supervision for Robot-Assisted Minimally Invasive Laser Surgery,"Table 5.1 shows the average depth of incision for all the experiments; standard deviation and coefficient of variation are also listed. Observed variabilities have a marginal significance, as the coefficients of variation are all well below 20 %. Further analysis of experimental data reveals that, once a delivery mode has been fixed, the ablation depth depends linearly on the applied exposure time. A simple linear regression is able to model the relation between these two quantities (Fig. 5.5) for both CW and RP. In both cases, the fitting error (normalized mean squared error, nMSE) is 0.02 %."
391,959,0.983,Ocean-Atmosphere Interactions of Gases and Particles,"inorganic carbon (DIC), total alkalinity (TA), temperature, salinity, and sea-surface pressure. This opens up the possibility of computing or estimating pCO2 from parameters that can be monitored on a basin- or near basin-scale. Sea surface temperature (SST) is remotely-sensed by satellites (see Sect. 5.1.2.1) and measured by floats. Sea surface salinity (SSS) is monitored by floats and satellite data will be available soon through the Aquarius (Le Vine et al. 2007, http://aquarius.nasa.gov/) and SMOS projects (see Sect. 5.1.2.2). DIC and TA cannot yet be remotelysensed. But for any individual ocean basin, TA can be estimated from SSS using a non-linear fit (Eden and Oschlies 2006). In addition, the SeaWiFS project provides satellite-derived estimates of the chlorophyll a concentration (see Sect. 5.1.2.3). The sea surface pCO2 and the air-sea CO2 flux vary over time. A synthesis study by Watson et al. (2009) presented annual CO2 flux estimates for the North Atlantic for the years 2002â2007. It demonstrated that the CO2 uptake is subject to large interannual variability. Bates (2007) reported high observed variability of CO2 fluxes due to increased wind speed over a 20 year period (1984â2005) at the Bermuda Atlantic Time Series (BATS) site. The occurrence of hurricanes in this region accounts for up to 29 % of the variability of summertime CO2 fluxes (Bates 2007) and the increased frequency of hurricanes can potentially impact by approximately 16 % the interannual variability of CO2 fluxes in the North Atlantic Ocean. In this context the North Atlantic Oscillation (NAO) is also a driver of variable wind speeds as it drives large scale climate variability over the North Atlantic (Hurrell 1995) and increasing values of NAO index correspond to increased CO2 fluxes in the northern"
165,367,0.983,New Methods for Measuring and Analyzing Segregation,"The difference of means formulations of indices of uneven distribution makes it relatively straightforward to implement segregation measurement in either conventional aspatial formulations or in spatial formulations. Aspatial versions of segregation indices are familiar because they are widely used in empirical studies. They are obtained by applying any of the computing formulas reviewed here using data for non-overlapping, bounded areas such as census tracts, block groups, or blocks. It is appropriate to designate the resulting scores as âaspatialâ because the spatial arrangements of the units (e.g., blocks, block groups, tracts) have no implications for the scores obtained. Spatial formulations would differ on this key point; namely, the spatial arrangement of units can potentially impact index scores. In truth, opportunities to compute spatial versions of indices of uneven distribution have always existed. But apparently this has not been widely appreciated. Or, more carefully, researchers have rarely taken advantage of this possible option. One simple way to implement popular indices of uneven distribution in either aspatial or spatial versions is to use computing formulas that give index scores as population averages for area-specific residential outcomes. Figures in Appendices present formulations of this type for all popular indices of uneven distribution. Here I note only two such formulations, one for D and one for S. Both take the general form 100 Ã (1 / T ) Ã Sy where y is a residential outcome for individuals scored on the basis of their area of residence. The value of D can be obtained using y = | p i â P | /2 PQ and the value of S can be obtained using y k = ( p i â P ) Â² / PQ . If y and p are calculated using only the data for the block the individual resides in, the calculations will yield the usual index score which is aspatial because how individual blocks are arranged in space has no impact on index scores. However, if one chooses to do so, one can calculate y and p based on spatially defined neighborhoods. For example, one could define the neighborhood as a âfirst orderâ contiguity neighborhood based on combining data for the block the individual resides in and also the blocks that are adjacent to that block. This is the only modification that is required; all other steps in the calculations remain the same."
311,689,0.983,The Physics of the B Factories,"Particle physics experiments of the past thirty years have conï¬rmed the Standard Model (SM) even at the quantum level, including quark mixing and CP violation. However, the observed matter-antimatter asymmetry of the universe indicates that there must be additional sources of CP violation, since the amount of CP violation implied by the CKM mechanism is insuï¬cient to create the observed matter-antimatter asymmetry. In fact, the excess of baryons over antibaryons in the universe Î = n B â nB (16.2.1) is small compared to the number of photons: the ratio is measured to be Î/nÎ³ â¼ 10â10 . Although it is conceivable that there might be regions in the universe consisting of antimatter, just as our neighborhood consists of matter, no mechanism is known which could, from the Big Bang, produce regions of matter (or antimatter) as large as we observe today. Furthermore, searches have been performed for sources of photons indicative of regions of matter and antimatter colliding. These searches failed to ï¬nd any large regions of antimatter. The conditions under which a non-vanishing Î can emerge dynamically from the symmetric situation Î = 0 have been discussed by Sakharov (1967). He identiï¬ed three ingredients 1. There must be baryon number violating interactions Heï¬ (ÎB = 0) = 0. 2. There must be CP violating interactions. If CP were unbroken, then we would have for every process i â f mediated by Heï¬ (ÎB = 0) the CP conjugate one with the same probability Î (i â f ) = Î (i â f )"
241,776,0.983,Second Assessment of Climate Change for the Baltic Sea Basin,"appear smaller than those in the other scenarios and a stabilising tendency at the end of the scenario simulation is observed (Neumann 2010; Neumann and Friedland 2011). Most of the scenario simulations for the Baltic Sea were performed with regionally limited ocean models with lateral boundaries in the Kattegat or Skagerrak. Due to the proximity of the lateral boundary, results for the Kattegat are not reliable. Instead, projections for the North Sea should be employed to study changes in the Kattegat and Skagerrak (Ãdlandsvik 2008; Holt et al. 2010). Only Madsen (2009) investigated both shallow seasâthe Baltic Sea and the North Seaâsimultaneously and found that warming is greater in the Baltic Sea than that in the North Sea. Holt et al. (2010) found in the A1B scenario that the shelf sea regions warm substantially more than the open ocean, by 1.5â4 Â°C depending on location. These results agree with observed warming trends in the Baltic Sea, North Sea and other shelf seas (Belkin 2009)."
344,185,0.983,Development Policies and Policy Processes in Africa : Modeling and Evaluation,"The UNU, WHO and FAO (2004) recommend that energy needs cannot be considered in isolation of other nutrients as âthe lack of one will influence the others.â Ecker and Qaim (2011) maintain that micronutrient deficiencies, especially in minerals and vitamins, are often even more widespread in developing countries than calorie deficiencies, which contributes to severe health problems in these countries. Looking beyond caloric availability is therefore critical, particularly when people suffer from multiple nutritional deficiencies as is often the case in developing countries, Malawi included. Hence, in the Malawi microsimulation model, Ecker et al. (2012) focus on a wider range of nutritional indicators. Rather than using consumption changes observed in the CGE model directly in the nutrition model, Ecker et al. (2012) adopt the two-stage micro-econometric model developed by Ecker and Qaim (2011) to first estimate consumption changes in response to household income changes.4 In the first stage food demand elasticities are estimated assuming a quadratic almost ideal demand system (QUAIDS). In the second stage the technical coefficients from the first-stage estimation are translated into own-price, cross-price and income elasticities for different nutrients, including calories, protein, iron, zinc, and vitamins A, B3 (riboflavin), B9 (folate), B12, and C. Elasticities are estimated separately for rural and urban households across the different household quintiles. These form the basis of the microsimulation model: CGE results on income changes for different household groups are now fed into the microsimulation model where elasticities are applied to estimate new deficiency levels across the various nutrients. From the discussion it should be apparent that the main difference between the two model frameworks lies in the specification of the microsimulation components and the way in which results from the âmacroâ model are linked to the âmicroâ level. In the Tanzania model caloric availability is calculated directly on the basis of changes in consumption quantities for different consumption items included in the CGE model. As discussed, these consumption changes are determined in an LES demand system, subject to relative price and income changes. In contrast, in the Malawi model, only changes in real household income are passed down to the micro-level. Changes in nutrient availability are calculated on the basis of income elasticities derived from a QUAIDS, a somewhat more flexible and advanced demand system, but one that stands distinct from the CGE modelâs LES demand system."
238,90,0.983,Nanoinformatics,"NÃ¸rskov et al. performed a series of systematic DFT calculations and proposed the semiempirical concept of the d-band model [3â5]. The model assumes that the d-electrons of transition metals play the most important role in chemisorption. This approach involves linear scaling between the energy of the d-band center (Îµd) relative to the Fermi level (EF) and the adsorption energy for a given adsorbate. The higher the d-states are in energy relative to the Fermi level, the emptier the antibonding states and the larger the adsorption energy of an adsorbed species on a surface. A calorimetric study by Lu et al. [13] subsequently provided experimental evidence to support the d-band model. This work showed moderate linear correlations between the experimental heats of adsorption of CO, H2, O2, and C2H4 on various metal surfaces and the positions of the d-band centers as calculated by Hammer and NÃ¸rskov [3]. The d-band model also predicts that adsorbate binding energies should correlate with one another [5]. Since the transition-state structures on different metals tend to be rather similar, the activation energy for an elementary reaction should exhibit a linear relationship with the energy change for the elementary reaction. Thus, the kinetic parameter for a catalytic reaction involving a metal can be written as Îµd â EF, equivalent to the position of the d-band center relative to EF. Recent experimental studies have demonstrated the validity of the d-band model when describing trends in catalytic activity [14â18]. As an example, Furukawa et al. found a relationship between the d-band centers of Ni and Ni3M (M = Ge, Nb, Sn, Ta, or Ti) intermetallics and their activation energies with regard to the H2âD2 equilibration [16]."
231,1178,0.983,North Sea Region Climate Change Assessment,"DBEM) were applied to the same present distribution datasets and the same environmental input parameters. As indicated by the test statistics, each method produced a plausible present distribution and estimate of habitats suitable for each species (14 commercial ï¬sh). When used to make projections into the future, the ensemble of models suggested northward shifts at an average rate of 27 km per decade (the current rate is around 20 km per decade for common ï¬sh in the North Sea, Dulvy et al. 2008). This modelling approach was extended to include several additional, commercial species (squid Loligo vulgaris, seabass, sardine, sprat, John dory, anchovy, plaice, herring, mackerel, halibut Hippoglossus hippoglossus, red mullet etc.) as part of a Defra study (Defra 2013). The species predicted to move the furthest were anchovy, sardine, Greenland halibut, John dory and seabass (i.e. E. encrasicolus, S. pilchardus, R. hippoglossoides, Z. faber and D. labrax respectively, see Fig. 12.4). By contrast Rutterford et al. (2015) used the same ï¬sh survey datasets for the North Sea, together with generalised additive models (GAMs), to predict trends in the future distribution of species, but came to the conclusion that ï¬sh species over the next 50 years will be strongly constrained by the availability of suitable habitat in the North Sea, especially in terms of preferred depths. The authors found no consistent pattern among species in predicted changes in distribution. On the basis of the GAM results the authors suggested that they did not expect or predict substantial further deepening (as previously observed by Dulvy et al. 2008), and that the capacity of ï¬sh to remain in cooler water by changing their depth distribution had been largely exhausted by the 1980s, that ï¬sh with preferences for cooler water are being increasingly exposed to higher temperatures, with expected physiological, life history and negative population consequences. Beaugrand et al. (2011) described a model to map the future spatial distribution of Atlantic cod. The model, which they named the non-parametric probabilistic ecological niche model (NPPEN), suggested that cod may eventually disappear as a commercial species from some regions including the North Sea where a sustained decline has already been documented; in contrast, the abundance of cod is likely to increase in the Barents Sea. Lenoir et al. (2011) applied the same NPPEN model with multiple explanatory variables (sea surface temperature, salinity, and bathymetry) to predict the distribution of eight ï¬sh species up to the 2090s for the Northeast Atlantic. This study anticipated that by the 2090s horse mackerel and anchovy would show an increased probability of occurrence in northern waters compared with the 1960s, that pollack Pollachius pollachius, haddock and saithe would show a decrease in the south, and that turbot Scophthalmus maximus and sprat would show no overall change in probability (â0.2 to +0.2) anywhere."
359,200,0.983,"Micro-, Meso- and Macro-Dynamics of the Brain","Based on the point-source equation, one can note the following: first, there is an inverse relationship between distance d and the amplitude of the resulting voltage deflection Ve, i.e., the farther away to recording site is from the location of the current point-source, the larger the attenuation of the amplitude of the Ve-deflection; the stronger the point-source I, the larger the Ve-deflection; finally, the conductivity of the extracellular medium critically impact propagation of the signals from the point-source to the recording site. Notably, when the source is not limited to a point but instead possesses physical extent, the approximation needs to be re-formulated accordingly to account for such"
103,274,0.983,Solar Particle Radiation Storms Forecasting and Analysis : The Hesperia Horizon 2020 Project and Beyond (Volume 444.0),"7.5 Predicting >500 MeV SEP Events by Using the UMASEP Scheme Solar energetic particles (SEPs) are sometimes energetic enough and the flux is high enough to cause air showers in the stratosphere and in the troposphere, which are an important ionization source in the atmosphere. >500 MeV solar protons are so energetic that they usually have effects on the ground, producing what is called a Ground Level Enhancement (GLE). One of the goals of the HESPERIA project was the development of a predictor of >500 SEP events at the near-earth (e.g. at geostationary orbit). The implemented predictor, called HESPERIA UMASEP-500, makes a lag-correlation between the SXR flux and high-energy differential proton fluxes of the GOES satellites. When the correlation estimation surpasses a threshold, and the associated flare is greater than a specific SXR peak flux, a >500 MeV SEP forecast is issued. The lag-correlation is carried out using the High-Energy UMASEP approach explained in NÃºÃ±ez (2015). In this project, this approach uses 1-min SXR and proton data. Firstly, it generates a bit-based time series from the SXR time-derivatives and three bit-based time series from the time-derivatives of each of the P9âP11 channels of the GOES6-GOES15 satellites. The â1sâ in each bit-based time series are set when its positive time derivative surpasses a percentage p of the maximum value of the time derivative in the present sequence of size L (beyond which no influence is assumed in the SEP event to be predicted); otherwise, the flux level is transformed into a â0â. To avoid false alarms due to relatively strong fluctuations during periods of low solar activity, a threshold d is necessary, which is the minimum value to consider it as positive fluctuation (i.e., a â1â). This forecasting approach creates a list of cause-consequence pairs as follows: it takes the first â1â of the SXR-based time series, and the first â1â of the proton-based time series, to create a pair; it then takes the second pair of â1sâ in each time series, and thus successively, until all the â1sâ of the SXR-based time series are inspected. After following this procedure, if a â1â does belong to any pair, it is classified as an âoddâ. For each pair, the pair separation between the SXR-based â1â and the proton-based â1â is calculated. An ideal magnetic connection is detected when a sequence of SXR-based â1sâ in a row is followed by a sequence of proton-based â1sâ in a row. In an ideal magnetically connected event, all pairs have the same temporal separation, and no odd â1â has been found; in other words, an ideal magnetic connection is detected when all recently-measured strongest rises in the SXR flux are followed, some minutes later (i.e. the lag), by all recently-measured strongest rises in a proton channel. We say that this ideal magnetic connection would have a Fluctuation Correlation of 1. In general, we need a formula, described in NÃºÃ±ez (2015), that calculates the Fluctuation Correlation between the bit-valued SXR-based time series and a proton-based time series. A >500 MeV SEP event is triggered when the lagcorrelation is greater than a threshold r, and the SXR intensity of the associated flare is greater than a threshold f."
311,650,0.983,The Physics of the B Factories,"Similar to the study of track reconstruction systematics, the KS0 reconstruction is studied using the exclusive decays of Dâ â D0 Ïs , D0 â Ï + Ï â KS0 . One measures the eï¬ciency of the displaced vertex requirement for the KS0 reconstruction by obtaining the numbers of KS0 candidates with and without reconstructing a KS0 vertex. The Belle method is described as follows. Two oppositely charged tracks that are identiï¬ed as pions are selected and their invariant mass is computed without applying a vertex constraint. A pair with invariant mass close to the nominal KS0 mass is selected as a KS0 candidate. Every KS0 candidate is combined with another Ï + Ï â pair to form a D0 candidate, which is required to pair with a slow charged pion to form a Dâ . To reduce the combinatorial background, a suitable mass range, estimated using simulations, is selected in the D0 mass and Îmâ² = mDâ â mD â mÏs . The uncertainty in Îmâ² is signiï¬cantly reduced with respect to mDâ because the contribution from the KS0 candidate momentum largely cancels in the subtraction and hence a tighter signal window can be applied due to a better resolution. Finally, the numbers of all KS0 particles and of those passing a displaced vertex selection are estimated by ï¬tting the candidate KS0 mass with and without requiring a displaced vertex, respectively. The control sample has suï¬ciently high statistics so that the study is extended to measure the eï¬ciency in terms of KS0 momentum and polar angle similar to the charged track study described in Section 15.1.1. Likewise the eï¬ciency of requiring a displaced vertex for Monte Carlo events can be estimated. Hence, the data-MC eï¬ciency ratio can be obtained. The systematic uncertainty that arises from the reconstruction of the two KS0 daughter pion tracks has to be added to the eï¬ciency uncertainty of the displaced vertex for the total KS0 systematic uncertainty. Since Î and KS0 decays have a similar topology, the Î systematic uncertainty can be estimated using the KS0 results. For the Belle full data sample, the total systematic uncertainty of the KS0 reconstruction is on average around 1% including track reconstruction systematic uncertainties. 15.1.2.2 Ratio of two D decays The performance of the KS0 reconstruction in data can be checked using the double ratio Î·(KS0 ) ="
142,92,0.983,Subseafloor Biosphere Linked To Hydrothermal Systems : Taiga Concept,"activities, not on subaerial hydrothermal activities. Thus, only the intra-oceanic arcs, totaling approximately 17,000 km (nearly 40 % of the subduction margins of the Earth) (Leat and Larter 2003), are considered, although not all intra-oceanic arc volcanism is submarine. The magma (crust)-addition rate in the western Pacific island arcs (a typical intra-oceanic arc) was estimated to be 30â95 km3/ km/Myr (Dimalanta et al. 2002). If we assume that all intraoceanic arcs have the same magma-addition rate, the crustal production rate of these arcs is estimated to be 0.51â1.6 km3/year. For simplicity, we use an average value of 1.1 km3/year, which corresponds to 5.5 % of the oceanic crustal production rate at mid-ocean ridges. It is also known that most of the intra-oceanic arcs accompany back-arc spreading or rifts, except for the Solomon and Aleutian arcs (Gerya 2011). Among these arcs, the Okinawa Trough (~1,000 km) (Hirata et al. 1991), the Mariana Trough (~1,300 km) (Hawkins and Melchior 1985), the Manus Basin (~120 km) (Reeves et al. 2011), the North Fiji Basin (~800 km) (Tanahashi et al. 1994), the Lau Basin (~2,000 km) (Parson and Wright 1996), the East Scotia Basin (~500 km) (Rogers et al. 2012), the Bransfield Basin (~300 km) (Janik 1997), the Andaman Basin (~80 km) (Rao et al. 1996), and the Marsili Basin (~70 km) (Ventura et al. 2013) are known to be currently active (Fig. 2.1). Thus, we assume here that all these currently active back-arc spreading centers (total length of ~6,200 km) have an average crustal thickness of 6 km and an average spreading rate of 5 cm/year (both values are the same as the mid-ocean ridge average), although their spreading rates and crustal thicknesses are known to be variable (slow to fast spreading and crustal thicknesses of a few km to >10 km). Consequently, the crustal production rate of the back-arc spreading is estimated to be 2.1 km3/year, corresponding to 10 % of the oceanic crustal production rate. It is also important to note that no active H2-enriched hydrothermal system associated with ultramafic rocks have ever been discovered in arc-backarc settings. However, the SED hydrothermal system characterized by CH4-enriched fluids is known in the Okinawa Trough (Kawagucci et al. 2011), which has ~1,000 km each of arc and back-arc systems. Taking into account the total crustal production rate and the fraction of the sediment-associated region, the hydrothermal fluid fluxes from the sediment-associated SED-ABA system and the rest of the ABA system are estimated to be 1.1  1014 and 7.8  1014 g/year, respectively. A summary of the estimated hydrothermal fluid fluxes from the global MOR and ABA systems is shown in Fig. 2.7. It is evident that the largest hydrothermal fluid flux (72.4 %) is provided by the MOR-B system. In contrast, the mass contributions of the ABA (12.0 %), MOR-U (7.1 %), and SED (8.5 %) systems are quite limited."
30,280,0.983,Determinants of Financial Development,"SARAR(1,1) model shows that the CDM credit flows in a country increase by 0.34 units if those in its neighbouring countries increase by one unit. The explanatory variables described in Section 6.2, except for EXPMANU , have been found to be closely related to CDM credit flows with the expected signs. In particular, the GS2SLS estimates show that the the geographic variables LATITUDE and ELEV are positively associated with CDM development. For the resource and commodity exporter dummies, EXPSERV is positively related, while RESPOINT , RESDIFF and RESCOFF are negatively related, to CDM development. All of the control variables including GDP03, POP03, ETHNIC, RELIGION and legal origin dummies (CIVLEG, COMLEG) are in general found significantly associated with CDM development and should be included in the model.115 With a row-standardized binary weighting matrix, Table 6.3 in general confirms the findings of Table 6.2 in terms of positive impacts of LATITUDE, ELEV and EXPSERV , and negative impacts of RESPOINT , RESDIFF and RESCOFF on CDM credit flows. Table 6.3 seems to provide stronger evidence than Table 6.2, especially for the spatial autoregressive coefficients, âÎ»â and âÏâ. According to the SARAR(1,1) model, the degree of neighbourhood effects for the CDM credit flows increases to 0.48. The finding on the positive association between absolute latitude and CDM credit flows is consistent with the literature. On the one hand, research by Diamond (1997), Gallup et al. (1999) and Sachs (2003a) suggests that countries in a tropical location in terms of a smaller absolute latitude are often associated with poor crop yields and production due to adverse ecological conditions such as fragile tropical soils, unstable water supply and prevalence of crop pests. On the other hand, tropical location can be characterized as an inhospitable disease environment, believed to be a primary cause for âextractiveâ institutions, in conjunction with weaker institutions according to the settler mortality hypothesis of Acemoglu et al. (2001). Countries further from the Equator are more likely to have better climate conditions and stronger institutions, which are conducive to CDM project development. The finding on the positive association between elevation and CDM credit flows is in line with recent research. It is widely known that the Earthâs average surface temperature rose by approximately 0.6â¦ C in the twentieth century and will rise a few degrees C in this century. Global warming is likely to raise the sea level and change the land area and elevation above sea level for many countries. Countries with higher elevations are therefore supposed to have more potential to attract CDM projects."
71,218,0.983,Advancing Culture of Living with Landslides: Volume 1 ISDR-ICL Sendai Partnerships 2015-2025 (Volume 1.0),"The maximum volumes obtained by this analysis are 50,000â25,000 m3 for cubic and prismatic volumes respectively (Fig. 7). The largest basal area obtained is 1361 m2. The tails of the volume distributions obtained are ï¬tted to negative power laws whose b-values are â0.57 and â0.55, respectively. As the concurrence of all the above mentioned hypotheses (i) to (iii) is highly unlikely and the assumptions conservative, these volumes set an upper limit for rockfalls in the study area. The size distribution of scars observed (see Fig. 3) is an empirical evidence of rockfalls that occurred in the past. Instead, the kinematically movable rock masses indicate"
297,1889,0.983,The R Book,"The idea behind hierarchical cluster analysis is to show which of a (potentially large) set of samples are most similar to one another, and to group these similar samples in the same limb of a tree. Groups of samples that are distinctly different are placed in other limbs. The trick is in deï¬ning what we mean by âmost similarâ. Each of the samples can be thought of a sitting in an m-dimensional space, deï¬ned by the m variables (columns) in the dataframe. We deï¬ne similarity on the basis of the distance between two samples in this m-dimensional space. Several different distance measures could be used, but the default is Euclidean distance (for the other options, see ?dist), and this is used to work out the distance from every sample to every other sample. This quantitative dissimilarity structure of the data is stored in a matrix produced by the dist function. Initially, each sample is assigned to its own cluster, and then the hclust algorithm proceeds iteratively, at each stage joining the two most similar clusters, continuing until there is just a single cluster (see ?hclust for details)."
141,288,0.983,Cyber-Physical Systems of Systems: Foundations â A Conceptual Model and Some Derivations: The AMADEOS Legacy,"The precision Ï of an ensemble of synchronized clocks denotes the maximum offset of (distance) respective ticks of the global time of any two clocks of the considered clock ensemble. The drift Ïc(t) of a physical clock describes the frequency ratio between the physical clock and the reference clock i.e., the rate of deviation of a local clock c at time t from global time [10]. Synchronization uncertainty Uc(t) is deï¬ned as an adaptive and conservative evaluation of the offset Îc(t) at any time t; uncertainty is such that Ac â¥ Uc(t) â¥ | Îc(t)| â¥ 0 [8]. Hence, accuracy Ac is an upper bound of uncertainty Uc(t) and consequently of the absolute value of the offset Îc(t). When a CS asks the current time to R&SAClock, the latter provides an enriched time value useful for time synchronization. The enriched time value is composed of a set of values: likelyTime, minTime, maxTime and FLAG. LikelyTime is the time value computed reading the local clock. minTime and maxTime represent left and right synchronization uncertainty margins with respect to likelyTime. They are based on synchronization uncertainty provided by the internal mechanisms of R&SAClock. Finally, the FLAG takes the value 1 if requirements on uncertainty are satisï¬ed, 0 otherwise. Details on R&SAClock and its implementation can be found in [8, 11]. It is evident that the main core of R&SAClock is the uncertainty evaluation algorithm that equips R&SAClock with the ability to compute the uncertainty. Such an algorithm relies on the Statistical Predictor and Safety Margin (SPS) algorithm. Each CS that uses the R&SAClock getTime method for getting synchronization information and each CS has the two main expectations: (i) a request for the time value"
213,271,0.983,Collider Physics Within The Standard Model : a Primer,"In the SM, the non-vanishing of the N parameter [related to the phase ' in (3.66) and (3.67)] is the only source of CP violation in the quark sector (we shall see that new sources of CP violation very likely arise from sector). Unitarity of Pthe neutrino D Ä±bc . the CKM matrix V implies relations of the form a Vba Vca In most cases these relations do not imply particularly instructive constraints on the Wolfenstein parameters. But when the three terms in the sum are of comparable magnitude, we get interesting information. The three numbers which must add to zero form a closed triangle in the complex plane (unitarity triangle), with sides of comparable length. This is the case for the tâu triangle shown in Fig. 3.7 (or, what is equivalent to a first approximation, for the dâb triangle): Vtd Vud C Vts Vus C Vtb Vub"
297,1712,0.983,The R Book,"Number of Observations: 50 Number of Groups: 5 The ï¬xed effects in this model are the means of the parameter values. To see the separate parameter estimates for each strain, use coef: coef(model2) E 34.09244 0.4533741 10.81642 B 28.01211 0.3238606 11.54848 C 49.64062 0.5193966 10.67127 A 53.20342 0.4426117 11.23660 D 93.04507 0.6440231 10.65410 Note that the rows of this table are no longer in alphabetical order but sequenced in the way they appeared in the panel plot (i.e. ranked by their maximum values). The parameter estimates are close to, but not equal to, the values estimated by nlsList (above) as a result of âshrinkageâ in the restricted maximum likelihood estimates (see p. 685). The efï¬ciency of the random effects model in terms of degrees of freedom is illustrated by contrasting the numbers of parameters estimated by nlsList (15 = 5 values for each of 3 parameters) and by nlme (7 = 3 parameters plus 4 variances), giving residual degrees of freedom of 35 and 43, respectively. If you want to draw the ï¬tted lines yourself, then repeat the scatterplot we did earlier: plot(enzyme,rate,pch=20+as.numeric(strain),bg=1+as.numeric(strain)) and use a loop to extract the three parameters from the coefï¬cients table coef(model) and to copy the colours for the ï¬tted lines: for(i in 1:5){ yv <- coef(model)[i,3]+coef(model)[i,1]*xv/(1+coef(model)[i,2]*xv) lines(xv,yv,col=(i+1)) }"
58,307,0.983,Enabling Things to Talk,"The functional view is defined by applying the methodology defined in Chap. 5 to functional decomposition as can be seen in Fig. 8.1. In a first step, the Unified Requirements are mapped to the different Functionality Groups of the IoT Functional Model. Next, clusters of requirements of similar functionality are formed and a Functional Component for these requirements defined. Finally, the Functional Components are refined after discussion with the technical work packages. The viewpoints used for constructing the IoT Functional View are hence: 1. The Unified Requirements; 2. The IoT Functional Model. Once all Functional Components are defined, the default function set, system use cases, sequence charts and interface definitions are made, which all can be found back in Carrez et al. (2013). The Functional View diagram is depicted in Fig. 8.2 and shows the nine functionality groups of the Functional Model. Note that: â¢ The Application FG and Device FG are out-of-scope of the IoT-A Reference Architecture and are coloured in yellow; â¢ Management FG and Security FG are transversal FGs and are coloured dark blue. For each of the Functionality Groups, the Functional Components (FC) are depicted."
142,214,0.983,Subseafloor Biosphere Linked To Hydrothermal Systems : Taiga Concept,"implied at the Urashima/Pika site, but it is at the Archean site, as indicated by high magnetization (Fujii et al. 2013). Another complexity is apparent from the genetic diversity at the Snail site: it is as large as those at the other three sites (Fig. 5.7b, c). This point is partly supported by the rather old age, up to 7.2 ka, at the Snail site, although only one exceptional datum shows such an old age. In contrast, a phylogeographical approach suggests a 0.2â1.0 Ma history for populations in the area (Hidaka et al. Chap. 27). This discrepancy has yet to be investigated. Furthermore, a complicated situation was observed in the Okinawa Trough. Bathyacmaea secunda as a vent endemic species showed greater genetic diversity at the Hakurei site in the Izena Hole, which was indicated as the number of mismatches of the COI sequences (Fig. 5.9b). It is apparently consistent with the longer life of the vents at the Hakurei site estimated as exceeding 16 ka from geochemical data. However, genetic diversities of the Shinkaicaris at Daiyon-Yonaguni Knoll were as great as those at Yoron Hole, although their ranges of geochronological ages were different. They were also considerably younger than results show for the Hakurei-site (Figs. 5.5 and 5.9a). This point might be explained by the high-larval dispersal potentials of the species, resulting in a higher rate of gene flow among local populations. In terms"
167,154,0.983,The Interconnected Arctic â UArctic Congress 2016,"processes described integrally by the lumped model); (iii) the projected climatology is depicted by the model parameters, and it provides the way for developmet of the advanced regional-oriented parameterization schemes. In this study, the MARCS model with the regional parameterization scheme was used to calculate the parameters of the SFDR PDFs over the Russian Arctic. However, the additional algorithms should be implemented to the model before developing regional-oriented schemes for the catchments located in the mid-latitudes. As the role of evaporation is more important in the general water balance on the south regions, this variable has to be considered (Kovalenko and Gaidukova 2011). In this study, the MARCS model was forced by the outputs from the climate model with big spatial resolution, which is usually cause the challenges in the physically-based spatially distributed hydrological models. However, it seems to not be a big issue for the hydrological models such as the MARCS model. The physical core of the model is a lumped model in form of a linear filter with stochastic components (see Kovalenko 1993 for details). The role of spatial resolution in the datasets used to force the MARCS model and the options connected with the regional climate projections can be issues for future studies. The vision for the future is changing continuously, and the set of climate change scenarios is renewed almost every 5 years since the meteorological models are improving unceasingly. The feature of the MARCS model is its general simplicity (only the projected statistics are evaluated instead of a time series), and is easy to perform for a regional scale assessment of the future water availability not only for an mean runoff value, but also for outliers (i.e for extreme hydrological events) under any chosen climate projection. These outliers are important for economists since they are usually the ones dealing with risks associated with weather/runoff extremes. The methods to evaluate the economic values from the outputs of the MARCS hydrological models are the topic of the future studies. Acknowledgments The study was supported by the Academy of Finland (contract 283101, project TWASE) and by the Ministry of Science and Education of the Russian Federation (contract 01 2014 58678)."
365,148,0.983,Climate Smart Agriculture : Building Resilience To Climate Change,"models. August was chosen, as it provides an early warning to projected yield, as the crop has already entered seed-pod filling. Generally, the predictions in this report range from average to above average yields for the primary growing regions in the United States. The exceptions are in southeastern Minnesota, where predictions are generally below the expected value. Yields, which have the greatest deviation above the expected values, include much of Illinois and southern Iowa. These areas had near average wetness and slightly below average temperatures, thereby promoting healthy growing conditions during the cornâs development. The cooler than average temperatures allowed many areas with some moisture deficit to achieve near average yields, since the cool temperatures limited the moisture stress in the crop. Figure 3b displays the predicted yield as metric tons per hectare. The area with the highest yields occurs in locations where corn tends to produce some of the best yields in the world, and these areas also had better than aveage growing conditions. Note that the low yields in northern Indiana (where yields are near the expected value) indictate that growing conditions are generally inferior, compared to some the neighboring crop districts. Figure 4 shows the wetness and temperature anomalies, which are used to predict corn yields for the center of the USA growing area. Predictions include data from May, June, July, August, the plot in fig. 4 displays the anomalies for July, which is the most important period in the determination of the yield. August is the time when seed pod filling occurs, after reproduction, it is the most critical period in the development of corn yield. The above-average temperatures in July across areas of Iowa and most of Minnesota introduce heat stress, which reduces potential yield. Fortunately, there was ample moisture across most of the area, so the negative impact of excessive heat is nominal, in terms of yield reduction. More soil mositure is available in portions of Indiana and Illinois, and these areas are the regions with better than expected yields. The parameters of the predictive model along with its calculation of yield are presented in Table 1. These values are presented by crop district for the state of Iowa. The location was chosen since it is the most important agricultural state for the production of corn. The slope for the trend of corn yields over the period of record is 0.16 (shared across the state), which means that the average annual increase in yield, due to improved seed stock and agricultural practices is 0.16 metric tons/ha/yr. The intercept for each crop district is unique, since some crop districts produce higher yields than others. The predicted yield is the model derived yield, in metric tons per hectare, for each crop district, based upon its wetness and temperature anomalies throughout the growing season to August 2015. The trended (expected) yield value is based on the 2015 crop season. The last column on the right is the percent variation from the expected yield, the parentheses means the value is negative. Figure 5 illustrates that some crop districts are slightly below the expected value in terms of yield. However, the majority of the crop districts had higher than expected yield. Therefore, at the end of August the state of Iowa as a whole is predicted to have higher than expected yield. At this time of the growing season the seedpods are approaching maturity, and they provide a reliable measurement of the final yield. The regression equation and statistical significance of each predictor variable in the model are presented in Table 2. The adjusted R2 for the model is 0.60 with an F-statistic of 28.46. The model has 211 degrees of freedom. The predictive variables"
231,432,0.983,North Sea Region Climate Change Assessment,"The Wadden Sea is characterised by relatively high SPM concentrations compared to the North Sea. To maintain these gradients, some type of accumulation process must be active (Postma 1954). Earlier explanations focusing on Wadden Sea processes included the biases described at the end of Sect. 3.8 (see also Postma 1954; van Straaten and Kuenen"
151,118,0.983,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"Despite uncertainties in partitioning and reactivity of CPYO, it is possible to estimate CPYOâs rate of formation and concentrations in distant atmospheres relative to CPY. These estimates can also be compared with monitoring data. It is assumed for illustrative purposes here, that CPY reacts with â¢OH to form CPYO in air or on surfaces with a molar yield of 30%; CPYO also reacts by the same mechanism. Half-lives are assumed to be 3 and 12 h for the reactions of CPY and CPYO, respectively. In the later evaluation, we assume a more conservative yield of 70%. A parcel of air containing M0 mol of CPY will change in composition with time and distance, forming CPYO, which in turn is degraded. This decay series is analogous to a radioactive decay series. The quantity of CPY (M1) will follow first order kinetics, which can be described as (16): = - M1 Â´ k1"
311,622,0.983,The Physics of the B Factories,"forbidden decay KL0 â Î¼Â± eâ . The experiment deï¬ned a signal region in two kinematic variables, the Î¼Â± eâ invariant mass (MÎ¼e ) and the KL0 candidateâs transverse momentum squared (PT2 ). The signal region was subsequently âblinded,â i.e., events falling within this region were not selected for viewing, while all selection criteria were ï¬nalized. Only after these criteria were ï¬nalized was this region unblinded and signal events counted. A similar technique was used by BNL E787 (Adler et al., 1996), which searched for the rare decay K + â Ï + Î½Î½, and by BNL E888 (Belz et al., 1996a,b), which searched for a longlived H dibaryon. The method was subsequently adopted by the Fermilab KTeV experiment (Alavi-Harati et al., 1999), which measured Ç«â² /Ç« in the K 0 -K 0 system; Fermilab E791 (Aitala et al., 1999b, 2001a), which measured rare/forbidden D meson decays; and the CERN NOMAD experiment (Astier et al., 1999), which searched for neutrino oscillations. As mentioned, the principle of a blind analysis is to not look at potential signal events before ï¬nalizing analysis criteria in order to avoid biasing the result. There are three main types of measurements this applies to: setting an upper limit, in which one wants to avoid selection criteria that bias one against signal events; measuring a branching fraction, in which one wants to avoid selections that bias one against background events (this can âsculptâ a signal peak); and precision measurements such as that of measuring mixing or CP -violation parameters, in which one wants to avoid selections or ï¬tting procedures that bias the result in a preferred direction. Some general examples of these cases are discussed below, followed by speciï¬c examples from Belle and BABAR. Not every measurement requires a blind analysis: usually when one searches for new particles and does not know a priori where to look, one inspects relevant distributions in an unblind manner. However, one still must be careful not to adjust selection criteria to increase or decrease the signal yield while looking at the signal events for feedback. A blind analysis is typically more time-consuming than an unblind one and, in the case of setting an upper limit, can produce a poor result (see below)."
307,184,0.983,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),Fig. 4.4 Contours of the function o; . Note that the open probability density function of the o;1 .x/ mutant is much greater than the open probability density function of the wild type for large values of the concentration and for large values of the mutation severity index 
45,606,0.983,Measurement and Control of Charged Particle Beams,"With such noise sources present, the evolution of the distribution function f (I, t) is no longer described by (11.1), but by a FokkerâPlanck equation of the form  6 1 â2 âf (I, t) (ÎI)2 f (I, t) + f (I, t) , (11.12) 2 âI 2 where now the angular brackets denote an average over the entire beam distribution, including the action variables, and over the noise. For example, if the FokkerâPlanck terms ÎI and (ÎI)2  are linear in I and constant, respectively, the equation reduces to D âf Î»If + (11.13) 2 âI where Î» = ÎI/Ît/I, and D â¡ (ÎI)2 /Ît. The beam then asymptotically approaches the distribution, fâ â exp(âI/Iâ ), with the equilibrium emittance (for the equality of rms emittance and average action see (1.14) and Ex. 1.1) Ç« = It=â = Iâ = (11.14) Using (11.13), this distribution is easily shown to be stationary: âfâ /ât = 0. The cooling of various particles can be coupled, e.g., in stochastic cooling the time resolution may be limited by the amplifier bandwidth, and on each passage through the cooler only the average position of several particles is measured and damped. Under these circumstances, the beam is fully cooled only if the individual particles exchange their positions within the beam, so that on successive turns the measured average position, which is damped, refers to different combinations of particles. This process of particle exchange is called âmixingâ."
175,746,0.983,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","If Î± = 0.05, then tÎ± = 1.65 and Eq. 6.199 becomes 0.057 â¤ Î¼x â¤ 0.11. Hence, based on the simulation output, one can be about 90% sure that the true mean failure frequency lies between 5.7 and 11%. This corresponds to a reliability between 89 and 94%. By performing additional simulations to increase the size of n, the width of this conï¬dence interval can be decreased. However, this increase in accuracy may be an illusion, because the uncertainty in the parameters of the streamflow model has not been incorporated into the analysis."
151,87,0.983,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"A comprehensive ecotoxicological risk assessment of CYP was developed for birds and mammals (Solomon et al. 2001) and aquatic environments (Giesy et al. 1999) that were near areas of application. The analysis of LRT of CPY and CPYO presented here extends those assessments to regions downwind of points of application. The approach taken in this study was to compile and evaluate data on concentrations of CPY and CPYO at locations both near to applications and remote from sources. This assessment of LRT thus goes beyond determination of CTD to include estimates of concentrations of CPY and CPYO in other environmental media such as rain, snow, and terrestrial phases as well as in the atmosphere at more remote locations, including high altitudes. This was accomplished by developing a relatively simple mass-balance model, predictions from which could be compared to available measured concentrations of CYP in air and other media. This can provide an order-of-magnitude test of the accuracy of the predictions of the model, and, in this way, make an indirect assessment of the relative importance of the included processes and parameters. The model can then serve as a semi-quantitative predictive framework that is consistent with observations. The equations included in the model enable examination of the effect of changes in parameters such as application rate, temperature, meteorology, distance from source and precipitation. Estimated concentrations in terrestrial and aquatic environments remote from areas of application can be used, in combination with toxicological data, to assess risk to organisms in those media and locations. Monitoring data. Reports of concentrations of CPY in air at a variety of locations are presented in Table 1, with comments on other influencing factors such as altitude. Also included are reports of concentrations of toxicologically-relevant transformation products, such as CPYO, if and when such information was available. Reports of concentrations of CPY in precipitation (rain and snow) are given in Table 2, while Table 3 provides data for water bodies and other terrestrial media."
107,239,0.983,"Symbiotic interaction : 5Th international Workshop, Symbiotic 2016, Padua, Italy, September 29â30, 2016, Revised Selected Papers","As a first step, we aimed to determine the optimal theta and alpha frequency band. Since inter-individual differences in the frequency range of the alpha and theta band are expected, the bands rendering the best performance were determined for every participant individually. For that purpose a sweep over frequency bands ranging from around 3 to 14 Hz using only one SPoC component was performed. The width of the bands scaled logarithmically with the frequency to obtain a higher resolution for lower frequencies. Figure 2 shows the correlation of predicted with the true levels for every participant as a function of frequency band. While for participants 1 and 6 the band that yields the best prediction is in the alpha range, for participants 3 to 5 the best prediction is achieved in the theta range. Interestingly, a decline of correlation can be observed for the 6.7â9.5 Hz band, which comprises the transition between the alpha and theta range. For participant 2, a first analysis showed very poor correlations across all frequencies. A closer inspection of the bandpower of the data projected onto the first three SPoC components over the time revealed that three outliers that were not detected by the artifact rejection procedure during the preprocessing"
238,291,0.983,Nanoinformatics,Fig. 8.7 a Number of calculations in both methods. b Calculation trajectory in the Kriging method. Red numbers indicate the position of the initial random sampling and the pink triangle shows the position of the most stable point found by the Kriging method [15]
311,1500,0.983,The Physics of the B Factories,"The interference between the SM W and H contributions is destructive. Consequently, the charged Higgs contribution suppresses the branching fraction relative to the SM expectation, resulting in rH < 1, unless the H Â± is suï¬ciently large that it dominates the SM W Â± contribu2 tion. The case where MB2 tan2 Î²/MH = 2 is indistinguishable from the SM. It is notable that Eq. (17.10.7) applies equally to the other leptonic decay modes, B + â Î¼+ Î½ and B + â e+ Î½. As the H Â± is expected to decrease the observed branching fraction, much of the present constraint on charged Higgs bosons results from the lower bound on the experimental value of the B + â Ï + Î½ branching fractions rather than the upper bound on the branching fraction. A much weaker bound is currently obtained from B + â Î¼+ Î½ decays because only upper limits on its branching fraction have been reported. The radiative decays B + â â+ Î½â Î³ are also of interest since the presence of the radiated photon can remove the helicity suppression of the purely leptonic modes, possibly by coupling the spin-0 B meson to the spin-1 W Â± boson through an intermediate oï¬-shell state (Burdman, Goldman, and Wyler, 1995). Consequently, the predicted branching fractions of B + â e+ Î½â Î³ and B + â Î¼+ Î½â Î³ are considerably larger than the corresponding non-radiative modes, in spite of an additional suppression by the factor Î±EM . The branching fractions for B + â â+ Î½â Î³ (with â = e, Î¼, Ï ) are predicted to be of order 10â6 independent of the lepton type, making these modes potentially accessible at the B Factories. They potentially provide an additional method to access |Vub |, and they are also a potential background to the non-radiative mode searches. The decay rate for B + â â+ Î½â Î³ is given by Î±EM G2F |Vub |2 5  2 MB ÏB fA (EÎ³ ) + fV2 (EÎ³ ) (1 â y)y 3 (17.10.8) where y = 2EÎ³ /MB . The axial-vector and vector B â Î³X form factors, fA and fV , respectively, are assumed to be equal in most models. The branching fraction can be approximated as (Korchemsky, Pirjol, and Yan, 2000) Î±EM G2F |Vub |2 2 5 fB MB ÏB 288Ï 2"
283,691,0.983,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","Some good cyclic LDPC codes with cycles of length 4 found using Algorithm 12.2, which may also be found using Algorithm 12.1, are tabulated in Table 12.2. A check based on Lemma 12.1 may be easily incorporated in Step 12 of Algorithm 12.2 to filter out cyclic codes whose Tanner graph has girth of 4. Figure 12.3 demonstrates the FER performance of several cyclic LDPC codes found by Algorithm 12.2. It is assumed that binary antipodal signalling is employed and the iterative decoder uses the RVCM algorithm described by Papagiannis et al. [23]. The FER performance is compared against the sphere packing lower bound offset for binary transmission. We can see that the codes [127, 84, 10] and [127, 99, 7], despite having cycles of length 4, are around 0.3 dB from the offset sphere packing lower bound at 10â4 FER. Figure 12.3c compares two LDPC codes of block size 255 and dimension 175, an algebraic code obtained by Algorithm 12.2 and an irregular code constructed using the PEG algorithm [10]. It can be seen that, in addition to having improved minimum Hamming distance, the cyclic LDPC code is 0.4 dB superior to the irregular code, and compared to the offset sphere packing lower bound, it is within 0.25 dB away at 10â4 FER. The effect of the error floor is apparent in the FER performance of the [341, 205, 6] irregular LDPC code, as"
273,150,0.983,Report on Global Environmental Competitiveness (2013),"the GEC indicator system and mathematical model must be an organic entirety that can comprehensively and precisely describe and reflect the level and characteristic of the entire environmental system and should follow the principle of being systematic. From the perspective of system theory, environmental system as a macro system may be further divided into many subsystems in multiple layers, which together determines the level of environmental competitiveness and connects the evaluation target with the indicators as organic entirety. From the perspective of methodology, human observation and cognition of complex problem can hardly be thorough once and for all; very often, we need to systematically decompose the problem into multiple layers and subsystems, step by step from global to local, from abstract to concrete, and from appearance to essence; this is a process of using layered cascade method in analysis, following the principle of layering. It is the continuation of the principle of system, requiring the indicator system to divide the indicators into distinct layers according to the structure of the macro system; and, the indicators of the lower layer should represent the meaning of the upper layer as much as possible, in order to avoid overlapping among the various indicators. In the hierarchical structure, each evaluation indicator shows its affiliation to different layers of indicators and the interactions in between. The higher the layer, the more comprehensive the indicator will be; and the lower the layer, the more concrete the indicator will be. Upper-layer indicators are the summarization of the lower-layer indicators and guide the establishment of the lower indicators; lower-layer indicators are the breakdown of the upper-layer indicators; hence an orderly systematic hierarchical structure is formed for convenient operation and utilization. In summary, an indicator system reflecting the environmental competitiveness of the environmental system must be systematic and hierarchical. 2. Principle of combining completeness and independence The constructed GEC indicator system and mathematical model as an organic whole should reflect not only the entire characteristics and comprehensive status of the environmental system in all countries from different angles and in an allround way, but also the key information of the system; the indicators should be concise and relatively independent and indicators in the same layer should be able to represent one of the aspects of the layered system, trying to avoid overlapping or inclusive causal relations; the entirety should be expressed in as less indicators as possible. 3. Principle of combining universality and comparability The indicators of GEC evaluation system should be able to understood and accepted by most people and universally applicable; they should consider the differences of the countries or regions around the globe and straightforwardly manifest the environmental competitiveness status of the countries or regions of the world. While considering the universality of the indicators, comparability should not be neglected. Which is to say, the selected indicators must be comparable indicators showing universality and at the same time with definite meaning as well as scope of statistics and scope in each country, as a way to guarantee the comparability in time and space. They can be compared with respective past and"
48,164,0.983,Time Predictions : Understanding and Avoiding Unrealism in Project Planning and Everyday Life (Volume 5.0),"17 Studies of indigenous cultures and small children suggest that our intuitive number system (approximate number system) is nonlinear, with increasing intervals at increasing magnitudes. This number system has several advantages. It is able to compress high numbers on a short numerical scale and it reflects the fact that a difference in one unit is more important for small numbers (2 vs. 3) than for large numbers (1000 vs. 1001). See, for example, [51]."
307,270,0.983,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"where N is a random variable taking on two possible values, one (open) and zero (closed). The stochastic release models studied above are derived by gluing together pieces of models of exactly this type. In this chapter, one additional effect is added: We now allow calcium to flow into the dyad through the LCCs. This flow depends on both the gradient of the concentration and of the electrical potential across the membrane dividing the extracellular space and the dyad. The process illustrated in Fig. 8.2 can be modeled as follows xN 0 D Nr vr .Ny"
105,413,0.983,"Computer and information Sciences : 31St international Symposium, Iscis 2016, KrakÃ³w, Poland, October 27â28, 2016, Proceedings (Volume 659.0)","consists of the following mechanisms: Slow Start, Congestion Avoidance, Fast Retransmit and Fast Recovery. The first two, determining an exponential and linear grow respectively, are responsible for increasing CWND in absence of losses in order to make use of all the available bandwidth. Congestion is detected by packet losses, which can be identified through timeouts or duplicate acknowledgements (Fast Retransmit). Since the latter are associated to mild congestion, CWND is just halved (Fast Recovery) and not reduced to 1 packet as after a timeout. Hence, the core of classical TCP congestion control is the AIMD (Additive-Increase/Multiplicative-Decrease) paradigm. Note that this approach provides congestion control, but does not guarantee fairness [6]. The TCP Vegas was the first attempt of a completely different approach to bandwidth management and is based on congestion detection before packet losses [3]. In a nutshell (see Sect. 2 for more details), TCP Vegas compares the expected rate with the actual rate and uses the difference as an additional congestion indicator, updating CWND to keep the actual rate close to the expected rate and, at the same time, to be able of making use of newly available channel capacity. To this aim TCP Vegas introduces two thresholds (Î± and Î²), which trigger an Additive-Increase/Additive-Decrease paradigm in addition to standard AIMD TCP behavior. The article [12] shows TCP Vegas stability and congestion control ability, but, in competition with AIMD mechanism, it cannot fully use the available bandwidth. The goal of our paper is to compare the performance of these two variants of TCP through fluid flow models. In more detail we investigated the influence of these two TCP variants on CWND changes and queue length evolution, hence also one-way delay and its variability (jitter). Moreover, we also evaluated the friendliness and fairness of the different TCP variants as well as their ability in using the available bandwidth in presence of both standard FIFO queues with tail drop and Active Queue Management (AQM) mechanisms in the routers. Another important contribution of our work is that we considered also the presence of background traffic and asynchronous flows. In the literature, traffic composed of TCP and UDP streams has been already considered, but in most works (for instance, in [5,13]) all TCP sources had the same window dynamics and UDP streams were permanently associated with the TCP stream. Instead, in this paper, extending our previous work presented in [4], the TCP and UDP streams are treated as separate flows. Moreover, unlike [9] and [14], TCP connections start at different times with various values of initial CWND. The rest of the paper is organized as follows. The fluid flow approximation models are presented in Sect. 2, while Sect. 3 discusses the comparison results. Finally, Sect. 4 ends the paper with some final remarks."
372,1020,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"9.4.2 Relative Performance of Fringe Detection Methods In the regime in which the phase noise limits the sensitivity, careful investigation of detection techniques is warranted. The most important of these have been examined by Rogers et al. (1995) to determine their relative performance. We assume in all cases that the visibility data from the correlator outputs have been averaged for a time equal to the coherence time, %c , discussed earlier. We have seen in Eq. (9.92) that incoherent averaging of N time segments of data reduces the level at which a signal is detectable by an amount proportional to N ""1=4 . Rogers et al. show that for a detection threshold for which the probability of a false detection is < 0:01% in a search of 106 values, the threshold of detection is lower than that without incoherent averaging (in effect, N D 1) by a factor 0:53N ""1=4 . This result is accurate only for large N, and they find empirically that for smaller N, the detection threshold decreases in proportion to N ""0:36 ; that is, the improvement with increasing N is greater when N is small. Table 9.2 includes the improvement factor 0:53N ""1=4 , together with other results that are discussed below. The fourth column of Table 9.2"
8,786,0.983,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","27.2 Thermodynamics of the Gas Phase and the SBM Given the grand partition function Z.Ë; V; / of a many-body system, all thermodynamic quantities can be determined by differentiation of ln Z with respect to its arguments. Here,  is the fugacity introduced to conserve a discrete quantum number, here the baryon number. The conservation of strangeness can be carried through in a similar fashion leading then to a further argument s of Z. Whenever necessary, we will consider Z to be implicitly dependent on s . The grand partition function is a Laplace transform of the level density .p; V; b/, where p is the four-momentum and b the baryon number of the many-body system enclosed in the volume V : Z.Ë; V; / D"
169,215,0.983,Riverine Ecosystem Management : Science For Governing Towards a Sustainable Future (Volume 8.0),"shading, cover, ï¬ow protection, type of structure, substrate, and embeddedness, are recorded at spawning grounds. As an example, Melcher and Schmutz (2010) monitored spawning habitats of 1250 nase (Chondrostoma nasus) in the river Pielach, Lower Austria. Spawning took place in April and ï¬sh spawned in shoals on shallow gravel bars that are easy to identify from the river bank. A grid of equally spaced points was laid over the spawning area (grid size 1 m2; see Fig. 7.2). Additionally, representative sites were sampled with different morphological characteristics within the study area to describe the entire available habitat. Furthermore, point measurements were taken interspersed at 2 m intervals along transects, resulting in hundreds of microhabitat measurements as graphically explained in Fig. 7.2. Statistical techniques In general statistical techniques such as generalized linear or generalized additive models (e.g., logistic or Poisson regression), artiï¬cial neural networks, classiï¬cation and regression trees (CARTs), and genetic algorithms can all be used to create a map of a species probability of occurrence at any point of interest (e.g., standardized sampling grid) in a river. With these models, data observed at each site is typically extracted from a âhabitat databaseâ and assembled by occurrence hierarchy; analyzed with statistics"
29,221,0.983,"Micro-, Meso- and Macro-Connectomics of the Brain","[namely, the left dorsal inferior frontal gyrus (IFG) and the left posterior superior temporal gyrus (STG) and sulcus (STS)], the authors showed that, when seeded in the left posterior STS, strong correlations with the left IFG were found in adults. For children, in contrast to adults, the analysis revealed strong correlations with the contralateral temporal region. The same observation was present when the seed was in the IFG. Within the same period in which homotopic connections prevail, a 5-year longitudinal study showed a linear increase with age of the left hemisphere involvement in the IFG during verb generation (Szaflarski et al. 2006), an additional demonstration that language left-hemisphere specialization develops first through callosal interactions. As a whole, these recent functional imaging studies show that anatomical and functional asymmetries of auditory primary areas are in place at birth, whereas in high-order language areas (IFG, STS, STG) leftward asymmetries develop slowly along with verbal acquisition, before reaching the adult pattern of a dominant intrahemispheric processing of language. To our knowledge, the exact time course and the physiological underpinnings of this developmental switch from inter- to intrahemispheric functioning during language processing remain to be established. It is not known whether this type of developmental scenario is also at stake for other left-lateralized function, such as praxis (Vingerhoets et al. 2013), or for rightlateralized functions, such as spatial attention."
45,163,0.983,Measurement and Control of Charged Particle Beams,"2.8.7 Emittance near Sum Resonance The driving term of the sum resonance Îº+ may also be inferred by measuring the emittance in the vicinity of this resonance [78]. However, we caution the reader that experimental experience with this scheme is scarce, and that there appear to be some uncertainties in the predictions by different theories and computer simulations. Using the formulae in [74] and [79], the vertical emittance should depend on the measured distance to the sum resonance, ÎQI,II,+ â¡ |QI + QII â p| (where p is the integer which minimizes the expression) as [78] Ç«y â Ç«x0"
311,507,0.983,The Physics of the B Factories,"Likelihood ratio plots. In the search for rare decays many observables are typically used and the signal may not be conï¬ned to an easily deï¬nable signal region as was possible in the example of Fig. 11.2.2. In these cases, projections of the data and model on a single observable can be deï¬ned using a likelihood ratio, rather than a series of cuts on each of the projected observables. For such a plot, the signal and background models are ï¬rst integrated over the plotted observable x to obtain the signal and background probabilities according to these models using only the information contained in the projected observables y and then combined in a likelihood ratio as follows: S(x, y)dx LR(y) = ! . (11.2.11) (f Â· S(x, y) + (1 â f )B(x, y)) dx"
95,479,0.983,Elements of Robotics,"This is essentially the Hebbian rule for ANNs (Sect. 13.5.2). wi (t) and wi (t + 1) are the iâth weights before and after the correction, Î· defines the learning rate, xi is the normalized input, and y is the desired output. Since the sign function is applied to the sum of the weighted inputs, y is 1 or â1, except on the rare occasions where the sum is exactly zero. Equation 14.7 corrects the weights by adding or subtracting a value that is proportional to the input, where the coefficient of proportionality is the learning rate. A small value for the learning rate means that the corrections to the weights will be in small increments, while a high learning rate will cause the corrections to the weights to be in larger increments. Once learning is completed, the weights are used to classify subsequent samples."
233,306,0.983,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"Phylogenetic Split Networks Rooted phylogenetic trees as shown in Fig. 1 are well understood. Here, both trees show that the common ancestor of the taxa considered has the ancestors of the two genera as direct descendants. In general, interior nodes indicate ancestral taxa of the leaf nodes, and the edge lengths give an estimate of the amount of change observed between nodes. However, if one wishes to combine the information in both trees, it becomes difficult to identify clear ancestors. For example, TCYB and TDCoH3 disagree whether G. sonneratii or G. varius is the basal Gallus species. In order to visualize these conflicts phylogenetic split networks have been devised. We start by describing splits. A split, denoted by A|B, is defined as a bipartition of the taxon set X into two disjoint subsets A and B, indicating that there is an observable amount of divergence between the two subsets. Every edge in a tree generates a split. If one removes an edge, the tree decomposes into two subtrees, each of which connects a unique set of leaves. TCYB has 17 splits (edges), while TDCoH3 has 15 splits (2 splits in TDCoH3 have zero length and are collapsed as they do not influence subsequent computations). Figure 2a shows the union set Î£ of 20 distinct splits occurring in the pheasant trees (Fig. 1). TCYB and TDCoH3 share the ten trivial splits Ï1, Ï2, â¦, Ï10 corresponding to external edges of the trees. The trees also share two non-trivial splits Ï13 and Ï16, where Ï16 corresponds to the internal edges separating Gallus from Polyplectron species. The remaining splits are unique to each tree."
297,1760,0.983,The R Book,"MCMC modelling involves two important practical considerations: the burn-in period and thinning. By throwing away the early values from the simulation, the effects of the initial conditions will have died away, leaving a better estimate of the posterior distribution. The burn-in period (from which the results are discarded) is often set to half of the chain. Because the hill-climbing process is based on a Markov chain, successive values of the parameters show strong serial correlations, so successive values typically give little extra information about the shape of the posterior distribution. You might choose to take one point in every hundred or so, to reduce the serial correlation. This is the thinning rate."
311,3073,0.983,The Physics of the B Factories,"are consistent with the SM, though the experimental errors are still relatively large. The constraints on the LR and RL mass insertions are nontrivial and are comparable to the ones following from B â Xs Î³, see lower panels in Fig. 25.2.4. The regions excluded by combining the measurements of the B â Xs Î³ branching fraction and SÏKS0 are also shown in Fig. 25.2.4 as colored regions. Further constraints on the corresponding mass insertions can be obtained from the other loop-induced observables. For instance, D0 âD0 mixing constrains the up-type mass insertions (Î´AB )12 , while (gâ2)Î¼ constrains the slepl ton mass insertions (Î´AB )22 , see, e.g., (Chang, Chang, Keung, Sinha, and Sinha, 2002; Chankowski, Lebedev, and Pokorski, 2005; Gabbiani, Gabrielli, Masiero, and Silvestrini, 1996; Hisano and Tobe, 2001)."
307,280,0.983,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"8.1.7 Monte Carlo Simulations of CICR In Fig. 8.4, we show the results of stochastic simulations using the model (8.3) and (8.4). The computations are based on the numerical scheme (8.13) and (8.14) with the parameters given in Table 8.1 and Ât D 0:01 ms. As initial conditions we have used x.0/ D c0 and y.0/ D c1 with both gates closed. From top to bottom, the transmembrane potential is given by V D 20, 0, 20, and 40 mV. The associated calcium concentrations of the dyad given by x D x.t/ are graphed in the left panels and the calcium concentrations of the JSR given by y D y.t/ are graphed in the right panels. In all cases, we show the solution for a time interval ranging from 0 ms to 1000 ms. The calcium concentration clearly depends on the transmembrane potential and we observe in particular that there is no activity for V D 40 mV, since the LCC is inactivated at that voltage. In Fig. 8.5, we show a detailed view of the case of V D 0 mV. In the upper part of the graph we show the state of the RyR (upper) and the LCCs (lower). The CICR mechanism is illustrated in the first part of the graph: The LCC opens at t  5 ms, but the release is too short-lived to trigger an RyR opening and we therefore observe just a minor increase in the dyad calcium concentration given by x. Next time, at t  9 ms, there is a new opening and now the channel is open for a longer time; there is an increase in x leading to opening of the RyR channel and then the concentration increases dramatically."
285,550,0.983,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","not expected to predict the data presented here but other values for the ceiling and floor SNRs than those proposed by the standard are needed to describe speech intelligibility of sharply filtered signals. Changing the filter type (HP or LP) had a small influence on the slope of the linear increase (or decrease) of SRTs before reaching the asymptote, confirming that the chosen low and high frequency regions contributed equally to speech intelligibility. But, the results showed that filtering the masker seemed to have a larger impact on speech intelligibility than filtering the target. SRTs were not symmetrical as the SNR was varied up or down with the same amount. The benefit was greater when the filtered part of the noise decreased rather than when the filtered part of the target increased. This result questions the uniformly distributed importance over the [â 15; + 15] interval adopted by the SII and suggests that a greater importance should be attributed to positive SNRs compared to negative SNRs. Studebaker and Sherbecoe (2002) derived intensity importance functions from intelligibility scores in five frequency bands. The functions they obtained differed from the one used in the SII calculation. Their functions are frequency-dependent and nonlinear along SNRs, which is in agreement with the findings of the present study. However, the derivation of their results yielded importance functions defined between SNRs of â 15 dB and + 45 dB. The results observed here suggested a floor SNR of â 13 dB only when the target was HP-filtered. For the other conditions, lower SNRs need to be considered. The importance function of Studebaker and Sherbecoe (2002) also attributes negative contribution to very high SNRs (> 30 dB on average across frequency bands) regarding speech intelligibility. This negative contribution might be due to high sound levels used in their study which could have led to poorer word recognition scores (Dubno et al. 2005). Their derived importance functions take into account both the effect of SNR and absolute speech level. It would be preferable to separate the influence of these two parameters. Their approach could however be inspiring in order to propose simple frequency-dependent SNR-importance functions, allowing modelling the SRTs measured in the present study."
311,994,0.983,The Physics of the B Factories,"In the case of time-dependent Dalitz plot analyses, the resonance parameterizations above are combined with the equation describing the time-dependent decay properties of the B and B meson as given in Equation 13.2.17. In this case, a great deal of attention has to be given to the tagging and resolution functions. Charmless B decays, especially those without access to tree decay diagrams, may have a large non-resonant contribution. This can be as high as 90% for B â KKK. The contribution is not uniform across the Dalitz diagram and so a parameterization must be adopted that depends on position in the Dalitz Plot. In some analyses, BABAR and Belle have adopted the same non-resonant parameterization but in most cases they diï¬er, which can complicate comparisons. The statistical errors on the measured fit fractions and CP parameters are often derived from fits to a large number of MC experiments generated with the fitted parameters obtained from the data. These MC experiments are also vital for understanding the minimization process. With a large number of floating parameters, the fit can sometimes have more than one local minimum. There can be systematic shifts in the fit caused by the starting values of the floating parameters. A number of techniques for investigating this eï¬ect have been applied, including using diï¬erent minimizers, scanning through a set of starting values, randomly initializing the starting values, and the use of genetic algorithms. Each has its benefits and drawbacks but there is no one method that works better than the others in all circumstances. The systematic uncertainties that aï¬ect the final result are very similar to those seen in other charmless B decays. However their eï¬ects can be modified since there are more opportunities for correlations between parameters and the fitted results are often reported as ratios rather than absolute numbers. Although the magnitude and phase of the complex coeï¬cients of the amplitude are sometimes transformed to a more orthogonal set of parameters, this"
217,492,0.983,Finite Difference Computing With Pdes : a Modern Software Approach,"Here, u is the displacement field,  is the stress tensor, I is the identity tensor, % is the mediumâs density, f are body forces (such as gravity), K is the mediumâs bulk modulus and G is the shear modulus. All these quantities may vary in space, while u and  will also show significant variation in time during wave motion. The acoustic approximation to elastic waves arises from a basic assumption that the second term in Hookeâs law, representing the deformations that give rise to shear stresses, can be neglected. This assumption can be interpreted as approximating the geological medium by a fluid. Neglecting also the body forces f , (2.130) becomes %u t t D r.Kr  u/ :"
353,215,0.983,"Disability, Health and Human Development","â¢ Functional Trajectoryi,t+1 of individual i at time t + 1 refers to the four categories of functional trajectory above (1) through (4) with (4) No functional difficulty in any wave as the reference category. â¢ Î± is the intercept; â¢ Deprivationi,t is a dummy variable equal to 1 if the individual experiences a deprivation in the previous wave (in turn with respect to education, morbidity, work, material wellbeing, economic insecurity, and multidimensional poverty), 0 otherwise; â¢ Î² is the coefficient of the deprivation status to be estimated. Results are reported in Table 6.2. â¢ xi,k,t is a set of k control variables at the individual, household or community level in time t (age categories, male, married, household head, household size, rural residence, distance to healthcare services); â¢ Î³k is the set of estimated coefficients of the k control variables; â¢ Îµi,t is the error term for person i at time t."
372,1380,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"The visibility V.u; v; w/ is entirely determined by V.u; v; w D 0/ through convolution with G. Thus, V.u; v; w D 0/ contains all the data that are required to provide an accurate image, limited only by the synthesized (dirty) beam. Nothing essential to the image is lost in the transition from three dimensions to two. The same convolution function G applies to projection in both directions, i.e., from .u; v; w D 0/ to .u; v; w/ and vice versa. Note that the convolving function is different for each .u; v; w/ data point. Cornwell et al. (2008) point out that this convolutional relationship between a two-dimensional and a three-dimensional one is due to the fact that the original brightness is confined to the two-dimensional surface of the celestial sphere. They also discuss the result in terms of the diffraction of the electric field over the w-coordinate space. The w-projection imaging procedure is as follows. First, the visibility data are gridded in .u; v; w/ and then projected onto the .u; v; w D 0/ plane. In"
49,141,0.983,Artificial Intelligence and Cognitive Science IV,"the target localization time density function, corresponding to the time when the main swarm entered the proximity of the target, (ii) the success rate uâ, corresponding to the limit of the cumulative target localization distribution function, representing the total fraction of robots that reached the target during the simulation and (iii) the mean target residence time tR , defined as the mean value on the residence time density distribution function, corresponding to the average time the particles spent in the proximity of the target. For details see Figure 2."
297,749,0.983,The R Book,"The mean of the Weibull distribution is (1 + Î± â1 )Âµ and the variance is Âµ2 ((1 + 2/Î±) â ((1 + 1/Î±))2 ), and the parameter Î± describes the shape of the hazard function (the background to determining the likelihood equations is given by Aitkin et al., 2009). For Î± = 1 (the exponential distribution) the hazard is constant, while for Î± > 1 the hazard increases with age and for Î± < 1 the hazard decreases with age. Because the Weibull, lognormal and log-logistic all have positive skewness, it is difï¬cult to discriminate between them with small samples. This is an important problem, because each distribution has differently shaped hazard functions, and it will be hard, therefore, to discriminate between different assumptions about the age-speciï¬city of death rates. In survival studies, parsimony requires that we ï¬t the exponential rather than the Weibull unless the shape parameter Î± is signiï¬cantly different from 1. Here is a family of three Weibull distributions with Î± = 1, 2 and 3 (red, green and blue lines, respectively):"
253,574,0.983,"Autonomous Driving : Technical, Legal and Social Aspects","17.4.1 A-Posteriori-Analyzes of Accident Data for âDriver Onlyâ/ âNo Automationâ Past and present a-posteriori-analyzes of accident-data collections with conventionally (human-) driven vehicles form the basis for direct insights into accident black spots and changes in real-life trafï¬c accidents. In this âdriver-onlyâ/ âno-automationâ category, there are neither warnings nor interventions in longitudinal and lateral guidance on the basis of environmental sensors. To illustrate this, the change in the numbers of accident deaths serves as one example (see Sect. 17.4.1.1), the impact of Electric Stability Control, or ESC, is another (see Sect. 17.4.1.2)."
145,281,0.983,"Evolution, Monitoring and Predicting Models of Rockburst : Precursor Information for Rock Failure","The classiï¬cation categories of rockburst tendency were strong (G1), moderate (G2), weak (G3), and no rockburst (G4). In other words, all of basic parameters were included in the three-dimensional matrix G Â¼ Ã°X1 ; X2 ; X3 ÃT , which formed the dataset of Bayesian model. These results were calculated as follows: According to those selected training samples, the empirical probability is p1 Â¼"
280,13,0.983,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"Abstract The border ocelli and adjacent parafocal elements are among the most diverse and finely detailed features of butterfly wing patterns. The border ocelli can be circular, elliptical, and heart-shaped or can develop as dots, arcs, or short lines. Parafocal elements are typically shaped like smooth arcs but are also often âV,â âW,â and âMâ shaped. The fusion of a border ocellus with its adjacent parafocal element is a common response to temperature shock and treatment with chemicals such as heparin and tungstate ions. Here I develop a new mathematical model for the formation of border ocelli and parafocal elements. The models are a reactiondiffusion model based on the well-established gradient-threshold mechanisms in embryonic development. The model uses a simple biochemical reaction sequence that is initiated at the wing veins and from there spreads across the field in the manner of a grass-fire. Unlike Turing-style models, this model is insensitive to the size of the field. Like real developmental systems, the model does not have a steady state, but the pattern is âread outâ at a point in development, in response to an independent developmental signal such as a pulse of ecdysone secretion, which is known to regulate color pattern in butterflies. The grass-fire model reproduces the sequence of Distal-less expression that determines the position of eyespot foci and also shows how a border ocellus and its neighboring parafocal element can arise from such a single focus. The grass-fire model shows that the apparent fusion of ocellus and parafocal element is probably due to a premature termination of the normal process that separates the two and supports the hypothesis that the parafocal element is the distal band of the border symmetry system. Keywords Mathematical model â¢ Eyespot â¢ Parafocal element â¢ Grass-fire model â¢ Temperature shock"
97,257,0.983,"Nature-Based Solutions To Climate Change Adaptation in Urban Areas : Linkages Between Science, Policy and Practice","At the level of Barcelona municipality, results show that the contribution of urban GI to climate change mitigation is very low (5187 t carbon sequestered in 2008), accounting for 0.47% of the overall city-based GHG emissions in that year. Similarly, NO2 removal by urban GI in the municipality of Barcelona (55 t/year) only represented 0.53% of the total city-based emissions in 2008, indicating a marginal air quality improvement. At the regional level (BMR), the provision of both regulating ESS shows similar spatial patterns (see Figs. 9.2a and 9.3a). Regulating ESS fluxes are especially relevant in periurban forest areas such as the mountain range of Collserola and other tree-covered sites located in the hinterland. However, NO2 removal in some of these areas (e.g., Montseny massif) is relatively low because pressure (pollutant concentrations) is also moderate (see Fig. 9.2b). The lowest provision values for both regulating ESS are located in urban and agricultural land. As expected, the highest pressure values are mostly located in the municipality of Barcelona and adjacent middle-size cities (see Figs. 9.2b and 9.3b). As observed in the local scale assessment, the urban core is characterized by a compact urban form, very high population density and a relative small share of inner green areas. The other middle-size municipalities, located both along the coastline and hinterland, show mostly middle to low pressure values. The higher spatial resolution of NO2 concentration compared to carbon emissions also reveals that high capacity roads are major sources of NO2 pollution. The spatial indicator of pressure related to air purification (annual mean NO2 concentration) expresses the remaining air pollution after regulating ESS provision (Guerra et al. 2014 refer to it as âESS mitigated impactâ). Thus, the resulting map (Fig. 9.2b) indirectly shows where regulating ESS provision cannot sustain a good air quality level according to the NO2 limit value set by the EU Air Quality Directive (40 Î¼g/m3). The carbon offsetting impact of urban GI is small on average (less than 5%) across BMR municipalities (see Fig. 9.3c). Only in 5 out of 164 BMR municipalities, the estimated carbon emissions are completely offset by carbon sequestration by the local vegetation. These municipalities are characterized by very low population density (less than 500 inhabitants) and predominance of forest land cover."
202,183,0.983,"Security of Networks and Services in an All-Connected World: 11th IFIP WG 6.6 International Conference on Autonomous Infrastructure, Management, and Security, AIMS 2017, Zurich, Switzerland, July 10-13, 2017, Proceedings","Evaluation of Network Load, Timeslot Sizes and Execution Times: Figures 6 and 7 compare the influence of the network load and timeslot sizes on the quality and time complexity of our algorithms. Backup demand of 100% and network capacity of 200 Mbps are used. As can be seen in Fig. 6, by increasing the number of scenarios, the percentage of admitted requests decreases and the SARA+ approach performs better with fine-grained timeslot sizes. We notice that the advance bandwidth reservation system gains more by deploying the SARA+ approach and with the 5-min timeslot size, shows up to 7.3% higher request admittance ratio. The time complexity of the approaches are evaluated in Fig. 7 for an increasing range of scenarios. This figure reveals that the granularity of timeslot size impacts the execution times of both approaches differently. While with 30-min timeslot size, the execution time of SARA+ is up to 147 milliseconds higher compared to the SARA approach, with 5-min timeslots, this time is up to 4.5 second lower. These results indicate that the quality and complexity of the advance bandwidth reservation system can be improved by deploying the SARA+ approach with fine-grained timeslot sizes. For further investigation of the execution time, we have assessed the impact of network capacity on the execution time, when the timeslot granularity of 5 min is used. This has been shown in Fig. 8. The number of scenarios is 7 and 14"
175,941,0.983,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","a given performance indicator, one can use multivariate linear analyses to evaluate the approximate impact on the performance indices of uncertainty in various parameters. As shown below, the impact depends upon the square of the sensitivity coefï¬cients (partial derivatives) and the variances and covariances of the parameter sets. For a performance indicator I = F(Y), which is a function F(â¢) of model outputs Y, that are in turn a function g(P) of input parameters P, one can use a multivariate Taylor series approximation of F to obtain the expected value and variance of the indicator:"
241,633,0.983,Second Assessment of Climate Change for the Baltic Sea Basin,"10.3.2.2 Regression Methods Regression models include linear and nonlinear relationships between predictors and the predictand (Benestad et al. 2008). Among them are the multiple regression (Murphy 1999), the CCA method (Busuioc et al. 1999), and the SVD method (Bretherton et al. 1992). The difference between these approaches is that the multiple regression minimises the root-mean-square errors (distance between predictions and observations), the CCA maximises the correlation, and the SVD maximises the covariance between two ï¬elds. Artiï¬cial neural networks also represent nonlinear regression models (Crane and Hewitson 1998). 10.3.2.3 Weather Classification Methods The weather classiï¬cation methods involve various strategies, such as analogues (Zorita and von Storch 1999; Timbal et al. 2008), circulation classiï¬cation schemes (BÃ¡rdossy and Caspary 1990; Jones et al. 1993), cluster analysis (CorteReal et al. 1999; Huth 2000), and neural nets. The analogue model involves searching the record of past events and taking the day that most closely matches the situation wanted to predict. Cluster analysis bases the predictions on a number of closest states (Wilks 1995), either by taking the mean of the days with close matches or by using the observed values for all days that match the predicted state, and constructing a statistical distribution (histogram). From this sample, or a ï¬tted probability density distribution, a random value may be drawn. Neural nets involve various adaptive learning algorithms, such as âartiï¬cial intelligenceâ (Wilby et al. 1998; Hewitson and Crane 2002). The analogue model, circulation classiï¬cation schemes, and cluster analysis all involve a re-sampling of past measurements. These re-sampling techniques suffer from one caveat that the tails of the distributions will be distorted because the sampling cannot produce new record-breaking values (Benestad 2008). Even stationary series are expected to produce new record-breaking events, given sufï¬ciently long intervals for observations. Theory of independent and identically distributed (iid) series shows that the expected occurrence of new record-breaking events will converge towards zero, but never actually become zero. Nevertheless, this implies that the upper and lower tails of the distribution of the results from the re-sampling methods may be distorted and that the"
311,2016,0.983,The Physics of the B Factories,"Two diï¬erent isobar models describe the data well. Both yield almost identical behavior in invariant mass (Fig. 19.1.25bâ19.1.25d). The results of the best fits (Model I: Ï2 /Î½ = 702.08/714, probability 61.9%; Model II: Ï2 /Î½ = 718.89/717, probability 47.3%) are summarized in Table 19.1.10. They find that the KÏ S-wave is not in phase with the P -wave at threshold as it was in the LASS scattering data. Both fitting models include significant contributions from K â (892), and each indicates that D0 â K â+ K â dominates over D0 â K ââ K + . This suggests that, in treelevel diagrams, the form factor for D0 coupling to K ââ is suppressed compared to the corresponding K â coupling. While the measured fit fraction for D0 â K â+ K â agrees well with a phenomenological prediction (Buccella, Lusignoli, Miele, Pugliese, and Santorelli, 1995) based on a large SU (3) symmetry breaking, the corresponding results for D0 â K ââ K + and the color-suppressed D0 â ÏÏ 0 decays diï¬er significantly from the predicted values. In a limited mass range, from threshold up to 1.02 GeV/c2 , they also measure the scalar amplitude using a model-independent partial-wave analysis. Agreement with similar measurements from D0 â K â K + KÂ¯0 decay (Aubert, 2005f), and with the isobar models considered here, is excellent. 19.1.4.7 D0 â Ï + Ï â Ï 0 This analysis from BABAR makes use of NS = 44780 Â± 250 signal and NB = 830 Â± 70 background events (Aubert, 2007w). Table 19.1.11 summarizes the results of the Dalitz plot analysis. The Dalitz plot distribution of the data is shown in Fig. 19.1.26(a-c). The distribution is marked by three destructively interfering ÏÏ amplitudes, suggesting an I = 0-dominated final state (Zemach, 1964). 19.1.4.8 Ds+ â Ï + Ï â Ï + BABAR has performed a Dalitz plot analysis of Ds+ â Ï + Ï â Ï + (Aubert, 2009i) and Ds+ â K + K â Ï + (del Amo Sanchez, 2011b) using 380 fbâ1 . The selection of the two channels is similar and it will be described only once."
219,1023,0.983,Economics of Land Degradation and Improvement â A Global Assessment for Sustainable Development,Determinants of Number of SLM Technologies Adopted: Poisson Regression Model The number of SLM technologies and the corresponding proportion of plots in which these technologies were applied are as presented in Table 20.8. The number of SLM technologies is thus a count variable (ranging from 0 to 6 in our case). Thus the assessment of the determinants number of SLM technologies adopted requires models that accounts for count variables. Poisson regression model (PRM) is typically the initial step for most count data analyses (Areal et al. 2008). PRM is preferred because it takes considers the non-negative and binary nature of the data (Winkelmann and Zimmermann 1995). The assumption of equality of the variance and conditional mean in PRM also accounts for the inherent heteroscedasticity and skewed distribution of nonnegative data (ibid). PRM is further preferred because the log-linear model allows for treatment of zeros (ibid). The reduced form of the Poisson regression is presented as follows:
142,295,0.983,Subseafloor Biosphere Linked To Hydrothermal Systems : Taiga Concept,"Bach (2009) examined the effect of temperature on the mineral assemblage and fluid composition produced by serpentinization of harzburgite (a peridotite consisting mainly of olivine and orthopyroxene) (Fig. 8.1). They reported that at temperatures below 315  C, the serpentinized rock was composed of typical serpentinite minerals, such as serpentine, brucite, magnetite, and minor secondary clinopyroxene. With increasing temperature, the amount of magnetite increased, and consequently, the concentration of H2 generated by serpentinization also increased with temperature up to ~360 mmol/kg. Above 315  C, however, olivine became stable and coexisted in equilibrium with other secondary minerals and fluid, and above 390  C it remained almost completely unaltered. Therefore, the amount of Fe converted to magnetite decreased as the temperature increased above 315  C, with the result that the H2 concentration in the fluid was lower at these higher temperatures. Contrary to expectation, theoretical modeling of these waterârock interactions does not always produce results that are quantitatively consistent with the experimental results. Therefore, further investigation is needed to"
362,354,0.983,Cloud-Based Benchmarking of Medical Image Analysis,"lungs, kidneys, adrenal glands, psoas major and rectus abdominis muscle bodies. In addition, we create atlases for three additional image and body regions: background (BKG), thorax and abdomen (THAB) and a body envelope (ENV) from annotations generated automatically as follows. BKG is created by thresholding the image followed by morphological processing in order to isolate the background from the body region. THAB is created as the dilated union of the aforementioned 20 structures and their bounding 3D ellipse, from which the structures are subtracted after dilation. Finally, ENV is defined as the image minus BKG and THAB. Note that ENV is a crude body envelope that comprises skin, fat, muscle and bone structures. Figure 11.8c, f illustrate the additional annotations. To create probabilistic atlases, we choose a representative image per modality from the dataset and use it as a reference onto which we register all remaining images in the modality via the method described in Sect. 11.2.1. We register each structure separately in a bounding box of a given margin in the intensity image, defined according to the corresponding annotation image, and apply the obtained transform subsequently to the annotation image. We accumulate annotations thus registered in a 3D histogram of reference image dimensions which is normalized to produce the corresponding probability map. Refer to Fig. 11.6a for an illustration of probabilistic atlases."
365,869,0.983,Climate Smart Agriculture : Building Resilience To Climate Change,"Table 2 reports the estimates of the endogenous switching regression model estimated by full information maximum likelihood with clustered standard errors at the woreda level.16 The first column presents the estimation of downside risk exposure by ordinary least squares (OLS) with no switching and with a dummy variable equal to 1 if the farm household adapted to climate change, 0 otherwise. The second, third and fourth columns present, respectively, the estimated coefficients of selection Eq. (1) on climate change adaptation, and of downside risk exposure, which is represented by skewness functions (4a and 4b) (i.e., the third central moments of production function (2) in regimes (1) and (2)), for adapters and non-adapters.17 Table A3 of the appendix shows the estimation of production function (2) in regimes (1) and (2) from which we derived the third central moments.18 The estimation of Eq. (1) suggests that key drivers of farm householdsâ decision to adopt some strategies in response to long-term changes in mean temperature and rainfall are represented by the information sources farm households have access to and the environmental characteristics of the farm. More specifically access to government extension, media, and climate information increase the likelihood to adapt. These findings are very consistent with what has been found elsewhere (e.g., Maddison 2006; Deressa et al. 2009; Hassan and Nhemachena 2008; Gbetibouo et al. 2010; Deressa et al. 2011; Di Falco et al. 2011). Farm households with highly fertile soils are less likely to adapt. This highlights that most adaptation intervention is implemented in medium fertility soils. Rainfall in both rainy seasons displays U-shaped behaviour.19 In addition, we find that literacy has a positive significant effect on adaptation as well as having experienced a flood in the past. This is also consistent with what has been found by Deressa et al. (2009) and Deressa et al. (2011). It may be argued that pooling different crops can induce some bias. There"
213,218,0.983,Collider Physics Within The Standard Model : a Primer,"In the first line the question mark refers to the issue of the Ës âgluon correlation. In fact, Ës tends to slide towards low values (Ës  0:113â0.116) if the gluon input problem is not fixed. Indeed, in the second line, taken from [254], the large error also includes an estimate of the ambiguity from the gluon density parametrization. One way to restrict the gluon density is to use the Tevatron and LHC high pT jet data to fix the gluon parton density at large x. Via the momentum conservation sum rule, this also constrains the small x values of the same density. Of course, in this way one has to go outside the pure domain of DIS. Further, the jet rates have been computed at NLO only. In a simultaneous fit of Ës and the parton densities from a set of data which, although dominated by DIS data, also contains Tevatron jets and DrellâYan production, the result was [287] Ës .mZ / D 0:1171 Ë 0:0014Câ¹ :"
255,353,0.983,Railway Ecology,"Effects on Wintering Shorebirds Shorebird Abundances The total number of shorebirds counted per winter varied between 9331 (2008) and 2166 (2009). Dunlin was the most abundant species, followed by the Black-tailed Godwit Limosa limosa. The full GLMM was signiï¬cantly different from the null model without the interaction term, thus pointing out the presence of impacts from the railway (Table 12.1). The number of birds counted in the impacted saltpans during the operation phase (2011) was signiï¬cantly lower than expected given the temporal trends observed at the control saltpans (Table 12.1)."
32,697,0.983,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","30.4 Conclusion One of the important factors ignored in the past analyses in e-marketing is âcolorsâ of products. This is so because it is difficult to define a color of a product, which typically consists of many different colors. The purpose of this research is to fill this gap by developing an algorithmic procedure for identifying the dominating color of a product by analyzing a digital image of the product. Since humans tend to clearly distinguish RED from GREEN as well as YELLOW from BLUE, the Euclidean distance in CIE-L a b is more consistent with the sensuous feeling of human for colors than the Euclidean distance in RGB. Accordingly, for analyzing color preferences of consumers in e-marketing, CIE-L a b is more appropriate than RGB. Based on this idea, we proposed the CCPV (Color-Class Profile Vector) which represents the overall impression of a digital image containing a product. Since each product has its color in the data base, these vectors can be utilized to"
8,737,0.983,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","Here we claim that the asymptotic part em=T0 of the mass spectrum is the same for pions and baryons. Qualitatively, this can be understood by considering all hadrons with a given baryon number b and a very large mass such that m bmN . For such large masses, the presence of a few baryons is irrelevant as most of the mass is due to excitation of non-baryonic degrees of freedom. Hence, for any fixed baryon number b, the asymptotic part of the mass spectrum must be the same and equal to the pionic one. This conclusion can be proved rigorously [37]. Consider now the baryonic partition function ln ZN D V"
285,409,0.983,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","Delays from each side must be matched for coincidence detection to occur. How? Coincidence requires microsecond precision, which may be achieved by a two-step process, where latencies from each ear are matched during development, and then precisely matched modulo 2Ï. This was first proposed by Gerstner et al. (1996), who postulated that the necessary degree of coherence in the signal arrival times could be attained during development by an unsupervised Hebbian learning rule that selects connections with matching delays from a broad distribution of axons with random delays. Our data support this matching hypothesis, with coarse delays laid down through normal development, and precise, i.e. Â± 20 Âµs modulo 2Ï, delays regulated by some as yet unknown activity dependent processes (Seidl et al. 2010). Anatomical data are consistent with coarsely matched latencies. For each frequency, latencies in barn owl NM and in the midline cross fibre tract are similar, consistent with a common path from the ear into the brain (see Fig. 7 in KÃ¶ppl 1997). Our previous measures of latency did not have the precision of the current intracellular recordings, with ipsilateral and contralateral delays at the dorsal and ventral borders of NL of 2.82 Â± 0.24 and 2.87 Â± 0.21 ms respectively (Carr and Konishi 1990). Note the standard deviations of about 200 Âµs in these early recordings of latency. Our current recordings support the hypothesis that within NL, latencies vary by multiples of 2Ï, and may be precisely regulated at a fine time scale, in order to create a cycle by cycle representation of the stimulus at the point(s) of coincidence detection (Funabiki et al. 2011; Ashida et al. 2013). Modulation by multiples of 2Ï is also consistent with cross-correlation and spike timing dependent plasticity models (Gerstner et al. 1996; Kempter et al. 1998; Pena and Konishi 2000; Fischer et al. 2008). Variability in response latency also characterizes mammalian auditory nerve and cochlear nucleus recordings (Sanes and Constantine-Paton 1985; Carney and Yin 1988; Young et al. 1988). Acknowledgments This research was sponsored by NIH DC00436 to CEC, by NIH P30 DC04664 to the University of Maryland Center for the Comparative and Evolutionary Biology of Hearing, by the German Research Foundation (DFG, Wa-606/12, Ke-788/1-3, 4) and the Bundesministerium fÃ¼r Bildung und Forschung (BMBF: 01GQ0972 and 01GQ1001A, Bernstein Collaboration Temporal Precision, 01GQ07101 to HW and 01GQ07102 to RK,), and by the cluster of Excellence, âHearing4allâ at the University of Oldenburg (GA)."
311,2817,0.983,The Physics of the B Factories,"The normalized Î·c transition form factor is shown in Fig. 22.7.5. The errors shown are combined statistical and Q2 -dependent systematic uncertainty. There is also a Q2 independent error equal to 4.3%. The main source of the systematic error is an uncertainty on the detection eï¬ciency. For the Î·c meson, the leading order formula for the light-meson transition form factor is modified to take into account the large mass of the c-quark. The term 1/x in Eq. (22.7.2) should be replaced with Q2 /[xQ2 + m2c (1 + 4xx)], where x = 1 â x, and mc is the c-quark mass. As a consequence, the Î³Î³ â Î·c transition form factor can be predicted by pQCD starting from Q2 = 0. However, the Q2 dependence of the form factor becomes rather insensitive to the shape of the Î·c DA, and is described by a monopole function with a pole parameter Î â¼ 10 GeV2 (Feldmann"
233,442,0.983,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"Consideration of Individual Sites When the data set is evaluated using Ws sums, the site harbouring the greatest phylogenetic divergence is Grand Sud, and to a lesser degree La Foa Canala and Riviere Bleue. Grand Sud never drops in rank when individual phylogenies are dropped, and La Foa Canala and Riviere Bleue never below the 6th rank. The lower bound Ws (mean â SD) for Grand Sud still ranks 4th compared to the mean value of all others sites when all possible combinations of phylogenies are rariï¬ed to the smallest number of phylogenies present at any one site (n = 5) (the lower bound Ws for La Foa Canala and Riviere Bleue drop to ranks 9 and 10). The lowest scoring site is Col des Roussettes, and low phylogenetic diversity is also found in Ningua Foret Sailles, Mt Mou, Mt Kaala, and Mt Humboldt. These sites never move above the 12th rank when individual phylogenies are dropped, and their upper bound (mean + SD) ranks in the lowest two thirds compared to the mean value of all other sites when all possible combinations of phylogenies are rariï¬ed to the smallest number of phylogenies present at any one site. When the same data set is evaluated using Ws ranks (summing the scores for the ï¬rst and second most divergent species for each phylogeny), the sites harbouring the greatest phylogenetic divergence are also La Foa Canala and Grand Sud. These sites never drop below the 3rd rank when individual phylogenies are dropped, and their lower bound (mean â SD) still ranks in the upper half compared to the mean value of all others sites when all possible combinations of phylogenies are rariï¬ed to the smallest number of phylogenies present at any one site (n = 5). The lowest scoring sites are Col des Roussettes, Mt Kaala and Ningua Forest Sailles. These sites never move above the 13th rank when individual phylogenies are dropped, and their upper bound (mean + SD) ranks in the lowest quarter compared to the mean value of all other sites when all possible combinations of phylogenies are rariï¬ed to the smallest number of phylogenies present at any one site."
285,554,0.983,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","Abstract Dynamic aspects of cochlear mechanical compression were studied by recording basilar membrane (BM) vibrations evoked by tone pairs (âbeat stimuliâ) in the 11â19 kHz region of the gerbil cochlea. The frequencies of the stimulus components were varied to produce a range of âbeat ratesâ at or near the characteristic frequency (CF) of the BM site under study, and the amplitudes of the components were balanced to produce near perfect periodic cancellations, visible as sharp notches in the envelope of the BM response. We found a compressive relation between instantaneous stimulus intensity and BM response magnitude that was strongest at low beat rates (e.g., 10â100 Hz). At higher beat rates, the amount of compression reduced progressively (i.e. the responses became linearized), and the rising and falling flanks of the response envelope showed increasing amounts of hysteresis; the rising flank becoming steeper than the falling flank. This hysteresis indicates that cochlear mechanical compression is not instantaneous, and is suggestive of a gain control mechanism having finite attack and release times. In gain control terms, the linearization that occurs at higher beat rates occurs because the instantaneous gain becomes smoothened, or low-pass filtered, with respect to the magnitude fluctuations in the stimulus. In terms of peripheral processing, the linearization corresponds to an enhanced coding, or decompression, of rapid amplitude modulations. These findings are relevant both to those who wish to understand the underlying mechanisms and those who need a realistic model of nonlinear processing by the auditory periphery. Keywords Basilar membrane Â· Hair cell Â· Compression Â· Gain control"
311,2893,0.983,The Physics of the B Factories,"The most important disadvantages of Bs0 meson studies at the Î¥ (5S) are the smaller initial number of Bs0 mesons, the lack of ability to resolve Bs0 oscillations (hence it is not possible to perform time-dependent CP analyses), and the necessity to choose between data taking at the Î¥ (5S) and Î¥ (4S). In general Bs0 meson studies with e+ eâ colliders at the Î¥ (5S) and at hadron-hadron machines are complementary and together provide the best coverage of the whole spectrum of the most interesting decay channels."
311,1932,0.983,The Physics of the B Factories,"system, while the sqÌ pair hadronizes into a strange meson independently of the leptonic current. This hadronization is described by related form factors; single one for spin-0 hadron and three form factors for vector meson. The form factors increase with momentum transfer q 2 â¡ M 2 (l+ Î½l ), as for maximal q 2 value the final state hadron is at rest in the initial D rest frame and an overlap of wave functions of initial and final states is largest. Measurements of shape of the form factor and its value at maximal q 2 allow one to test theoretical calculations performed with either HQET based models or with LQCD. Since semileptonic decays often serve as a reference for hadronic decays, it is important to check whether the Ds+ semileptonic decays mirror"
16,707,0.983,"Autonomous Control For a Reliable internet of Services : Methods, Models, Approaches, Techniques, Algorithms, and Tools","Figure 10 shows that the previous findings in ns-3 were accurate enough to explain the possible effect of CCAs in other LTE deployments. In fact, some deficiencies such as the ones regarding Hybrid Slow-Start and Westwood+ are more harmful than in simulation environment, causing a greater gap between the available capacity and the achieved one. In general three are the most important features to be underlined. First, the drastic back-off application of Westwood+ leads the CCA to be incapable of achieving the maximum capacity even within 20 s of transmission. Looking at the growth tendency, the CCA may well take around 1 min to convergence which is an unacceptable value in order to provide a good service to the UEs. Second, the underperformance of Hybrid Slow-Start is more remarkable in this testbed and the results prompt a convergence time around 4.5 s. The performance difference with Standard Slow-Start (present in NewReno and Illinois) could be cushioned if the transmission is long enough (average value of CUBIC is close to NewReno or Illinois in 20 s transmission). However, the impact in short-lived flows would be more notable. Third, the performance of NewReno and Illinois are very similar and the only distinction appear due to the greater aggressiveness of Illinois for its delay-awareness. Nevertheless, the utilized femtocells give a very good channel quality regardless the mobility pattern, movement patterns or speed. Thus, the âsignal quality ringsâ that are present in real-world could not be represented. Therefore, in order to better understand the performance tradeoff of CUBIC, NewReno and Illinois in congestion avoidance phase during mobility circumstances, an additional analysis was demanded."
389,354,0.983,Impacts of the Fukushima Nuclear Accident on Fish and Fishing Grounds,"in both the internal organs and muscle of ayu (internal organs: t = â3.855, p < 0.001; muscle: t = â2.809, p = 0.006) (Fig. 17.6). The concentrations in the internal organs of ayu were positively correlated with those in the riverbed samples (i.e., fish prey) that were collected simultaneously with the ayu (t = 8.197, p < 0.001). In contrast, there was no correlation between the concentrations in the ayu muscle and the riverbed samples (t = â1.202, p = 0.261; Fig. 17.6). Thus, we conclude that herbivorous fish assimilate radiocesium from the microalgae and silt on the riverbed stones as they forage. Between 2011 and 2013, the activity concentration of radiocesium in the internal organs and the muscle of ayu declined, mainly because of the half-life of 134Cs (2.07 years), which is considerably shorter than the 30.1 years for 137Cs. However, the concentration of 137Cs in the whole ayu body tended to decrease during 2011 (Iguchi et al. 2013). Therefore, the decrease in the concentration of 137Cs in ayu cannot be explained only by the half-life of 134Cs. The activity concentration of 137Cs in the internal organs, which represented the majority of the 137Cs in ayu, was correlated with that in the riverbed samples. Therefore, the decrease of 137Cs in the riverbed, which may have been caused by flushing out of the contaminated soil from the mountains, would explain the decrease of 137Cs in ayu. In European lakes, Cs concentrations in fish muscle peaked a few years after the Chernobyl disaster (Jonsson et al. 1999; Smith et al. 2000). Then, the rate of decrease in muscle 137Cs concentrations was initially rapid, but later slowed. Conversely, in the rivers of Fukushima, the radiocesium contamination levels in ayu peaked immediately after the FNPP accident. The concentrations then decreased slowly, fluctuating with the transport of fresh polluted sediment from the mountains following snowmelt and typhoon events (Figs. 17.5 and 17.6). Ayu fork length was correlated not with concentrations of radiocesium in the internal organs but with that in the muscle (internal organs: t = â1.168, p = 0.246; muscle: t = 4.329, p < 0.001). The concentration of 137Cs in fish increases with fish size according to a power law relationship because of changes in prey items (Smith et al. 2002). For instance, the concentration in northern pike (Esox lucius) increased as the trophic level of prey increased from plankton to invertebrates and then to small fish. Indeed, the level of radiocesium contamination in fish at Fukushima increased according to the order herbivores (i.e., ayu) < omnivores < piscivores at Fukushima (Mizuno and Kubo 2013). A positive correlation was also observed for ayu body size relative to the muscle concentration of radiocesium. Thus, in this case, the positive correlation between ayu body size and radiocesium concentrations in their muscle could not be explained by a change in feeding patterns because the prey size and the prey items do not change as the ayu grows. Also, the time (season) of collection had no effect on the activity concentrations of radiocesium in ayu muscle. The activity concentration of radiocesium in fish is a function of uptake and elimination rates. In hatchery-reared ayu, assimilation efficiency decreases as the fish grow (Akutsu et al. 2001). Larger ayu therefore need much more food per unit weight gain than smaller individuals. Thus, at our study site, larger ayu have greater potential to accumulate radiocesium from microalgae on the riverbed stones than smaller ayu."
214,529,0.983,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"Resistances (7) or impedances; retarding forces on flows of water or nutrients. Resolution (4) the length of the ï¬nest-described scale in a model. Typically the horizontal scale (two dimensions in space) of a single grid box in a model. Respiration (2) gas exchange between solid and gas forms. In the case of plants, consuming carbon dioxide and releasing oxygen. In the case of animals (and bugs, microbes), consuming oxygen and releasing carbon dioxide. Rheology (6) the study of the flow of matter like liquids, or solids (like sea ice), under conditions in which they respond with plastic flow rather than deforming elastically in response to an applied force. Ridging (6) regions of thicker ice that stick up above the surface due to compression or convergence of sea ice. Salience (12) relevant, useful, having a prominent signal. Salinity (6) salt content of water. Sampling uncertainty (9) uncertainty or error introduced by having observations only at limited points and sparsely sampling an observed quantity (like temperature or precipitation). Scenario uncertainty (1) the uncertainty in a projection or prediction of the future due to uncertainties in inputs (boundary conditions) to a model over time. Scenarios (9) multiple, possible descriptions of what might happen in the future. Often used as inputs to models. Sensitivity (5) the degree to which a system will respond to an input of given strength. Systems with larger positive feedbacks have higher sensitivity. Shared Socioeconomic Pathways (SSPs) (10) a framework that combines climate forcing and socioeconomic conditions. These two dimensions describe situations in which mitigation, adaptation, and residual climate damage can be evaluated. The core is a limited set of ï¬ve SSP narratives that describe the main characteristics of future human development pathways including population, urbanization, and economic development. SSPs are the starting point for the identiï¬cation of internally consistent assumptions for the quantiï¬cation of emissions (similar to RCPs). Different modeling tools can be used to develop quantiï¬cations of these storylines, including factors like population, economic development, land use, and energy use. Shortwave radiation (5) energy in the ultraviolet or visible and near-visible portion of the electromagnetic spectrum (0.4â1.0 millionths of a meter, in wavelength). These are the wavelengths emitted by the sun, and shortwave is used to distinguish from radiation emitted by terrestrial (low-temperature sources). Signal (10) the real portion of some observed relationship, the opposite of noise."
359,206,0.983,"Micro-, Meso- and Macro-Dynamics of the Brain","Fig. 4 Main contributors of extracellular signals. (a, top) Postsynaptic currents influence extracellular voltage recordings. Overlay of events elicited by single action potentials of an interneuron and the resulting distribution of unitary postsynaptic amplitudes in rodent hippocampus. (a, bottom) The distributed nature of sinks and sources induced by postsynaptic currents. A single excitatory synapse (solid circle) is activated along an apical branch and its impact is propagated along the entirety of extracellular space due to the spatially distributed morphology of the excitatory neuron and the presence of passive return currents. (b, top left) Spike triggered average EAP waveform of a layer 5 pyramidal neuron with the intact EAP waveform (red) and when the EAP negativity is missing (black; window of 0.6 ms around spike initiation time is substituted by a spline). (Top right) Extracellular traces composed using the L5 pyramid EAP waveform (red: using the intact EAP waveform; black using the de-spiked EAP waveform). As observed, the typical EAP-related negativity is missing whereas the remainder of the waveform is attributed to slow afterpotential currents. (Bottom) Mean spectral density as a function of temporal frequency for the intact and de-spiked EAP waveform of the L5 pyramid (green: no spiking; red: 1 Hz; black: 8 Hz; blue: 30 Hz). As observed, the effect of spike afterpotentials (even in the absence of the salient EAP-negativity) can impact voltage recordings in frequencies as low as ~20 Hz. (c) Impact of Ca-dependent dendritic spikes on extracellular voltage recordings. A computational model of a layer 5 pyramidal neuron in the presence (left) and absence (right) of the Ca-hot-zone (location shown by the arrow) is used to emulate the electric field produced by a single neuron. The simulated depth LFP for the two cases is shown by the traces. The presence of the Ca-hot-zone and elicitation of a Ca-spike give rise to a strong, long-lasting event in the superficial regions of the cortex. Figure contributions are from (a, top) Bazelot et al. (2010), (a, bottom) LindeÌn et al. (2010), (b) Anastassiou et al. (2015), (c) simulations by A.S. Shai and C.A. Anastassiou"
103,409,0.983,Solar Particle Radiation Storms Forecasting and Analysis : The Hesperia Horizon 2020 Project and Beyond (Volume 444.0),"The goodness of fit describes how well the model predictions fit a set of observations. One way to evaluate the goodness of the fit, in case the measurement errors are known, is to construct a weighted sum of squared residuals (for details see Agueda 2008). The 2 estimator does not work very well for SEP events because during impulsive events the maximum intensities can be several orders of magnitude higher than the intensities observed during the decay phase, thus emphasizing the peak period. Therefore a better goodness-of-fit estimator is provided by the sum of the squared logarithmic differences between the observational and the modeled data. This estimator gives an equal weight of all relative residuals instead of just emphasizing the goodness of fit at the time of maximum. By evaluating the goodness of the fit under different interplanetary transport conditions (different values of ), one can objectively discern the âbest fitâ scenario (-value and associated injection profile) by minimizing the values of the goodness-of-fit estimator."
65,379,0.983,Handbook of Ocean Wave Energy,"In Table 7.7 there is a summary of results from the gradually more sophisticated calculations. First one can note thatâin this caseâthe simplest wave-drift estimate gives 40 times as large value as the one founded on diffraction theory. This is important in relation to the wind and current force. The Morison wave force for a regular sinusoidal wave is very dependent on the assumed wave period, while the Morison approach for irregular waves gives some better signiï¬cance, however some 20 % overestimation. Shaded values will be used in the design as they are considered as most realistic."
65,485,0.983,Handbook of Ocean Wave Energy,"Based on the performance of the device obtained in irregular waves (the influence of 3D waves will be assessed through a parametric study) and after optimization of the PTO load in the tested sea states, the mean annual energy production (MAEP) of the device can be estimated. Table 9.2 illustrates how the performance of a device, based on tank testing with sea states, is calculated and can be presented. The upper part summarizes the wave characteristics and corresponding performance results for the full-scale model based on CWR obtained through tank tests. The bottom part presents the resulting values that give an overall overview of the performance. Deï¬nitions of the different terms are given after the table. An estimation of the actual generated electrical power (Pel) can also be added if the efï¬ciency of the PTO system (Î·PTO) of the full-scale device is known. This value can (normally) not be deducted from the tank tests as the rest of the conversion"
165,313,0.983,New Methods for Measuring and Analyzing Segregation,"In the case of White-Black segregation as measured by the separation index (S), the regression would include a dummy variable for âWhiteâ coded 1 if White and 0 if Black to predict the residential outcome of pairwise contact with Whites ( y = p ). The value of the estimated regression intercept (b0) will indicate the average contact with Whites for Blacks (i.e., the baseline group coded 0 on race). The value of the unstandardized regression coefficient for White (b1) will indicate the extent to which the White mean for contact with Whites (i.e., the group coded 1 on race) deviates from the Black mean for contact with Whites. Accordingly, the value of the regression coefficient also will exactly equal the value of the segregation index score; that is, b1 = S . (And, for the sake of completeness, mean contact with Whites for Whites will be given by b0 + b1 .) At one level, this is not surprising as most readers will already be aware that bivariate dummy variable regression is mathematically equivalent to a difference of means comparison. But it is a new development in segregation measurement theory to interpret a segregation index score as the effect of race in a micro-level process of residential attainment. Thinking in this way opens up new avenues for exploring and interpreting segregation. Table 9.5 reports results for a series of bivariate regressions of the type just described estimated using the micro-level data set for Houston, Texas introduced earlier in this chapter. Recall that this data set reconstitutes the block group-level summary tabulations so the information in the tabulation is organized in a data set appropriate for performing individual-level attainment analysis. Cells in the tabulation are treated as cases and are coded on independent variables â race, poverty status, and family type â to suit the needs of the analysis. Dependent variables relat-"
389,180,0.983,Impacts of the Fukushima Nuclear Accident on Fish and Fishing Grounds,"There was no difference in the 134Cs/137Cs concentration ratio between sardine and anchovy and among the respective measurement specimens. Considering that the half-life for 134Cs is 2.1 years, the 134Cs/137Cs concentration ratio in these small"
149,94,0.983,Sago Palm : Multiple Contributions To Food Security and Sustainable Livelihoods,"Starch yield of sago palm varies greatly depending on both the habitat and the folk variety. Figure 4.2a shows the result of a random amplified polymorphic DNA (RAPD) analysis on a total of 38 population samples collected from 22 sites in the Malay Archipelago and 1 site in Papua New Guinea (PNG) (Ehara et al. 2003a). Among the 38 samples, 16 are spineless and 22 are spiny, including 14 gray or 2 with brown banding on the abaxial side of petiole/rachis, and 3 having reddish pith. The resultant UPGMA dendrogram divides the populations into two main groups. Group A includes subgroup A1 consisting of populations mainly distributed over the western part of the Malay Archipelago and subgroup A2 consisting of three populations from Southeast Sulawesi and two from Mindanao. Group B includes 12 populations collected in the eastern part of the Malay Archipelago: subgroup B1 consisting of 6 populations from Seram and subgroup B2 consisting of 2 from Seram and 4 from Ambon. From this analysis, a relationship between the genetic distance and geographical distribution of sago palms was found. A smaller genetic variation in the western part than in the eastern part of the Malay Archipelago was also found, which indicated that the more genetically varied populations are distributed in the eastern area. According to Vavilovâs theory about centers of origin of cultivated plants, the origin of a plant taxon is the place where the highest diversity of that taxon is found. The origin of a plant species can be considered to be the place where the greatest number of varieties and other variants are found within the taxon. Based on the RAPD analysis, the greater genetic variation found in the eastern Malay Archipelago, including the Maluku Islands, supports the traditional hypothesis that the area from the Maluku Islands to New Guinea Island is the center of origin of the sago palm. On the other hand, the genetic distance between the spineless and spiny populations is not necessarily farther than that between spineless populations or spiny populations in the dendrogram. This result indicated that the presence of spines on the petiole/rachis appears to be unrelated to genetic distance, which supports Rauwerdinkâs proposition (1986) to recognize the spineless sago palms and the spiny sago palms in the same taxon as M. sagu. Furthermore, no definite relationships were also found between the other morphological characteristics such as the banding pattern on petiole/rachis, the color of pith, and the genetic distances of populations. KjÃ¦r et al. (2004) have examined the relationships between various characteristics representing morphological features and genetic distances using amplified fragment length polymorphism (AFLP) analysis of sago palm populations growing in Papua New Guinea and reported that no correlations were found between different morphological characteristics and genetic distances. In communities that are highly dependent on sago palm, many folk varieties are recognized based on morphological traits. However, according to the past genetic diversity analyses conducted by Ehara et al. (2003a) and KjÃ¦r et al. (2004), the genetic diversity of the sago palm is not very high. Abbas et al. (2010) reported that a simple sequence repeat analysis of chloroplast DNA of plants collected from various locations in Papua Province of Indonesia has found that they are divided into three groups with about 77% belonging to one group. In any case, it is considered that there is no"
80,344,0.983,Innovations in Quantitative Risk Management (Volume 99.0),"3 Generalised Archimedean Copula Models for Currency Exchange Rate Baskets In order to study the joint tail dependence in the investment or funding basket, we consider an overall tail dependence analysis which is parametric model based, obtained by using flexible mixtures of Archimedean copula components. Such a model approach is reasonable since typically the number of currencies in each of the long basket (investment currencies) and the short basket (funding currencies) is 4 or 5. In addition, these models have the advantage that they produce asymmetric dependence relationships in the upper tails and the lower tails in the multivariate model. We consider three models; two Archimedean mixture models and one outer power transformed Clayton copula. The mixture models considered are the Clayton-Gumbel mixture and the Clayton-Frank-Gumbel mixture, where the Frank component allows for periods of no tail dependence within the basket as well as negative dependence. We fit these copula models to each of the long and short baskets separately. Definition 2 (Mixture Copula) A mixture copula is a linear weighted combination of copulae of the form: Î»i Ci (u; Î¸ i ), C M (u; Î¸ ) ="
255,358,0.983,Railway Ecology,"and whether it could be compensated by the use of the other saltpans or other places outside the study area. We applied an experimental design that provides a spatial and temporal context to integrate the evaluation of potential impacts of railways and other line infrastructures on wetlands. Our approach surpasses the usual estimators of mortality due to the effect of barrier and evaluates the effects on the spatial and temporal distribution, survival and behaviour. This decision was made due to the nature of this railway and the mitigation measures that were decided a priori to decrease the probability of negative impacts over the aquatic birds that uses the area crossed by this infrastructure. Indeed, the construction of a viaduct over the most critical areas was a decision that decreased land reclamation on the saltpans. On the other hand, this railway was meant from the start to be used for commercial purposes, transporting containers from the harbor to the main national line. The expected trafï¬c and speed of the trains was thus limited and conditioned to speciï¬c daily periods, thus decreasing the barrier effect of this infrastructure and, hopefully, lowered the expected mortality due to collisions with the trains. In fact, during the study period, only two casualties were reported: a common Sandpiper (Actitis hypoleucos) and an unidentiï¬ed gull. The main guidelines described in this book pertaining to future studies in railway ecology seem mostly designed to terrestrial environments (and species) and long stretches of continuous railway beds. This study dealt (albeit in a preliminary way) with some of them, namely by (1) studying species with different ecological traits (even if belonging to the same taxonomic group), (2) using an experimental analysis, which includes comparison between a control phase with the period of construction and after, (3) studying the effect of habitat fragmentation (in this case, how the birds responded to the loss/degradation of the impacted saltpans both in the breeding season and in winter, and (4) a preliminary study of the population persistence in the area after the railway construction. In our view, the most interesting feature of the present study is the integration of several spatial scales, from the macroscale of other case studies (see Chaps. 7 and 11) to the microscale of the habitat. This option was imposed by the nature of the problem itself and allowed us to go beyond the usual evaluation of mortality of adult birds due to collisions (Chap. 7) or the general patterns of disturbance and habitat loss (Chap. 11), and changed the focus of analysis to the impact of disturbance (both on the breeding and wintering birds) and chick mortality at smaller scales (habitat), a feature that is seldom attempted. We have developed a conceptual methodology where the rationale is hypothesis-based and uses a Before-After-Control-Impact comparative approach (as advised in the guidelines of Chap. 19). We think that it can be easily extended to the same or other aquatic bird species and habitats affected by railways in the wetland areas, enabling a multi-scale habitat analysis that can be combined with the usual broad scope procedures. Although the method has the potential to be improved and reï¬ned, we feel that it represents an add-on to the arsenal of methods of analysis in future studies of railway ecology, as those proposed in Chap. 19."
65,490,0.983,Handbook of Ocean Wave Energy,"The representation of the performance enables to visualise the various parameters over the different sea states. In this case, the non-dimensional performance peaks at sea state two while the maximum wave energy contribution peaks between sea state three and four. The mean annual energy production (MAEP) could be increased by trying to match these peaks better. This could be done by increasing the size (scaling ratio) of the full-scale device (which has been discussed in Sect. 9.2.5.3). However, Pabs is in the same range in sea state three, four and ï¬ve. This is an advantage as the device will require roughly the same capacity of PTO system for these wave states (leading to a high capacity factor), which is most-likely not the case if the peak of the non-dimensional performance is close or beyond the peak of the wave energy contribution curve. The Prob * Pabs curve shows that most of the energy will in average be absorbed in sea state 2, which correspond to 1 m (Hs) waves, and the least in sea state ï¬ve."
372,1828,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"15.1.3 Assumptions in the Derivation and Application of the van CittertâZernike Theorem At this point, it is convenient to collect and review the assumptions and limitations that are involved in the theory of the interferometer response. 1. Polarization of the electric field. Although the electric fields are vector quantities with directions that depend on the polarization of the radiation, the components received by antennas from different elements of the source can be combined in the manner of scalar quantities. The fields are measured by antennas at P1 and P2 , and each antenna responds to the component of the radiation for which the polarization matches that of the antenna. If the fields are randomly polarized and the antennas are identically polarized, then the signal product in Eq. (15.4) represents half the total power at each antenna. However, the antenna polarizations do not have to be identical since, in general, the interferometer system will respond to some combination of components of the source intensity determined by the antenna polarizations. The ways in which the antenna polarizations can be chosen to examine all polarizations of the incident radiation are described in Sect. 4.7.2. Thus, the scalar treatment of the field involves no loss of generality. 2. Spatial incoherence of the source. The radiation from any point on the source is statistically independent from that from any other point. This applies almost universally to astronomical sources and permits the integration in Eq. (15.6) by allowing cross products representing different elements of the source to be omitted. The Fourier transform relationship provided by the van CittertâZernike theorem requires the source to be spatially incoherent. Spatial coherence and incoherence are discussed in Sect. 15.2. Note that an incoherent source gives rise to a coherent or partially coherent wavefront as its radiation propagates through space. If this were not the case, the mutual coherence (or visibility) of an incoherent source, measured by spaced antennas, would always be zero. 3. Bandwidth pattern. The assumption required in going from Eqs. (15.4) to (15.5), that .R2 ! R1 /=c is less than the reciprocal bandwidth .'$/!1 , can be written ld u"
297,711,0.983,The R Book,"Various tests for normality are described on p. 346. Here we are concerned with the task of comparing a histogram of real data with a smooth normal distribution with the same mean and standard deviation, in order to look for evidence of non-normality (e.g. skew or kurtosis). par(mfrow=c(1,1)) fishes <- read.table(""c:\\temp\\fishes.txt"",header=T) attach(fishes) names(fishes) [1] ""mass"" mean(mass) [1] 4.194275 max(mass) [1] 15.53216 Now the histogram of the mass of the ï¬sh is produced, specifying integer bins that are 1 gram in width, up to a maximum of 16.5 g: hist(mass,breaks=-0.5:16.5,col=""green"",main="""")"
30,34,0.983,Determinants of Financial Development,"This study mainly investigates key determinants of five specific indices of financial development discussed in more depth below. For each financial index, there are three samples on which the investigation is based: the whole sample, a developing country sample and a smaller sample for which the La Porta et al. (1998) data are available. The whole sample is the main focus of the analysis. The developing countries in the settler mortality dataset of Acemoglu et al. (2001) form the main part of the developing country sample here. Looking at the smaller La Porta et al. (1998) sample makes it possible to examine whether differences in legal tradition, reflected in the protection of shareholdersâ and creditorsâ rights, determine cross-country differences in financial development. The countries included are listed in Appendix Table A2.3. Note that the transition economies and small economies with a population of less than 500,000 in 1990 are excluded from the sample. The information on the transition economies and population size is from the World Bank Global Development Network Database (GDN) and the Penn World Table 6.2 from Heston et al. (2006), respectively. 2.2.2"
145,224,0.983,"Evolution, Monitoring and Predicting Models of Rockburst : Precursor Information for Rock Failure","The detailed process to set up predicting methods for rockmass instability is as follows: (1) Determine the monitoring and predicting area of rockmass, partitioning and numbering them. To get a distinct monitoring network, numbers correspond to the change of incremental stress and incremental displacement. (2) Drill hole for stress and displacement monitoring. The bottom of drilling hole for stress monitoring should be located in the rockmass that was easy to produce instability and it cannot be located in the virgin of rockmass. The deepest anchor point of borehole for vibration twist-action multipoint displacement instrument should be located in the virgin of rockmass. (3) Construct the borehole in the direction vertical to the excavation face according to the radial size of vibration twist-action multipoint displacement instrument. The drilling length should be more than 3 times the radius of excavation space. In general, the diameter of borehole is 45â76 mm. Record the drilling length as L and the displacement value as Dl (with units in mm). The deepest displacement monitoring points should be located in the region of initial stress. The measurement range of multipoint displacement instrument is 0â20 mm with an accuracy of 0.2 mm and sensitivity of less than 0.01 mm/F. To master the strain change status in rockmass failure, the temperature measurement range is â40 to 140 Â°C with an accuracy of Â±0.50 Â°C, the water pressure resistance is greater than"
214,168,0.983,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"Climate models of the atmosphere come in many shapes and sizes, ranging from simple idealized models with no dimensions, to complex models with high spatial resolution over particular regions or the whole planet. Both regional and global climate models are valuable. Regional models are often coupled to other scales to perform detailed experiments. The processes represented in an atmosphere model include the equations of motion and the various forcing terms that come from energy flows, motion, and transformations. Moisture is critical in the transformation and storage of energy. Models have to approximate processes occurring on many scales with parameterizations to derive the source and sink terms for the equations of motion and heat that are iterated forward in time. Key processes include clouds, radiative flows of heat, and chemistry, as well as small-scale forcing of motions. Representing the variations in processes on small scales correctly can be difï¬cult. Most of the uncertainty in the models results from missing or incorrect processes and particularly from the interactions across scales that cannot be represented. Feedbacks between processes can be complex and are not easy to observe. Cloud feedbacks are highly uncertain in atmosphere modeling. We return to many of these ideas in Chaps. 6 and 7, on the other model components of the climate system. Key Points â¢ A hierarchy of atmosphere models exist. â¢ Global models reproduce the general circulation patterns of the climate. â¢ Physical processes in atmosphere models are complex. Clouds and radiation are key processes. Water is critical for moving heat in the atmosphere. â¢ Small-scale (subgrid) variations make parameterization difï¬cult in atmosphere models. â¢ Feedbacks between processes are important, and clouds are the most uncertain. Open Access This chapter is distributed under the terms of the Creative Commons AttributionNoncommercial 2.5 License (http://creativecommons.org/licenses/by-nc/2.5/) which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited. The images or other third party material in this chapter are included in the workâs Creative Commons license, unless indicated otherwise in the credit line; if such material is not included in the workâs Creative Commons license and the respective action is not permitted by statutory regulation, users will need to obtain permission from the license holder to duplicate, adapt or reproduce the material."
213,274,0.983,Collider Physics Within The Standard Model : a Primer,"which, in the limit of equal mui , is clearly vanishing due to the unitarity of the CKM matrix V. Thus the result is proportional to mass differences. For K 0 âKN 0 mixing, the contribution of virtual u quarks is negligible due to the small value of mu and the contribution of the t quark is also small due to the mixing factors Vts Vtd  O.A2 5 /. The dominant c quark contribution to the real part of the box diagram quark-level amplitude is approximately of the form (see, for example, [176]): Re Hbox D"
283,339,0.983,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","different field sizes q. To demonstrate a comparison the parameters of AG codes with shortened BCH codes in fields with small sizes and characteristic 2 is given. AG codes of length n, dimension k have minimum distance d = n â k â g + 1 where g is called the genus . Notice that n â k + 1 is the distance of a maximum distance (MDS) separable code. The genus g is then the Singleton defect s of an AG code. The Singleton defect is simply the difference between the distance of a code and the distance some hypothetical MDS code of the same length and dimension. Similarly a BCH code is a code with length n, dimension k, and distance d = n â k â s + 1 where s is the Singleton defect and number of non-consecutive roots of the BCH code. Consider Table 8.1, which compares the parameters of AG codes from three curves with genera 3, 7, and 14 with shortened BCH codes with similar code rates. At high rates, BCH codes tend to have better minimum distances or smaller Singleton defects. This is because the roots of the BCH code with high rates are usually cyclically consecutive and thus contribute to the minimum distance. For rates close to half, AG codes are better than BCH codes since the number of non-consecutive roots of the BCH code is increased as a result of conjugacy classes. The AG codes benefit from the fact that their Singleton defect or genus remains fixed for all rates. As a consequence AG codes significantly outperform BCH codes at lower rates. However, the genera of curves with many points in small finite fields are usually large and as the length of the AG codes increases in F8 , the BCH codes beat AG codes in performance. Tables 8.2 and 8.3 show the comparison between AG and BCH codes in fields F16 and F32 , respectively. With larger field sizes, curves with many points and small genera can be used, and AG codes do much better than BCH codes. It is worth noting that Tables 8.1, 8.2 and 8.3 show codes in fields with size less than 49."
283,330,0.983,"Error-Correction Coding and Decoding : Bounds, Codes, Decoders, Analysis and Applications","These three binary codes, the RS-based (150, 75, 19) and (150, 75, 22) codes together with the (150, 75, 23) shortened cyclic code have been simulated using binary transmission for the AWGN channel. The decoder used is a modified Dorsch decoder set to evaluate 2 Ã 107 codewords per received vector. This is a large number of codewords and is sufficient to ensure that quasi-maximum likelihood performance is obtained. In this way, the true performance of each code is revealed rather than any shortcomings of the decoder. The results are shown in Fig. 7.3. Also shown in Fig. 7.3, for comparison purposes, is the sphere packing bound and the erasure-based binomial bound discussed in Chap. 1. Interestingly, all three codes have very good performance and are very close to the erasure-based binomial bound. Although not close to the sphere packing bound, this bound is for non-binary codes and there is an asymptotic loss of 0.187 dB for rate 21 binary codes in comparison to the sphere packing bound as the code length extends towards â. Comparing the three codes, no code has the best overall performance over the entire range of NEb0 , and, surprisingly the dmin of the code is no guide. The reason for this can be seen from the Hamming distances of the codewords decoded in error for"
30,196,0.983,Determinants of Financial Development,"time dummies. The CCEP estimator has been shown to be asymptotically unbiased and consistent as Nâ > â for both T fixed or T â > â, and to have generally satisfactory finite sample properties. Appendix Table A5.3 presents the time series properties for three continuous variables, the financial liberalization index (FLi,t ), GDP per capita in PPP terms (GDPi,t ) and trade openness (OPENi,t ). It contrasts a panel unit root test proposed by Pesaran (2007) in the presence of cross section dependence with the Maddala and Wu (1999) Fisher test, which is associated with the assumption of cross section independence of the error term and does not require a balanced panel. The Pesaran (2007) approach augments the standard ADF regression with cross section averages of lagged levels and first differences of individual series, to control for cross section dependence. The Maddala and Wu (1999) Fisher test is then applied to this more general setting. With cross sectionally independent errors, the Maddala and Wu (1999) Fisher test cannot reject the null of non-stationarity for FLi,t , GDPi,t and OPENi,t when we do not allow for a trend. With a trend, the series of GDPi,t and OPENi,t are close to being found as stationary. When we allow for a trend, Pesaranâs test shows that we can almost reject the null of non-stationarity for FLi,t , GDPi,t and OPENi,t at the 10% significance level94 , suggesting that FLi,t , GDPi,t and OPENi,t may not be I(1) variables. However, this result should be interpreted with caution since there are reservations as to the power and reliability of these tests. This analysis also employs a normal within groups (WG) approach to estimating the one-way fixed effects models (country fixed effects included), as estimated by AM, with non-robust standard errors. How important controlling for error dependence across countries and over time is for this context can be examined by comparing the WG estimates and CCEP estimates. The consistency of the one-way WG estimator for the dynamic homogeneous model is justified by the length of the time series,95 but this estimator is biased in small samples because of the lagged dependent variable bias. The country fixed effects can be eliminated by an idempotent (covariance) transformation matrix as in within groups estimation."
30,95,0.983,Determinants of Financial Development,"Note: The dependent variable FD is the index of overall financial development over the period 1990â99. The variable description is in Appendix Table A2.1. The Gets analysis yields coefficients and t -values for the variables in the final model. There are 64 observations in the whole sample, 44 observations in the developing country sample and 40 observations in the La Porta sample."
222,478,0.983,Ecosystem Services For Well-Being in Deltas : integrated Assessment For Policy Analysis,"In addition to changes in total river volume discharged during the year, it is also important to consider the nature of freshwater input and tidal influence. The timing and intensity of freshwater input is closely related to the intensity of the monsoon as illustrated by the left hand panel of Fig. 17.10 which shows the daily river discharge entering the model at a site close to the city of Pabna, situated on the Padma (Ganges) River. In âwetâ years (blue and green curves) there is an early and sharp monsoon onset around the middle of July (day 200). A more gradual and later onset (around day 220âearly August) occurs during the two âdry yearsâ (pink and red curves). The future tidal range is projected to become larger, although not in a spatially homogenous fashion. Combined with an increase in mean sea level, this leads to an increase of salt intrusion into the delta (e.g. see Fig. 17.11). The spatial structure of the front is complex and âpulsesâ in and out on a tidal time scale of 12.4 hours. There is also considerable vertical structure in the profile of salinity, with a fresher layer overlying salty waters. Tides advect salt water in and out, but also contribute to mixing in the vertical. Figure 17.12 shows the distribution of mean and maximum annual salinity in the study area under the baseline scenario. A strong east-west profile is observed, with freshwaters in the eastern section, dominated by"
65,496,0.983,Handbook of Ocean Wave Energy,"To begin with, a general appreciation of the hydrodynamic behaviour of the device should be made. This can in practice be done in regular waves and without any PTO loading, by making various short tests (0.5â2 min each) where the wave height is maintained constant and the wave periods are each time incremented. This should be repeated for constant wave periods and increasing wave heights, and it could for example be used to identify the resonance frequency of the structure or of the wave activated body and show the range of effect of the wave conditions. A similar approach could possibly be used to investigate the influence of different conï¬gurations, for example if the device has an adaptable geometry, weight or floating level. After the hydrodynamic behaviour, the sensitivity and relevant working range of the PTO loading adjustment system have to be assessed. In this case, the load should be increased again in batches (0.5â2 min each) for a couple of the tested wave conditions. In practice, this can be done by incrementing the load between each batch by 10 % of its full range and repeated for the smallest, one or two medium and the largest sea states. Although these tests are not crucial, they often lead to a signiï¬cant gain in time. Note that in order to maintain the same wave energy content in between regular and irregular waves, the signiï¬cant wave height (Hm0) from the irregular waves has to be divided by â2 to obtain the wave height for the regular waves, while maintaining the same wave period (T = Te). However, in the case that the response or performance of the device is mostly dependent on the wave period, it might be beneï¬cial to match the wave period in regular waves with Tp, as this is the dominant wave period in irregular waves. The actual performance assessment is based on long-crested irregular waves, having a speciï¬c wave spectral shape (e.g. JONSWAP spectrum with É£ = 3.3). Each individual IW lab test should have a length of 1000 peak wave periods (for statistical robustness), which should take about 20â30 min, depending on the scale. Moreover, in each wave condition the PTO load needs to be optimised for optimal energy production (as presented in Fig. 9.12). Ideally, an exact reproduction of the waves should be performed in between those tests. Depending on the complexity"
280,37,0.983,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"the midline and/or the veins must be either more rapid or slower than movement elsewhere. One way to accomplish this is by having a required metabolite or precursor to the reaction distributed in a pattern that is symmetrical to the midline. A clear candidate for this is the gradient left behind by the midline pattern that preceded the formation of the focal spot (Fig. 1.8). This midline concentration gradient decays only gradually, and its profile depends on the parameter values and initial fuel distribution. The hypothesis then is that the shape of the parafocal elements is determined by a gradient left behind by the process that formed the focus. This idea can be tested computationally. Figures 1.10a and 1.11 show a sample of the diversity of parafocal element shapes that can be produced by this model. Although these shapes closely mimic those of real parafocal elements (e.g., Fig. 1.9), the shape of the ocellus is not circular, as would typically be the case. To produce both perfectly circular eyespots and the right diversity of parafocal element shapes, it is necessary to assume that the focus could be the source of two different signals (one perhaps initiated by notch and the other by Distal-less) that use different substrates. If one signal uses a homogeneously distributed substrate, it will produce a circular eyespot (Fig. 1.10b), and if the other uses the gradient left behind by the focus-forming process, it produces the parafocal element. Interestingly, this second source also produces an arc-shaped pattern on the proximal side of the eyespot (Fig. 1.10câg). This finding is consistent with SÃ¼ffertâs idea that the parafocal element is the distal band of the border symmetry system: the parafocal element and the proximal arc produced by the second source make up paired bands of the border symmetry system. These model results also support the ideas about the nature of parafocal elements and border symmetry systems proposed by Otaki (Dhungel and Otaki 2009; Otaki 2009, 2011)."
84,321,0.983,Eye Tracking Methodology,"13.2 Dwell-Time Fixation Detection The dwell-time fixation detection algorithm depends on two characterization criteria: 1. Identification of a stationary signal (the fixation) 2. Size of time window specifying an acceptable range (and hence temporal threshold) for fixation duration An example of such an automatic saccade/fixation classification algorithm, suggested by Anliker (1976), determines whether M of N points lie within a certain distance D of the mean (Î¼) of the signal. This strategy is illustrated in Fig. 13.3a where two N -sized windows are shown. In the second segment (positioned over a hypothetical saccade), the variance of the signal would exceed the threshold D indicating a rapid positional change, i.e., a saccade. The values of M, N , and D are determined empirically. Note that the value of N defines an a priori sliding window of sample times where the means and variances are computed. Anliker denotes this algorithm as the position-variance method because it is based on the fact that a fixation is characterized by relative immobility (low position variance) whereas a saccade is distinguished by rapid change of position (high position variance)."
230,306,0.983,Marine Anthropogenic Litter,"7.3 Spatial and Temporal Patterns in the Abundance of Microplastics Our understanding about the distribution and the factors affecting the distribution of microplastics in the oceans is limited and much of the sampling to date has been opportunistic utilizing existing research programs (research cruises, educational programs, routine plankton monitoring) to collect material. There has also been some targeted microplastic sampling and attempts to make formal comparisons in the abundance of microplastics between locations (Browne et al. 2010, 2011). Existing data indicate that microplastics are widely distributed in surface waters, in shallow waters (Browne et al. 2011; Hidalgo-Ruz et al. 2012), in deepsea sediments (Van Cauwenberghe et al. 2013) and in the digestive tract of a range of organisms living within these habitats (Lusher 2015). With the exception of heavily contaminated areas such as shipbreaking yards (Reddy et al. 2006), the abundance of microplastics would appear to be relatively low in surface waters and sediments (see Lusher 2015). By volume it is apparent, however, that sediments are more contaminated than surface waters. However, because of their ubiquity, the total quantity of microplastics in the environment is considerable and in some locations represents the most numerous type of debris present (Browne et al. 2010). This ubiquity is also demonstrated by encounters when considered by marine species, of which around 10 % are with microplastics (Secretariat of the Convention on Biological Diversity and Scientific and Technical Advisory Panel GEF 2012). In terms of spatial patterns in abundance, at a global scale Browne et al. (2011) detected a weak relationship between the abundance of microplastics and human population density. Extensive sampling by Law et al. (2010) demonstrated the role of large-scale physical factors leading to increased abundance in the North Atlantic gyre far from the nearest land. She matched abundance data form the ocean surface with model predictions based on physical factors indicating that, at large scales, factors driving the abundance of debris can be used to make predictions about relative abundance (Law et al. 2010; Fig. 7.5). Formal comparisons also demonstrated patterns at smaller spatial scales with locations previously used for the dumping of sewage sludge having greater quantities of microplastic than control areas (Browne et al. 2011). In addition, intertidal sediments on shores that were downwind in relation to prevailing wind direction can have greater quantities of microplastic than those on shorelines that were up-wind (Browne et al. 2010). Targeted sampling has also indicated extremely high microplastic abundance near to a plastic processing plant in Sweden (NorÃ©n 2008). However, while the role of some potential sources including sewage and industrial spillage have been demonstrated together with the influence of physical factors leading to accumulation of debris in particular locations, our collective understanding of the relative importance of these factors in influencing spatial patterns of distribution or in making predictions about such is limited. Only a handful of studies have considered temporal patterns in the abundance of microplastics. Thompson et al. (2004) in the northeast Atlantic and Goldstein"
238,285,0.983,Nanoinformatics,"data for the âinitial atomic conï¬gurationsâ are used as the descriptors. This choice enables one to predict the grain boundary energy without performing structure and energy calculations. The selected descriptors, such as the minimum and maximum bond lengths are listed in Table 8.1. In addition to these descriptors, their square, inverse, exponential and exponential inverse values were considered. As a result, 83 descriptors were obtained, which were standardized to align their average and variance to zero and one, respectively. The nonlinear support vector machine (SVM) method was used for regression analysis. In this study, the most stable structures and metastable structures of Î£5 [001]/(210), Î£ 5[001]/(310), Î£17[001]/(410), and Î£17[001]/(350) were considered for construction of the prediction model. We have selected those grain boundaries as the training data based on the variance of tilt angles and computational costs for their calculations."
359,115,0.983,"Micro-, Meso- and Macro-Dynamics of the Brain","modular organization. How then could modular grid scale result from a smooth underlying gradient? One possible scenario is that module grid scale is determined by network dynamics acting on a graded underlying scale parameter. Attractor models of grid cells predict that all cells in a circuit must have the same grid spacing (as well as orientation and pattern deformation) to generate a stable grid pattern (Welinder et al. 2008). Within a grid network determined by attractor dynamics, there will likely be some tolerance to small variations in the scale-parameter distribution across cells, so that when the network is initiated, the effects of population dynamics dominate individual cells enough to coordinate all cells into a common pattern, cancelling out individual variation. In a sense, this âspatial synchronizationâ acts similarly to synchronization in the temporal domain; originally observed by Huygens in 1665, coupled oscillators settle on a mean frequency that entrains all the individual oscillators, even in the presence of relatively large variations in individual frequencies. But what would happen if the scale parameter distribution exhibits too large spread? The variation may become too large to entrain all units into one coherent pattern, and the pattern may fraction into sub-ensembles that each center on a mean frequency that the ensemble can sustain. This way, by having a network self-organize from a very wide, continuous scale parameter distribution, such as channel expressions along an axis in MEC, several local modules of internal spatial consistency could arise from the unstable global pattern. We observed convincing signs of independence between modules within animals, in terms of pattern geometry and rescaling responses. To incorporate this finding into the suggested mechanism above, one can suppose that, during development, learning strengthens connections within spatially synchronized ensembles but weakens connections between spatially desynchronized cells. In agreement with this possibility, grid cell pairs with similar spatial phase show stronger functional connectivity than pairs with dissimilar phase (Dunn et al. 2015). If two cells have a similar spatial phase, their coordinated firing in space will cause coordinated firing in time, a prerequisite for many forms of long-term potentiation (LTP; Bi and Poo 1998). Enhancement of connections between grid cells with a similar phase would lead to the development of functional ensembles intermingled in the same tissue, with strong inter-ensemble connectivity and weak crossensemble connectivity, in effect decoupling the ensembles functionally. A testable prediction from this idea is that very young animals, which have yet to achieve complete module decoupling, will display grid cells with poor spatial regularity because the network cannot sustain a coherent grid pattern based on cross-ensemble interactions. As the animal explores more space, decoupling will at some point become complete enough for cells to self-organize into modules with coherent and regular grid patterns. Such a transition may be rapid, as it may involve a âtipping pointâ after which network dynamics kick in to entrain the ensemble. In two studies that characterized grid cells in early development in rats, grid patterns were indeed not very regular initially (Langston et al. 2010; Wills et al. 2010). Only at the age of about 4 weeks, 1â2 weeks after the beginning of outbound exploration, did regular"
302,237,0.983,Freshwater Microplastics : Emerging Environmental Contaminants?,"To date, two models have been presented that are able to simulate the transport of plastic debris in freshwater rivers with high spatial and temporal resolution [35â 37]. Both models are framed by the authors as theoretical models, that is, they are supposed to be valid with respect to the design criteria and in agreement with existing theory, but they are not yet validated against measured data for plastic debris (Table 1). Modeling the Transport of Plastic Debris in the Dommel River (The Netherlands) The model by Besseling et al. [35, 36] is the first model that simulated the fate of nano- up to centimeter (i.e., macroplastic)-sized plastic particles in a river (see [24] for review). The model is based on the NanoDUFLOW hydrological model [96, 109] and includes advective transport of particles, their homo- and heteroaggregation, biofouling, sedimentation/resuspension, degradation of plastic, and burial in the sediment. This implies that all processes mentioned in Sect. 3 were accounted for. Although not yet formally validated for plastic particles because of lacking monitoring data, earlier model simulations for nano-CeO2 showed good agreement with measured nano-CeO2 submicron particles in the same river [96]. The model can be implemented for other catchments using DUFLOW Modeling Studio [110] and allows for the inclusion of tributaries and diffuse as well as point sources (e.g., WWTPs) [96]. To simulate the transport of plastic debris, parameter values were set based on literature data. Data for the attachment efficiency for heteroaggregation are scarce and therefore were also determined experimentally. A 40 km stretch of the river Dommel (the Netherlands) was modeled with a spatial resolution of 477 sections of an average 87.7 m length and with section widths ranging from 8 to 228 m. The effect of all processes was calculated per section and the result was passed on to the next. An upstream point source with known mass concentration was used as a boundary condition at time zero, based on an average order of magnitude of published concentrations of microplastics in freshwaters. Scenario studies aimed at identifying how plastic debris of all sizes and densities would be distributed along the river. Realistic flow data were used. Impacts of long-term variability in"
139,429,0.983,Programming for Computations - MATLAB/Octave (Volume 14.0),"d) Consider the physical application from Sect. 5.1.4. Run this case with the  rule and  D 1=2 for the following values of t: 0.001, 0.01, 0.05. Report what you Filename: rod_ThetaRule.m. Remarks Despite the fact that the Crank-Nicolson method, or the  rule with  D 1=2, is theoretically more accurate than the Backward Euler and Forward Euler schemes, it may exhibit non-physical oscillations as in the present example if the solution is very steep. The oscillations are damped in time, and decreases with decreasing t. To avoid oscillations one must have t at maximum twice the stability limit of the Forward Euler method. This is one reason why the Backward Euler method (or a 2-step backward scheme, see Exercise 5.3) are popular for diffusion equations with abrupt initial conditions. Exercise 5.6: Compute the diffusion of a Gaussian peak Solve the following diffusion problem: @2 u D Ë 2; u.x; 0/ D p 2 2"
285,585,0.983,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","levels of 65 and 70 dB, respectively), while the SOAE level increases remained fairly unchanged. This resulted in a less symmetrical shape of the SOAE Bounce compared to the reference recording. Consequently, the time constants of the fitted function with CAS shortened significantly."
311,2431,0.983,The Physics of the B Factories,"mÎcc = mN + 2(mÎc â mN ) + (mÎ£c â mÎc ), (19.4.11) where the terms represent the base nucleon mass, the mass oï¬set for two charm quarks, and a hyperfine correction. Evaluating this we obtain mÎcc â 3720 MeV/c2 . However, this estimate should be treated with some skepticism: we have assumed that the coeï¬cients A, B â² , and C â² are the same for all baryons, but in practice these terms have some scale dependence (e.g. on the spatial extent of the wavefunction). There are numerous more rigorous theoretical predictions (see, e.g., Roberts and Pervin (2008) and the references therein), including increasingly precise estimates from Lattice QCD (Liu, Lin, Orginos, and Walker-Loud, 2010). Most estimates lie between 3600 and 3700 MeV/c2 . To date, sightings of Îcc states have been reported only at the SELEX experiment, a forward spectrometer in which a hyperon151 beam (composed of Î£ â , p, and Ï â ) struck a fixed target of copper or diamond. SELEX claimed observation of Îcc at a mass of 3519 MeV/c2 in + â + + â the Îc K Ï and pD K final states (Mattson et al., 2002; Ocherashvili et al., 2005). In each case the signature is a small, narrow signal on top of a smaller background: an excess of 15.9 events above an estimated backâ + ground of 6.1Â±0.5 for Î+ c K Ï , and of 5.4 above 1.6Â±0.4 + â for pD K . The observations were controversial (Kiselev and Likhoded, 2002), primarily because the lifetime and the production rate of Îcc at SELEX were far from ex+ pectations. The theory expectation for the Îcc lifetime is approximately 200â250 fs across a number of models (Chang, Li, Li, and Wang, 2008), compared to a reported upper limit of 33 fs. Even more surprising, by comparing the relative yields of Î+ c and Îcc and correcting for acceptance and additional decay modes, SELEX estimated that 20% of its sample of 1,630 Î+ c came from Îcc decays (presumably with a further contribution of similar order from ). This runs counter to expectations: it is much more diï¬cult to produce a baryon with more than one unit of flavor because two heavy quark-antiquark pairs need to be created within a narrow enough kinematic window"
359,20,0.983,"Micro-, Meso- and Macro-Dynamics of the Brain","equivalence between space and time. In addition, the cross correlation is strongly modulated by theta. The lags of the local maximum, on theta time scales, correlate with the time taken to traverse between the place fields. The ratio of these lags reflects the degree of compression. A recent study explicitly tested the link between theta phase precession and theta sequencing as rats explored a novel linear track (Feng et al. 2015). This study found that phase precession was observed on the first trial, though theta sequences were not. The sequencing emerged rapidly, by the second trial, and this development coincided with a decrease in the phase variability in which cells fired upon place field entry. Therefore, theta sequencing seems to be a natural consequence of a group of cells that phase precess at the same rate (slope) and begin firing at the same phase (Dragoi and BuzsaÌki 2006). It is unknown what causes cells to fire at more reliable theta phases. The known importance of inhibitory cells in dictating firing phase (Royer et al. 2012) and the hypothesized role of inhibition in phase precession (Kamondi et al. 1998; Geisler et al. 2010; Losonczy et al. 2010; Stark et al. 2013) suggest a potential candidate for this phase alignment may be plasticity between excitatory and inhibitory cells. Interestingly, cells recorded at the same site tended to have more uniform phases upon place field entry (Feng et al. 2015), consistent with models in which interneurons coordinated place cells within the range of their axonal arbor. There is growing evidence that theta sequences represent a meaningful segmentation of space. In one experiment that addressed this issue, rats were habituated to a linear track and the place field order and theta sequences were identified. Then, the track was expanded, a manipulation known to cause concomitant increases in place field size (OâKeefe and Burgess 1996). Remarkably, the theta time-scale lag remained fixed, thereby causing an increase in the compression of the amount of behavioral time represented within a theta cycle (Diba and BuzsaÌki 2008). A recent experiment found that the magnitude of compression observed within each theta sequence varied significantly according to where the rat was on the maze. The amount of space represented ahead of, or behind, the rat varied systematically according to where the rat was relative to the experimentally defined landmarks (Gupta et al. 2012). This heterogeneity of theta sequence content suggests that one role of theta could be to divide space into meaningful segments. In the aforementioned study, theta sequences could have chunked space according to the physical geometry or due to some process related to route planning. To dissociate these two possibilities, rats were trained to traverse around a circular track, collecting rewards by waiting a variable amount of time at each of three locations (Wikenheiser and Redish 2015). Rats had a choice to stay and wait for a reward or run to the next location, which was the optimal strategy if the wait time for reward at the more distant site was shorter (Wikenheiser et al. 2013). When activity on the late phases of theta was analyzed, there was a strong correlation between the distance the rat was about to run and the places represented by the active cells. Different cells spiked in the same location depending on where the rat would run next. Importantly, there was no relationship between the distance the rat had just run and the distances represented in these late theta phases. These data"
372,1674,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"Appendix 13.1 Importance of the 22-GHz Line in WWII Radar Development The history of the 22-GHz transition of water vapor is quite interesting. The water vapor molecule has different moments of inertia about its three axes of rotation, and hence its rotational spectrum is complex, as shown in Fig. A13.1. The rotational energy levels were first determined through measurements of the infrared spectra by Randall et al. (1937). Van Vleck noted in an MIT Radiation Laboratory report (Van Vleck 1942) that these energy levels indicated the existence of an allowable microwave line in the range of 1.2â1.5 cm (20â25 GHz), due to a chance nearcoincidence of two energy levels in adjacent rotational ladders. Lying at an energy level of 447 cm!1 above the ground state, corresponding to a temperature of 640 K, the line has a Boltzmann population factor at atmospheric temperature that is about 0.1. Van Vleck calculated that atmospheric opacity along horizontal path lengths due to the absorption of H2 O and absorption in the wing of the O2 lines near 60 GHz would cause problems for radars operating at short centimeter wavelengths. However, there was little empirical data about the pressure-broadening constants for the line widths [see Eq. (13.43) and Fig. 13.7] of these lines, and the estimates Van Vleck used were almost three times larger than than their actual values. Therefore, he substantially overestimated the absorption of O2 and underestimated the absorption of H2 O at 1.25-cm wavelength. Nonetheless, he raised an important"
349,274,0.983,Methods for Measuring Greenhouse Gas Balances and Evaluating Mitigation Options in Smallholder Agriculture,"There are different approaches to account for soil carbon stocks and stock changes, and they all aim at providing a measure of mass of SOC per unit ground area. The spatial coordinate approach calculates stocks considering the amount of carbon contained within a given volume of soil, which is defined by the sampled area and the depth referenced to the surface level. With this approach, the average SOC stock for a given depth interval (d) is calculated according to the following formula:"
213,135,0.983,Collider Physics Within The Standard Model : a Primer,"which we have alreadyP introduced. R is dimensionless and is given in perturbation theory by1 R D NC i Q2i F.t; Ës /, where F D 1 C O.Ës /. We have already mentioned that for this process the âanomalous dimensionâ function vanishes, i.e., .Ës / D 0, because of electric charge non-renormalization by strong interactions. Let us recall how this happens in detail. The diagrams that are relevant for charge renormalization in QED at 1-loop are shown in Fig. 2.12. The Ward identity that follows from gauge invariance in QED requires the vertex (ZV ) and the self-energy (Zf ) renormalization factors to cancel, and the only divergence remains in Z , the vacuum polarization of the photon. Hence, the charge is only renormalized by the photon vacuum polarization blob, and it is thus universal (the same factor for all fermions, independent of their charge) and not affected by QCD at 1-loop. It is true that at higher orders the photon vacuum polarization diagram is affected by QCD (for example, at 2-loops we can exchange a gluon between the quarks in the loop), but the renormalization induced by the divergent logs from the vacuum polarization diagram remain independent of the nature of the fermion to which the photon line is attached. The gluon contributions to the vertex (ZV ) and to the self-energy (Zf ) cancel, because they have exactly the same structure as in QED, and there is no gluon contribution to the photon blob at 1-loop, so that .Ës / D 0. At the 1-loop level, the diagrams relevant for the computation of R are shown in Fig. 2.13. There are virtual diagrams and also real diagrams with one additional gluon in the final state. Infrared divergences cancel between the interference term of the virtual diagrams and the absolute square of the real diagrams, according to the"
233,356,0.983,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"Thus, by summing across branches instead of species, substituting branch occurrence for species occurrence, and including a branch length weighting, we are able to adapt the classic rarefaction formula for species richness for the purposes of calculating expected Phylogenetic Diversity (Eq. 6). Note this solution is equivalent to that of Nipperess and Matsen (2013) but is expressed in an expanded form for the specific case of calculating rooted PD. Equation 6 is very similar to the solution for"
297,1815,0.983,The R Book,"Three of the seven variables were chosen for use (Sepal, Leaf and Petiole); four variables were assessed and rejected (Petals, Internode, Bract and Fruit). The key has four nodes and hence three dichotomies. As you see, the misclassiï¬cation error rate was an impressive 0 out of 120. It is noteworthy that this classiï¬cation tree does much better than the multivariate classiï¬cation methods described in Chapter 25. For classiï¬cation trees, the print method produces a great deal of information print(model1) node), split, n, deviance, yval, (yprob) * denotes terminal node 1) root 120 332.70 I (0.2500 0.2500 0.2500 0.25) 2) Sepal<3.53232 90 197.80 I (0.3333 0.3333 0.3333 0.00) 4) Leaf<2.00426 60 83.18 I (0.5000 0.5000 0.0000 0.00) 8) Petiole<9.91246 30 0.00 II (0.0000 1.0000 0.0000 0.00) * 9) Petiole>9.91246 30 0.00 I (1.0000 0.0000 0.0000 0.00) * 5) Leaf>2.00426 30 0.00 III (0.0000 0.0000 1.0000 0.00) * 3) Sepal>3.53232 30 0.00 IV (0.0000 0.0000 0.0000 1.00) * The node number is followed by the split criterion (e.g. Sepal < 3.53 at node 2). Then comes the number of cases passed through that node (90 in this case, versus 30 going into node 3, which is the terminal node for taxon IV). The remaining deviance within this node is 197.8 (compared with zero in node 3 where all the individuals are alike; they are all taxon IV). Next is the name of the factor level(s) left in the split (I, II and III in this case, with the convention that the ï¬rst in the alphabet is listed), then a list of the empirical probabilities (the fractions of all the cases at that node that are associated with each of the levels of the response variable â in this case the 90 cases are equally split between taxa I, II and III and there are no individuals of taxon IV at this node, giving 0.33, 0.33, 0.33 and 0 as the four probabilities). There is quite a useful plotting function for classiï¬cation trees called partition.tree, but it is only sensible to use it when the model has two explanatory variables. Its use is illustrated here by taking the two most important explanatory variables, Sepal and Leaf: model2 <- tree(Taxon~Sepal+Leaf,taxonomy); partition.tree(model2) This shows how the phase space deï¬ned by sepal length and leaf width has been divided up between the four taxa, but it does not show where the data fall. We could use points(Sepal,Leaf) to overlay the points, but for illustration we shall use text. We create a vector called label that has a for taxon I, b for II, and so on: label <- ifelse(Taxon==""I"", ""a"", ifelse(Taxon==""II"",""b"",ifelse(Taxon==""III"",""c"",""d""))) Then we use these letters as a text overlay on the partition.tree like this: text(Sepal,Leaf,label,col=1+as.numeric(factor(label)))"
70,266,0.983,Optics in Our Time,"angle in a very interesting way. In part (a) of the sample, nine electrons will scatter through 0.5 or more, in part (b) about 17 will scatter, while part (c) shows a much larger number of about 95 electrons scattering by more than 0.5 . However, in part (d) the number scattered depends on the angle that the incident beam of electrons makes with the sample atoms in what is known as âBragg diffractionâ [6]. If a small aperture (i.e. a hole cut in a metal plate) is placed below the specimen such that any electrons scattered by 0.5 or more are stopped by this plate whilst those scattered by less than this angle go through, then a number of different âin-valueâ electron signals could be collected below the small aperture. The position of this aperture is therefore the key part in the working of the TEM. The detectors used in the TEM also vary from principally detecting the crystallinity of the sample, via the distribution of its atoms; or they determine its atomic number, via the measurement of very small energy losses that the incident electrons suffer in passing through the thin sample. The TEM is a more complex instrument to manufacture and operate than the SEM. Moreover, its price is normally several times more than that of the SEM. The number of TEM instruments worldwide is perhaps less than 10 % of the installed user base of SEMs. It should, however, be understood that the information gathered from either instrument normally complements the other rather than being an alternative to it."
118,108,0.983,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"by U.S. industries while MELCOR was developed by the United States Nuclear Regulatory Commission (US NRC) [5]. These codes basically calculate the thermal response of the core, dealing with the entire progression from the initiating event to the radionuclide releases to the environment, which is called the âsource term.â Therefore, the initial inventory and the release properties for each nuclide are required as input parameters. These values are usually calculated by a burn-up code, such as ORIGEN or CORSOR. The entire progression from the initial event includes damage in the RPV and PCV and consequent leakage of water and steam. After the accident, another code named âSevere Accident analysis code with Mechanistic, Parallelized Simulations Oriented towards Nuclear fields (SAMPSON)â [6], developed by Nuclear Power Engineering Corporation (NUPEC), has been improved by Institute of Applied Energy in Japan. The merit of the SAMPSON code is the fact that there is no factor adjusted by the user."
49,357,0.983,Artificial Intelligence and Cognitive Science IV,"The decay rate parameter 7 modifies the extent to which the previous activities of the neuron affect its current state. Therefore, when the neurons are set with large 7 values their activities will be changing more slowly over time as compared to those neurons set with smaller 7 values. In this experiment, 256 input-output neurons were set to 7 = 2 while the hidden neurons consisted of two different categories where each had different time integration constant. The first category comprise of 60 fast neurons with 7 = 5 and the second of 20 slow neurons set to 7 = 70. These two categories are attempting to capture the dynamics of complex behavioral patterns by flexible recombination of motor primitives into novel sequences of actions. As described in the introduction, multiple timescale networks have been suggested as the underlying system that facilitates this behavioral compositionally. The network is fully connected and hence every neuron is connected to every other neuron including itself. There is one exception where the slow neurons are not directly connected to the input-output layer but rather indirectly via the fast neurons. The continuous time integration model of the MTRNNâs neurons were defined by the differential equation 4 while the actual membrane potentials are calculated by its numerical approximation defined by (8). 4,> = ?1 â A B 4, + A Câ0â3 60 50, D"
45,93,0.983,Measurement and Control of Charged Particle Beams,"which is commonly applied. Figure 2.11 illustrates the error involved in approximating (2.28) by (2.29). The difference between the two expressions becomes important if Qx,y is close to an integer or half integer resonance, and for large changes ÎK [24]. A recent beta function measurement at the Fermilab Recycler is depicted in Fig. 2.12. The nonlinear dependence is well described by the complete (2.28). Care has to be taken that the applied change in quadrupole strength does not alter the beam orbit, which happens if the beam is off-center in the quadrupole whose strength is varied. If the orbit changes, part of the measured tune shift could then be caused by the closed-orbit variation at the sextupole magnets elsewhere in the accelerator. If a strong effect on the orbit is observed, the orbit should first be corrected with the help of steering correctors before the new (shifted) tune value is measured. Sometimes, several magnets are connected to the same power supply, and then the strengths Ki (i = 1, ..., m) of m quadrupoles must be changed simultaneously, all by the same amount ÎK. The above result is easily generalized to this case: the induced tune change is related to the average beta function at the m quadrupoles via Î²x,y m â Â±4Ï ÎQx,y /(mÎK). However, the disadvantage of averaging over several quadrupoles is that beta beating may be less evident."
139,495,0.983,Programming for Computations - MATLAB/Octave (Volume 14.0),"where % is the density of the beam, A is the area of the cross section, E is Youngâs modulus, and I is the moment of the inertia of the cross section. The most important parameter of interest is !, which is the frequency of the beam. We want to compute the frequencies of a vibrating steel beam with a rectangular cross section having width b D 25 mm and height h D 8 mm. The density of steel is 7850 kg=m3 , and E D 21011 Pa. The moment of inertia of a rectangular cross section is I D bh3 =12. a) Plot the equation to be solved so that one can inspect where the zero crossings occur. Hint When writing the equation as f .Ë/ D 0, the f function increases its amplitude dramatically with Ë. It is therefore wise to look at an equation with damped amplitude, g.Ë/ D e Ë f .Ë/ D 0. Plot g instead. b) Compute the first three frequencies. Filename: beam_vib.m. Open Access This chapter is distributed under the terms of the Creative Commons AttributionNonCommercial 4.0 International License (http://creativecommons.org/licenses/by-nc/4.0/), which permits any noncommercial use, duplication, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, a link is provided to the Creative Commons license and any changes made are indicated. The images or other third party material in this chapter are included in the workâs Creative Commons license, unless indicated otherwise in the credit line; if such material is not included in the workâs Creative Commons license and the respective action is not permitted by statutory regulation, users will need to obtain permission from the license holder to duplicate, adapt or reproduce the material."
231,1033,0.983,North Sea Region Climate Change Assessment,"The importance of addressing critical temporal scales in climate impact research has been documented for various lakes in the North Sea region (see Adrian et al. 2012; Sect. 10.2). A closer look at sub-hourly measurements showed that the rate of increase in the daily minima (night-time water temperature) exceeded that of the daily maxima (daytime water temperature) (Wilhelm et al. 2006). The consequences of this day-night asymmetry for the biota are unclear, but may contribute to some of the unexplained changes observed in ecosystem dynamics over time. Day-to-day variation in respiration seems to be common in lakes worldwide, including those in the North Sea region (Solomon et al. 2013). Daily variation in gross primary production explained 5â85 % of the daily variation in respiration. Solomon et al. (2013) found respiration to be closely coupled to gross primary production at a diurnal-scale in oligotrophic and dystrophic lakes, but more weakly coupled in mesotrophic and eutrophic lakes. Known changes in the thermal regime of lakes in the North Sea region (Sect. 10.2) operate over a broad range of temporal scales, and are closely related to lake morphometry: on sub-daily (Wilhelm and Adrian 2008) to weekly scales in polymictic lakes (Wagner and Adrian 2009b), and on weekly to monthly scales in monomictic or dimictic lakes (Gerten and Adrian 2000; Livingstone 2003). While variation in the timing of spring overturn affects underwater light conditions and thus the start of algal growth (Weyhenmeyer et al. 1999; Gerten and Adrian 2000; Peeters et al. 2007), variation in the timing of summer stratiï¬cation affects water temperature and internal nutrient loading and subsequent plankton development and species composition in productive lakes (Wilhelm and Adrian 2008; Wagner and Adrian 2011). Differences in water temperature between mixed and stratiï¬ed periods can be up to 5 Â°C within days or a just few weeks in summer, favouring thermophilic copepod species for example (Wagner and Adrian 2011). Changes in phenology in abiotic and biotic variables (see Sect. 10.4.2) operate on time scales of weeks (Weyhenmeyer et al. 1999; Gerten and Adrian 2000; Straile et al. 2003). Thus, in terms of their duration, seasons should be deï¬ned by cardinal events within the lake itself, rather than by ï¬xed calendar dates. Important markers successfully used to deï¬ne phenology-adjusted seasons in lakes include temperature thresholds, ice-off dates, the timing of the clear-water phase, and periods of stable thermal stratiï¬cation (Rolinski et al. 2007; Wagner and Adrian 2009a; Huber et al. 2010). Wagner et al. (2012) proposed a seasonal classiï¬cation scheme tuned to speciï¬c hydrographic-sensitive phases for dimictic lakes across a latitudinal gradient: inverse stratiï¬cation (winter), spring overturn, early stratiï¬cation and the"
295,427,0.983,interface oral Health Science 2016 : innovative Research On BiosisâAbiosis intelligent interface,"Wettability is another important feature of the material surface. This basic physical parameter can be quantified by the value of the surface contact angle (CA), which is first proposed by Thomas Young in 1805 [27]. Generally, a surface with a contact angle of less than 90Â° is considered as a hydrophilic one and if the contact angle is equal to 0Â°, it is considered as a super-hydrophilic surface. On the contrary, the hydrophobic surface is defined as the one with the value of CA more than 90Â°. The roughness would also influence the wettability of material surfaces. Previous studies have shown that the surface contact angle of the pure titanium was about 70â90Â° regardless of surface roughness. However, after a serial surface roughening process including acid etching and sandblasting, the CA of the surface could reach up to 150Â°. Moreover, it is reported that the CA of the surface rarely exceeds 120Â° if the material is only treated by chemical methods rather than roughening. Therefore, the hydrophobic surfaces with the CA ranging from 125 to 180Â° often call for a combination of different treatments [28â30]. Surfaces with different wettability produce diverse biological effects of osseointegration, and the impact can be divided into the following four aspects: protein and biological macromolecular adhesion to the material surface, biological behavior of different cells on the surface, the formation of the bacteria biofilm, and in vivo study of osseointegration. There are different views on the adhesion of protein on the surfaces with different wettability. Previous studies have shown that the fibronectin adhesion is much more facilitated on a hydrophilic surface than on the hydrophobic surface, and the fibronectin adhering to the hydrophilic surface can maintain a better biological activity which could promote osteoblast adhesion and differentiation [7, 31]. However, Tugulu et al. [32] found that the super-hydrophilic surface treated by diluted alkaline solution would reduce the adhesion of the fibrinogen and thereby reduce the inflammation around the implant, which enhances the potential of the promotion of implant osseointegration in vivo [33]. Besides, the adhesion of other proteins which play important roles in osteogenic differentiation such as vitronectin and type I collagen is also affected by surfaces with diverse characteristics [34]. More recently, some results demonstrate that the expression of genes related to the osteogenic differentiation of mesenchymal stem cells on the super-hydrophilic surface is higher than the level of the expression of the cells on the hydrophobic"
213,305,0.983,Collider Physics Within The Standard Model : a Primer,"where all theoretical numbers are taken from [59]. The dominant ambiguities arise from the hadronic term. The lowest order (LO) vacuum polarization contribution can be evaluated from the measured cross-sections in eC e ! hadrons at low energy via dispersion relations (the largest contribution D 6949 Ë is from the final state) [155, 239], with the result aLO   10 43. The higher order (HO) vacuum polarization contribution (from 2-loop diagrams containing a hadronic insertion) is given by aHO D 98:4 Ë 0:7 [239].   10 The contribution of the light-by-light (LbL) scattering diagrams is estimated to be D 116 Ë 40 [290]. Adding the above contributions, the total hadronic   10 result is reported as D .6967 Ë 59/  10 11 : ahadronic"
49,476,0.983,Artificial Intelligence and Cognitive Science IV,"The chromosome C is composed of three parts C = {C1 ,C2 ,C3 }, which are represented by: (C1 ) virtual RB designed on P0 , (C2 ) searched optimal granularities, i.e. numbers of MFs and (C3 ) definitions of MFs constructed on P000 , respectively. In all three parts of a chromosome the parameters for all variables are contained and the chromosome, which has the best value of the fitness function (14) will have the best interpretable KB, too. 4.3.2"
120,63,0.983,Genome Editing in Neurosciences,"library element, depending on the details of the screen. This number is necessary to average out noise in the assay itself, and also heterogeneity in the genetic perturbation induced in each cell, as well as inherent variability in the response of the screened cells to the perturbation. (Graham and Root 2015). Thus, for a CRISPR gRNA library that contains approximately four gRNAs per protein-coding gene, the 80,000 library elements should each be targeted to approximately 1000 cells (thus 80 million cells in total across all replicates). Reducing either biological or technical variability, for example by employing a more homogeneous cell population, can reduce the number of cells needed in each screen. The time between injection of the library and harvesting of the cells for analysis will be determined by experimental goals and could range from several days to months, depending on the rate of progression of the CNS phenotype being screened."
32,643,0.983,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","Around the middle of Fig. 28.6, we can see three nodes with larger degrees than others. Their colors are red, green, and yellow from left to right, respectively. These nodes are the hubs of the communities in the sense that they have the largest values of both in-degree Wiin and out-degree Wiout in each of their communities. It is interesting that those nodes for the red and yellow communities have the largest absolute values of the difference Ä±i D Wiout Wiin in each of their communities. We also note that all of the words corresponding to these three nodes are classified as positive in the dictionary of emotional words. In Table 28.1, we summarize these features of the community structure. These features mean the following. In each of the communities, there exists key emotional words. However, these words have different directions of influence. While the word âlikeâ in the red community is most influential to arouse emotion of others, the one âidolâ in the yellow community appears most under influence of other emotional words. In the green community, the word with the largest absolute value of the difference Ä±i is different from the one with the largest values of in-degree and outdegree. Moreover, the word with the largest absolute value of the difference in the green community is negative while the one with the largest in/out-degrees there is positive. Such differences in the three communities could imply how emotional exchanges differ among them. In order to see such differences in more detail, we show, in Table 28.2, a list of words with which the key emotional words have larger values of in-degrees/out-degrees. This table indicates how the key emotional words affect others or how they are used under influence of others within the same communities. In Table 28.2, for each of the three communities, three words are listed in the order of in-degree/out-degree which its key word has. For each of the word listed, we also show its order, its value of in-degree/out-degree, and its nature, i.e., positive or negative. The values of in-degree/out-degree of these words listed are not so large considering the total values of in-degree/out-degree of the key words. This Table 28.1 We summarize characteristics of the three communities shown in Fig. 28.6 Color The number of nodes The word wi with the largest in/out-degrees In-degree Wiin of wi Out-degree Wiout of wi The word wj with the largest absolute value of difference Ä±j D Wjout Wjin The difference Ä±j of wj"
311,1144,0.983,The Physics of the B Factories,"eï¬ect of neglecting the penguin amplitude has been estimated in models based on factorization and heavy quark symmetry, and the corrections are expected to be a few percent (Xing, 1998, 2000). Significant deviation of S in B 0 â D(â)Â± D(â)â decays with respect to sin 2Ï1 determined from b â ccs transitions, or a large non-zero value of C, could indicate physics beyond the SM (Grossman and Worah, 1997; M. Gronau and Pirjol, 2008; Zwicky, 2007). The B 0 â D(â)Â± D(â)â candidates are formed from oppositely charged D(â) mesons reconstructed in the following channels: Dâ+ â D0 Ï + , Dâ+ â D+ Ï 0 , D0 â K â Ï + , D0 â K â Ï + Ï 0 , D0 â K â Ï + Ï â Ï + , D0 â KS0 Ï + Ï â , and D+ â K â Ï + Ï + . Belle also uses the D0 â K + K â , D+ â KS0 Ï + and KS0 Ï + Ï 0 channels. In the B 0 â Dâ+ Dââ mode, B 0 candidates where both Dâ mesons decay to DÏ 0 are excluded because of its smaller branching fraction and larger backgrounds. At least one D meson is required to decay via D+ â K â Ï + Ï + for the B 0 â D+ Dâ decay. Both BABAR and Belle also analyze these decays using partially reconstructed events. However, while Belle includes these events in their analysis of fully reconstructed events, BABAR performs a separate B 0 â Dâ+ Dââ analysis of partially reconstructed events. For the partial reconstruction method one Dââ (or a Dâ â K + Ï â Ï â ) is fully reconstructed as described in the previous para+ graph. For the other Dâ+ , only a slow pion Ïslow the decay Dâ+ â D0 Ïslow , is reconstructed. The details of the partial reconstruction technique are described in Section 7.3. Due to low B meson CM momentum and small energy release in the Dâ+ decay, the momenta of Ïslow and D(â)â are almost back-to-back. This signature is used as a discriminator in Belleâs analysis. BABAR on the other hand exploits the kinematics of the event and calculates the B four-momentum up to an unknown azimuthal angle around the direction of the fully reconstructed Dâ . BABAR uses the median value for this angle based on simulation to calculate the recoil mass of the unreconstructed D0 and uses this recoil mass as a fit variable to separate signal and background. Belle requires the CM momenta of the reconstructed mesons in the Dâ+ Dâ mode to satisfy 1.63 GeV/c < pâD(â)â < 1.97 GeV/c and pâÏ+ <"
311,1926,0.983,The Physics of the B Factories,"An unbinned extended maximum likelihood fit is used to extract the signal yields. The fit employs two variables: for the Î¥ (1S) â Î¼+ Î¼â final state, the fit used the mass recoiling against the dipion system (Equation 18.4.3) and the invariant mass of the lepton pair. Monte Carlo simulations are used to verify these variables are uncorrelated, and the total likelihood is the product of the likelihoods in each dimension. For the Î¥ (1S) â Ï + Ï â final state, only the dipion recoil mass is used. The use of a ratio of branching fraction to the two possible final states allows for the cancellation of common systematic uncertainties. The two final-state samples are simultaneously fitted using the likelihood functions, and the result of the fit is RÏ Î¼ = 1.006 Â± 0.013 (statistical"
394,171,0.983,Biotechnologies for Plant Mutation Breeding : Protocols,"The optimization of a mutagen dose before the commencement of a large-scale experiment is crucial for the successful mutagenic treatment. Simple pot tests for seedling emergence and growth reduction are routinely used to select a critical dose, i.e., a dose that results in a 50â70 % growth reduction at the seedling stage (Maluszynski et al. 2003). Such a dose causes a very high degree of sterility and lethality of M1 plants, and therefore, for plant breeding projects that are aimed at improving only one or two characters in a well-adapted variety, doses of a mutagen that cause less than a 30 % growth reduction should be applied (Maluszynski et al. 2009). Too high a concentration of a mutagenic agent leads to a high lethality in the M1 populations, whereas too low a concentration of the mutagen may result in a low density of mutations. A lower frequency of mutations requires a larger mutagenized population, which is associated with higher costs and labor. Therefore, in treatments that are used for the creation of TILLING populations, doses that induce the highest frequency of point mutations in M2 plants, regardless of the somatic effects in M1, are sometimes applied (Martin et al. 2009). Nevertheless, the somatic effect of the mutagen should be known before a large-scale treatment in order to estimate the size of the M1 population required for development of a TILLING population. Alkylating agents such as EMS and MNU mainly induce GC>AT transitions (Till et al. 2003; Maluszynski et al. 2003); however, inversions and translocations have also been observed at low frequencies after MNU treatment (Szarejko and Maluszynski 1980). These mutagens cause DNA damage by transferring a methyl (âCH3) group to the oxygen and nitrogen atoms of the nucleotide bases. A wide spectrum of lesions can be obtained, with the biological effect of these lesions ranging from less harmful to those that lead to cell death. It has been proven that methylation at the O6 position of guanine has the strongest mutagenic property, as the O6-meG mispairs with T, and after DNA replication, the transition of G/C to A/T occurs (Richardson et al. 1986; Kleibl 2002; Warren et al. 2006). The alkylation of guanine in a nontranscribed (sense) DNA strand leads to its mispairing with thymine and after replication to its replacement by adenine (G>A transition). The alkylation of guanine in the transcribed (antisense) strand results in the C>T transition. MNU is considered to be a very strong chemical mutagen, which is sometimes called a âsupermutagenâ (Maluszynski et al. 2009). A detailed characterization of mutation types and frequencies induced by MNU in barley genome was reported by Kurowska et al. in 2012. The majority (63.6 %) of the MNU-induced nucleotide changes were transitions, with a similar rate of G>A and C>T substitutions. This indicates a lack of bias in the repair of the O6-methylguanine lesions between DNA strands. However, a strong specificity of the nucleotide surrounding the O6-meG at the 1 position was observed. Purines formed 81 % of the nucleotides identified at the 1 site (Kurowska et al. 2012). In studies performed on rice, G>A and C>T transitions were predominant and accounted for 50 % and 42 % of all mutations detected, respectively (Suzuki et al. 2008). Similarly, in soybean, 90 % of the observed nucleotide changes formed these two types of transitions (Cooper et al. 2008). A similar share of G>A and C>T transitions that has been reported in barley, rice, and soybean indicates a lack of bias in the repair of the O6-meG lesions between DNA strands."
70,945,0.983,Optics in Our Time,"One fascinating concept in quantum mechanics is the possibility to encode quantum information in different ways. In the simple example of the polarization of light, there are three bases in which one can encode one bit of information (see . Fig. 18.4 ). These are the horizontal and vertical (H/V) basis, the diagonal and anti-diagonal (D/A) basis, and the left- and right-circular (L/R) basis. One can encode a bit in the H/V basis by considering 0 to be horizontal polarization and 1 to be vertical polarization. If a photon encoded in either H or V polarization is measured in any of the other two bases, its information cannot be extracted. For example, in the case of measurements made in the D/A basis, in 50 % of the cases, a diagonally polarized photon will be observed; in the other cases, the photon will be measured as anti-diagonally polarized. This property is the main ingredient for quantum cryptography, as we will see later. Furthermore, in higher-dimensional systems, fundamental properties of mutually unbiased bases are still open questions that are signiï¬cant for quantum communication."
372,998,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"Fig. 9.8 Probability that one or more samples of the fringe amplitude will exceed the value Zm =) in the absence of a signal, as given by Eq. (9.70). The curves are labeled by the number of samples measured."
297,587,0.983,The R Book,"Histograms are excellent for showing the mode, the spread and the symmetry (skew) of a set of data, but the R function hist is deceptively simple. Here is a histogram of 1000 random integers drawn from a Poisson distribution with a mean of 1.7. With the default âprettyâ scaling to produce eight bars, the histogram produces a graphic that does not clearly distinguish between the zeros and the ones:"
71,162,0.983,Advancing Culture of Living with Landslides: Volume 1 ISDR-ICL Sendai Partnerships 2015-2025 (Volume 1.0),"Sensitivity Analysis Yavari-Ramshe and Ataie-Ashtiani (2015) performed a series of sensitivity analysis on the effects of landslide constitutive structure and landslide rigidity on both the sliding mass deformations and the water surface fluctuations. The authors demonstrated the importance of landslide rheological behaviour, constitutive structure and deformations on LGW characteristics based on the comparison of 2LCMFlow numerical results with two sets of experimental data on SMLs (Ataie-Ashtiani and Najaï¬-Jilani 2008) and SALs (Ataie-Ashtiani and Nik-Khah 2008). The key results of their simulations are schematically illustrated in Figs. 10 and The 2LCMFlow model is able to simulate the interactions between water and a variety of the sliding material from dense and dry material to loose and saturated masses (Yavari-Ramshe and Ataie-Ashtiani 2015). This ability is based on applying different values of landslide constitutive parameters, k1 and k2 , in Eq. (7). As it can be observed Fig. 10, dense material deforms into a thick front and thin tail proï¬le and initiates a LGW consist of a larger wave crest"
38,435,0.983,The GEO Handbook on Biodiversity Observation Networks,"Unlike many structural and functional attributes at the ecosystem level, most biological entities at the species and genetic levels of biodiversity cannot be readily detected through remote sensing. Notable exceptions include the emerging use of very high spatial resolution imagery to identify individual organisms of certain large-bodied, and conspicuous, animal species (e.g., penguins; Fretwell et al. 2012), and the use of hyperspectral sensors to detect variation in plant species composition in the top layer of vegetation communities (Leutner et al. 2012). These developments offer considerable potential for direct derivation of spatially-complete mapping of temporal change from remote sensing, for at least a subset of biological entities. However this still leaves a very large proportion of our planetâs biological diversity that is effectively invisible to satellite-borne remote sensing, both at the species level and, even more so, at the genetic level. In situ monitoring of change in these components of diversity at selected locations may provide all the information that is needed for some applicationsâe.g., for monitoring the performance of local-scale management actions (Lindenmayer et al. 2012). Estimating change across large spatial extentsâe.g., across a whole ecoregion, country or continent, or across the entire planetâposes a much greater challenge for in situ monitoring, particularly if these changes need to be mapped at relatively ï¬ne spatial resolution across the entire extent of interest (Ferrier 2011; Jetz et al. 2012; Pereira and Cooper 2006). We here"
275,59,0.983,Foundations of Trusted Autonomy,"The CTW distribution is tightly related to the Solomonoff-Hutter distribution (2.1), the primary difference being the replacing of computer programs with context trees. Naively computing CTW (et | Ã¦<t at ) takes double-exponential time. However, the CTW algorithm [86] can compute the prediction CTW (et | Ã¦<t at ) in O(D) time. That is, for fixed D, it is a constant-time operation to compute the probability of a next percept for the current history. This should be compared with the infinite computational resources required to compute the Solomonoff-Hutter distribution M. Despite its computational efficiency, the CTW distribution manages to make a weighted prediction based on all context trees within the maximum depth D. The relative weighting between different context trees changes as the history grows, reflecting the success and failure of different context trees to accurately predict the next percept. In the beginning, the shallower trees will have most of the weight due to their shorter code length. Later on, when the benefit of using longer contexts start to pay off due to the greater availability of data, the deeper trees will gradually gain an advantage, and absorb most of the weight from the shorter trees. Note that CTW handles partially observable environments, a notoriously hard problem in AI. MC-AIXI-CTW. Combining the MCTS algorithm for planning with the CTW approximation for induction yields the MC-AIXI-CTW agent. Since it is history based, MC-AIXI-CTW handles hidden states gracefully (as long as long-term dependencies are not too important). The MC-AIXI-CTW agent can run on a standard desktop computer, and achieves impressive practical performance. For example, MC-AIXI-CTW can learn to play Rock Paper Scissors, TicTacToe, Kuhn Poker,"
77,251,0.983,A Life Course Perspective on Health Trajectories and Transitions (Volume 4.0),"In the end, the analyst will obtain, for each individual j, a value for: (a) the intercept  0 , which defines the predicted value of Y when age D 0; (b) the slope  1 , which defines the predicted change (either positive for growth or negative for decline) of Y for each unit change (e.g., one year, one month) in age; (c) the standard error of the estimate, which is closely related to the variability of the errors Ei at each time point and which defines the quality of the overall age prediction of Y. Note that to interpret the intercept, it is customary to center age around a meaningful value, such as the average of the sample (by subtracting from each individualâs age the sample average age). The intercept is then the predicted Y value for an individual"
45,206,0.983,Measurement and Control of Charged Particle Beams,"If the number of correctors equals the number of BPMs, then M is a square matrix. In this case, (3.17) reduces to simply Î¸ = M â1 x. Otherwise the general form is taken. If m > n the matrix is overdetermined. For m < n the number of unknown corrector strengths exceeds the number of measurements, so that an independent measurement should be made after changing some parameter, for example, the beam energy. In a linear accelerator, (3.12) must be modified [17] to include the energy scaling factor Ei /Ej , which reflects the transverse damping due to the longitudinal acceleration. This introduces n additional unknown variables, so that additional measurements are required to constrain the solution. As motivation for the algorithms to be used below in the discussion of beam-based alignment and dispersion-free steering, the solution (3.17) can be equivalently formulated in terms of a minimization procedure, which is well adapted to computational evaluation. The function to be minimized, given by (3.13), is xj â Mji Î¸i , (3.18)"
97,49,0.983,"Nature-Based Solutions To Climate Change Adaptation in Urban Areas : Linkages Between Science, Policy and Practice","Urban temperatures can be strategically handled through a network of planned urban green space. This includes the selection of appropriate surfaces, their spatial organisation and management. Studies have shown that urban parks have a cooling effect in the range of 1 Â°C during the daytime, with indications that larger parks have a larger effect as well as systems including trees (Bowler et al. 2010). The surface type will also influence the cooling effect of the blue or green infrastructure. For instance, surface temperatures of water is lower compared to vegetated areas which in turn are markedly cooler than streets and roofs (Leuzinger et al. 2010). This means that there is a larger cooling effect per unit surface water as compared to a vegetated park system (Å½uvelaAloise et al. 2016). This effect varies with time of the day, with largest differences between park and water bodies during daytime. Several studies therefore suggest that in order to maximise the use of space for urban cooling more focus should be placed on inclusion of water bodies as well as concentrating these surfaces in the city centres as compared to an alternative approach with smaller parks distributed over the city in general (Å½uvela-Aloise et al. 2016; Skoulika et al. 2014). There is also a substantial seasonality in the effect of urban vegetation, with stronger effects in summer than early spring. While these broad differences in cooling occur, there"
349,279,0.983,Methods for Measuring Greenhouse Gas Balances and Evaluating Mitigation Options in Smallholder Agriculture,"where: Ï is the SOC stocks variance yi represents each calculated SOC stock in that stratum Î¼st is mean SOC stock associated with the stratum st n is the number of observations in that stratum The average SOC stock for the area of study (landscape) is calculated considering both the mean SOC stock obtained for each stratum and the area occupied by each stratum. Therefore, the calculation is as follows:"
118,181,0.983,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"the dose to become below 20 mSv/year is longer than that, but artificial decontamination can effectively shorten the time, particularly if the natural dispersion is slow (0.05 yearâ1). The results of actual measurement shown in Fig. 4.8 are consistent with this observation. In Fig. 4.8, it is observed that the dose rate comparison between September 18, 2011 (Fig. 4.8a) and September 28, 2013 (Fig. 4.8b) shows that the yellow zone, which corresponds to 1,000â3,000 kBq/ m2 initial contamination, actually decreased to below 20 mSv/year, as indicated by the arrow in the figure. Similarly, the red zone shrank while the orange zone increased."
202,133,0.983,"Security of Networks and Services in an All-Connected World: 11th IFIP WG 6.6 International Conference on Autonomous Infrastructure, Management, and Security, AIMS 2017, Zurich, Switzerland, July 10-13, 2017, Proceedings","traffic rate (rrel ), which can be higher than 100% if the function replicates or encodes traffic. Dependencies, in turn, may be of two different types: between a VNF and an outgoing link, or between two distinct VNFs. Outgoing link dependencies represent VNFs that should selectively be placed on one of the sub-flows (e.g., a firewall that succeeds an anomaly-based IDS only for suspicious traffic). Dependencies between VNFs, on the other hand, indicate that the dependent function must be present in each and every sub-flow in the chain (e.g., the cache servers succeeding a load balancer). In Fig. 1(b), we represent two possible chains for the service described in Fig. 1(a). Notice that VNFs 2 and 3 selectively appear in the sub-flows of VNF 1, while VNF 4 is present in both subpaths. Moreover, bandwidth and processing demands are determined according to the relative traffic of each outgoing link. Although structurally similar, the left chain (i.e., VNF-FG 1) requires less network and computing resources, which at the end results in lower costs for both clients and providers. As an outcome of the SFC composition problem, VNF-FG 1 would be sent for embedding in the provider infrastructure. Table 1 details the notation used in the model proposed in the following section. The first part of the table introduces the parameters of a Network Service Request. The second part explains the sets of nodes of an augmented graph used to build the ILP model. Finally, the table shows the ILP variables. Before jumping into the next section, it is worth mentioning that the outcome of the chain composition stage is one complete service chain (VNF-FG) with regard to one predefined objective and that the amount of required capacities depends on the amount of data handled by that VNF instance."
311,467,0.983,The Physics of the B Factories,"It is important to understand the Ît resolution in detail as this is of a similar magnitude to the average separation between the BCP and Btag proper decay times. Thus, this resolution has a signiï¬cant eï¬ect on the extraction of S and C from a time-dependent analysis. Diï¬erent approaches are used to understand resolution eï¬ects at the B Factories. BABAR adopts a parametric approach to describe the Ît resolution, whereas Belle characterizes resolution eï¬ects according to their physical source. Both approaches work well and provide a good dePhys scription of resolution for use in time-dependent analyses. Figure 10.4.1 shows the fÂ± and FÂ±Phys distributions The nominal BABAR Ît resolution function has a triple for S = 0.7 and C = 0.0, where both dilution and resPhys Gaussian form, where the mean Î¼i and width si of the olution eï¬ects are considered. The distribution fÂ± two central Gaussian components are scaled by Ïât on an smeared out considerably as a result of experimental resoevent-by-event basis. The three Gaussians are denoted by lution when computing F Phys . The eï¬ect of dilution serves Gi , where i = core, tail, and outlier, in order of increasing to reduce the reconstructed asymmetry between B 0 - and width. The resolution function is given by B 0 -tagged events. This can be seen as a reduction in the asymmetry between F+ and Fâ in comparison with the Rsig (Î´t, Ïât ) = fcore Gcore (Î´t, Î¼core Ïât , score Ïât ) + true distributions f+ and fâ . ftail Gtail (Î´t, Î¼tail Ïât , stail Ïât ) + foutlier Goutlier (Î´t, Î¼outlier , soutlier ) . (10.4.2)"
151,188,0.983,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"Distribution of CPY across the U.S. Intensity of use was a factor in the selection of watersheds. County sales data (Fig. 1) cannot be used to determine the precise location of product use, but it does provide a general indication of the spatial distribution of CPY use. Areas in the U.S. with the greatest intensity of use from 2010 to 2011 (depicted in blue in Fig. 1, see SI for color map) include Kern, Tulare, Santa Cruz, Fresno counties in central California; Lancaster County in southeastern Pennsylvania; and Calhoun, Decatur, and Mitchell counties in southeast Georgia. Other areas of high use at a lesser density are depicted by the green shading in Fig. 1 (see SI for color map). National vulnerability assessment of CPY runoff. A national vulnerability assessment was conducted to characterize the potential for CPY to be transported beyond a treated field in runoff water and eroded sediment throughout the conterminous United States. The assessment involved use of the National Pesticide Tool (NPAT) to simulate the relative runoff potential of CPY for all major agricultural soil types."
302,132,0.983,Freshwater Microplastics : Emerging Environmental Contaminants?,"likely due to the combined contribution of both wastewater and runoff. The levels observed in CSOs are higher than the levels in strict separated runoff. For two of the three samples, concentrations were higher than the concentrations in wastewater. A settlement of particles during dry weather periods and their re-suspension when the flow increases was described on the literature [22]. A similar behavior could be expected for the fibers. The level of fibers also depends on the previous rain events: the first sampling was conducted after a long period of heavy rainfall, which might have induced a decrease in the amount of fibers in the sewer system. Levels of fragments vary between 35 and 3,100 fragments L 1. The levels are especially high even if they vary by two orders of magnitude. Lower concentrations of fragments in comparison to fibers can be observed except for the event presenting the highest runoff contribution and volume."
105,55,0.983,"Computer and information Sciences : 31St international Symposium, Iscis 2016, KrakÃ³w, Poland, October 27â28, 2016, Proceedings (Volume 659.0)","This study extended and analysed the previous study in [7], applying the Taguchi experimental design method to obtain the best parameter settings with different run-time budgets. We trained the system using 2 instances from 6 and 9 domains separately and tracked the effects of each parameter level over time. The experimental results show that good values for three of the parameters are relatively easy to predict, but the performance is less sensitive to the value of the fourth (DoS), with different values doing well for different instances and very similar,"
95,383,0.983,Elements of Robotics,"Since averaging is an integrating operator that removes abrupt changes in intensities, it is not surprising that the differential operator can be used to detect abrupt changes that represent edges. Figure 12.8b plots the intensity against the row number along a single column of Fig. 12.8a, although the intensities are shown as lines instead of as discrete points. The intensity doesnât change for the first three pixels, then it rapidly increases and continues at the higher level. The first derivative f â² of a function f is zero when f is constant, positive when f increases and negative when f decreases. This is shown in Fig. 12.9a. An edge can be detected by searching for a rapid increase or decrease of the first derivative of the image intensity. In practice, it is better to use the second derivative. Figure 12.9b shows a plot of f â²â² , the derivative of f â² in Fig. 12.9a. The positive spike followed by the negative spike indicates a transition from dark to light; if the transition were from light to dark, the negative spike would precede the positive spike. There are many digital derivative operators. A simple but effective one is the Sobel filter. There are two filters, one for detecting horizontal edges (on the left) and other for detecting vertical edges (on the right): â1 â2 â1 ï£° 0 0 0ï£» 1 2 1"
82,400,0.983,Fading Foundations : Probability and The Regress Problem,"This example is nonuniform (i.e. the conditional probabilities, Î±n and Î²n , are not the same for different n), and it is in the usual class. It is shown in (A.18) in Appendix A.5 that Eq.(8.1) reduces to P(q) ="
359,113,0.983,"Micro-, Meso- and Macro-Dynamics of the Brain","Fig. 6 Head direction representational density increases along the dorsoventral axis in MEC layer 3. Each doughnut represents a head direction cell population, and each cell is represented as a circle on the doughnut. The location and size of the circle represent preferred head direction and tuning specificity, respectively. Given populations of equal size (same number of rings; dorsal to ventral as left to right), and the same directional input, ventral populations will have a larger proportion (P) of its cell population be active to any input compared to more dorsal populations due to broader tuning (color gradient shows each cellâs activity level; red is maximum)"
199,79,0.983,Synchronized Factories : Latin America and the Caribbean in the Era of Global Value Chains,"Indeed, this test addresses the potential causality issue mentioned earlier. In particular, the test implies examining a cross-country, cross-sector interaction effect, the so-called difference-indifferences estimation. The estimation seeks to alleviate the potential endogeneity problem associated with cross-country regressions. The difference-in-differences estimator would suffer from reverse causality if the FDI flow of a given sector compared to those of other sectors had a causal effect on the overall level of logistics infrastructure. This seems much less likely to be the case than in the more common cross-country regressions, in which total FDI flows could have a causal effect on the overall level of logistics infrastructure investment. In particular, Appendix B âSpecification for the Model of Vertical FDI and Logistics Infrastructureâ shows that the main finding holds under least squares and negative binomial estimations, as well as after the inclusion of parent, subsidiary, and sector-fixed effects, and under the more stringent parent-subsidiary, sector-fixed effects. In a longer version of this analysis we also show that the results are sufficiently robust to explicitly include additional covariates in the model; see Blyde and Molina (2013)."
349,254,0.983,Methods for Measuring Greenhouse Gas Balances and Evaluating Mitigation Options in Smallholder Agriculture,"6. Annual changes in C stock for each transition by dividing changes in C stock by the length of the study period (expressed in Mg CO2 equivalent haâ1) 7. Total annual emission and total sequestration and net changes of C stock in the landscape (expressed in Mg CO2 equivalent haâ1) Because the principal scaling approach relies on similarity-based relationships (e.g., allometric equations) that are scale invariant, the same steps are equally relevant for whole-farms or landscapes, irrespective of the spatial extent. Furthermore, since the results are expressed in CO2 equivalent haâ1 it is possible to integrate these measures with those from other GHG sources and sinks such as soil carbon or trace gas emissions from soils."
26,45,0.983,Cognitive Supervision for Robot-Assisted Minimally Invasive Laser Surgery,"how matter affects the propagation of a laser beam. When laser light is incident on a material, three basic effects can be observed, as illustrated in Fig. 2.7: â¢ Transmission refers to the undisturbed propagation of light through the material. The material is said to be transparent: light travels through it without any attenuation, maintaining the same direction of propagation. The energy density entering the medium equals the one that escapes it. â¢ Attenuation occurs when the energy associated with the laser beam is lost within the material volume. The propagation of the beam through the material results in an attenuation of its energy density, either because energy is absorbed by the material or it is dispersed into it. Basic attenuation mechanisms are absorption and scattering [5]. â¢ Reflection and Refraction occur when light crosses the boundary between materials with different optical properties. A fraction of the incident wave is returned from the surface of the material (reflection), while the remaining part propagates into it (refraction). Refraction is usually associated with a change in the speed and direction of propagation. Reflection and refraction are strongly related to each other by the Fresnelâs Equations [8]. In general, these effects might occur simultaneously in a combined fashion, i.e. when light impinges on the surface of a material part of it is reflected, part is transmitted and part is either absorbed or dispersed into the material. The proportions of light which is transmitted, returned or dispersed are dictated by the incident wavelength and the optical properties of the material. The principle of energy conservation holds: the sum of the energies associated with each of these interactions gives the total amount of incident energy. In the scope of this doctoral thesis, we shall focus on a specific set of laser-matter interactions, in which the target material consists of biological tissue. Most relevant laser-tissue interactions are induced by the absorption of laser energy within the tissue volume. Among these, thermal interactions play a crucial role in laser surgery, as we will find later on. In the next section, we present the fundamentals of lasertissue interaction mechanisms, with particular focus on the effects that laser radiation can produce on tissues."
289,1335,0.983,"Programming Languages and Systems : 27th European Symposium on Programming, ESOP 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings","benefit is inconsistent and very small. And TD+BU does not seem to balance the precision/performance tradeoff particularly well. Precise Object Representation Often Helps with Precision at a Modest Cost to Performance. Figure 3 shows a representative sample of scatter plots illustrating the tradeoff between ALLO, CLAS, and SMUS. In general, we see that the highest points tend to be ALLO, and these are more to the right of CLAS and SMUS. On the other hand, the precision gain of ALLO tends to be modest, and these usually occur (examining individual runs) when combining with AP+SO. However, summary objects and ALLO together greatly increase the risk of timeouts and low performance. For example, for eclipse the row of circles across the bottom are all SO-only. The Precision Gains of POLY are More Modest than Gains Due to Using AP+SO (over AP). Figure 4 shows scatter plots comparing INT and POLY. We investigated several groupings in more detail and found an interesting interaction between the numeric domain and the heap abstraction: POLY is often better than INT for AP (only). For example, the points in the upper left of bloat use AP, and POLY is slightly better than INT. The same phenomenon occurs in luindex in the cluster of triangles and circles to the upper left. But INT does better further up and to the right in luindex. This is because these configurations use AP+SO, which times out when POLY is enabled. A similar phenomenon occurs for the two points in the upper right of pmd, and the most precise points for hsqldb. Indeed, when a configuration with AP+SO-INT terminates, it will be more precise than those with AP-POLY, but is likely slower. We manually inspected the cases where AP+SO-INT is more precise than AP-POLY, and found that it mostly is because of the limitation that access paths are dropped through method calls. AP+SO rarely terminates when coupled with POLY because of the very large number of dimensions added by summary objects."
311,318,0.983,The Physics of the B Factories,"One requires knowledge of the B-meson four-momentum, pB , to solve this equation. The direction of the motion of the B is not known, but itâs momentum is suï¬ciently +2pB pÏf cos Î¸BÏf + 2pB pÏs cos Î¸BÏf cos Î¸f s small (on average 0.34 GeV/c) compared to the typical (7.3.4) values of the magnitudes of lepton and Dâ momenta so +2pB pÏs sin Î¸BÏf sin Î¸f s cos Ï. that the three-momentum of the B meson can be set to The Ï-dependent missing mass is then calculated as, m(Ï) = zero. The neutrino invariant mass can then be computed p2D0 (Ï). The value of Ï is not constrained by kinemat- as ics and may be chosen arbitrarily: BABAR deï¬nes in Aus MÎ½ = â ED â Eâ â (pDâ + pâ ) , (7.3.8) bert (2004p) the missing mass for partially reconstructed B 0 â Dââ Ï + decays to be mmiss = 21 [mmax + mmin ], where mmax and mmin are the maximum and minimum where the energy of the B meson is taken to be half of the values of m(Ï), while in analysis of partially reconstructed CM energy. Figure 7.3.3 shows the distribution of partially B 0 â Dâ+ Dââ decays BABAR chooses the value for which reconstructed B 0 â Dââ â+ Î½â decays (Ïs â combinations) cos Ï = 0.62, which is the median of the correspond- from Aubert (2006s). The signal events produce a promiing Monte Carlo distribution for signal events obtained nent peak at MÎ½2 â 0 with spread around 0.850 GeV2/c4 using generated momenta, and deï¬nes the missing mass while background events are distributed in a wide range, mmiss = mmiss (cos Ï = 0.62) (Lees, 2012k). For signal can- dropping sharply to zero where there is a lack of phase didates, the mmiss variable peaks at the nominal D0 mass space. mD0 , with a spread of about 3 MeV/c2 , while the background is smoothly distributed, dropping oï¬ just above the D mass due to lack of phase space. The distribution 7.4 Recoil B-meson reconstruction of mmiss for partially reconstructed B 0 â Dââ Ï + decays B-meson decays to a ï¬nal state with one or more neuis shown in Fig. 7.3.2 (Aubert, 2004p). trinos oï¬er very little or even no kinematic constraints which are usually exploited in B decay searches in order 7.3.2 B â D âÂ± âÎ½â decays to distinguish these decays from continuum and BB backgrounds, as described in Sections 7.1 and 7.3 and Chapter The partial reconstruction technique of semileptonic 9. Prominent examples of such decays are: p2D0 (Ï) = m2B + (pÏf + pÏs )2 â 2EB (EÏf + EÏs )"
372,685,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"7.2.6 Phase-Locked Loops and Reference Frequencies Some practical points in the implementation of LO systems should be briefly mentioned. In two of the schemes described above, an oscillator at the antenna is controlled by a phase-locked loop. Details of the design of phase-locked loops are given, for example, by Gardner (1979), and here we mention only the choice of the natural frequency of the loop. Unless the natural frequency is about an order of magnitude less than the frequency at the inputs of the phase detector, the loop response may be fast enough to introduce undesirable phase modulation at the phase detector frequency. In the system in Fig. 7.5, the frequency of the input signals to the phase detector is the offset frequency !1 "" !2 , an upper limit on which has been placed by consideration of the reflections in the line. Also, the bandwidth of the noise to which the loop responds is proportional to the natural frequency. These considerations place an upper limit on the natural frequency of the loop, which in turn limits the choice of the oscillator to be locked. An oscillator with inherently poor phase stability (when unlocked) requires a loop with a higher natural frequency than does a more stable oscillator. Crystal-controlled oscillators are highly stable and require loop natural frequencies of only a few hertz. They are especially suitable for long transmission lines because the noise bandwidth of the loop is correspondingly small. With crystal-controlled oscillators at the antennas, it is possible to send out the reference frequency in bursts, rather than continuously. Signals traveling in opposite directions can then be separated by time multiplexing, and no frequency offset is required. However, the change in impedance of the circuits at the ends of the cable when the direction of the signal is reversed could become a limiting factor in the accuracy of the round-trip phase measurement. Systems of this type have been designed for several large arrays (Thompson et al. 1980; Davies et al. 1980). In addition to the establishment of a phase-locked oscillator at each antenna at a reference frequency (equal to ! in Fig. 7.4, !2 in Fig. 7.5, and !1 C !2 in"
65,495,0.983,Handbook of Ocean Wave Energy,"Part 1: Identiï¬cation and characterisation of the system â Stability and GM centre of mass and Identify hydrostatic and hydrodynamic buoyance etc response of the WEC â Asses the natural period of oscillations and decay response â Asses the hydrodynamic response in regular waves â In regular waves asses the required range of Identify the sensitivity and range of the PTO the PTO and sensors, and adapt them and sensors accordingly to the maximum Identify the influence of physical parameters, Perform elementary regular wave batches which are intended to be assessed in depth in (0.5â2 min tests with increasing wave period), irregular waves later while changing one variable at the time between batches. This could give a good feeling on the importance of the parameter â¨ Possibly make adjustments on design, sensors and/or scaling ratio, as now the natural period of oscillation and the range of usage of the sensors and PTO have been identiï¬ed Part 2: actual test campaign Irregular waves on reference setup â Optimise PTO load for each sea state â Duration of tests is 1000 waves (relative to Tp) Optimisation of the design â Asses the influence of alterations to the design, in order to optimise its power performance, hydrodynamic behaviour, â¦. Start with tests in sea state 2 and 3, as these contribute the most to the MAEP â The PTO load needs to be optimised for each sea state â These tests will also provide the ï¬nal RAOâs Asses the influence of additional physical â e.g. mooring conï¬guration, water depth, parameters oblique waves â¦ â The PTO load needs to be optimised for each sea state Asses the influence of additional â This is of importance when later trying to estimate the performance of the device for environmental parameters different wave conditions, e.g. water depth, oblique waves, 3D waves, â¦ â The PTO load needs to be optimised for each sea state â¨ The data of each test should be processed after each individual test in order to be able to compare the results with the one of previous tests"
372,1732,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"14.2 Scattering Caused by Plasma Irregularities Understanding the propagation of radiation in a random medium is an important problem in many fields. The signals from cosmic radio sources propagate through several ionized random media, including the ionized interstellar gas of our Galaxy, the solar wind, and the ionosphere. In the observerâs plane, there are two effects. First, the amplitude varies with the position of the observer, which leads to temporal amplitude variations if there are relative motions among the source, scattering medium, and observer. Second, the image of the source is also distorted in a frequency-dependent manner. Much of the research in this area has been motivated by the attempt to understand the observational characteristics of pulsars [see, e.g., Gupta (2000)]. Propagation effects in the turbulent troposphere are described in Chap. 13."
256,227,0.983,"Supercomputing Frontiers : 4Th Asian Conference, Scfa 2018, Singapore, March 26-29, 2018, Proceedings","Abstract. A real-time simulation of the environmental dynamics of radioactive substances is very important from the viewpoint of nuclear security. Since airflows in large cities are turbulent with Reynolds numbers of several million, large-scale CFD simulations are needed. We developed a CFD code based on the adaptive mesh-reï¬ned Lattice Boltzmann Method (AMR-LBM). AMR method arranges ï¬ne grids in a necessary region, so that we can realize a high-resolution analysis including a global simulation area. The code is developed on the GPU-rich supercomputer TSUBAME3.0 at the Tokyo Tech, and the GPU kernel functions are tuned to achieve high performance on the Pascal GPU architecture. The code is validated against a wind tunnel experiment which was released from the National Institute of Advanced Industrial Science and Technology in Japan Thanks to the AMR method, the total number of grid points is reduced to less than 10% compared to the ï¬ne uniform grid system. The performances of weak scaling from 1 nodes to 36 nodes are examined. The GPUs (NVIDIA TESLA P100) achieved more than 10 times higher node performance than that of CPUs (Broadwell). Keywords: High performance computing  GPU  Lattice boltzmann method Adaptive mesh reï¬nement  Real-time wind simulation"
231,1562,0.983,North Sea Region Climate Change Assessment,"affected by their boundary conditions, that is, the GCMs that are being downscaled. In a way, GCM uncertainty could be likened to emission uncertainty in the sense that a particular RCM projection is conditional to the choice of the emission scenario and the boundary conditions. The latter comprise large-scale inflow and outflow (winds, temperature, humidity) into and from the regional domain, from the driving GCM. RCMs can also be provided with boundary conditions from global reanalyses (e.g. Dee et al. 2011), which is often the case in model evaluation studies as comparison with actual observations is more straightforward than in the case of runs with boundary conditions from GCMs. A key motivation of RCMs is that they facilitate simulations at higher resolution. Today, RCMs are starting to provide climate simulations at resolutions of 1â10 km, compared to around 25 km some 5â10 years ago, and 50 km or more some 10â15 years ago."
311,2287,0.983,The Physics of the B Factories,"where the error includes statistical and systematic uncertainties.142 The main contributions to the systematic uncertainty on AÎ arise from similar sources as in the yCP measurements, from possible biases in the acceptance dependence on the decay time and the assumption of an equal mean of the resolution function in both decay modes used (see Section 19.2.3.1). From the AÎ value one can conclude there is currently no significant sign of CP violation at the sensitivity level of 0.25%. It should be noted that the decay time integrated CP violating asymmetry, as defined in Eq. (19.2.63) of Section 19.2.6, also receives contributions from all the types of CP violation, and the CP violation in decay (AfD ) appears in the linear order. Upon the summation of Eq. (19.2.65) and Eq. (19.2.95) we get CP + AÎ = AD"
362,456,0.983,Cloud-Based Benchmarking of Medical Image Analysis,"where each element is the probability of the latent topic given this case. The similarity between different cases is then measured by the Euclidean distance between the probability vectors, followed by the k-NN method for retrieval as introduced in Sect. 14.2.1. During the experiments, we empirically fixed the number of latent topics to 20, i.e. H = 20."
311,2871,0.983,The Physics of the B Factories,"23.3.3 Cabibbo favored decays Bs0 â Ds(â)+ Ds(â)â Belle has also used Î¥ (5S) data to measure Bs0 â (â)+ (â)â Ds Ds decays. An initial study was done using 23.6 fbâ1 of data (Esen, 2010), while a subsequent study uses the full 121.4 fbâ1 data set (Esen, 2013). The fi+ nal states reconstructed consist of D(s) Dsâ , Dsâ+ Dsâ + ââ + âÂ± â â+ ââ Ds D(s) (â¡ Ds Ds ), and Ds Ds . These are expected to be mostly CP -even, and their partial widths are expected to dominate the diï¬erence in widths between the two Bs0 CP eigenstates, ÎÎCP (Aleksan, Le Yaouanc, Oliver, PeÌne, and Raynal, 1993). This parameter is equal to ÎÎs / cos Ï12 , where ÎÎs is the decay width diï¬erence between the mass eigenstates, and Ï12 is the CP -"
231,1179,0.983,North Sea Region Climate Change Assessment,"French scientists from IFREMER have used a delta GAM/GLM approach to model future plaice and red mullet distribution in the eastern English Channel and southern North Sea (see Vaz and Loots 2009). Abundance of each species was related to depth, sediment type, bottom salinity and temperature. Results suggested that climate change may strongly affect the future distribution of plaice. For large plaice (>18 cm), distribution will still be centred in the southern part of the North Sea, however for young individuals, the predicted distribution is anticipated to shift north-westwards and to the Dogger Bank area in particular (as has already been observed, see van Keeken et al. 2007; Engelhard et al. 2011). Model outputs indicate that the distribution of red mullet will not change dramatically but that for young individuals (deï¬ned as <17.3 cm), the offshore habitat situated on the Dogger Bank may become increasingly favourable. Older individuals seemed little affected by the simulated change in environment, but they may beneï¬t from higher juvenile survival and expand their area of occupation as a result. There are some concerns about the validity of the bioclimate envelope approach for predicting the future distribution of commercially important ï¬sh species (see Jennings and Brander 2010; Heath et al. 2012). First, it may not be possible to assess temperature preferences from current distributions because the observed distributions are modiï¬ed by abundance, habitat, predator and prey abundance and competition. Second, there may be barriers to dispersal (although this is typically less of an issue in the sea than on land) and species will move at different rates and encounter different local ecologies as temperature changes (Davis et al. 1998). A more detailed, physiologically-based approach has been taken by some authors, whereby the detailed dynamics of individual animals are modelled, often by linking complex biophysical models (forced with the output from Global Climate Models) to sub-routines which replicate the behaviour/characteristics of eggs, larvae, juveniles or adults. Teal et al. (2008) reported a study of plaice and sole distribution in the North Sea, in which they predicted size- and season-speciï¬c ï¬sh distributions based on the physiology of the species, temperature and food conditions in the sea. This study combined state-of-the-art dynamic energy budget (DEB) models with output from a biogeochemical ecosystem model (ERSEM) forced with observed climatic data for 1989 and 2002, with contrasting temperature and food conditions. The resulting habitat quality maps were in broad agreement with observed ontogenetic and seasonal changes in distribution as well as with long-term observed changes in distribution. The technique has recently been extended to provide future projections up to year 2050, assuming moderate climate warming (L. Teal, pers. comm. IMARES, Netherlands)."
70,584,0.983,Optics in Our Time,"the eyeâs pupil, optical aberrations, and intraocular scattering. Diffraction blurs the images formed through instruments with a limited aperture due to the wave nature of the light. The effect of diffraction is usually small and only can be noticed with small pupils. The impact of the ocular aberrations in the eyeâs image quality is more signiï¬cant for larger pupil diameters. The pupil of the eye varies diameter from around 2â8 mm in diameter. This corresponds approximately to an aperture range from f/8 to f/2, values which can be compared with the typical values in a camera objective. . Figure 12.5 shows an example of realistic retinal images of letters for the same eye for small (3 mm) and a larger (7 mm) pupil. Note how aberration degrades the image for larger pupils. The amount of aberrations for a normal eye with about 5 mm pupil diameter (f/4 aperture) is approximately equivalent to less than 0.25 D of defocus, a small error typically not corrected when dealing in the clinic with refractive errors. The particular shapes of the eyeâs lenses, refractive index distribution, and particular geometry are responsible for the limited optical quality of the eye compared with artiï¬cial optical systems. A normal eye has at least six times lower quality than a good (diffraction-limited) artiï¬cial optical system. Each eye produces a peculiar retinal image depending on the optical aberrations present. This can be demonstrated by how a point source is projected in the retina. For example, the shape of stars would depend on our image quality. . Figure 12.6"
107,75,0.983,"Symbiotic interaction : 5Th international Workshop, Symbiotic 2016, Padua, Italy, September 29â30, 2016, Revised Selected Papers","(i.e., non-virtual) hand, resting on the arrow keys of a PC keyboard. The questionnaires used 5-point Likert-style scales, while the emotion classiï¬cation used a three-alternative forced choice. Implicit Measures. The usersâ electrocardiography (ECG), EDA and right-handed biaxial accelerometer (ACC) were recorded using a Brain Products QuickAmp biosignal ampliï¬er at a sample rate of 1,000 Hz (Fig. 5). ACC data were transformed to movement speed by taking the root mean square of the ï¬rst derivative over both axes (i.e., the Euclidean distance traveled per ms). Physiological sensors to measure EDA were placed on the index and middle ï¬ngers of the left hand, and to measure ECG were placed on the chest. EDA commonly is used as an emotional arousal indicator [31] or to describe the intensity of an experience. Electronically, it represents the potential diï¬erence between two areas of the skin. Two key measurements of the EDA are tonic (low frequency baseline conductivity level indicates slow changes in arousal) and phasic (higher frequency indicates rapid changes in arousal). The average, raw EDA was taken as a tonic measure of autonomic activity. Furthermore, the measure was transformed to reï¬ect phasic arousal related to non-speciï¬c events [31] by applying a continuous, local, positive peak (of at least 0.5 Î¼S) detection algorithm and calculating the rate of skinconductivity responses per second. Finally, the latency (in ms) between heartbeats, or inter-beat interval (IBI), was used to calculate the average IBI (inversely related to heart rate)."
297,885,0.983,The R Book,"A great deal of statistical information comes in the form of counts (whole numbers or integers): the number of animals that died, the number of branches on a tree, the number of days of frost, the number of companies that failed, the number of patients who died. With count data, the number 0 is often the value of a response variable (consider, for example, what a 0 would mean in the context of the examples just listed). The analysis of count data is discussed in more detail in Chapters 14 and 15. The dictionary deï¬nition of contingency is: âA possible or uncertain event on which other things depend or are conditionalâ (OED, 2012). In statistics, however, the contingencies are all the events that could possibly"
372,373,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"of the electronics and cannot be assumed to remain constant from one observing session to another. Making observations (i.e., measuring the coherency vector) of sources for which the polarization parameters are already known is clearly a way of determining the leakage and gain terms. The number of unknown parameters to be calibrated is proportional to the number of antennas, na , but the number of measurements is proportional to the number of baselines, na .na ! 1/=2. The unknown parameters are therefore usually overdetermined, and a least-meansquares solution may be the best procedure. For any antenna with orthogonally polarized receiving channels, there are seven degrees of freedom, that is, seven unknown quantities, that must be calibrated to allow full interpretation of the measured Stokes visibilities. This applies to the general case, and the number can be reduced if approximations are made for weak polarization or small instrumental polarization. In terms of the polarization ellipses, these unknowns can be regarded as the orientations and ellipticities of the two orthogonal feeds and the complex gains (amplitudes and phases) of the two receiving channels. When the outputs of two antennas are combined, only the differences in the instrumental phases are required, leaving seven degrees of freedom per antenna. Sault et al. (1996) make the same point from the consideration of the Jones matrix of an antenna, which contains four complex quantities. They also give a general result that illustrates the seven degrees of freedom or unknown terms. This expresses the relationship between the uncorrected (measured) Stokes visibilities (indicated by primes) and the true values of the Stokes visibilities, in terms of seven ' and Ä± terms: 32 3 Iv0 ! Iv 'CC 'C! Ä±C! !jÄ±!C 6Q0 ! Qv 7 6 'C! 'CC Ä±CC !jÄ±!! 7 6Qv 7 6 v 76 7 4U 0 ! Uv 5 D ! 2 4 Ä±C! ! Ä±CC 'CC j'!! 5 4Uv 5 : Vv0 ! Vv !jÄ±!C !jÄ±!! j'!! 'CC"
167,141,0.983,The Interconnected Arctic â UArctic Congress 2016,"Abstract Climate warming has been and will continue to be faster in the Arctic compared to the other domains of the world, which generates major challenges for human adaptation. Among others, the development of socio-economic infrastructure and strategic planning requires long-term projections of water availability and extreme hydrological events. In this context, it is preferable that the projections of river runoff should be performed statistically, allowing the evaluation of economical risks and costs for hydraulic structures, which are connected to changes in hydrological extremes. In this study, the hydrological model MARCS (MARcov Chan System) is suggested as a tool to simulate the parameters of probability density functions (PDFs) of maximal runoff or peak flow, based on climate projections of the Representative Concentration Pathways. Following that, the PDFs of the maximal runoff were constructed within the Pearson Type III distributions to estimate the runoff values of a small exceedance probability. To evaluate the risks and costs of a long-term investment based on the future projections of river maximal discharge of 1 % probability, simple calculations were performed for the new bridge over the Nadym River as an example."
285,879,0.983,"Physiology, Psychoacoustics and Cognition in Normal and Impaired Hearing","The rate profile for healthy band-suppressed IC model neurons has peaks at the formant frequencies (Fig. 4a, magenta). These cells respond more strongly at frequencies where the low-frequency fluctuations are reduced (Fig. 2d, orange arrows). The peaks in the rate profile at the formant frequencies are robust in the model hybrid population (Fig. 4a, green). This model result motivates further physiological and computational investigation of these cells. The hybrid neuron and model in Fig. 3 are most strongly driven by a contrast in neural fluctuations (i.e., strong neural fluctuations in the 8â30 Hz range and reduced fluctuations in the 30â200 Hz range); other MTF shapes are also observed for hybrid neurons (Krishna and Semple 2000). Model IC responses with impaired AN inputs (Fig. 4b) differ dramatically from the healthy responses. The band-enhanced rate profile (Fig. 4b, blue) has peaks approximately where the healthy model has valleys, and vice versa. The impaired model profile is explained by reduced synchrony capture: the impaired AN model synchronizes to F0 whenever the fibres are adequately driven by stimulus energy (see Fig. 2f). Thus the impaired band-enhanced rate profile reflects the energy spectrum of the vowel, unlike the healthy rate profile, which is inversely related to energy. The impaired band-suppressed (Fig. 4b, magenta) and hybrid (Fig. 4b, green) model responses also have peaks near F1 and F2. Unlike the healthy case, the inhibition from the shifted peaks in the impaired band-enhanced model suppresses the peaks in the band-suppressed and hybrid models. These results emphasize the fact that the strong rate profiles in the healthy IC model are not simply explained by the rate vs.BF profile of the AN. The healthy band-suppressed and hybrid profiles are created by a synchrony-to-rate transformation between the AN and IC and enhanced by disinhibition at the formant frequencies. In general, the rate profiles for all three response types are qualitatively different for this mildly impaired model due to differences in the F0-synchronized rate between the healthy and impaired AN models (Fig. 2d, f)."
70,339,0.983,Optics in Our Time,"The measurement of small distances is a fundamental problem of interest since the early days of science. It has become even more important due to recent interest in nanoscopic and mesoscopic phenomena in biophysics. Starting from the invention of the optical microscope around 400 years ago, todayâs optical microscopy methodologies can basically be divided into lens-based and lensless imaging. In general, far-ï¬eld imaging is lens-based and thus limited by criteria such as the Rayleigh diffraction limit which states that the achievable resolution in the focus plane is limited to approximately half of the wavelength of illuminating light. Further limitation arises from out-of-focus light, which affects the resolution in the direction perpendicular to the focal plane. Many methods have been suggested to break these limits. Lens-based techniques include confocal, nonlinear femtosecond, or stimulated emission depletion microscopy which have been recognized by the Nobel Prize. They have achieved remarkable ï¬rst results, as shown in . Fig. 7.1 . Also non-classical features such as entanglement, quantum interferometry, or multi-photon processes can be used to enhance resolution. However, there is still great interest in achieving nanometer distance measurements by using optical illuminating far-ï¬eld imaging only. Recently, new schemes were proposed [45â47] to measure the distance between two adjacent two-level systems by driving them with a standing wave laser ï¬eld and measuring the far-ï¬eld resonance ï¬uorescence spectrum, which is motivated by the localization of single atom inside a standing wave ï¬eld to distances smaller than the Rayleigh limit Î»/2. The basic idea is that in a standing wave, the effective driving ï¬eld strength depends on the position of the particles (. Fig. 7.22 ). Thus, each particle generates a sharp sideband peak in the spectrum, where the peak position directly relates to the subwavelength position of the particle. As long as the two sideband peaks can be distinguished from each other, the position of each particle can be recovered. However, when the interatomic distance decreases, the two particles can no longer be considered independent. Due to the increasing dipoleâdipole interaction between the two particles, the ï¬uorescence spectrum becomes complicated. It was found, however, that the"
280,161,0.983,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"The ecological significance of wing pattern plasticity in Bicyclus anynana is becoming increasingly well understood. In particular, exposed eyespots serve a cryptic function in the dry season, whereas they serve a deflection function in the wet season. Nonexposed eyespots serve a sexual signaling function and display their own patterns of plasticity, distinct from those of exposed eyespots. In addition, patterns of plasticity for each eyespot and for each of the color components within an eyespot are very eyespot-specific and need to be studied in isolation. The physiological basis of eyespot size plasticity in this species, unfortunately, focused for a very long time on a developmental period of low temperature sensitivity (the early pupal stage) instead of the more highly sensitive wandering larval stage of development. So, much of the early work in this system needs to be read and interpreted with caution. More recent experiments have clarified the developmental window and the physiological basis for size plasticity of both dorsal and ventral eyespots, and we have only begun to explore how different homologous wing pattern elements respond to the same environmental cue in different ways. Still, much work still remains to be done. For instance, as pointed out above, different"
391,370,0.983,Ocean-Atmosphere Interactions of Gases and Particles,"The first technique, called the radon deficit method, assumes that 222Rn and 226Ra are in secular equilibrium in the mixed layer and below, and the observed deficit of radon in the surface ocean is due to evasion of 222Rn out of the ocean (e.g. Bender et al. 2011; Peng et al. 1979; Roether and Kromer 1984). The second technique involves the deliberate simultaneous injection of two volatile tracers with very different diffusion coefficients (3He and SF6) in the mixed layer of the ocean or into coastal seas, and the subsequent monitoring of their concentrations with time (e.g. Wanninkhof et al. 1993; Watson et al. 1991). Dilution will not alter the 3He/SF6 ratio, whereas the gas exchange of 3He is faster than SF6, and so the 3He/SF6 ratio will decrease with time, and allow the gas transfer velocity to be determined."
372,434,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"where x%1 and y%1 are the aperture dimensions, and D% is the aperture separation, all measured in wavelengths. The sinc-squared functions in (5.14) represent the power pattern of the uniformly illuminated rectangular apertures, and the cosine term represents the fringe pattern. In early instruments, the relative magnitude of the spatial sensitivity was controlled only by the field distribution over the antennas, but image processing by computer enables the magnitude to be adjusted after an observation has been made."
311,1579,0.983,The Physics of the B Factories,"17.12 B decays to baryons Editors: Roland Waldi (BABAR) Min-Zu Wang (Belle) Hai-Yang Cheng (theory) Additional section writers: Thomas Hartmann Baryons and antibaryons have to be produced in pairs in the Standard Model, therefore most mesons cannot decay to baryons for lack of energy. The only baryonic D decay is Ds+ â pn, which has only just enough energy to proceed. But the phase space of the two or four quarks in a purely hadronic weak decay of a B meson is much larger than that of charmed or light mesons, and leaves ample freedom for high multiplicities and for the production of Figure 17.12.1. Diï¬erential B â Î+ c X production rate per baryon-antibaryon pairs. Î¥ (4S) from BABAR (Aubert, 2007p), Belle (Seuster, 2006), and That B decays to baryons play an important role be- CLEO (Crawford et al., 1992) versus the momentum fraction came evident by the large proton multiplicities found by xp = p/pmax in the Î¥ (4S) rest frame. Also shown is the diï¬er0 ARGUS and CLEO at the Î¥ (4S) (Albrecht et al., 1989b; ential Îc production rate normalized to match the peak of the Îc rate. Crawford et al., 1992). The interest in decays to baryons was increased when ARGUS claimed the observation of B decays to ppÏ Â± and ppÏ + Ï â (Albrecht et al., 1988b). Subsequently, baryonic of protons and antiprotons in an average B decay is B decays were studied extensively by theorists around the np + np  = 0.080 Â± 0.004. (17.12.1) early 1990s with the focus on the tree-dominated twobody decay modes. Experimental studies were first led by Some of these protons come from Î decays; the multiCLEO, but with the accumulating data at the B Factories, plicity of Î baryon production has been determined to be BABAR and Belle came to dominate the field. nÎ + nÎ  = 0.040 Â± 0.005 (Albrecht et al., 1989b; CrawThe features of B decays to baryons reflect the prop- ford et al., 1992). erties of both the weak interaction, and the hadronization Inclusive particle spectra in (scaled) momentum are of quarks. One or two qq pairs have to be produced out of obtained from data at the Î¥ (4S) energy by subtracting the vacuum to produce a baryon-antibaryon pair, similar the spectra obtained oï¬ resonance, scaled to the on resoto jet fragmentation. nance luminosity and cross section. The scaled momentum This section presents the inclusive production of bar- of a particle of mass m is given by x = p/p  with the yons in B meson decays (Section 17.12.1), then exclusive maximum center-of-mass momentum s/4 â m2 . two-body decays (Section 17.12.2), followed by the more Integration extrapolated spectra yields the parfrequent multibody final states with a baryon-antibaryon ticle multiplicity. pair plus one or more mesons (Section 17.12.3). ComIf protons were the only stable baryons, any baryplex phenomena are seen in multibody decays, and our treatment includes dedicated discussions of threshold en- onic event would have one proton and one antiproton. hancement (Section 17.12.3.3), multiplicity eï¬ects (Sec- This would imply a 4% branching fraction into baryon antion 17.12.3.4), and angular correlations (Section 17.12.3.5). tibaryon + X (ignoring decays with two pairs). But since Finally, radiative and semileptonic decays with baryons in there are also neutrons, a more sophisticated analysis is the final state are discussed (Sections 17.12.4 and 17.12.5 required to obtain the total baryonic branching fraction. respectively). Theoretical interpretations and model pre- Such an analysis has been performed by the ARGUS coldictions for each of these topics are discussed within the laboration (Albrecht et al., 1992c) using in addition to the at the the corresponding section. Baryon number violating de- proton and Î multiplicities the fractions of events baryon-antibaryon pairs, baryon pairs, cays have been presented in the previous section, 17.11.6. baryon ââ pairs (where â is an electron or muon). The baryon-antibaryon fraction allows for the elimination of the unknown contribution of neutrons to the final state, 17.12.1 Inclusive decays into baryons while the remaining fractions help to establish baryonflavor correlations. The result is The inclusive production of protons and antiprotons from Î¥ (4S) decays, i.e., an admixture of B + , B â , B 0 , and B 0 , B(B â B1 B2 X) = (6.8 Â± 0.5 Â± 0.3)% (17.12.2) has been measured by ARGUS and CLEO (Albrecht et al., 1993a; Crawford et al., 1992). The combined multiplicity where B represents a generic baryon."
297,1208,0.983,The R Book,"This shows that temperature is by far the most important factor affecting ozone concentration (the longer the branches in the tree, the greater the deviance explained). Wind speed is important at both high and low temperatures, with still air being associated with higher mean ozone levels (the ï¬gures at the ends of the branches). The interaction structure is relatively simple (compare with the other air pollution example on p. 768), but there is a hint of an interaction between wind and radiation and between wind and temperature. We could include these in an initial complex model, degrees of freedom permitting. w2 <- windË2 t2 <- tempË2 r2 <- radË2 tw <- temp*wind wr <- wind*rad tr <- temp*rad wtr <- wind*temp*rad Armed with this background information we can begin the linear modelling. We start with the most complicated model: this includes curvature terms for each variable, all three two-way interactions and a three-way interaction: model1 <- lm(ozone~rad+temp+wind+t2+w2+r2+wr+tr+tw+wtr) summary(model1) Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 5.683e+02 2.073e+02 2.741 0.00725 ** -3.117e-01 5.585e-01 -0.558 0.57799 -1.076e+01 4.303e+00 -2.501 0.01401 * -3.237e+01 1.173e+01 -2.760 0.00687 ** 5.833e-02 2.396e-02 2.435 0.01668 * 6.106e-01 1.469e-01 4.157 6.81e-05 *** -3.619e-04 2.573e-04 -1.407 0.16265 2.054e-02 4.892e-02 0.420 0.67552 8.403e-03 7.512e-03 1.119 0.26602 2.377e-01 1.367e-01 1.739 0.08519 . -4.324e-04 6.595e-04 -0.656 0.51358 Residual standard error: 17.82 on 100 degrees of freedom Multiple R-squared: 0.7394, Adjusted R-squared: 0.7133 F-statistic: 28.37 on 10 and 100 DF, p-value: < 2.2e-16 There looks to be rather little scope for model simpliï¬cation, so we shall do it all by hand (rather than using step, for instance, not least because this is prone to remove main effects that are still present in interactions, or the linear parts of quadratic terms that would best be retained). We start by removing the highest-order interaction. An excellent feature of R is that the p values are âp values on deletionâ so we do not have to use anova to compare the models produced by stepwise deletions: model2 <- update(model1,~.-wtr) summary(model2) Coefficients: (Intercept)"
311,1514,0.983,The Physics of the B Factories,Figure 17.10.5. Constraint on the ratio of the two vacuum expectation values tan Î² and the charged Higgs mass in the type II of two Higgs doublet model. The green regions indicate the excluded regions at a conï¬dence level of 95%.
139,126,0.983,Programming for Computations - MATLAB/Octave (Volume 14.0),"sum SN .t/. The trial function can run a loop where the user is asked for the bn values in each pass of the loop and the corresponding plot is shown. You must find a way to terminate the loop when the experiments are over. Use M=500 in the calls to plot_compare and error. f) Choose f .t/ to be a straight line f .t/ D 1 t on Å ; Â. Call trial(f, 3) and try to find through experimentation some values b1 , b2 , and b3 such that the sum of sines SN .t/ is a good approximation to the straight line. g) Now we shall try to automate the procedure in f). Write a function that has three nested loops over values of b1 , b2 , and b3 . Let each loop cover the interval Å 1; 1Â in steps of 0:1. For each combination of b1 , b2 , and b3 , the error in the approximation SN should be computed. Use this to find, and print, the smallest error and the corresponding values of b1 , b2 , and b3 . Let the program also plot f and the approximation SN corresponding to the smallest error. Filename: fit_sines.m. Remarks 1. The function SN .x/ is a special case of what is called a Fourier series. At the beginning of the 19th century, Joseph Fourier (1768â1830) showed that any function can be approximated analytically by a sum of cosines and sines. The approximation improves as the number of terms (N ) is increased. Fourier series are very important throughout science and engineering today. (a) Finding the coefficients bn is solved much more accurately in Exercise 3.12, by a procedure that also requires much less human and computer work! (b) In real applications, f .t/ is not known as a continuous function, but function values of f .t/ are provided. For example, in digital sound applications, music in a CD-quality WAV file is a signal with 44100 samples of the corresponding analog signal f .t/ per second. Exercise 2.19: Count occurrences of a string in a string In the analysis of genes one encounters many problem settings involving searching for certain combinations of letters in a long string. For example, we may have a string like gene = âAGTCAATGGAATAGGCCAAGCGAATATTTGGGCTACCAâ"
391,122,0.983,Ocean-Atmosphere Interactions of Gases and Particles,"of the effects on DMS flux have yielded very different outcomes: Kloster and co-workers predict a 10 % reduction in global DMS flux by the end of the century (Kloster et al. 2007); Vallina et al. (2007) found only a 1 % increase in net global DMS concentrations upon a 50 % increase in atmospheric CO2; Cameron-Smith and co-workers simulated a 50 % increase in DMS flux to the atmosphere over large regions of the Southern Ocean when atmospheric CO2 increases to 970 ppm (Cameron-Smith et al. 2011). The latter result was related to concurrent sea-ice changes and ocean ecosystem composition shifts. In a comparison with previous model outcomes, the authors concluded that increasing model complexity appears to be associated with reduced DMS emission at the equator and increased emissions at high latitudes. The variation in model outcomes illustrates how important it is to improve our understanding of the underlying ecosystem processes. The different scenarios presented in Fig. 1.1 show that under different environmental conditions the DMS yield from DMSP can shift by possibly an order of magnitude. This can potentially result in changes in DMS emission of several hundreds of percent, an extent that the recent publication from Quinn and Bates (2011), which argues for the retirement of the CLAW hypothesis (see Sect. 1.2.3.2), did not consider possible. Whether such shifts are plausible at the global scale remains to be seen. Since DMS in mechanistic models is dynamically linked to the production of DMSP by phytoplankton, validating this model parameter would help us to understand the underlying controlling factors. Building a DMSP database comparable to the current DMS database would therefore be of great value."
65,156,0.983,Handbook of Ocean Wave Energy,"couplings between wave components, but in this case they are between sets of three wave components that cause energy transfer via non-linear interactions. When the waves are dispersive these interactions cannot be created, which is why they only occur in very shallow water. The effect of triad wave-wave interactions is to generate a second peak in the wave spectrum at twice the frequency of the original spectrum, which is bound to the main frequency peak in the sense that it travels with the same phase velocity. Unfortunately, currently third generation wave models are unable to correctly model these bound waves, and the triad wave-wave interaction source term is estimated based on the wave spectrum and water depth; however, these source terms have been found to be reasonable approximations in most cases. â¢ The ï¬nal source term typically included in wave models is the depth-induced wave breaking (surf-breaking) source term. This source term typically assumes that a ï¬xed proportion of the energy in any wave is lost when it breaks. Thus it is necessary to make an estimate on the proportion of waves that break at any particular water depth for any particular wave spectrum. This may be done by making some assumptions about the distribution of wave heights and the relative water depth in which these waves will break. Perhaps surprisingly, laboratory observations suggest that the spectral shape is not affected by wave breaking and so the energy removed due to wave breaking, and thus the strength of the depth-induced wave breaking source term, is typically assumed to be proportional to the wave energy spectrum, with the coefï¬cient of proportionality dependent on the proportion of breaking waves."
219,141,0.983,Economics of Land Degradation and Improvement â A Global Assessment for Sustainable Development,"Relation to Land Cover Strata At the resolution of this global study (i.e. 8-km pixel), many sub-classes of scattered land cover/use (e.g. slash-and-burn ï¬eld, mountain paddy rice terraces and fruit plantations) will be dissimulated. Thus, we used 7 broad land use/cover classes (see Fig. 4.11) aggregated from 23 classes of the Globcover 2005â2006 data (Bicheron et al. 2008). The spatial pattern of long-term (1982â2006) NDVI decline with correction of RF and AF effects and masking of saturated NDVI zone versus main land cover/use types is shown in Fig. 4.11. The related statistics for regions in the"
233,214,0.983,Biodiversity Conservation and Phylogenetic Systematics : Preserving Our Evolutionary Heritage in An Extinction Crisis (Volume 14.0),"Fig. 4 (a) A single environmental gradient (thick black line) and three selected sites (black dots). Each hypothetical branch/lineage, centred at a demand point, graphically is represented in the figure by a point above its demand point, at a vertical distance equal to one-half of its distribution extent on the gradient. Branch/lineage points in the figure are gray if no selected site overlaps with the range-extent of the branch/lineage. Branch/lineage âaâ would be captured by the middle site only, branch/lineage âbâ is not sampled by any sites as its extent is too small; it is therefore coloured gray. Branch/lineage âcâ is captured by two sites. The height to the top of the gray area above any demand point reflects the total number of branch/lineages centered at that point that are not overlapped by any selected sites. ED is the sum of the resulting triangular gray areas. When richness varies along the gradient, the corresponding weights on demand points can be interpreted as if we are calculating a volume when counting-up unrepresented branch/lineages to obtain the ED score. (b) If the hollow-circle site is added to the selected set indicated by the black dots, the ED value (number of branch/lineages not represented) will be reduced by the amount equal to the white-striped area. This ED-complementarity equals x*y/2, where x and y are distances from the hollow circle"
311,1260,0.983,The Physics of the B Factories,"Figure 17.7.8. Signal enhanced projections of a) ÎE, b) mES from the BABAR B â a1 Ï analysis. Points represent onThe constant Î» is |Vus |/|Vud | = |Vcd |/|Vcs | while fK , fÏ , resonance data, dotted lines the sum of all backgrounds, and solid lines the full ï¬t function. These plots are made with a fa1 (1260) , and fK1 are the decay constants of K, Ï, a1 (1260), cut on the signal-to-continuum likelihood ratios excluding the and K1 mesons, respectively. The branching fraction mea- variable being plotted (from Aubert (2007ae))."
231,998,0.983,North Sea Region Climate Change Assessment,"signal depends on the morphometry and mixing characteristics of the lake. The NAO signal persists only through spring in shallow, polymictic MÃ¼ggelsee, but throughout much of the following summer in the shallow, dimictic Heiligensee, and throughout the whole of summer and autumn in the much deeper, dimictic Stechlinsee. Thus, although an NAO signal is likely to be present to some extent in the temperature of all lakes within the North Sea region, individual lake characteristics are certain to result in a large degree of variability in the strength and persistence of this signal. In the context of the NAO, one other phenomenon should be mentioned: the late 1980s climate regime shift. In the late 1980s, an abrupt regime shift occurred in the atmospheric, oceanic, terrestrial, limnological and cryospheric systems in many regions of the world. Evidence suggests that this large-scale regime shift involved abrupt changes in the AO and NAO (Alheit et al. 2005; Rodionov and Overland 2005; Lo and Hsu 2010), had a substantial impact on air temperature in northern Europe (Lo and Hsu 2010), and affected ï¬sh populations in the North Sea (Reid et al. 2001; Alheit et al. 2005). It is not surprising therefore that a regime shift in lake temperature in the late 1980s was also detected in MÃ¼ggelsee (Gerten and Adrian 2000, 2001), and it can be fairly conï¬dently hypothesised that a similar regime shift, at least in lake surface temperatures, is likely to have occurred in many lakes within the North Sea region. In summer, a more regional approach to determining the effects of climatic forcing on lakes is necessary owing to the smaller spatial scales of the weather systems involved. In the case of the UK, the Lamb synoptic weather classiï¬cation system (Lamb 1950) has proven useful. For both Windermere, the largest lake in the English Lake District, and Lough Feeagh, a lake located near the west coast of Ireland, the highest lake surface temperatures were recorded during a westerly circulation type in winter (corresponding to a positive NAO index), but a southerly circulation type in summer (George et al. 2007b). On a multi-annual time scale, large-scale regional coherence is greater in winter than in summer. However, on shorter time scales the opposite appears to be the case. Short-term, high-resolution surface temperature measurements in Scottish Highland lochs (Livingstone and Kernan 2009) show a high degree of regional coherence in daily means from late spring to autumn, but much lower coherence in winter and early spring. Short-term regional coherence is high in summer because the surface mixed layer is thin and surface temperatures respond sensitively to climate forcing, and is high in autumn because surface temperatures are dominated by convective cooling, which is governed by regionally coherent air temperature. Short-term coherence is comparatively low in winter because fluctuations in lake surface temperature are small and may be buffered by partial"
222,473,0.983,Ecosystem Services For Well-Being in Deltas : integrated Assessment For Policy Analysis,"Water levels in the delta are controlled by a balance between river and tidal flow, acting on different timescales. Throughout the year the situation can change, from tides controlling the water levels in the dry season to dominance by river flow during the monsoon. Some sensitivity to model bottom roughness is seen in the simulations, though the magnitude of these changes is small in the context of tidal range and seasonal water level changes associated with freshwater. Figure 17.6 shows modelled surface salinity from the baseline simulation described in Table 17.1. As there is a continuous data set from the model, the mean and maximum are calculated from the full annual data set, while Fig. 17.2 compares point observations with a modelled map for the March to May period. It may be seen that the salinity is higher in the west of the study region around the Sundarbans forest. This is because the majority of freshwater discharges occur through the eastern distributaries, especially the Meghna Estuary, and the western delta is composed mainly of tidal creeks with little or no freshwater input. In the FVCOM delta model, there is only one point of freshwater input (marked on Fig. 17.5). The network of river distributaries then carries this to the Bay of Bengal. The model may be missing additional freshwater inputs from smaller distributaries and Indian rivers that enter the"
372,196,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"baseline. The interferometer power pattern is of the form A.l/ cos .2$ul/FB .l/, and the response of the interferometer to the source is ÅA.l/ cos .2$ul/FB .l/' $ I1 .l/. Most interferometers for operation at meter wavelengths, that is, at frequencies below about 300 MHz, use antennas that are arrays of fixed dipoles. At such long wavelengths, it is possible to obtain large collecting areas and still have wide enough beams that some minutes of observing time are obtained as a source passes through in sidereal motion. Often the bandwidth of such low-frequency instruments is small, so that the bandwidth pattern, FB .l/, is wide and this factor can be omitted. Also, the antenna beams are usually wider than the source and sufficiently wide that several cycles of the fringe pattern can be measured as the source transits the beam. So in the nontracking case, the essential form of the response is also represented by Eq. (2.19). However, fixed antennas with nontracking beams are mainly a feature of the early years of radio astronomy, and in more recent meter-wavelength arrays, the phases of individual dipoles, or small clusters of dipoles, can be adjusted to provide steerable beams."
84,379,0.983,Eye Tracking Methodology,"Note that a uniform grid is not a necessity. The transition matrix can be composed from arbitrarily defined AOIs. For example, Ellis and Stark (1986) compared transition matrices of airline pilots viewing a Cockpit Display of Traffic Information or CDTI. The CDTI was fixed with 8 AOIs. Krejtz et al. (2015) provide other examples. How to interpret the meaning of Ht ? Weiss et al. (1989) note that in a transition matrix, a small Ht suggests dependencies between the fixation points, whereas a large Ht suggests a random scanning pattern. Stated another way, entropy refers to the âexpected surpriseâ of a given gaze transition. Minimum entropy of 0 suggests no expected surprise, meaning that a gaze transition is always expected to the same jth AOI. Maximum entropy, on the other hand, suggests maximum surprise, since transition from source AOI to any destination AOI is equally likely, and hence whichever occurs results in maximum expected surprise. More formally, the term â pi j log2 pi j in (14.16) is the transitionâs contribution to system entropy, modeled by its probability multiplied by its surprisal (Hume and Mailhot 2013). The key difference between Krejtz et al. (2015) approach and that of Ellis and Stark (1986) is that Krejtz et al. consider self-transitions. The use of a grid also makes the transition matrix analysis content-independent. The benefit of contentindependence is that it allows estimation of transition matrices irrespective of the expected AOIs in the scene. All that is required is knowledge of the dimensions of the screen to establish different grid granularities. This makes the statistical analysis portable among different experimental designs. Krejtz et al. (2015) compute a transition matrix by setting matrix elements pi j to the number of transitions from the ith source AOI to the jth destination AOI for each participant. The matrix is then normalized relative to each source AOI (i.e., per row). In practice, it is possible that no transitions from the ith AOI are observed. This leads to a zero matrix row sum and division by zero. When this occurs, each of the row entries is set to their uniform transition distribution, namely pi j = 1/s, where s is the number of AOIs, thereby modeling an equally likely probability of transitioning to any other AOI given this ith source AOI (hence maximum âsurpriseâ). The benefit of this implementation decision is that it leads to the construction of a transition matrix that is regular (specifically a right stochastic matrix), with all entries positive and non-zero, facilitating stationary entropy calculation via Eigen analysis. Note that setting each of the pi j entries to 1/s would lead to a uniform matrix with maximum entropy equal to log2 s bits per transition. Indeed, maximum entropy is used to normalize the empirical entropy obtained from each transition matrix. That is, statistical comparison of mean entropies per experimental condition is facilitated by computing Ht per individual participant and per condition, then normalizing, HÌt = Ht / log2 s. This results in a table of entropies (each entropy computed from an individualâs transition matrix) for each of experimental conditions and each of the participants. Analysis of variance (ANOVA) is then used to test for differences in mean (normalized) entropy per condition. An example of entropy analysis is given in Chap. 15."
241,1323,0.983,Second Assessment of Climate Change for the Baltic Sea Basin,"calculated the fraction of the ï¬ndings that are consistent with the local warming. For Europe, they found that 94 % of the studies investigating change in the physical systems and 90 % of the studies on changes in the biological systems ï¬nd changes that are consistent with the observed warming. While such an approach is valuable in providing an overview of ï¬ndings across systems, there is the potential danger of sampling issues influencing the results. If process models of the system are available, the uncertainties in the response to changes in the various drivers (climate and other) can be fully explored. Such an end-to-end attribution analysis, however, is so far not available for the Baltic Sea area."
372,1058,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"loops with time constants in the range 0.1â1 s, so that short-term performance becomes that of the crystal oscillator. Details of how these loops are implemented are given by Vanier et al. (1979). The performance of the crystal oscillator is very important because unless it has high spectral purity, the phase-locked loops involved in generating the local oscillator signal from the frequency standard will not operate properly (Vessot 1976). We first consider a frequency standard as a âblack boxâ that puts out a stable sinusoid at a convenient frequency such as 5 MHz, or some higher frequency, at which the crystal oscillator is locked to the atomic process. The performance of various devices is shown in Fig. 9.17. These somewhat idealized plots show that the Allan variances of the standards have three regions: short-term noise dominated by either white-phase or white-frequency noise; flicker-frequency noise, which gives the lowest value of Allan variance and is therefore referred to as the âflicker floorâ; and finally, for long periods, random-walk-of-frequency noise. Two other parameters can be specified, a drift rate and an accuracy. The drift rate is the linear change in frequency per unit time interval. Note that if the standard drives a clock, then a constant drift rate results in a clock error that accumulates as time squared. The accuracy refers to how well the standard can be set to its nominal frequency. The performance parameters are summarized in Table 9.4."
194,26,0.983,Dynamic Dispatch for Method Contracts Through Abstract Predicates,"For exact instances of Cell, the footprint is defined as the set containing all object fields (this.*), with val being the only field the same predicate in (7) is thus equivalent to select Any (h1 , c, Cell::val) = select Any (h2 , c, Cell::val) in that case."
238,293,0.983,Nanoinformatics,"Comparing the virtual screening method and the Kriging method, the efï¬ciency of the virtual screening is superior to that of the Kriging method. However, one has to construct a predictor in order to maintain this great efï¬ciency. The most important advantage of the Kriging method is its wide applicability. No training is needed for the Kriging method, and thus it can be easily applied to other GBs in other materials. To show the wider applicability of the Kriging method, we used it to conduct similar studies on oxide interfaces. In particular, we applied the Kriging approach to grain boundaries of metal oxides including MgO, TiO2, and CeO2 which commonly exhibit more complex structures than metals. Four kinds of metal oxide grain boundaries, namely rock-salt-MgO Î£5[001]/(210) and Î£5[001]/(310), rutile-TiO2 Î£5[001]/(210), and fluorite-CeO2 Î£3[110]/(111) were selected to test the applicability of the present method. These grain boundaries have different complexities; the number of termination planes for MgO Î£5[001]/(210) and Î£5[001]/(310) is one (Fig. 8.9a, b), whereas that for TiO2 Î£5[001]/(210) and CeO2 Î£3[110]/(111) is two (Fig. 8.9c, d). The same Kriging method was applied to these oxides GBs. Two hyper-parameters, pre-distribution and kernel parameter, were set to 0 and 3.0"
297,1947,0.983,The R Book,"You need to use the function ppp to convert your coordinate data into an object of class ppp representing a point pattern data set in the two-dimensional plane. Our next dataframe contains information on the locations and sizes of 3359 ragwort plants in a 30 m Ã 15 m map: data <- read.table(""c:\\temp\\ragwortmap.txt"",header=T) attach(data) names(data) [1] ""xcoord"" ""ycoord"" ""type"" The plants are classiï¬ed as belonging to one of four types: skeletons are dead stems of plants that ï¬owered the year before, regrowth are skeletons that have live shoots at the base, seedlings are small plants (a few weeks old) and rosettes are larger plants (one or more years old) destined to ï¬ower this year. The function ppp requires separate vectors for the x and y coordinates: these are in our ï¬le under the names xcoord and ycoord. The third and fourth arguments to ppp are the boundary coordinates for x and y respectively (in this example c(0,3000) for x and c(0,1500) for y). The ï¬nal argument to ppp contains a vector of what are known as âmarksâ: these are the factor levels associated with each of the points (in this case, type is either skeleton, regrowth, seedling or rosette). You give a name to the ppp object (ragwort) and deï¬ne it like this: ragwort <- ppp(xcoord,ycoord,c(0,3000),c(0,1500),marks=type) You can now use the object called ragwort in a host of different functions for plotting and statistical modelling within the spatstat library. For instance, here are maps of the point patterns for the four plant types separately: plot(split(ragwort),main="""") regrowth"
175,1461,0.983,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","flow, from steady to unsteady flow, and from uniform to gradually or rapidly varying non-uniform flow. Urban drainage ditches normally have uniform cross sections along their lengths and uniform gradients. Because the dimensions of the cross sections are typically one or two orders of magnitude less than the lengths of the conduit, unsteady free surface flow can be modeled using one-dimensional flow equations. When modeling the hydraulics of flow it is important to distinguish between the speed of propagation of the kinematic wave disturbance and the speed of the bulk of the water. In general the wave travels faster than the water particles. Thus if water is injected with a tracer the tracer lags behind the wave. The speed of the wave disturbance depends on the depth, width, and velocity of the flow. Flood attenuation (or subsidence) is the decrease in the peak of the wave as it propagates downstream. Gravity tends to flatten, or spread out, the wave along the channel. The magnitude of the attenuation of a flood wave depends on the peak discharge, the curvature of the wave proï¬le at the peak, and the width of flow. Flows can be distorted (changed in shape) by the particular channel characteristics. Additional features of concern to hydraulic modelers are the entrance and exit losses to the conduit. Typically at each end of the conduit is a manhole. Manholes are storage chambers that provide access (for men and women) to the conduits upstream and downstream. Manholes induce some additional head loss. Manholes usually cause a major part of the head losses in sewer systems. A manhole head loss represents a combination of the and contraction losses. For pressure flow, the head loss HL due to contraction can be written as a function of the downstream velocity, VD, and the upstream and downstream flow cross-sectional areas AU and AD. HL Â¼ K VD2 =2g Â½1"
311,363,0.983,The Physics of the B Factories,"tial background from low momentum tracks can be reduced by correlating the direction of the slow pion and the remaining tracks from the Btag decay. Since the slow pion and the D0 are emitted nearly at rest in the DâÂ± frame, the slow pion direction in the Btag rest frame will be along the direction of the D0 decay products and opposite to the remainder of the Btag decay products. This direction can be approximately determined by calculating the thrust axis of the Btag decay products. The thrust is calculated using both charged tracks and neutral clusters not used in the reconstruction of Brec . The following variables provide useful discriminating power: â q, the charge of the track. â pâ , the momentum of the slow pion candidate in the Î¥ (4S) center-of-mass frame. â plab , the momentum of the slow pion candidate in the laboratory frame. â Î¸lab , the polar angle in the laboratory frame. â cos Î¸ÏT , the cosine of the angle between the slow pion direction and the Btag thrust axis in the Î¥ (4S) centerof-mass frame. â LK , the PID likelihood of the track to be a kaon. PID information helps to reject the contribution from low momentum kaons ï¬ying in the thrust direction. â Le , the PID likelihood of the track to be a electron. This helps to reject background from electrons produced in photon conversions and Ï 0 Dalitz decays. 8.5.4 Correlation of kaons and slow pions In events where both a charged kaon and a slow pion candidate (e.g. from a Dâ+ â D0 (â K â X)Ï + decay) are found, the corresponding ï¬avor tagging information can potentially be improved by using the angular correlation between the kaon and slow pion. A kaon and a slow pion of opposite charge (i.e. agreeing ï¬avor tag) that are emitted in approximately the same direction in the Î¥ (4S) centerof-mass frame can provide a combined tag with a relatively low mistag fraction. In addition to the information used to identify kaons and slow pions, the following discriminating variable can be used:"
280,169,0.983,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"Eyespots, concentric bands of pigment patterning, constitute one of the most studied pattern elements on the wings of butterflies (c.f., Fig. 6.3 for an example). Each eyespot develops around a focus, a small group of cells that sends out a morphogenetic signal that determines the synthesis of circular patterns of pigments in their surroundings. In this work, we consider a model that provides a possible mechanism underlying the determination of the number and locations of eyespots on the wing surface. The model we consider, first described by Sekimura et al. (2015), provides a mechanism that places the foci around which eyespots form in various locations on the entire wing surface. We do not address here subsequent stages of eyespot formation that occurs after the development of the foci. The model we consider is based on that of Nijhout (1990). The main novelty of the work in Sekimura et al. (2015) was to illustrate that simply changing the conditions assumed to hold at the proximal veins was sufficient to determine whether or not an eyespot formed in a given wing cell. In the present work, we extend the investigations of the models proposed in Sekimura et al. (2015). We show that it is possible to determine the location of eyespots within a wing cell simply by changing the conditions that are assumed to hold at the lateral wing veins that bound the wing cell. Furthermore, we illustrate that it is possible, using a two-stage model, to recapitulate the results of artificial selection experiments in terms of selection and location of eyespots in butterfly wings."
372,1320,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"11.2 Maximum Entropy Method 11.2.1 MEM Algorithm An important class of image-restoration algorithms operates to produce an image that agrees with the measured visibility to within the noise level, while constraining the result to maximize some measure of image quality. Of these, the maximum entropy method (MEM) has received particular attention in radio astronomy. If I 0 .l; m/ is the intensity distribution derived by MEM, a function F.I 0 / is defined, which is referred to as the entropy of the distribution. F.I 0 / is determined entirely by the distribution of I 0 as a function of solid angle and takes no account of structural forms within the image. In constructing the image, F.I 0 / is maximized within the constraint that the Fourier transform of I 0 should fit the observed visibility values. In astronomical image formation, an early application of MEM is that of Frieden (1972) to optical images. In radio astronomy, the earliest discussions are by Ponsonby (1973) and Ables (1974). The aim of the technique, as described by Ables, is to obtain an intensity distribution consistent with all relevant data but minimally committal with regard to missing data. Thus, F.I 0 / must be chosen so that maximization introduces legitimate a priori information but allows the visibility in the unmeasured areas to assume values that minimize the detail introduced. Several forms of F.I 0 / have been used, which include the following: F1 D """
219,75,0.983,Economics of Land Degradation and Improvement â A Global Assessment for Sustainable Development,"Some socio-economic characteristics of a land user, as well as some plot-level characteristics can be considered exogenous (such as age of the user or slope of the plot), while others can be considered the result of the surrounding institutional environment or endogenous. Depending on the focus and time span of analysis, some characteristics can be considered exogenous or endogenous. For instance the level of education of a land user is not likely to change in the short term, but its changes throughout generations can be considered the outcome of education policies."
311,3033,0.983,The Physics of the B Factories,"has a structure similar to the SM one. The most important feature is that there is a hierarchy similar to the one in the quark flavor sector of the SM. The first two generations are much lighter than the third generation. Also, the mixing between the third and the first two generations is much smaller than the one between the first two generations (i.e. Vcb , Vub âª Vus ). If this pattern is also present in NP with roughly the same hierarchies and directions of flavor breaking as in the SM, then FCNCs generated by NP will not be dangerously large. This insight can be formalized using symmetries and goes by the name of general MFV (GMFV). It is more general than cMFV, but coincides with the most general form of MFV, if Yukawa couplings are perturbative. Since GMFV is more general, there are also less correlations between observables. For instance, depending on which operators dominate, the new CP violating phases in Bd0 â B 0d and Bs0 â B 0s mixing are either exactly the same or there is a new CP violating phase only in Bs0 â B 0s mixing as we will see below. Supersymmetry The supersymmetric (SUSY) extensions of the SM are some of the most popular NP models. SUSY relates fermions and bosons. For example, the gauge bosons have their fermion superpartners and fermions have their scalar superpartners. SUSY at the TeV scale is motivated by the fact that it solves the SM hierarchy problem. The quantum corrections to the Higgs mass are quadratically divergent and would drive the Higgs mass to the Planck scale â¼ 1019 GeV, unless the contributions are canceled. In SUSY models they are canceled by the virtual corrections from the superpartners. The minimal SUSY extension of the SM refers to the scenario where all the SM fields obtain superpartners but there are no other additional fields. This is the Minimal Supersymmetric Standard Model (MSSM). SUSY cannot be an exact symmetry since in that case superpartners would have the same masses as the SM particles, in clear conflict with observations. Diï¬erent mechanisms of SUSY breaking have very diï¬erent consequences for flavor observables. In complete generality the MSSM has more than a hundred parameters, most of them coming from the so-called soft SUSY breaking terms (the SUSY breaking terms with dimensionful couplings, such as e.g. masses, so that the divergence is at most logarithmic). If superpartners exist at the TeV scale the most general form with O(1) flavor breaking coeï¬cients is excluded due to flavor constraints. This has been dubbed the SUSY flavor problem (or in general the NP flavor problem)."
311,2930,0.983,The Physics of the B Factories,"The models have also been tested directly against the development and tuning of such models. One has to keep in mind, however, that models contain a relatively large data in BABAR for the Î+ c spectrum (Aubert, 2007p). Eight models are tested within Jetset by re-weighting number of parameters which might aï¬ect the light and with varying parameter values so as to minimize the Ï2 heavy hadron fragmentation descriptions. of a comparison with the data. The UCLA model (Chun and Buchanan, 1998) is fitted similarly, whereas HERWIG (Marchâ esini et al., 1992) has no relevant free parameters. In gen- Î+ c Îc X Events eral, the fitted parameters are diï¬erent between mesons and baryons, and between light and heavy hadrons. BABAR has studied events containing both a Î+ c and a Îc These results indicate the need for diï¬erent treatment (Aubert, 2010b), following a study by the CLEO collabof heavier hadrons and provide new, precise input for the oration (Bornheim et al., 2001). CLEO found â¼4 times"
372,326,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"which can be combined with Eq. (A4.1) or (A4.2) to express p as a function of .A; E/ or .H; Ä±/. If the beam has elongated contours and width comparable to the source under observation, rotation of the beam causes the effective intensity distribution to vary with hour angle. This is particularly serious in the case of observations to reveal the structure of the most distant Universe, for which foreground sources need to be accurately removed. For the Australia Pathfinder Array (DeBoer et al. 2009), the 12-m-diameter antennas have altazimuth mounts, with a third axis that allows the reflector, feed supports, and feeds to be rotated about the reflector axis so the beam pattern and the angle of polarization remain fixed relative to the sky."
320,382,0.983,Managing Protected Areas in Central and Eastern Europe Under Climate Change,"(frequency and temporal distribution) appear the most important. In this regard, temporal variability of precipitation and flood dynamics were analysed in order to search for possible trends that can be brought about by climate change. Analysis of precipitation in the Biebrza Valley was done on the basis of data recorded in rain gauges located in Laskowiec (1996â2011 dataset) and Burzyn (1970â2010) (see location on the Fig. 14.1). Flood analysis was done on the basis of discharge data for the River Biebrza in Osowiec in the years 1951â2011. The flood threshold of river discharge (bankful flow) was set up on the basis of multi-year observations as 25 m3/s (Grygoruk et al. 2011b). Also the largest floods (over the threshold of the median of the highest annual discharges of Biebrza in Osowiec: Q > 84.1 m3/s, which is a flood of 50 % exceedence frequency) were analysed in order to reveal the temporal dynamics of flooding in particular seasons. Those sizeable floods are important from an ecological point of view, as they cover a large extent of the valley and induce the development and function of riparian zones as well as a network of ox-bow lakes, entailing water exchange between the river and a large share of the floodplain. Temporal variability in amounts of precipitation recorded in the Burzyn rain gauge during hydrological âsummerâ (MayâOctober) and âwinterâ (NovemberâApril) was analysed for each particular year (Fig. 14.2a), as well as the ratio of winter to summer precipitation (Fig. 14.2b). A slightly decreasing trend in summer volumes of precipitation within the years 1970â2010 can be observed, along with the considerable trend in increasing winter precipitation. One can conclude that the share of precipitation in the cold part of the year increases in the total amount of annual precipitation. However, a vast increase in rainfall intensity in the summer can be observed (Fig. 14.2c) â although in general summers seem to be drier, the increasing temporal concentration of precipitation entails possible flooding and remains a challenge for management and ecosystems. This observation confirms the results of Liszewska and Osuch (2000), who stated that more extreme weather conditions can occur as a major consequence of climate change in Central Europe. Similar results for the analysis of precipitation data recorded in the climate monitoring station of the IMGW (Institute of Meteorology and Water Management) in BiaÅystok (50 km from the Biebrza Valley), in the years 1971â2010, were observed by Grygoruk et al. (2011a). Hence, the precipitation dynamics and their temporal distribution analysed herein on the basis of data from the Laskowiec and Burzyn rain gauges most likely reflect the long-term regional trend. The increasing temporal concentration and intensity of summer sums of rainfall correspond with the results of temporal analysis of the largest floods (Fig. 14.2e). It indicates a significant increase in summer flooding in the first decade of the twentyfirst century, compared to the second half of the twentieth century. Also, the total volume of individual summer floods (calculated as the amount of water momentary stored in the Lower Biebrza Basin during the flood events; Fig. 14.2f) increased in the last 15 years. Temporal analysis of snowmelt floods in the Biebrza Valley (Fig. 14.2d) indicates the continuing trend of the earlier occurrences of floods in 1950â2012. The analysis of the start of the spring flood was emphasised since this process induces the ecosystem services of marshes, such as their role in strictly temporal fish spawning and nutrient removal (Okruszko et al. 2011). It was"
241,740,0.983,Second Assessment of Climate Change for the Baltic Sea Basin,"Christensen 2007), selecting the eastern Baltic Sea area for further analysis. They found that RCMs tend to produce a reasonable annual cycle of temperature but clearly overestimate precipitation in winter and underestimate precipitation in summer. By applying their bias correction method to daily precipitation, they were able to bring the model results much closer to observations. The same correction function would then be applied to the output of scenario simulations, under the assumption that the causes that produce the mean biases in the models remain unchanged under a future climate. The corrected values of precipitation and temperature were used by ApsÄ«te et al. (2010) to drive a hydrological model to estimate changes in run-off in the eastern Baltic Sea catchment area. Future run-off will be modulated by changes in two factors. On the one hand, evaporation will tend to increase due to higher air temperatures, while on the other, precipitation is expected to increase, as simulated by most RCMs participating in the PRUDENCE project (Christensen and Christensen 2007). A surprising result of the study by ApsÄ«te et al. (2010) was that the ï¬rst factor seemed to be more important in the future, and river run-off would tend to decrease according to the RCM simulations analysed. Also, important is that the annual cycle of run-off tended to change considerably, with the late spring maximum observed in the present climate shifting to earlier seasons even into the months of January and February. This is a consequence of the rising temperatures and an earlier onset of the melt season, as well as changes in the annual cycle of precipitation and increased evaporation. This represents a major shift in the annual run-off cycle that may have profound consequences on many economic sectors should it remain unmanaged by reservoirs. However, the study by ApsÄ«te et al. (2010) is based on the mean of data simulated by 21 models and does not indicate the spread of individual simulations. Previous projections of river run-off into the Baltic Sea indicated that the uncertainty was large enough to encompass a broad range of projections, from slight reductions to substantial increase (Graham 2004). In a further study aimed at reconstructing run-off in the past 500 years, Hansson et al. (2011) also applied a statistical downscaling method using predictors from climate reconstructions of atmospheric circulation and temperature. Although the study is not focused on future projections but rather on past evolution of run-off, their ï¬ndings about past variability in river run-off were also interpreted in the context of future climate change. Hansson et al. (2011) briefly indicated that if their statistical downscaling model is correct, run-off would tend to increase in the northern Baltic Sea and decrease in the southern Baltic Sea. This result is mainly driven by the signal of increasing temperature in the northern Baltic Sea catchment area and by a decrease in precipitation in Central Europe. Chap. 12 deals explicitly with run-off projections."
151,97,0.983,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"Selected values presented for CPY in Tables 5 and 6 are well established and judged to be accurate within a factor of approximately 2 but, in some cases, ranges are given to reflect the variability and uncertainty in values. Since CPYO has been less studied, the values presented in Table 7 are subject to more uncertainty than those for CPY and must be treated as tentative. The vapor pressure and solubility were used only to estimate the air-water partition coefficient KAW and the Henryâs Law constant, H. The octanol-water partition coefficient (KOW) was used only indirectly to estimate the organic carbon water partition coefficient (KOC) in the TAPL3 LRT model but, since there are extensive empirical data on KOC, these empirical values were used directly. The octanol-air"
173,188,0.983,"intelligent Human Computer interaction : 9Th international Conference, Ihci 2017, Evry, France, December 11-13, 2017, Proceedings","Several researchers have developed menus which attempt to speed up selection in large sets of items presented on a small cellphone screen. Kim et al. [22] developed a 3D Menu which utilizes the depth cue. The researchersâ formal evaluation reveals that as the number of items gets larger and the task complexity is increased, the 2D organization of items is better. For a small number of items, the 3D Menu yields a better performance. Foster and Foxcroft [13] developed the Barrel Menu which consists of three horizontally rotating menu levels stacked vertically; the top level represents the main menu items. Selecting an item from a level is achieved by rotating the level left or right, resulting in the child elements of the current item being displayed in the menu level below. Francone et al. [14] developed the Wavelet Menu, which expands upon the initial Marking Menus by Kurtenbach and Buxton [21]. Bonnet and Appert [7] proposed the Swiss Army Menu which merges standard widgets, such as a font dialog, into a Radial Menu layout. Zhao et al. [35] used an Eyes-Free Menu with touch input and reactive auditory feedback."
228,214,0.983,Theory and Applications of ordered Fuzzy Numbers : a Tribute To Professor Witold KosiÅski,"Abstract This chapter concerns an issue of comparing fuzzy numbers. The relationship of similarity is probably the most widely used and most difficult to determine the measure of compliance precisely. Analysis of the similarity between two objects is an essential tool in biology, taxonomy, and psychology, and is the basis for reasoning by analogy. This chapter describes methods for determining the similarity used in fuzzy logic. Many of them were dedicated only to triangular or trapezoidal fuzzy numbers. This was a computing inconvenience and raised the question about the axiological basis for such comparisons. The authors have proposed two new approaches to comparing fuzzy numbers using one of the known fuzzy number extensions that are Ordered Fuzzy Numbers (OFNs). This has allowed us to simplify operations and eliminate said dualism. Two order-sensitive defuzzification methods are presented in the chapter. For OFN numbers with positive order (compliant with the direction of the OX axis increase) the results of defuzzifications are results for numbers of different notations, for example, L-R, whereas for numbers with negative orders, the defuzzification result changes. An important part of the chapter is a catalogue of the shapes of numbers in OFN notation. This is probably the first summary of basic shapes of those numbers with the results of defuzzifications using several methods. Keywords Fuzzy logic Â· Ordered Fuzzy Numbers Â· OFN Â· Defuzzification"
45,122,0.983,Measurement and Control of Charged Particle Beams,"A typical set of optimization scans for both beams is illustrated in Fig. 2.19. Small waist shifts were obtained by changing the strength of the last three quadrupoles (the âfinal tripletâ) and of one quadrupole further upstream. Part of the linear coupling was corrected by means of a skew quadrupole at the same betatron phase as the final triplet. The residual dispersion at the IP was corrected using two normal and two skew âI quadrupole pairs in the chromatic correction system (CCS), excited equally with opposite sign. Chromaticity was compensated by the two pairs of CCS-sextupoles. The strength of these sextupoles could also be varied to minimize the vertical spot size and, thus, to minimize the change in focusing with particle energy. As it turned out, the IP spot sizes proved to be relatively insensitive to the exact"
311,2000,0.983,The Physics of the B Factories,"suppresses the false kinematical singularity at s = 0 in the physical region near the ÏÏ threshold (the Adler zero (Adler, 1965)). The parameter values used in this analysis are listed in Section 13, and are obtained from a global analysis of the available ÏÏ scattering data from threshold up to 1900 MeV/c2 (Anisovich and Sarantsev, scatt 2003). The parameters fuv , for u = 1, are all set to zero since they are not related to the ÏÏ scattering process. Similarly, for the P vector we have"
372,1263,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"the three quantities in expressions (10.90)â(10.92), the intensity values correspond to the specific line of interest, continuum features having been subtracted out. In obtaining the best estimates for these three quantities, it should be noted that including ranges of .l; m; vr / that contain no discernable emission only adds noise to the results. Exploring the relationships between three-dimensional images in .l; m; vr / and the three-dimensional distribution of the radiating material is an astronomical concern. As a simple example, consider a spherical shell of radiating material. If the material is at rest, it will appear in .l; m; vr / space as a circular disk in the plane of zero velocity, with brightening at the outer edge. If the shell is expanding with the same velocity in all directions, it will appear in .l; m; vr / space as a hollow ellipsoidal shell. Interpretation of observations of rotating spiral galaxies is more complex. An example of a model galaxy is given by Roelfsema (1989), and a more extensive discussion can be found in Burton (1988)."
175,497,0.983,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","1. Randomly generate an initial population of strings of allocation variable values, ensuring that each allocation value (gene) is no less than 0 and no greater than 5. In addition, any set of allocations A1, A2, and A3 that sum to more than 6 will be considered infeasible and discarded. 2. Pair individuals and determine if a crossover is to be performed on each pair, assuming the probability of a crossover is 50%. If a crossover is to occur, we will determine where in the string of numbers it will take place, assuming an equal probability of a crossover between any two numbers. 3. Determine if any number in the resulting individual strings is to be mutated, assuming the probability of mutation of any particular number (gene) in any string (chromosome) of numbers as 0.10. For this example, a mutation reduces the value of the number by 1, or if the original number is 0, mutation changes it to 5. After mutation, all strings of allocation values (the genes in the chromosome) that sum to more than 6 are discarded. 4. Using Eq. 5.9, evaluate the âï¬tnessâ (total beneï¬ts) associated with the allocations represented by each individual string in the population. Record the best individual string of allocation values from this and previous populations. 5. Return to Step 1 above if the change in the best solution and its objective function value is signiï¬cant; Otherwise terminate the process. These steps are performed in Table 5.3 for three iterations using a population of 10. The best solution found so far is 222: that is, x1 = 2, x2 = 2, x3 = 2. This process can and should continue. Once the process has converged on the best solution it can ï¬nd, it may be prudent"
307,318,0.983,Computing Characterizations of Drugs for Ion Channels and Receptors Using Markov Models (Volume 111.0),"Fig. 9.8 Using the probability density functions of the states OO, OC, (CO+BO), and (CC+BC), we compute the expected concentrations of the dyad (x) and the JSR (y). The wild type is denoted by Ä± and the LCC mutation index  increases from one to three along the solid line. In the dashed from 0 to 100 ms 1 . We observe that as kbc line, we keep  D 3 and increase the value of kbc increases, the expected concentrations are completely repaired. The simulations are performed using V D 0 mV"
311,2248,0.983,The Physics of the B Factories,"Arecs at present. For this mode one corrects for the forward backward and detection eï¬ciency asymmetries with an inclusive correction obtained by subtracting 0 + 0 + D + âKS D + âKS after integrating over the from ACP entire (pÏ , cos Î¸Ï , cos Î¸D+ ) space. This technique yields a systematic uncertainty of 0.18%, which originates from the statistical uncertainty of the selected Ds+ â ÏÏ + sample. Similar methods have been used in the analysis of D+ â Ï + Î· (â²) decays (Won, 2011). Recently, the BABAR collaboration has developed another data-driven method (del Amo Sanchez, 2011i) to determine the charge asymmetry in the track reconstruction as a function of the magnitude of the track momentum and its polar angle, in the analysis of D+ â KS0 Ï + decays. B mesons are produced in the process e+ eâ â Î¥ (4S) â BB. This production mechanism is free of any physics-induced charge or flavor asymmetry. The CP violation in the later decays of B mesons must vanish if one takes a completely inclusive sample of B meson decay products. Hence the inclusive Î¥ (4S) â BB events provide a very large control sample in which any asymmetry is detector induced. However, data recorded at the Î¥ (4S) resonance also include continuum production e+ eâ â qq (q = u, d, s, c), where there is a non-negligible"
311,1777,0.983,The Physics of the B Factories,"to the same production channel, Belle measured the absolute production rate and established that X(3940) â Dâ D is a prominent decay mode; searches were made for X(3940) â DD and J/ÏÏ without evidence for any signals. The lower and upper limits on the branching fractions for these modes set in Abe (2007f) were withdrawn by Belle in Pakhlov (2008), as the inclusive peak in the earlier analysis, used to provide the denominator of the fraction, may have contributions from more than one state. Since the X(3940) is a candidate for a conventional charmonium state, it is also discussed in Section 18.2.1.3. The production mechanism constrains it to have positive charge conjugation since this is an electromagnetic process and the initial state virtual photon and the accompanying J/Ï have the same C. Furthermore, all of the known states that are observed via this production mechanism have J = 0. Although the reason for this is not understood, it is plausible that this state also has J = 0. Thus, the most likely quantum numbers of the X(3940) are J P C = 0++ or 0â+ ; the apparent absence of the DD decay channel favors 0â+ ."
187,363,0.983,Managing The Complexity of Critical infrastructures : a Modelling and Simulation Approach (Volume 90.0),"â¢ Qk is the availability level of Service k (if Qk = 0 the Service is fully unavailable). Qk depends explicitly on time and describes the pattern followed by the outage of the k-th Service during the time course of the Crisis. The function Qk(t) is the outcome of the Impact Analysis. The elements rk(tij) are the measure of the relevance of the Service k for the Wealth achievement in a given Sector element. For this reason, they will be identiï¬ed as Service Access Wealth (SAW) indices. They may be different from each other: a Sector element can be more vulnerable to the absence of a given PS and, thus, its Wealth most affected if that speciï¬c PS would fail. We then consider a closure relation, such as"
324,299,0.983,"Solving the Powertrain Puzzle : 10th Schaeffler Symposium April 3/4, 2014","The optimization models and tools shown in Figure 10 are leveraged to project combined fuel consumption [3]. To enter the corresponding data in the GT-Power model, the reduction in friction in the timing drive that is responsible for 0.5 to 2 percent of the overall loss in efficiency [4] must be examined in greater detail. Friction can never be eliminated altogether. At the same time, however, losses must be minimized and the influential factors and interactions within the systems must be understood. By utilizing friction in a targeted manner to optimize the chain drive, the damping properties it affords can make a significant impact on limiting peak points in dynamic force. The majority of tribological systems in an internal combustion engine as it is operated or being started encounter the different types of friction (static, boundary, and hydrodynamic friction) at different frequencies."
175,840,0.983,"Water Resource Systems Planning and Management : An introduction To Methods, Models, and Applications","The probability distribution of the flows in Table 7.1 can be approximated by a histogram. Histograms can be created by subdividing the entire range of random variable values, e.g., flows, into discrete intervals. For example, let each interval be 2 units of flow. Counting the number of flows in each interval and then dividing those interval counts by the total number of counts results in the histogram shown in Fig. 7.9. In this case, just to compare this with what will be calculated later, the ï¬rst flow, q1, is ignored."
372,617,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"In this case, there is no smearing of the intensity distribution, but the beam suffers a radial smearing that has the desirable effect of reducing distant sidelobes. Therefore, this mode of observation is well suited for imaging wide fields. The improvement in the beam results from the increase in the number of .u; v/ points measured, an effect that is also used in multifrequency synthesis discussed in Sect. 11.6."
365,257,0.983,Climate Smart Agriculture : Building Resilience To Climate Change,"The explanatory variables x include different household head characteristics (i.e. age, education, gender, membership of socio-political organization), household characteristic (agricultural member ratio, farm size, income, risk attitude and ethnicity (only in model for Vietnam)), distance to district town and province dummies. The justification of these variables and their expected direction of influence are assumed to be identical with those in Equation 1a. In addition, however, we include the respondentâs perceptions of changes in climate-related parameters like rainfall, temperature and wind as these perceptions may influence the choice of adaptation measures in different ways. The multinomial logit model makes the assumption of independence of irrelevant alternatives (IIA) (Long and Freese 2006). We use the Hausman test to verify this assumption."
214,390,0.983,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"Different strategies can be used to better quantify the different types of uncertainty in climate model projections. One of the best, which can be used for quantifying all three types of uncertainty, is to run ensembles, or multiple simulations. These simulations are typically done to gauge one kind of uncertainty, but they can be appropriately mixed and matched to understand all three types of uncertainty. The use of ensembles is now also common in weather forecasting. Weather forecasting suffers from model uncertainty and especially initial condition uncertainty, but usually not scenario uncertainty as we have described it. However, sometimes external events such as ï¬re smoke can signiï¬cantly affect the weather. In weather forecasting, ensembles of a forecast model are typically run in parallel: multiple model runs at the same time. Then the spread of results of these parallel runs is analyzed. The simulations usually differ from each other by perturbing the initial conditions (slight variations in temperature, for example) and/or by altering the parameters in the model. Sometimes forecasters consult different models, creating another sort of ensemble. Altering initial conditions tests the initial condition"
8,867,0.983,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","29.2 On Formation of QGP in Heavy Ion Collisions Maurice Jacob begins his presentation at 11:20, 22 June, 1982. âHeavy ion collisions offer the possibility to reach very high densities and very high temperatures over extended domains, many times larger than the size of a single hadron. The energy densities considered are of the order of 0.5â1.5 GeV/fm3 and the relevant temperatures are in the 200 MeV range. The great interest of reaching such conditions originates from recent developments in Quantum Chromodynamics, QCD, which make it very plausible that, while color confinement should prevail under standard circumstances, deconfinement should occur at sufficiently high density and (or) sufficiently high temperature. Under such conditions a new phase of matter, a quark-gluon plasma, is likely to exist. This phase should be viewed as due to a coalescence, or perhaps a percolation, of hadrons into larger entities and not as an actual separation of free quarks! . . . âOver an extended volume where the required density or temperature conditions would prevail, one expects that the properties of the physical vacuum would be modified. While the normal vacuum excludes the gluon field, the color-equivalent of the dielectric constant being zero (or practically zero), one would get a new vacuum state where quarks and gluons could propagate while interacting perturbatively. âThe equivalent of the dielectric constant would now be unity. The required conditions may be reached at high enough densities, hadrons being squeezed into one another, or at high enough temperature, the calculation of the partition function no longer favoring confining configurations whereby a color flux tube of fixed cross section extends between two color sources. The temperature at which the phase transition is expected to occur depends on the density, or on the quark chemical potential. One may thus separate two phases, a hadron phase and a quark gluon plasma, on a density-temperature diagram.. . . âThe presence of a phase transition could long be expected from phenomenological models with an exponentially increasing hadron spectrum. The limiting Hagedorn temperature, obtained as the specific heat of the hadron gas diverges, can"
372,773,0.983,Interferometry and Synthesis in Radio Astronomy : Third Edition (Edition 3),"calculating here, is of interest mainly for weak signals.) For a measurement period !, NN D 2&""!, which is commonlyp106 â1012 . From Eq. (8.14), the threshold of â10!6 . In terms detectability of a signal is given by % NN ' 1, that is, % ' 10p of the signal bandwidth and measurement duration, Rsn1 D % 2&"" !. Now for observations of a point source with identical antennas and receivers, % is equal to the ratio of the resulting antenna temperature to the system temperature, TA =TS . Thus, the present result is equal to that given by Eq. (6.45) for an analog correlator with continuous unsampled inputs and TA # TS . Before leaving the subject of unquantized sampling, we should consider the effect of sampling at rates other than the Nyquist rate. Successive sample values from any one signal are then no longer independent. We consider a sampling frequency that is Ë times the Nyquist rate3 and a number of samples N D ËNN . The sample interval is !s D .2Ë&""/!1 . Samples spaced by q!s , where q is an integer, have a correlation coefficient that, from Eq. (8.4), is equal to R1 .q!s / D"
34,135,0.983,"Habitats and Biota of the Gulf of Mexico: Before the Deepwater Horizon Oil Spill: Volume 2: Fish Resources, Fisheries, Sea Turtles, Avian Resources, Marine Mammals, Diseases and Mortalities","Mexico (Louisiana and Texas) (SEDAR 7 2005). Young (to age 8) red snapper in the eastern Gulf of Mexico tend to have a higher reproductive output at age compared to those in the western Gulf. A single stock of red snapper in the Gulf of Mexico has been suggested by genetic studies (Camper et al. 1993; Gold et al. 1997; Heist and Gold 2000), which may result from the lack of sufficient time since the Pleistocene epoch for spatially separated substocks of red snapper in the Gulf of Mexico to have become genetically distinct, or from enough mixing to maintain homogeneity in the population. However, phenotypic differences have been identified in growth, maturation, abundance, age/size compositions, prey compositions, and fishery dynamics between the eastern and western Gulf of Mexico. To account for such differences between the two areas in the stock assessment, the Gulf of Mexico red snapper stock is considered to consist of the two substocks. Although there is evidence of large differences in life history and population dynamics at fine spatial scales, such as among different reefs (Gallaway et al. 2009), more studies are needed to evaluate the potential existence of metapopulation structure."
34,1403,0.983,"Habitats and Biota of the Gulf of Mexico: Before the Deepwater Horizon Oil Spill: Volume 2: Fish Resources, Fisheries, Sea Turtles, Avian Resources, Marine Mammals, Diseases and Mortalities","14.4.3 Biodiversity The role of biodiversity is not a category of disease, and biodiversity can be considered a tool to evaluate catastrophic events treated above in Section 14.4.2. We treat it separately because of its relationship with health. Since parasites comprise about half the Earthâs biota, they form a critical component of biodiversity of the Gulf. An abundance of parasites, with an emphasis on helminths, indicates a healthy ecosystem or a healthy host species. This method provides an especially powerful marker because most helminths have three, plus or minus two hosts, in a specific cycle. Consequently, the presence of that specific helminth in a habitat indicates that all members of the cycle are or had recently been present in the habitat. Because of this cycle, the absence of helminths provides indicators of disruption. The reason for the indication is that adverse impacts on the corresponding intermediate host (or final host) for the species or the population result in fewer species or smaller parasite populations. Fewer parasite species or parasite populations show there was a disruption, even if it was not otherwise apparent."
77,351,0.983,A Life Course Perspective on Health Trajectories and Transitions (Volume 4.0),"Figure 10.7 shows the period effects â that is, the period residuals estimated in the random part of the model. These effects are in general small, although some are statistically significant at the 95 % level: people were in general healthier than average in 1991 and 2003, and less healthy in 2000. Finally, model 3 in Table 10.1 shows that allowing the age effect to vary at the person level improves the model fit substantially (the DIC declined by over 2500). People differ in their life course trends of their GHQ measure, and this is expressed by the person-level coverage intervals11 in Fig. 10.8. It can be seen that those with higher GHQ scores will experience a greater increase in GHQ scores over their lifetime than those with lower GHQ scores, and so the variance between individuals is greater amongst older people than younger people â this is a result of the positive covariance term (0.077) in model 3 of Table 10.1. This model is for illustration only â one would normally add additional time varying and time invariant control variables (for example employment status, social position, income, wealth and ethnicity) in an attempt to account for the unexplained variation in the random part of the model. One could also further extend the model in any of the other ways mentioned above. It is also worth noting that, whilst this model presented here uses a continuous outcome, other outcomes could be used with different link functions (for example, if you wanted to analyse a binary health outcome, a logit or probit version of this model could be used)."
118,200,0.982,Reflections On The Fukushima Daiichi Nuclear Accident : Toward Social-Scientific Literacy and Engineering Resilience,"5.2 Regionally Disaggregated DNE21 DNE21 is an integrated assessment model that provides a framework for evaluating the optimal energy mix to stabilize low atmospheric CO2 concentrations. The recent version of the DNE21 model [1] has featured a more detailed representation of regional treatments, including nuclear and renewable energy. The model seeks the optimal solution that minimizes the total system cost, in multiple time stages for the years from 2000 to 2100 at 10-year intervals in multiple regions, under various kinds of constraints, such as amount of resource, energy supply and demand balance, and CO2 emissions. The model is formulated as a linear optimization model, in which the number of the variables is more than one million. Figure 5.1 shows the division framework of world regions and assumed transportation routes. In the DNE21 model, the world is divided into 54 regions. In the model, large countries such as the United States, Russia, China, and India are further divided into several sub-regions. Furthermore, in order to reflect the geographical distribution of the site of regional energy demand and energy resource production, each region consists of âcity nodesâ shown as round markers in Fig. 5.1 and âproduction nodesâ shown as square markers, the total number of which amounts to 82 points. The city node mainly shows representative points of intensive energy demand, and the production node exhibits additional representative points for fossil fuel production to consider the contribution of resource development in remote districts. The model takes detailed account of intra-regional and inter-regional transportation of fuel, electricity, and CO2 between these points."
32,181,0.982,"Proceedings of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014","clusters become non-functional. Thus in interdependent networks only the giant mutually-connected cluster is of interest. Unlike clusters in regular percolation whose size distribution is a power law with a p-dependent cutoff, at the final stage of the cascading failure process just described only a large number of small mutual clusters and one giant mutual cluster are evident. This is the case because the probability that two nodes that are connected by an A-link and their corresponding two nodes are also connected by a B-link scales as 1=NB , where NB is the number of nodes in network B. So the centrality of the giant mutually-connected cluster emerges naturally and the mutual giant component plays a prominent role in the functioning of interdependent networks. When it exists, the networks preserve their functionality, and when it does not exist, the networks split into fragments so small they cannot function on their own. In Fig. 8.3 we present a schematic representation of an example of a tree-like network of networks, composed of five networks. The cascading failure process is applied by removing 1 p nodes, and calculating the size of the mutual giant component, P1 . We present (Fig. 8.2) a comparison between P1 of n D 1; 2; 5 networks, and show that the network of networks system is more vulnerable to cascading failures. Finally, we show (Fig. 8.2) the analytical"
141,30,0.982,Cyber-Physical Systems of Systems: Foundations â A Conceptual Model and Some Derivations: The AMADEOS Legacy,"Precision: The precision of an ensemble of synchronized clocks denotes the maximum offset of respective ticks of the global time of any two clocks of the ensemble over the IoD. The precision is expressed in the number of ticks of the reference clock. The precision of an ensemble of clocks is determined by the quality of the oscillators, by the frequency of synchronization, by the type of synchronization algorithm and by the jitter of the synchronization messages. Once the precision of the ensemble has been established, the granularity of the global time follows by applying the reasonableness condition. Reasonableness Condition: The reasonableness condition of clock synchronization states that the granularity of the global time must be larger than the precision of the ensemble of clocks. We distinguish between two types of clock synchronization, internal clock synchronization and external clock synchronization. Internal Clock Synchronization: The process of mutual synchronization of an ensemble of clocks in order to establish a global time with a bounded precision. There are a number of different internal synchronization algorithms, both non-fault tolerant or fault-tolerant, published in the literature (see e.g., [13], and many others). These algorithms require the cooperation of all involved clocks. External Clock Synchronization: The synchronization of a clock with an external time base such as GPS. Primary Clock: A clock whose rate corresponds to the adopted deï¬nition of the second. The primary clock achieves its speciï¬ed accuracy independently of calibration. The term master clock is often used synonymously to the term primary clock. If the clocks of an ensemble are externally synchronized, they are also internally synchronized with a precision of |2A|, where A is the accuracy. Accuracy: The accuracy of a clock denotes the maximum offset of a given clock from the external time reference during the IoD, measured by the reference clock. The external time reference can be a primary clock or the GPS time."
26,31,0.982,Cognitive Supervision for Robot-Assisted Minimally Invasive Laser Surgery,"2.1 Physical Properties of Light Our understanding of the nature of light is based on the fact that it exhibits properties of both electromagnetic (EM) waves and elementary particles, this characteristic is known as the wave-particle duality [1]. First, we consider the description of light as EM waves; the terms light and EM radiation are used interchangeably throughout this dissertation. Light can be regarded as the perturbation produced by the interplay of mutually related electric and magnetic fields. Each pair of electric/magnetic fields is characterized by a common wavelength Î» and oscillate perpendicular with each other and at right angle to the direction of propagation, as shown in Fig. 2.1. In general, EM radiation is classified according to the range of wavelengths it contains. The EM spectrum covers all the possible wavelengths of EM radiation: a limited interval is shown in Fig. 2.2, ranging from longer wavelengths (Microwaves) to shorter (XRays). A fundamental physical property of EM waves is the frequency Î½, which relates to the wavelength through the relation c = Î½ Â· Î», where c is the speed of EM waves in vacuum. The classic picture of light as EM waves can be revisited in the perspective of the quantum theory. In its simplest form, the theory describes light as the"
173,348,0.982,"intelligent Human Computer interaction : 9Th international Conference, Ihci 2017, Evry, France, December 11-13, 2017, Proceedings","3 Experimental Setup The experiments to examine performance of co-training in MWE detection require the following four tasks to be performed: 1. Two different views (two groups of features) of data set must be determined 2. The classiï¬er pairs must be chosen 3. MWE data set composed of both positive and negative samples must be prepared/selected. 4. Labeled, unlabeled and testing data set sizes must be set. 5. Evaluation measures must be determined. We propose to use linguistic and statistical features as two different views on MWE data set. In this study, the linguistic view includes 8 linguistic features listed below: 1. Partial variety in surface forms (PVSF_m and PVSF_n): In MWE detection studies, it is commonly accepted that MWEs are not observed in a variety of different surface forms in language. As a result, the histogram presenting the occurrence frequencies of different surface forms belonging to the same MWE is expected to be non-uniform [12]. We measured variety in surface forms in two different ways that are called as PVSF_m and PVSF_n features based on the surface form histogram,"
214,520,0.982,Demystifying Climate Models : a Users Guide To Earth System Models (Volume 2.0),"Convection (6) mass motions within a fluid resulting in transport and mixing of the properties of that fluid. Motions that are predominantly vertical and driven by buoyancy forces arising from density gradients with light air (or water) beneath denser air (or water). Coriolis force (6) or Coriolis effect; an effect where a mass moving in a rotating system experiences an apparent force (the Coriolis force) acting perpendicular to the direction of motion and to the axis of the rotation. On the earth, the effect tends to deflect moving objects to the right in the Northern Hemisphere and to the left in the Southern Hemisphere. The Coriolis force is zero at the equator (since an object moves parallel to the axis of rotation), and a maximum at the poles. Coupled climate system model (4) a class of climate model in which at least two different subsystems of earthâs climate system are allowed to interact. A coupled model would typically couple the atmosphere, ocean, and land, sometimes also atmospheric chemistry so that different parts interact with each other. Coupled Model Inter-comparison Project (CMIP) (11) a standard experimental protocol for studying the output of coupled atmosphere-ocean general circulation models. http://cmip-pcmdi.llnl.gov/. Coupling (3) the interaction of two or more items, things, or processes. Credibility (12) scientiï¬cally trusted or believable. Cryosphere (2) the places on the earth where water is in solid form, frozen into ice or snow. Deep ocean (6) the region of the ocean below the thermocline. Deformable solid mechanics (7) the branch of continuum mechanics that studies the behavior of solid materials, especially their motion and deformation under the action of forces, temperature changes, phase changes, and other external or internal agents. Disruptive innovation (10) innovation that helps create a new market and value network, and eventually disrupts an existing market and value network (over a few years or decades), displacing an earlier technology. Domain (5) a speciï¬c and limited region. Downscaling (5) method used to obtain local-scale weather and climate information, from regional-scale atmospheric variables that are provided by GCMs. Two main forms of downscaling technique exist. One form is dynamical downscaling, where large-scale model output (from the GCM) is used to drive a regional, numerical model at higher spatial resolution to simulate local conditions in greater detail. The other form is statistical downscaling, where a statistical relationship is established from observations between large-scale variables, like atmospheric surface pressure, and a local variable, like the wind speed at a"
311,1682,0.982,The Physics of the B Factories,"Similar results have been obtained by BABAR, using a 384 fbâ1 data sample (Aubert, 2010g). The D0D0 and D+Dâ final states are fully reconstructed, selecting twophoton events  (in the no-tag mode) by requiring a large missing mass ( (pe+ eâ â pDD )2 ) and a small transverse momentum of the DD system. Additionally, the energy deposited in the calorimeter unmatched to any chargedparticle track should not exceed 400 MeV. A peak in the DD invariant mass distribution near 3.93 GeV/c2 is also clearly seen in BABAR data. The combined eï¬ciencycorrected DD invariant mass spectrum is fitted with a relativistic Breit-Wigner signal function convolved with a mass-dependent Gaussian resolution function and a background lineshape taking the DD threshold into account."
151,209,0.982,Ecological Risk Assessment for Chlorpyrifos in Terrestrial and Aquatic Systems in North America (Volume 231.0),"Although the monitoring data on CPY provide relevant insight in quantifying the range of concentrations in surface waters, few monitoring programs have sampled at a frequency sufficient to quantify the time-series pattern of exposure. Therefore, numerical simulations were used to characterize concentrations of CPY in water and sediment for three representative high exposure environments in the U.S. The fate of CPY in the environment is dependent on a number of dissipation and degradation processes. In terms of surface waters, fate in soils is a major driver of the potential for runoff into surface waters and results from a number of dissipation studies in the laboratory were characterized. Aerobic degradation of CPY exhibits bi-phasic behavior in some soils; initial rates of degradation are greater than overall rates by factors of up to threefold. Along with fate in water, these data were considered in selecting parameters for the modeling concentrations in surface waters. An assessment of vulnerability to runoff was conducted to characterize the potential for CPY to be transported beyond a treated field in runoff water and eroded sediment across the conterminous U.S. A sensitivity analysis was performed on use practices of CPY to determine conditions that resulted in the highest potential runoff of CPY to aquatic systems to narrow the application practices and geographical areas of the country for selecting watersheds for detailed modeling. The selected focus-watersheds were Dry Creek in Georgia (production of pecans), Cedar Creek in Michigan (cherries), and Orestimba Creek in California (intensive agricultural uses). These watersheds provided realistic but reasonable worst-case predictions of concentrations of CPY in water and sediment. Estimated concentrations of CPY in water for the three watersheds were in general agreement with ambient monitoring data from 2002 to 2010 in the datasets from US Geological Survey (USGS), California Department of Pesticide Regulation (CDPR), and Washington State Department of Ecology (WDOE). Maximum daily concentrations predicted for the watershed in California, Georgia, and Michigan were 3.2, 0.041, and 0.073 Î¼g Lâ1, respectively, with the 28-d aerobic soil metabolism half-life and 4.5, 0.042, and 0.122 Î¼g Lâ1, respectively, with the 96-d soil halflife. These estimated values compared favorably with maximum concentrations measured in surface water, which ranged from 0.33 to 3.96 Î¼g Lâ1. For sediments, the maximum daily concentrations predicted for the watersheds in California, Georgia, and Michigan were 11.2, 0.077, and 0.058 Î¼g kgâ1, respectively, with the 28-d half-life and 22.8, 0.080, and 0.087 Î¼g kgâ1, respectively, with the 96-d soil half-life. CYP was detected in 12 samples (10%) out of 123 sample analyses that existed in the USGS, CDPR, and WDOE databases. The concentrations reported in these detections were from <2.0, up to 19 Î¼g kgâ1, with the exception of one value reported at 58.6 Î¼g kgâ1. Again, the modeled values compared favorably with these measured values. Duration and recovery intervals between toxicity threshold concentrations of 0.1 and 1.0 Î¼g Lâ1 were also computed. Based on modeling with the half-life of 28 d, no exceedance events were identified in the focus watersheds in Georgia or Michigan. Using the half-life of 96 d, only three events of 1-d duration only were identified in the Michigan focus-watershed. Frequency of exceedance was greater in the California focus watershed, though the median duration was only 1-d."
224,143,0.982,Ester Boserupâs Legacy on Sustainability : Orientations for Contemporary Research,"Another yield-based approach is the Ï-factor (Dietrich 2011; Dietrich et al. 2012). Although it requires similar inputs to those of the TE approach, it was developed as a direct measure of agricultural land-use intensity. The Ï-factor is based on the idea that yield can be considered an indicator of agricultural land-use intensity that is affected by variations in environmental conditions. When two locations with identical environmental conditions growing the same crop are compared, any difference in yield can be attributed to the differences in agricultural land-use intensity in these two locations. If environmental conditions differ, however, the differences in land-use intensity are superimposed by variations in environmental conditions. The Ï-factor approach tackles this problem by comparing observed agricultural yields with a reference yield that would be achieved at each site with the same level of input intensity. This reference yield is constructed to ensure that only the differences in environmental conditions are reflected by setting land-use intensity to a constant level. The Ï-factor can then be defined as the ratio of actual yield to reference yield. Variability of the Ï-factor can thus legitimately be assumed to be solely caused by differences in agricultural land-use intensity, whereas the variations in the reference value can be assumed to be purely environmentally determined. Dietrich et al. (2012) calculate and map the Ï-factor at the global scale by using the âLund-Potsdam-Jena dynamic global vegetation model with managed landâ (LPJmL) (Bondeau et al. 2007). Actual yields are based on the national yield data of the Food and Agriculture Organization of the United Nations (FAO) (FAOSTAT 2011), downscaled by the LPJmL. Reference yields are computed by simulating spatial patterns of crop yields under constant management practices (all managementdependent parameters in the model are set to a constant level). The results of this analysis are shown in Fig. 5.2. The FAO data used by Dietrich et al. (2012) contains only information on harvest yields (yield per area for each harvest event) but not the yields per unit area under land use (land-use yields), which causes changes in cropping"
223,343,0.982,Knowledge and Action (Volume 9.0),"Some Linguistic Evidence of Semantic Domain Knowledge A central thesis of this chapter is that the semantic domains, as structured by conceptual spaces, form an important part of semantic knowledge. In this section I present linguistic evidence that the development of semantic knowledge can appropriately be described as the development of separable semantic domains. In the analysis of child language data, the establishment of a word in the vocabulary of children is often analyzed for the average frequency of the wordâs usage at a certain age.7 Typically, the frequency of a wordâs usage starts at or close to zero, increases rapidly, then levels off once the word is established in the vocabulary.8 The resulting curve thus has an S shape. I hereafter call the interval during which usage increases rapidly the establishment period for a word. I can now formulate a general hypothesis concerning semantic domains: If one word from a domain is learned during a certain establishment period, then other (common) words from the same domain tend to be learned during roughly the same period. In order to test this hypothesis, I have analyzed data from the Child Language Data Exchange System (CHILDES) corpus and have used the publicly available web-based ChildFreq application, a highly efficient tool for such investigations.9 In this chapter I can present only a few examples from my analysis. For most of the domains discussed in the previous section, words are established during the language spurt that takes place between 12 and 24 months of age. This observation holds in particular for the different regions of the category domain. For example, consider the region of fruits, part of the category domain. Figure 12.2 shows the frequency curves for the names of several of the most common fruits: apple, banana, pear, grape, and orange. These words have an establishment period"
311,242,0.982,The Physics of the B Factories,"obtain high statistics electron and muon samples. The control sample is divided into 70 bins (10 momentum bins in 500 MeV/c steps and 7 polar angle bins). The eï¬ciencies of the lepton identiï¬cations estimated using this process are shown in Fig. 5.4.1 (d) and (e). Since the above process leads to low track-multiplicity events, inclusive J/Ï events (J/Ï â â+ ââ ) are also used as a control sample, mainly for the estimation of a possible performance diï¬erence between low-multiplicity events and hadronic events. The mis-identiï¬cation rates of the lepton identiï¬cation for pions and kaons, are studied using a control sample of KS0 â ÏÏ and Dâ+ â D0 Ï + â K â Ï + Ï + ."
339,348,0.982,Etiology and Morphogenesis of Congenital Heart Disease : From Gene Function and Cellular interaction To Morphology,"Distal outgrowth and fusion of the mesenchymalized AV endocardial cushions are essential morphogenetic events in AV valvuloseptal morphogenesis. BMP-2 myocardial conditional knockout (cKO) mice die by embryonic day (ED) 10.5 [1] at the initial stage for the formation of endocardial cushions, hampering investigation of the role of BMP-2 in AV valvuloseptal morphogenesis at the later stages. In our previous study, we localized BMP-2 and type I BMP receptors, BMPR1A and Alk2, in AV endocardial cushions [2, 3]. Based on their expression patterns, we hypothesize that autocrine signaling by BMP-2 within mesenchymalized AV cushions plays a critical role during AV valvuloseptal morphogenesis. To test this hypothesis, we employed recently generated endocardial/endocardial cushion-specific cre-driver line Nfact1Cre. Unlike a previously generated Nfatc1enCre line whose cre-mediated recombination is restricted to AV Y. Sugi (*) â¢ J.L. Burnside Department of Regenerative Medicine and Cell Biology, Medical University of South Carolina, 171 Ashley Avenue, Charleston, SC 2925, USA e-mail: sugiy@musc.edu B. Zhou Department of Genetics, Pediatrics, and Medicine (Cardiology), Albert Einstein College of Medicine of Yeshiva University, Bronx, NY, USA K. Inai Pediatric Cardiology, Heart Institute, Tokyo Womenâs Medical University, Tokyo, Japan Y. Mishina Department of Biological Materials Science, School of Dentistry, University of Michigan, Ann Arbor, MI, USA # The Author(s) 2016 T. Nakanishi et al. (eds.), Etiology and Morphogenesis of Congenital Heart Disease, DOI 10.1007/978-4-431-54628-3_22"
77,254,0.982,A Life Course Perspective on Health Trajectories and Transitions (Volume 4.0),"Given that the repeated assessments of Y of all individuals are analyzed simultaneously, Eq. 8.2 necessitates the addition of the subscript j, which identifies the individual. Moreover, this approach supposes one set of growth parameters, that is, one intercept and one slope, per individual, which again justifies the subscript j on both parameters  0 and  1 . Technically, it is not correct to say that an intercept and a slope value are estimated for each individual. That is, the model does not explicitly estimate a  0j and a  1j value for each individual j. The model presupposes, however, that each individual may have an intercept and a slope value that deviate from the central (population average) values, which are indicated by Ë 0 and Ë 1 and which are explicitly estimated. What are also estimated are the inter-individual variances, due to the individual deviations U0j and U1j around the central values, and possibly the covariance between U0j and U1j . The variances of the U0j and U1j are defined by the parameters  2 I and  2 S , respectively, and their covariance by the parameter  IS . Lastly, the errors of prediction Eij are not individually estimated. These are often assumed to have a constant variance in time, estimated by the parameter  2 E , and to be uncorrelated in time. In sum, then, in its"
8,435,0.982,"Melting Hadrons, Boiling Quarks: From Hagedorn Temperature to Ultra-Relativistic Heavy-Ion Collisions at CERN: With a Tribute to Rolf Hagedorn","This is then also the probability of finding two specified particles. They may be the initial (elastic scattering) or some definite other ones, e.g., p C p ! A C B. Between Eq. (19.30) and the differential cross-section come, of course, some further considerations (flux factors, centrality condition, influence of the actual masses of A and B), which have been treated in another paper [4]. The main point is that, if we compare Eq. (19.30) with the numerical result of Eq. (19.2) which fits the observed large angle scattering well [4], we find that T0 should have a value such that 1=T0 D 6:2. Thus in this case, T0 D 1:1m D 151 MeV :"
145,304,0.982,"Evolution, Monitoring and Predicting Models of Rockburst : Precursor Information for Rock Failure","follow, Barton method only considered the UCS and the major principal stress, but the brittleness and integrity of rock were not included in the model. However, 10# ore body and other rock stratums were not particularly intact, the predicting result of Barton method might be slightly higher than the actual value, its predicting result was strong rockburst grade; Rock brittleness method didnât contain the major principal stress, which was the base factor for the rockburst tendency. It was important that the stress caused the accumulated energy of rockmass, its predicting result was medium rockburst grade, and this result was the same as the fuzzy matter-element model. However, in other predicting result of rock brittleness method, they might be away from the true value because of lacking the most important factor in stress; Impact tendency energy method only considered the before and after peak value, rock brittleness was only one of factors of rockburst tendency. However, the parameters, such as the stress and the intact degree, were important factors in the predicting result. In this research, we could ï¬nd that the curve was mitigated. So the predicting of impact tendency energy method was quiet low; Rock integrity method didnât include the major principal stress, rock brittleness and the impact tendency energy, which were the base factors of energy accumulation. That is to say, these traditional methods exited the limitations for rockburst tendency. However, fuzzy matter-element model were obtained using all factors of traditional method, the predicting result using fuzzy matter-element method for rockburst was medium in 1391 level of C1b rock strata in 10# ore body,"
295,402,0.982,interface oral Health Science 2016 : innovative Research On BiosisâAbiosis intelligent interface,"Table 19.1 shows the chemical compositions of Ti-(6-18)MnLM. The lower amount of Mn content in comparison to the nominal value is caused by the evaporation of Mn during melting processes [17]. Mn loss of up to 25 % was expected [17]. However, the observed difference (2â10 %) is considerably smaller in the Ti-MnLM, thanks to the shorter melting time required by this method. Figures 19.1 and 19.2 show the optical micrographs and XRD profiles of the Ti-(6-18)MnLM, respectively. As shown in Fig. 19.2, only diffraction peaks attributed to Î² planes are detected in the XRD profiles of Ti-(9-18)MnLM. However, diffraction peaks that can be attributed to Ï and Î± or Î±â² phases are also detected in the XRD profile of Ti-6MnLM. A concentration of approximately 6.3 mass% Mn is required to fully retain the Î² phase upon quenching [4]. However, the Mn content of Ti-6MnLM used for this study is 5.60 mass%. Furthermore, the presence of the athermal Ï phase has been confirmed by the TEM observations. Figure 19.3aâd shows Table 19.1 Chemical compositions of Ti-Mn alloys fabricated by CCLM (mass%) Alloy Ti-6MnLM Ti-9MnLM Ti-13MnLM Ti-18MnLM"
64,132,0.982,Infrastructure and Economic Growth in Asia,"the simulation periods is the variable of interest for estimating poverty and inequality changes across the different scenarios. According to the methodology followed in this study, this variable is affected by the change in consumer prices as well as in household revenues, here corresponding to incomes from wage and selfemployment activities. Consistently with the CGE model, we also took into account the different marginal propensity to consumption for constrained and non-constrained households. Initially, the FIES is processed to classify constrained and non-constrained households. A logit model specifies the probability of being a non-constrained household (Yi Â¼ 1; Yi Â¼ 0 if constrained), which is defined as: has access to formal credit institutions, has saved or has a savings account. The logit model shown in Eq. 1 estimates the probability that a given household h is non-constrained ( ph,nc). By implication, the complement of ph,nc gives the probability that a given household h is constrained ( ph,c). LogitÃ°Ï h Ã Â¼ Î± Ã¾ Î²v Xh Ã¾ Îµh with Ï h Â¼ EÃ°Y h jXh Ã"
45,63,0.982,Measurement and Control of Charged Particle Beams,"the strength of nonlinear fields. Furthermore, optimizing and controlling the tunes improves the beam lifetime and the dynamic aperture, and can reduce beam loss or emittance growth during acceleration. For example, Fig. 2.2 shows the variation of the extracted vertical beam size as a function of the vertical betatron tune which was measured at the SLC electron damping ring. Other effects including space charge, ionized gas molecules, the beam-beam interaction, and radiation damping can affect the tune signal, which may be seen for example in the shape of the beam response to a swept-frequency excitation. An example showing the dramatic effect of the nonlinear beam-beam force is shown in Fig. 2.3. Fast decoherence and filamentation, head-tail damping or instabilities may make it difficult to extract a clean and reproducible tune signal. Conversely, the strong influence of various phenomena on the tune signal also implies that all these processes can be studied by means of tune measurements. In the following we will describe three methods of measuring the fractional part of the tune. These approaches fall into two different categories: (1) precision tune measurements and (2) tune tracking (the latter aims at monitoring and controlling fast changes, e.g., during acceleration). For simplicity, the fractional part of the tune will also be denoted by Q."
30,63,0.982,Determinants of Financial Development,"three information criteria: the Akaike Information Criterion (AIC), the Hannan-Quinn Criterion (HQ) and the Schwarz Criterion (SC). The output also includes three mis-specification tests (Chow test, Normality test and Heteroscedasticity test).21 The Gets results in Table 2.2 are the final models for three samples, respectively, in Appendix Table A2.7, which clearly shows the variables included in the GUM and in the final model. In Table 2.2, the BMA analysis for the whole sample yields a subset inclusive of four âotherâ variables (GDP90, POP90, ETHPOL and EURFRAC), two geographic variables (REGEAP and AREA), four policy variables (CTRADE, EXPPRIM, SDBMP and SDPI) and five institutional variables (CIVLEG, COMLEG, DURABLE, KKM and PCI). Given no rejection of the mis-specification tests, the Gets analysis for the whole sample yields a subset inclusive of three âotherâ variables (GDP90, POP90 and EURFRAC), two geographic variables (LATITUDE and AREA), one policy variable (SDTT ) and three institutional variables (CIVLEG, KKM and PCI). Both the BMA and Gets analyses on the whole sample unanimously suggest that three âotherâ variables (GDP90, POP90 and EURFRAC), one geographic variable (AREA) and three institutional variables (CIVLEG, KKM and PCI) are the main determinants for FD."
202,157,0.982,"Security of Networks and Services in an All-Connected World: 11th IFIP WG 6.6 International Conference on Autonomous Infrastructure, Management, and Security, AIMS 2017, Zurich, Switzerland, July 10-13, 2017, Proceedings","of NFV-RA as instances with less arriving bandwidth are easier to be embedded in the substrate network. The mean run time of our model considering all performed scenarios was 1.33 s, in comparison to the heuristic with a mean run time of 0.028 s. Also, it is important to note that the objective function of our ILP was restricted here to the minimization of the total link bandwidth in the resulting VNF-FG (to be comparable with the existing heuristic). However, this objective may change depending on the operatorâs goals to several ones, such as: minimization of the number of created instances, minimization of the total processing capacities, etc."
173,71,0.982,"intelligent Human Computer interaction : 9Th international Conference, Ihci 2017, Evry, France, December 11-13, 2017, Proceedings","We observe that as the size of the network is greatly reduced comprising a single memory block and the number of memory cells being either 8 or 10, the accuracy is not as great as observed in Sect. 4.1. Hence, in order to accomplish the same level of accuracy as obtained in Sect. 4.1, the number of iterations for the training process was increased. The performances can be referred to in Fig. 2, showing"
364,190,0.982,Sustainable Living with Environmental Risks,"Capture fishery is an industry that enables humans to utilize wild seafoods as a resource, although, if overfished, breeding individuals become short of supply, making it impossible to secure the resources in the future. Accordingly, theories of resource management for appropriate fishing have long existed, although such theories typically ignore uncertainty. Of the conventional resource management theories, one such theory that ignores uncertainty is the classical theory of âmaximum sustainable yield,â or MSY, that appears in fisheries and wildlife management (Clark 1990). In the classical MSY theory, we consider the reproduction curve as shown in Fig. 9.1. The formulae are shown at the end of the chapter. Biological populations are characterized by exponential growth in population numbers. Calculated in the simplest possible way, the per capita growth rate of a population would increase constantly, like the relationship between the amount of a bank deposit and interest (corresponding to the thin line in Fig. 9.1). However, the per capita growth rate in fact decreases as the stock increases because availability of food, habitat, and other resources becomes limited. Therefore, the surplus production, or the product of the population number and the per capita growth rate, forms a peak, corresponding to the bold curve in Fig. 9.1. In addition, the population does not increase beyond a threshold, which is called carrying capacity, denoted by K in Fig. 9.1. The surplus production produces a curve of one crest in relation to the stock, as in Fig. 9.1, and with an intermediate stock level, the largest catches can be continued. The catches are called MSY, which in the case of Fig. 9.1 is 500,000 t. If the surplus production is larger than the catches (implied by the broken line in Fig. 9.1), the resources increase. If it is smaller, they decrease. The equilibrium stock biomass under the given catch level is the intersection of the surplus production curve and the catch"
280,410,0.982,Diversity and Evolution of Butterfly Wing Patterns : An integrative Approach,"13.3.2 Variation by Wing Cell Variation in pattern elements was also dependent on the wing cell in which the pattern elements are located. There was little variation in NED, NSP, NCP, or NEP for wing cells Sc+R1, M2, and M3 between seasonal forms for all clades because these eyespots are typically reduced or absent in ventral hind wings in both the wet and dry season forms. In contrast, the Rs, M1, and Cu1 wing cells showed significant differences in NED and NCP across clades, but only NED varied significantly between seasonal forms. Junonia almana (Fig. 13.3) provides a clear example of the differences in seasonal eyespot size variation across wing cells. In this species, the Sc+R1 eyespot is absent in both seasonal forms. The M2 and M3 eyespots are absent in the wet season form, but are present and highly reduced in the"
